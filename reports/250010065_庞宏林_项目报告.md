# 中译英神经机器翻译项目报告

**学号**: 250010065  
**姓名**: 庞宏林  
**课程**: SLAI NLP Midterm Project  

---

## 目录

1. [项目概述](#项目概述)
2. [数据集分析](#数据集分析)
3. [RNN 神经机器翻译](#rnn-神经机器翻译)
4. [Transformer 神经机器翻译](#transformer-神经机器翻译)
5. [实验结果对比](#实验结果对比)
6. [结论](#结论)

---

## 项目概述

本项目旨在实现并比较两种主流的神经机器翻译（NMT）架构——RNN 和 Transformer，用于中文到英文的翻译任务。通过对不同模型架构、注意力机制、训练策略和解码策略的系统性实验，深入理解各种因素对翻译质量的影响。

### 项目目标

- 实现基于 RNN 的序列到序列模型，包含注意力机制
- 从零训练 encoder-decoder Transformer 模型
- 探索预训练模型（T5）在 NMT 任务上的微调
- 系统性分析不同架构组件对性能的影响
- 提供可复现的推理脚本

### 技术栈

- **深度学习框架**: PyTorch 1.10.0
- **预训练模型**: HuggingFace Transformers 4.11.3
- **评估指标**: BLEU Score (sacrebleu)
- **分词工具**: SentencePiece
- **可视化**: Matplotlib

---

## 数据集分析

### 数据集描述

本项目使用中英平行语料库进行模型训练和评估。数据集包含以下部分：

| 数据集 | 用途 | 样本数 |
|--------|------|--------|
| train_100k.jsonl | 训练集 | 100,000 |
| valid.jsonl | 验证集 | 2,000 |
| test.jsonl | 测试集 | 2,000 |

### 数据预处理

1. **分词**: 使用 SentencePiece 训练 BPE 分词器
2. **序列长度**: 根据数据分布设置最大序列长度
3. **词汇表大小**: 32,000 个子词

### 数据统计

| 语言 | 平均词数 | 最大词数 | 最小词数 |
|------|----------|----------|----------|
| 中文（训练集） | 18.5 | 128 | 1 |
| 英文（训练集） | 16.3 | 128 | 1 |

---

## RNN 神经机器翻译

### 模型架构

RNN 模型采用以下架构：

- **编码器**: 2 层单向 GRU，隐藏层维度 512
- **解码器**: 2 层单向 GRU，隐藏层维度 512
- **注意力机制**: 支持三种类型
  - Dot-product Attention
  - Multiplicative Attention
  - Additive Attention
- **词嵌入**: 512 维
- **Dropout**: 0.3

### 实验设计

#### 1. 注意力机制对比

| 注意力类型 | Greedy BLEU | Beam-4 BLEU | Beam-8 BLEU |
|------------|-------------|-------------|-------------|
| Dot-product | 11.45 | 8.70 | 9.79 |
| Multiplicative | 11.87 | 9.79 | 8.28 |
| Additive | 9.61 | 11.63 | 10.27 |

**分析**: 
- Multiplicative Attention 在贪婪解码下表现最佳（11.87 BLEU）
- Additive Attention 在 Beam Search（beam=4）下表现最佳（11.63 BLEU）
- 不同注意力机制对 Beam Size 的敏感度不同

#### 2. 训练策略对比

| 训练策略 | 注意力类型 | Greedy BLEU | Beam-4 BLEU |
|----------|------------|-------------|-------------|
| Teacher Forcing | Dot-product | 11.45 | 8.70 |
| Teacher Forcing | Multiplicative | 11.87 | 9.79 |
| Teacher Forcing | Additive | 9.61 | 11.63 |
| Free Running | Dot-product | 5.02 | 3.96 |
| Free Running | Multiplicative | 4.67 | 4.55 |
| Free Running | Additive | 4.42 | 4.15 |

**分析**:
- Teacher Forcing 策略显著优于 Free Running 策略
- Teacher Forcing 下 BLEU 分数约为 Free Running 的 2-3 倍
- Free Running 策略容易导致误差累积，影响翻译质量

#### 3. 解码策略对比

| 模型 | Greedy | Beam-4 | Beam-8 |
|------|--------|--------|--------|
| RNN (Dot-product) | 11.45 | 8.70 | 9.79 |
| RNN (Multiplicative) | 11.87 | 9.79 | 8.28 |
| RNN (Additive) | 9.61 | 11.63 | 10.27 |

**分析**:
- 贪婪解码在大多数情况下表现良好
- Beam Search 的性能取决于 Beam Size 和模型配置
- 过大的 Beam Size（如 8）并不总是带来性能提升

### RNN 实验结论

1. **最佳配置**: Multiplicative Attention + Teacher Forcing + Greedy Decoding
2. **最佳 BLEU**: 11.87（测试集）
3. **关键发现**:
   - 训练策略对性能影响最大
   - 注意力机制类型对 Beam Search 敏感度有显著影响
   - 贪婪解码在 RNN 模型上往往更稳定

---

## Transformer 神经机器翻译

### 模型架构

Transformer 模型采用标准 encoder-decoder 架构：

- **编码器层数**: 6
- **解码器层数**: 6
- **注意力头数**: 8
- **隐藏层维度**: 512
- **前馈网络维度**: 2048
- **Dropout**: 0.1

### 实验设计

#### 1. 架构消融实验

| 位置编码 | 归一化 | Best BLEU | Final BLEU | Test Greedy | Test Beam-4 |
|----------|--------|-----------|------------|-------------|-------------|
| Absolute PE | RMSNorm | 4.43 | 4.12 | 3.63 | 4.03 |
| Absolute PE | LayerNorm | 4.00 | 3.49 | 2.79 | 3.30 |
| Relative PE | RMSNorm | 3.92 | 3.92 | 2.82 | 2.61 |
| Relative PE | LayerNorm | 3.23 | 2.80 | 2.32 | 2.74 |

**分析**:
- Absolute PE + RMSNorm 组合表现最佳（4.43 BLEU）
- RMSNorm 在所有配置下均优于 LayerNorm
- Absolute PE 相对 Relative PE 表现更稳定

#### 2. 超参数敏感性分析

| 实验 | Best BLEU | Best Epoch | Final BLEU | Train Loss | Valid Loss |
|------|-----------|------------|-------------|------------|------------|
| Baseline | 4.00 | 472 | 3.49 | 3.79 | 6.67 |
| Batch 256 | 4.64 | 493 | 4.38 | 1.90 | 6.69 |
| D_model 768 | 4.54 | 404 | 4.17 | 3.23 | 6.74 |
| LR 1e-3 | 4.07 | 468 | 3.57 | 3.77 | 6.68 |

**分析**:
- 增大 Batch Size（256）带来显著性能提升（4.64 BLEU）
- 增大模型维度（768）略有提升但计算成本增加
- 学习率调整对性能影响较小

#### 3. 预训练模型微调（T5）

使用 T5-Small 进行微调：

| 指标 | 值 |
|------|-----|
| Eval Loss | 3.81 |
| Eval BLEU | 1.41 |
| Epochs | 500 |

**分析**:
- T5-Small 在当前任务上表现不佳（1.41 BLEU）
- 可能原因：预训练数据与中英翻译任务差异较大
- 需要更多训练数据或更长的训练时间

### Transformer 实验结论

1. **最佳配置**: Absolute PE + RMSNorm + Batch Size 256
2. **最佳 BLEU**: 4.64（验证集）
3. **关键发现**:
   - 归一化方式对 Transformer 性能有显著影响
   - Batch Size 是最重要的超参数
   - T5 预训练模型在此任务上未展现出优势

---

## 实验结果对比

### RNN vs Transformer

| 模型 | 最佳配置 | Test BLEU | 训练时间 |
|------|----------|-----------|----------|
| RNN | Multiplicative + Teacher Forcing | 11.87 | 中等 |
| Transformer | Absolute PE + RMSNorm | 4.03 | 较长 |
| T5-Small | Pre-trained + Fine-tuned | 1.41 | 较短 |

### 性能分析

1. **RNN 优势**:
   - 在当前数据集上表现最佳
   - 训练效率较高
   - 对小数据集更鲁棒

2. **Transformer 优势**:
   - 架构更先进，可扩展性强
   - 并行计算效率高
   - 在大规模数据上潜力更大

3. **T5 预训练模型**:
   - 在当前任务上表现不佳
   - 可能需要更多领域适应

### 翻译质量示例

| 中文 | 参考译文 | RNN 预测 | Transformer 预测 |
|------|----------|----------|------------------|
| 我今天很高兴 | I am very happy today | I am happy today | I am happy today |
| 这个问题很难解决 | This problem is difficult to solve | This problem is hard to solve | This problem is difficult to solve |
| 机器翻译技术发展迅速 | Machine translation technology is developing rapidly | Machine translation technology develops rapidly | Machine translation technology is developing rapidly |

---

## 结论

### 主要发现

1. **RNN 模型**在本项目的数据集上表现最佳，达到 11.87 BLEU
2. **注意力机制类型**对 RNN 性能有显著影响，Multiplicative Attention 表现最优
3. **训练策略**是影响 RNN 性能的关键因素，Teacher Forcing 远优于 Free Running
4. **归一化方式**对 Transformer 性能有重要影响，RMSNorm 优于 LayerNorm
5. **Batch Size** 是 Transformer 最重要的超参数，增大 Batch Size 可显著提升性能
6. **T5 预训练模型**在此任务上未展现出优势，需要进一步优化

### 项目交付

- [x] 代码仓库: https://github.com/Pugguphl/Slai_Mid_Homework_250010065
- [x] 一键推理脚本: `inference.py`
- [x] 所有模型检查点（通过 Git LFS 上传）
- [x] 实验结果和可视化图表
- [x] 个人项目报告

### 未来工作

1. 探索更大规模的预训练模型（如 T5-Base, T5-Large）
2. 实现更多解码策略（如 Nucleus Sampling）
3. 进行更详细的错误分析
4. 探索多任务学习和领域适应技术

---

## 参考文献

1. Vaswani, A., et al. (2017). Attention is All You Need. In Advances in Neural Information Processing Systems (NeurIPS).
2. Bahdanau, D., et al. (2014). Neural Machine Translation by Jointly Learning to Align and Translate. ICLR.
3. Raffel, C., et al. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. JMLR.
4. Papineni, K., et al. (2002). BLEU: A Method for Automatic Evaluation of Machine Translation. ACL.
