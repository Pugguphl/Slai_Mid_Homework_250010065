{
  "best_global_step": 246800,
  "best_metric": 1.4050730653141843,
  "best_model_checkpoint": "/mnt/afs/250010065/SLAI_NLP_Mid_project/zh-en-nmt-midterm/experiments/logs/t5_small/checkpoint-246800",
  "epoch": 500.0,
  "eval_steps": 500,
  "global_step": 1542500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.03241491085899514,
      "grad_norm": 1.6797802448272705,
      "learning_rate": 4.800000000000001e-06,
      "loss": 4.5726,
      "step": 100
    },
    {
      "epoch": 0.06482982171799027,
      "grad_norm": 1.0331605672836304,
      "learning_rate": 9.800000000000001e-06,
      "loss": 4.3618,
      "step": 200
    },
    {
      "epoch": 0.09724473257698542,
      "grad_norm": 0.9222862124443054,
      "learning_rate": 1.48e-05,
      "loss": 4.1515,
      "step": 300
    },
    {
      "epoch": 0.12965964343598055,
      "grad_norm": 1.4852453470230103,
      "learning_rate": 1.9800000000000004e-05,
      "loss": 4.0236,
      "step": 400
    },
    {
      "epoch": 0.1620745542949757,
      "grad_norm": 1.1541982889175415,
      "learning_rate": 2.48e-05,
      "loss": 3.9762,
      "step": 500
    },
    {
      "epoch": 0.19448946515397084,
      "grad_norm": 0.9210267066955566,
      "learning_rate": 2.98e-05,
      "loss": 3.9351,
      "step": 600
    },
    {
      "epoch": 0.22690437601296595,
      "grad_norm": 0.9970620274543762,
      "learning_rate": 3.48e-05,
      "loss": 3.9247,
      "step": 700
    },
    {
      "epoch": 0.2593192868719611,
      "grad_norm": 1.0250582695007324,
      "learning_rate": 3.9800000000000005e-05,
      "loss": 3.8842,
      "step": 800
    },
    {
      "epoch": 0.2917341977309562,
      "grad_norm": 0.9728512763977051,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 3.8498,
      "step": 900
    },
    {
      "epoch": 0.3241491085899514,
      "grad_norm": 0.9381589889526367,
      "learning_rate": 4.9800000000000004e-05,
      "loss": 3.8169,
      "step": 1000
    },
    {
      "epoch": 0.3565640194489465,
      "grad_norm": 0.9093391299247742,
      "learning_rate": 4.999688614985404e-05,
      "loss": 3.8367,
      "step": 1100
    },
    {
      "epoch": 0.3889789303079417,
      "grad_norm": 0.8823839426040649,
      "learning_rate": 4.9993642555951996e-05,
      "loss": 3.7899,
      "step": 1200
    },
    {
      "epoch": 0.4213938411669368,
      "grad_norm": 0.9424126148223877,
      "learning_rate": 4.9990398962049955e-05,
      "loss": 3.7958,
      "step": 1300
    },
    {
      "epoch": 0.4538087520259319,
      "grad_norm": 0.8339336514472961,
      "learning_rate": 4.998715536814791e-05,
      "loss": 3.77,
      "step": 1400
    },
    {
      "epoch": 0.4862236628849271,
      "grad_norm": 1.1111314296722412,
      "learning_rate": 4.9983911774245866e-05,
      "loss": 3.7718,
      "step": 1500
    },
    {
      "epoch": 0.5186385737439222,
      "grad_norm": 0.8459322452545166,
      "learning_rate": 4.9980668180343824e-05,
      "loss": 3.7747,
      "step": 1600
    },
    {
      "epoch": 0.5510534846029174,
      "grad_norm": 0.8715502023696899,
      "learning_rate": 4.9977424586441776e-05,
      "loss": 3.7546,
      "step": 1700
    },
    {
      "epoch": 0.5834683954619124,
      "grad_norm": 0.8342731595039368,
      "learning_rate": 4.9974180992539735e-05,
      "loss": 3.7504,
      "step": 1800
    },
    {
      "epoch": 0.6158833063209076,
      "grad_norm": 0.9409109950065613,
      "learning_rate": 4.9970937398637694e-05,
      "loss": 3.7476,
      "step": 1900
    },
    {
      "epoch": 0.6482982171799028,
      "grad_norm": 1.8225523233413696,
      "learning_rate": 4.9967693804735646e-05,
      "loss": 3.7431,
      "step": 2000
    },
    {
      "epoch": 0.6807131280388979,
      "grad_norm": 0.9544966220855713,
      "learning_rate": 4.9964450210833604e-05,
      "loss": 3.7417,
      "step": 2100
    },
    {
      "epoch": 0.713128038897893,
      "grad_norm": 0.9057488441467285,
      "learning_rate": 4.996120661693156e-05,
      "loss": 3.7253,
      "step": 2200
    },
    {
      "epoch": 0.7455429497568882,
      "grad_norm": 0.9065762758255005,
      "learning_rate": 4.995796302302952e-05,
      "loss": 3.7116,
      "step": 2300
    },
    {
      "epoch": 0.7779578606158833,
      "grad_norm": 0.9931285977363586,
      "learning_rate": 4.995471942912748e-05,
      "loss": 3.7322,
      "step": 2400
    },
    {
      "epoch": 0.8103727714748784,
      "grad_norm": 0.9617375135421753,
      "learning_rate": 4.995147583522543e-05,
      "loss": 3.7178,
      "step": 2500
    },
    {
      "epoch": 0.8427876823338736,
      "grad_norm": 0.8823535442352295,
      "learning_rate": 4.994823224132339e-05,
      "loss": 3.7404,
      "step": 2600
    },
    {
      "epoch": 0.8752025931928687,
      "grad_norm": 0.984575092792511,
      "learning_rate": 4.994498864742135e-05,
      "loss": 3.7064,
      "step": 2700
    },
    {
      "epoch": 0.9076175040518638,
      "grad_norm": 0.9684839844703674,
      "learning_rate": 4.99417450535193e-05,
      "loss": 3.6906,
      "step": 2800
    },
    {
      "epoch": 0.940032414910859,
      "grad_norm": 0.8473362326622009,
      "learning_rate": 4.993850145961726e-05,
      "loss": 3.6977,
      "step": 2900
    },
    {
      "epoch": 0.9724473257698542,
      "grad_norm": 0.7248867154121399,
      "learning_rate": 4.993525786571521e-05,
      "loss": 3.7144,
      "step": 3000
    },
    {
      "epoch": 1.0,
      "eval_bleu": 0.6681105934635609,
      "eval_loss": 3.7649145126342773,
      "eval_runtime": 5.288,
      "eval_samples_per_second": 93.041,
      "eval_steps_per_second": 1.513,
      "step": 3085
    },
    {
      "epoch": 1.0048622366288493,
      "grad_norm": 0.8666132688522339,
      "learning_rate": 4.993201427181317e-05,
      "loss": 3.6911,
      "step": 3100
    },
    {
      "epoch": 1.0372771474878444,
      "grad_norm": 0.857800304889679,
      "learning_rate": 4.992877067791113e-05,
      "loss": 3.6687,
      "step": 3200
    },
    {
      "epoch": 1.0696920583468394,
      "grad_norm": 0.8115066289901733,
      "learning_rate": 4.992552708400908e-05,
      "loss": 3.6751,
      "step": 3300
    },
    {
      "epoch": 1.1021069692058347,
      "grad_norm": 0.9506049752235413,
      "learning_rate": 4.992228349010704e-05,
      "loss": 3.6704,
      "step": 3400
    },
    {
      "epoch": 1.1345218800648298,
      "grad_norm": 0.791263222694397,
      "learning_rate": 4.9919039896205e-05,
      "loss": 3.6736,
      "step": 3500
    },
    {
      "epoch": 1.1669367909238249,
      "grad_norm": 0.7485505938529968,
      "learning_rate": 4.991579630230295e-05,
      "loss": 3.6679,
      "step": 3600
    },
    {
      "epoch": 1.1993517017828201,
      "grad_norm": 0.9193425178527832,
      "learning_rate": 4.991255270840091e-05,
      "loss": 3.6793,
      "step": 3700
    },
    {
      "epoch": 1.2317666126418152,
      "grad_norm": 0.9649174809455872,
      "learning_rate": 4.990930911449886e-05,
      "loss": 3.6664,
      "step": 3800
    },
    {
      "epoch": 1.2641815235008105,
      "grad_norm": 0.7906646132469177,
      "learning_rate": 4.990606552059682e-05,
      "loss": 3.6765,
      "step": 3900
    },
    {
      "epoch": 1.2965964343598055,
      "grad_norm": 0.8507904410362244,
      "learning_rate": 4.990282192669478e-05,
      "loss": 3.6478,
      "step": 4000
    },
    {
      "epoch": 1.3290113452188006,
      "grad_norm": 0.900198221206665,
      "learning_rate": 4.989957833279273e-05,
      "loss": 3.6549,
      "step": 4100
    },
    {
      "epoch": 1.3614262560777957,
      "grad_norm": 0.9227755665779114,
      "learning_rate": 4.989633473889069e-05,
      "loss": 3.656,
      "step": 4200
    },
    {
      "epoch": 1.393841166936791,
      "grad_norm": 0.8107711672782898,
      "learning_rate": 4.989309114498865e-05,
      "loss": 3.6749,
      "step": 4300
    },
    {
      "epoch": 1.426256077795786,
      "grad_norm": 1.0376216173171997,
      "learning_rate": 4.988984755108661e-05,
      "loss": 3.6481,
      "step": 4400
    },
    {
      "epoch": 1.4586709886547813,
      "grad_norm": 0.7833175659179688,
      "learning_rate": 4.988660395718456e-05,
      "loss": 3.6524,
      "step": 4500
    },
    {
      "epoch": 1.4910858995137763,
      "grad_norm": 0.9450885057449341,
      "learning_rate": 4.988336036328252e-05,
      "loss": 3.64,
      "step": 4600
    },
    {
      "epoch": 1.5235008103727714,
      "grad_norm": 0.933602511882782,
      "learning_rate": 4.988011676938048e-05,
      "loss": 3.6573,
      "step": 4700
    },
    {
      "epoch": 1.5559157212317665,
      "grad_norm": 0.7349653840065002,
      "learning_rate": 4.9876873175478436e-05,
      "loss": 3.65,
      "step": 4800
    },
    {
      "epoch": 1.5883306320907618,
      "grad_norm": 0.8325422406196594,
      "learning_rate": 4.987362958157639e-05,
      "loss": 3.6482,
      "step": 4900
    },
    {
      "epoch": 1.620745542949757,
      "grad_norm": 0.7393724322319031,
      "learning_rate": 4.987038598767435e-05,
      "loss": 3.6471,
      "step": 5000
    },
    {
      "epoch": 1.653160453808752,
      "grad_norm": 0.7905532121658325,
      "learning_rate": 4.9867142393772306e-05,
      "loss": 3.6355,
      "step": 5100
    },
    {
      "epoch": 1.6855753646677472,
      "grad_norm": 0.7634092569351196,
      "learning_rate": 4.986389879987026e-05,
      "loss": 3.6369,
      "step": 5200
    },
    {
      "epoch": 1.7179902755267422,
      "grad_norm": 0.7685794234275818,
      "learning_rate": 4.9860655205968216e-05,
      "loss": 3.6298,
      "step": 5300
    },
    {
      "epoch": 1.7504051863857373,
      "grad_norm": 0.915002703666687,
      "learning_rate": 4.9857411612066175e-05,
      "loss": 3.6304,
      "step": 5400
    },
    {
      "epoch": 1.7828200972447326,
      "grad_norm": 0.8249419927597046,
      "learning_rate": 4.985416801816413e-05,
      "loss": 3.6202,
      "step": 5500
    },
    {
      "epoch": 1.8152350081037278,
      "grad_norm": 0.8105984926223755,
      "learning_rate": 4.9850924424262086e-05,
      "loss": 3.6394,
      "step": 5600
    },
    {
      "epoch": 1.847649918962723,
      "grad_norm": 0.9352609515190125,
      "learning_rate": 4.9847680830360045e-05,
      "loss": 3.6446,
      "step": 5700
    },
    {
      "epoch": 1.880064829821718,
      "grad_norm": 0.9292877912521362,
      "learning_rate": 4.9844437236458e-05,
      "loss": 3.6332,
      "step": 5800
    },
    {
      "epoch": 1.912479740680713,
      "grad_norm": 0.9372442960739136,
      "learning_rate": 4.9841193642555955e-05,
      "loss": 3.6229,
      "step": 5900
    },
    {
      "epoch": 1.9448946515397083,
      "grad_norm": 0.9329570531845093,
      "learning_rate": 4.983795004865391e-05,
      "loss": 3.6037,
      "step": 6000
    },
    {
      "epoch": 1.9773095623987034,
      "grad_norm": 0.8103736639022827,
      "learning_rate": 4.9834706454751866e-05,
      "loss": 3.6365,
      "step": 6100
    },
    {
      "epoch": 2.0,
      "eval_bleu": 0.9132883486169052,
      "eval_loss": 3.7527167797088623,
      "eval_runtime": 6.1765,
      "eval_samples_per_second": 79.657,
      "eval_steps_per_second": 1.295,
      "step": 6170
    },
    {
      "epoch": 2.0097244732576987,
      "grad_norm": 0.8591398596763611,
      "learning_rate": 4.9831462860849825e-05,
      "loss": 3.6147,
      "step": 6200
    },
    {
      "epoch": 2.0421393841166937,
      "grad_norm": 0.8726011514663696,
      "learning_rate": 4.982821926694778e-05,
      "loss": 3.6171,
      "step": 6300
    },
    {
      "epoch": 2.0745542949756888,
      "grad_norm": 0.7608801126480103,
      "learning_rate": 4.9824975673045736e-05,
      "loss": 3.6012,
      "step": 6400
    },
    {
      "epoch": 2.106969205834684,
      "grad_norm": 0.8192885518074036,
      "learning_rate": 4.9821732079143694e-05,
      "loss": 3.584,
      "step": 6500
    },
    {
      "epoch": 2.139384116693679,
      "grad_norm": 0.9664657711982727,
      "learning_rate": 4.9818488485241646e-05,
      "loss": 3.6164,
      "step": 6600
    },
    {
      "epoch": 2.1717990275526744,
      "grad_norm": 0.8984004259109497,
      "learning_rate": 4.9815244891339605e-05,
      "loss": 3.6105,
      "step": 6700
    },
    {
      "epoch": 2.2042139384116695,
      "grad_norm": 0.7975320219993591,
      "learning_rate": 4.9812001297437564e-05,
      "loss": 3.6167,
      "step": 6800
    },
    {
      "epoch": 2.2366288492706645,
      "grad_norm": 0.8556150794029236,
      "learning_rate": 4.9808757703535516e-05,
      "loss": 3.5952,
      "step": 6900
    },
    {
      "epoch": 2.2690437601296596,
      "grad_norm": 0.9917799234390259,
      "learning_rate": 4.9805514109633474e-05,
      "loss": 3.5874,
      "step": 7000
    },
    {
      "epoch": 2.3014586709886546,
      "grad_norm": 0.8774116039276123,
      "learning_rate": 4.980227051573143e-05,
      "loss": 3.581,
      "step": 7100
    },
    {
      "epoch": 2.3338735818476497,
      "grad_norm": 0.8475454449653625,
      "learning_rate": 4.979902692182939e-05,
      "loss": 3.5832,
      "step": 7200
    },
    {
      "epoch": 2.366288492706645,
      "grad_norm": 0.8292209506034851,
      "learning_rate": 4.979578332792735e-05,
      "loss": 3.5974,
      "step": 7300
    },
    {
      "epoch": 2.3987034035656403,
      "grad_norm": 0.9741121530532837,
      "learning_rate": 4.97925397340253e-05,
      "loss": 3.5995,
      "step": 7400
    },
    {
      "epoch": 2.4311183144246353,
      "grad_norm": 0.9680750370025635,
      "learning_rate": 4.978929614012326e-05,
      "loss": 3.5854,
      "step": 7500
    },
    {
      "epoch": 2.4635332252836304,
      "grad_norm": 0.8713087439537048,
      "learning_rate": 4.978605254622122e-05,
      "loss": 3.6188,
      "step": 7600
    },
    {
      "epoch": 2.4959481361426255,
      "grad_norm": 0.8612383008003235,
      "learning_rate": 4.978280895231917e-05,
      "loss": 3.5888,
      "step": 7700
    },
    {
      "epoch": 2.528363047001621,
      "grad_norm": 0.9502345323562622,
      "learning_rate": 4.977956535841713e-05,
      "loss": 3.5876,
      "step": 7800
    },
    {
      "epoch": 2.560777957860616,
      "grad_norm": 0.9232063293457031,
      "learning_rate": 4.977632176451508e-05,
      "loss": 3.596,
      "step": 7900
    },
    {
      "epoch": 2.593192868719611,
      "grad_norm": 0.7830067276954651,
      "learning_rate": 4.977307817061304e-05,
      "loss": 3.5806,
      "step": 8000
    },
    {
      "epoch": 2.625607779578606,
      "grad_norm": 0.8996331095695496,
      "learning_rate": 4.9769834576711e-05,
      "loss": 3.5794,
      "step": 8100
    },
    {
      "epoch": 2.658022690437601,
      "grad_norm": 0.8579016923904419,
      "learning_rate": 4.976662341874798e-05,
      "loss": 3.5972,
      "step": 8200
    },
    {
      "epoch": 2.6904376012965967,
      "grad_norm": 0.8599563837051392,
      "learning_rate": 4.976337982484593e-05,
      "loss": 3.5792,
      "step": 8300
    },
    {
      "epoch": 2.7228525121555913,
      "grad_norm": 0.7435423135757446,
      "learning_rate": 4.976013623094389e-05,
      "loss": 3.5904,
      "step": 8400
    },
    {
      "epoch": 2.755267423014587,
      "grad_norm": 0.8878489136695862,
      "learning_rate": 4.975689263704185e-05,
      "loss": 3.5916,
      "step": 8500
    },
    {
      "epoch": 2.787682333873582,
      "grad_norm": 0.6734551787376404,
      "learning_rate": 4.97536490431398e-05,
      "loss": 3.586,
      "step": 8600
    },
    {
      "epoch": 2.820097244732577,
      "grad_norm": 0.843337893486023,
      "learning_rate": 4.975040544923776e-05,
      "loss": 3.5657,
      "step": 8700
    },
    {
      "epoch": 2.852512155591572,
      "grad_norm": 0.8057633638381958,
      "learning_rate": 4.974716185533572e-05,
      "loss": 3.5782,
      "step": 8800
    },
    {
      "epoch": 2.884927066450567,
      "grad_norm": 0.9760859608650208,
      "learning_rate": 4.974391826143367e-05,
      "loss": 3.5723,
      "step": 8900
    },
    {
      "epoch": 2.9173419773095626,
      "grad_norm": 0.8412949442863464,
      "learning_rate": 4.974067466753163e-05,
      "loss": 3.5803,
      "step": 9000
    },
    {
      "epoch": 2.9497568881685576,
      "grad_norm": 0.7343051433563232,
      "learning_rate": 4.973743107362958e-05,
      "loss": 3.5941,
      "step": 9100
    },
    {
      "epoch": 2.9821717990275527,
      "grad_norm": 0.7558436393737793,
      "learning_rate": 4.973418747972754e-05,
      "loss": 3.5902,
      "step": 9200
    },
    {
      "epoch": 3.0,
      "eval_bleu": 0.585025333444598,
      "eval_loss": 3.7381343841552734,
      "eval_runtime": 6.1926,
      "eval_samples_per_second": 79.45,
      "eval_steps_per_second": 1.292,
      "step": 9255
    },
    {
      "epoch": 3.0145867098865478,
      "grad_norm": 0.7880876660346985,
      "learning_rate": 4.97309438858255e-05,
      "loss": 3.5631,
      "step": 9300
    },
    {
      "epoch": 3.047001620745543,
      "grad_norm": 0.9972786903381348,
      "learning_rate": 4.972770029192345e-05,
      "loss": 3.5578,
      "step": 9400
    },
    {
      "epoch": 3.079416531604538,
      "grad_norm": 0.742519736289978,
      "learning_rate": 4.972445669802141e-05,
      "loss": 3.5679,
      "step": 9500
    },
    {
      "epoch": 3.1118314424635334,
      "grad_norm": 0.8826646208763123,
      "learning_rate": 4.9721213104119367e-05,
      "loss": 3.5504,
      "step": 9600
    },
    {
      "epoch": 3.1442463533225284,
      "grad_norm": 0.7517140507698059,
      "learning_rate": 4.971796951021732e-05,
      "loss": 3.5588,
      "step": 9700
    },
    {
      "epoch": 3.1766612641815235,
      "grad_norm": 0.9241285920143127,
      "learning_rate": 4.971472591631528e-05,
      "loss": 3.5546,
      "step": 9800
    },
    {
      "epoch": 3.2090761750405186,
      "grad_norm": 0.7730284333229065,
      "learning_rate": 4.9711482322413236e-05,
      "loss": 3.5471,
      "step": 9900
    },
    {
      "epoch": 3.2414910858995136,
      "grad_norm": 0.7868578433990479,
      "learning_rate": 4.9708238728511195e-05,
      "loss": 3.5575,
      "step": 10000
    },
    {
      "epoch": 3.2739059967585087,
      "grad_norm": 0.7329961061477661,
      "learning_rate": 4.9704995134609154e-05,
      "loss": 3.5642,
      "step": 10100
    },
    {
      "epoch": 3.306320907617504,
      "grad_norm": 0.7275894284248352,
      "learning_rate": 4.9701783976646125e-05,
      "loss": 3.5543,
      "step": 10200
    },
    {
      "epoch": 3.3387358184764993,
      "grad_norm": 0.9888803958892822,
      "learning_rate": 4.969854038274408e-05,
      "loss": 3.5582,
      "step": 10300
    },
    {
      "epoch": 3.3711507293354943,
      "grad_norm": 0.7633535265922546,
      "learning_rate": 4.9695296788842035e-05,
      "loss": 3.5413,
      "step": 10400
    },
    {
      "epoch": 3.4035656401944894,
      "grad_norm": 0.8675153255462646,
      "learning_rate": 4.9692053194939994e-05,
      "loss": 3.5401,
      "step": 10500
    },
    {
      "epoch": 3.4359805510534844,
      "grad_norm": 0.7103675603866577,
      "learning_rate": 4.968880960103795e-05,
      "loss": 3.5554,
      "step": 10600
    },
    {
      "epoch": 3.46839546191248,
      "grad_norm": 0.8221423029899597,
      "learning_rate": 4.968559844307493e-05,
      "loss": 3.5357,
      "step": 10700
    },
    {
      "epoch": 3.500810372771475,
      "grad_norm": 0.7904021143913269,
      "learning_rate": 4.968235484917288e-05,
      "loss": 3.5593,
      "step": 10800
    },
    {
      "epoch": 3.53322528363047,
      "grad_norm": 0.9460060000419617,
      "learning_rate": 4.967911125527084e-05,
      "loss": 3.5398,
      "step": 10900
    },
    {
      "epoch": 3.565640194489465,
      "grad_norm": 0.8974375128746033,
      "learning_rate": 4.96758676613688e-05,
      "loss": 3.5414,
      "step": 11000
    },
    {
      "epoch": 3.59805510534846,
      "grad_norm": 0.9071260690689087,
      "learning_rate": 4.967262406746675e-05,
      "loss": 3.5415,
      "step": 11100
    },
    {
      "epoch": 3.6304700162074557,
      "grad_norm": 0.8502393364906311,
      "learning_rate": 4.966938047356471e-05,
      "loss": 3.5611,
      "step": 11200
    },
    {
      "epoch": 3.6628849270664503,
      "grad_norm": 0.7328165173530579,
      "learning_rate": 4.966613687966267e-05,
      "loss": 3.5417,
      "step": 11300
    },
    {
      "epoch": 3.695299837925446,
      "grad_norm": 0.7291750311851501,
      "learning_rate": 4.966289328576063e-05,
      "loss": 3.5695,
      "step": 11400
    },
    {
      "epoch": 3.727714748784441,
      "grad_norm": 0.8253569006919861,
      "learning_rate": 4.965964969185859e-05,
      "loss": 3.5445,
      "step": 11500
    },
    {
      "epoch": 3.760129659643436,
      "grad_norm": 0.7859416604042053,
      "learning_rate": 4.965640609795654e-05,
      "loss": 3.5315,
      "step": 11600
    },
    {
      "epoch": 3.792544570502431,
      "grad_norm": 0.8389645218849182,
      "learning_rate": 4.96531625040545e-05,
      "loss": 3.5709,
      "step": 11700
    },
    {
      "epoch": 3.824959481361426,
      "grad_norm": 0.7343776226043701,
      "learning_rate": 4.964991891015245e-05,
      "loss": 3.5557,
      "step": 11800
    },
    {
      "epoch": 3.8573743922204216,
      "grad_norm": 0.8770685791969299,
      "learning_rate": 4.964667531625041e-05,
      "loss": 3.5513,
      "step": 11900
    },
    {
      "epoch": 3.8897893030794166,
      "grad_norm": 0.9362691640853882,
      "learning_rate": 4.964343172234837e-05,
      "loss": 3.5615,
      "step": 12000
    },
    {
      "epoch": 3.9222042139384117,
      "grad_norm": 0.707268238067627,
      "learning_rate": 4.964018812844632e-05,
      "loss": 3.5344,
      "step": 12100
    },
    {
      "epoch": 3.9546191247974067,
      "grad_norm": 0.8025115132331848,
      "learning_rate": 4.963694453454428e-05,
      "loss": 3.5376,
      "step": 12200
    },
    {
      "epoch": 3.987034035656402,
      "grad_norm": 0.8135232329368591,
      "learning_rate": 4.9633700940642237e-05,
      "loss": 3.5404,
      "step": 12300
    },
    {
      "epoch": 4.0,
      "eval_bleu": 0.6307012100847111,
      "eval_loss": 3.7309165000915527,
      "eval_runtime": 6.1573,
      "eval_samples_per_second": 79.905,
      "eval_steps_per_second": 1.299,
      "step": 12340
    },
    {
      "epoch": 4.019448946515397,
      "grad_norm": 0.8447877168655396,
      "learning_rate": 4.963045734674019e-05,
      "loss": 3.536,
      "step": 12400
    },
    {
      "epoch": 4.051863857374392,
      "grad_norm": 0.7224787473678589,
      "learning_rate": 4.962721375283815e-05,
      "loss": 3.5391,
      "step": 12500
    },
    {
      "epoch": 4.084278768233387,
      "grad_norm": 0.7559031248092651,
      "learning_rate": 4.9623970158936106e-05,
      "loss": 3.5234,
      "step": 12600
    },
    {
      "epoch": 4.116693679092383,
      "grad_norm": 0.9309917688369751,
      "learning_rate": 4.962072656503406e-05,
      "loss": 3.5223,
      "step": 12700
    },
    {
      "epoch": 4.1491085899513775,
      "grad_norm": 0.8326523900032043,
      "learning_rate": 4.961748297113202e-05,
      "loss": 3.5204,
      "step": 12800
    },
    {
      "epoch": 4.181523500810373,
      "grad_norm": 0.8649126887321472,
      "learning_rate": 4.961423937722997e-05,
      "loss": 3.5287,
      "step": 12900
    },
    {
      "epoch": 4.213938411669368,
      "grad_norm": 0.8267214894294739,
      "learning_rate": 4.961099578332793e-05,
      "loss": 3.5079,
      "step": 13000
    },
    {
      "epoch": 4.246353322528363,
      "grad_norm": 1.0432873964309692,
      "learning_rate": 4.9607752189425886e-05,
      "loss": 3.5347,
      "step": 13100
    },
    {
      "epoch": 4.278768233387358,
      "grad_norm": 0.7139114141464233,
      "learning_rate": 4.960450859552384e-05,
      "loss": 3.5224,
      "step": 13200
    },
    {
      "epoch": 4.311183144246353,
      "grad_norm": 0.886198103427887,
      "learning_rate": 4.96012650016218e-05,
      "loss": 3.5309,
      "step": 13300
    },
    {
      "epoch": 4.343598055105349,
      "grad_norm": 0.9162964224815369,
      "learning_rate": 4.9598021407719756e-05,
      "loss": 3.5082,
      "step": 13400
    },
    {
      "epoch": 4.376012965964343,
      "grad_norm": 0.8677259683609009,
      "learning_rate": 4.9594777813817714e-05,
      "loss": 3.5319,
      "step": 13500
    },
    {
      "epoch": 4.408427876823339,
      "grad_norm": 0.9781454801559448,
      "learning_rate": 4.9591534219915666e-05,
      "loss": 3.5306,
      "step": 13600
    },
    {
      "epoch": 4.4408427876823335,
      "grad_norm": 0.8927208781242371,
      "learning_rate": 4.9588290626013625e-05,
      "loss": 3.5119,
      "step": 13700
    },
    {
      "epoch": 4.473257698541329,
      "grad_norm": 1.0158946514129639,
      "learning_rate": 4.9585047032111584e-05,
      "loss": 3.5242,
      "step": 13800
    },
    {
      "epoch": 4.5056726094003245,
      "grad_norm": 0.8837922811508179,
      "learning_rate": 4.958180343820954e-05,
      "loss": 3.5304,
      "step": 13900
    },
    {
      "epoch": 4.538087520259319,
      "grad_norm": 0.8325304388999939,
      "learning_rate": 4.9578559844307495e-05,
      "loss": 3.5056,
      "step": 14000
    },
    {
      "epoch": 4.570502431118315,
      "grad_norm": 0.9770678281784058,
      "learning_rate": 4.957531625040545e-05,
      "loss": 3.5208,
      "step": 14100
    },
    {
      "epoch": 4.602917341977309,
      "grad_norm": 0.7594199776649475,
      "learning_rate": 4.957207265650341e-05,
      "loss": 3.5273,
      "step": 14200
    },
    {
      "epoch": 4.635332252836305,
      "grad_norm": 0.7928706407546997,
      "learning_rate": 4.9568829062601364e-05,
      "loss": 3.5146,
      "step": 14300
    },
    {
      "epoch": 4.667747163695299,
      "grad_norm": 0.7732164263725281,
      "learning_rate": 4.956558546869932e-05,
      "loss": 3.5062,
      "step": 14400
    },
    {
      "epoch": 4.700162074554295,
      "grad_norm": 0.7874186635017395,
      "learning_rate": 4.956234187479728e-05,
      "loss": 3.5378,
      "step": 14500
    },
    {
      "epoch": 4.73257698541329,
      "grad_norm": 0.8824012875556946,
      "learning_rate": 4.9559098280895233e-05,
      "loss": 3.5158,
      "step": 14600
    },
    {
      "epoch": 4.764991896272285,
      "grad_norm": 0.8137038350105286,
      "learning_rate": 4.955588712293221e-05,
      "loss": 3.5088,
      "step": 14700
    },
    {
      "epoch": 4.7974068071312805,
      "grad_norm": 0.8203847408294678,
      "learning_rate": 4.955264352903017e-05,
      "loss": 3.5154,
      "step": 14800
    },
    {
      "epoch": 4.829821717990275,
      "grad_norm": 0.8928176164627075,
      "learning_rate": 4.954939993512813e-05,
      "loss": 3.5043,
      "step": 14900
    },
    {
      "epoch": 4.862236628849271,
      "grad_norm": 0.9409657120704651,
      "learning_rate": 4.954615634122608e-05,
      "loss": 3.5109,
      "step": 15000
    },
    {
      "epoch": 4.894651539708266,
      "grad_norm": 0.7923205494880676,
      "learning_rate": 4.954291274732404e-05,
      "loss": 3.5054,
      "step": 15100
    },
    {
      "epoch": 4.927066450567261,
      "grad_norm": 0.7994697093963623,
      "learning_rate": 4.953966915342199e-05,
      "loss": 3.5066,
      "step": 15200
    },
    {
      "epoch": 4.959481361426256,
      "grad_norm": 0.926158607006073,
      "learning_rate": 4.953642555951995e-05,
      "loss": 3.501,
      "step": 15300
    },
    {
      "epoch": 4.991896272285251,
      "grad_norm": 0.7580047249794006,
      "learning_rate": 4.953318196561791e-05,
      "loss": 3.5229,
      "step": 15400
    },
    {
      "epoch": 5.0,
      "eval_bleu": 0.6592546159668585,
      "eval_loss": 3.729104518890381,
      "eval_runtime": 6.2421,
      "eval_samples_per_second": 78.819,
      "eval_steps_per_second": 1.282,
      "step": 15425
    },
    {
      "epoch": 5.024311183144246,
      "grad_norm": 0.8578425645828247,
      "learning_rate": 4.952993837171586e-05,
      "loss": 3.487,
      "step": 15500
    },
    {
      "epoch": 5.056726094003242,
      "grad_norm": 0.8427363634109497,
      "learning_rate": 4.952669477781382e-05,
      "loss": 3.5067,
      "step": 15600
    },
    {
      "epoch": 5.0891410048622365,
      "grad_norm": 0.7459796071052551,
      "learning_rate": 4.952345118391178e-05,
      "loss": 3.4981,
      "step": 15700
    },
    {
      "epoch": 5.121555915721232,
      "grad_norm": 0.8611906170845032,
      "learning_rate": 4.952020759000973e-05,
      "loss": 3.4891,
      "step": 15800
    },
    {
      "epoch": 5.153970826580227,
      "grad_norm": 0.7575498819351196,
      "learning_rate": 4.951696399610769e-05,
      "loss": 3.4769,
      "step": 15900
    },
    {
      "epoch": 5.186385737439222,
      "grad_norm": 0.7891557216644287,
      "learning_rate": 4.951372040220565e-05,
      "loss": 3.4988,
      "step": 16000
    },
    {
      "epoch": 5.218800648298217,
      "grad_norm": 0.8713921904563904,
      "learning_rate": 4.95104768083036e-05,
      "loss": 3.4873,
      "step": 16100
    },
    {
      "epoch": 5.251215559157212,
      "grad_norm": 0.8601099252700806,
      "learning_rate": 4.950723321440156e-05,
      "loss": 3.4976,
      "step": 16200
    },
    {
      "epoch": 5.283630470016208,
      "grad_norm": 0.8631005883216858,
      "learning_rate": 4.950398962049951e-05,
      "loss": 3.5095,
      "step": 16300
    },
    {
      "epoch": 5.316045380875202,
      "grad_norm": 0.7837833762168884,
      "learning_rate": 4.950074602659747e-05,
      "loss": 3.5081,
      "step": 16400
    },
    {
      "epoch": 5.348460291734198,
      "grad_norm": 0.8504587411880493,
      "learning_rate": 4.949750243269543e-05,
      "loss": 3.5043,
      "step": 16500
    },
    {
      "epoch": 5.3808752025931925,
      "grad_norm": 0.9819002151489258,
      "learning_rate": 4.949425883879339e-05,
      "loss": 3.499,
      "step": 16600
    },
    {
      "epoch": 5.413290113452188,
      "grad_norm": 0.7411984205245972,
      "learning_rate": 4.9491015244891345e-05,
      "loss": 3.4931,
      "step": 16700
    },
    {
      "epoch": 5.4457050243111835,
      "grad_norm": 0.900629460811615,
      "learning_rate": 4.9487804086928316e-05,
      "loss": 3.493,
      "step": 16800
    },
    {
      "epoch": 5.478119935170178,
      "grad_norm": 0.8661559820175171,
      "learning_rate": 4.9484560493026275e-05,
      "loss": 3.514,
      "step": 16900
    },
    {
      "epoch": 5.510534846029174,
      "grad_norm": 1.045635461807251,
      "learning_rate": 4.948131689912423e-05,
      "loss": 3.4997,
      "step": 17000
    },
    {
      "epoch": 5.542949756888168,
      "grad_norm": 0.9000488519668579,
      "learning_rate": 4.9478073305222186e-05,
      "loss": 3.5048,
      "step": 17100
    },
    {
      "epoch": 5.575364667747164,
      "grad_norm": 0.8551437854766846,
      "learning_rate": 4.9474829711320145e-05,
      "loss": 3.4743,
      "step": 17200
    },
    {
      "epoch": 5.607779578606159,
      "grad_norm": 0.8399674892425537,
      "learning_rate": 4.9471586117418103e-05,
      "loss": 3.493,
      "step": 17300
    },
    {
      "epoch": 5.640194489465154,
      "grad_norm": 0.8089284896850586,
      "learning_rate": 4.946834252351606e-05,
      "loss": 3.4899,
      "step": 17400
    },
    {
      "epoch": 5.672609400324149,
      "grad_norm": 0.9978110194206238,
      "learning_rate": 4.9465098929614014e-05,
      "loss": 3.4806,
      "step": 17500
    },
    {
      "epoch": 5.705024311183144,
      "grad_norm": 0.7624926567077637,
      "learning_rate": 4.946185533571197e-05,
      "loss": 3.4773,
      "step": 17600
    },
    {
      "epoch": 5.7374392220421395,
      "grad_norm": 0.7706282138824463,
      "learning_rate": 4.945861174180993e-05,
      "loss": 3.5073,
      "step": 17700
    },
    {
      "epoch": 5.769854132901134,
      "grad_norm": 0.8986166715621948,
      "learning_rate": 4.9455368147907884e-05,
      "loss": 3.4777,
      "step": 17800
    },
    {
      "epoch": 5.80226904376013,
      "grad_norm": 0.849778950214386,
      "learning_rate": 4.945212455400584e-05,
      "loss": 3.498,
      "step": 17900
    },
    {
      "epoch": 5.834683954619125,
      "grad_norm": 0.8570396304130554,
      "learning_rate": 4.94488809601038e-05,
      "loss": 3.492,
      "step": 18000
    },
    {
      "epoch": 5.86709886547812,
      "grad_norm": 0.807104229927063,
      "learning_rate": 4.944563736620175e-05,
      "loss": 3.4758,
      "step": 18100
    },
    {
      "epoch": 5.899513776337115,
      "grad_norm": 0.9720584750175476,
      "learning_rate": 4.944239377229971e-05,
      "loss": 3.4737,
      "step": 18200
    },
    {
      "epoch": 5.93192868719611,
      "grad_norm": 1.0952348709106445,
      "learning_rate": 4.943915017839767e-05,
      "loss": 3.482,
      "step": 18300
    },
    {
      "epoch": 5.964343598055105,
      "grad_norm": 0.7507976293563843,
      "learning_rate": 4.943590658449562e-05,
      "loss": 3.4961,
      "step": 18400
    },
    {
      "epoch": 5.9967585089141,
      "grad_norm": 0.9111955165863037,
      "learning_rate": 4.943266299059358e-05,
      "loss": 3.4761,
      "step": 18500
    },
    {
      "epoch": 6.0,
      "eval_bleu": 0.7970503778240688,
      "eval_loss": 3.715768575668335,
      "eval_runtime": 6.2508,
      "eval_samples_per_second": 78.71,
      "eval_steps_per_second": 1.28,
      "step": 18510
    },
    {
      "epoch": 6.0291734197730955,
      "grad_norm": 0.8710297346115112,
      "learning_rate": 4.942941939669153e-05,
      "loss": 3.4704,
      "step": 18600
    },
    {
      "epoch": 6.061588330632091,
      "grad_norm": 0.9526450634002686,
      "learning_rate": 4.942617580278949e-05,
      "loss": 3.4765,
      "step": 18700
    },
    {
      "epoch": 6.094003241491086,
      "grad_norm": 0.8353201746940613,
      "learning_rate": 4.942296464482647e-05,
      "loss": 3.4644,
      "step": 18800
    },
    {
      "epoch": 6.126418152350081,
      "grad_norm": 0.7566527724266052,
      "learning_rate": 4.941972105092443e-05,
      "loss": 3.4615,
      "step": 18900
    },
    {
      "epoch": 6.158833063209076,
      "grad_norm": 0.875457763671875,
      "learning_rate": 4.941647745702238e-05,
      "loss": 3.4777,
      "step": 19000
    },
    {
      "epoch": 6.191247974068071,
      "grad_norm": 0.9333813786506653,
      "learning_rate": 4.941323386312034e-05,
      "loss": 3.4946,
      "step": 19100
    },
    {
      "epoch": 6.223662884927067,
      "grad_norm": 1.037817120552063,
      "learning_rate": 4.94099902692183e-05,
      "loss": 3.475,
      "step": 19200
    },
    {
      "epoch": 6.256077795786061,
      "grad_norm": 0.877434492111206,
      "learning_rate": 4.940674667531625e-05,
      "loss": 3.4872,
      "step": 19300
    },
    {
      "epoch": 6.288492706645057,
      "grad_norm": 1.012824296951294,
      "learning_rate": 4.940350308141421e-05,
      "loss": 3.4737,
      "step": 19400
    },
    {
      "epoch": 6.3209076175040515,
      "grad_norm": 0.8615337014198303,
      "learning_rate": 4.940025948751217e-05,
      "loss": 3.4645,
      "step": 19500
    },
    {
      "epoch": 6.353322528363047,
      "grad_norm": 0.9462717175483704,
      "learning_rate": 4.939701589361012e-05,
      "loss": 3.4556,
      "step": 19600
    },
    {
      "epoch": 6.3857374392220425,
      "grad_norm": 0.8230668306350708,
      "learning_rate": 4.939377229970808e-05,
      "loss": 3.468,
      "step": 19700
    },
    {
      "epoch": 6.418152350081037,
      "grad_norm": 0.8385706543922424,
      "learning_rate": 4.939052870580603e-05,
      "loss": 3.4688,
      "step": 19800
    },
    {
      "epoch": 6.450567260940033,
      "grad_norm": 0.9133042097091675,
      "learning_rate": 4.938728511190399e-05,
      "loss": 3.4669,
      "step": 19900
    },
    {
      "epoch": 6.482982171799027,
      "grad_norm": 0.8159068822860718,
      "learning_rate": 4.938404151800195e-05,
      "loss": 3.4771,
      "step": 20000
    },
    {
      "epoch": 6.515397082658023,
      "grad_norm": 0.900025486946106,
      "learning_rate": 4.9380797924099906e-05,
      "loss": 3.4575,
      "step": 20100
    },
    {
      "epoch": 6.547811993517017,
      "grad_norm": 0.8121553063392639,
      "learning_rate": 4.937755433019786e-05,
      "loss": 3.4718,
      "step": 20200
    },
    {
      "epoch": 6.580226904376013,
      "grad_norm": 0.765598475933075,
      "learning_rate": 4.937431073629582e-05,
      "loss": 3.4812,
      "step": 20300
    },
    {
      "epoch": 6.612641815235008,
      "grad_norm": 1.0192369222640991,
      "learning_rate": 4.9371067142393776e-05,
      "loss": 3.4746,
      "step": 20400
    },
    {
      "epoch": 6.645056726094003,
      "grad_norm": 0.8071571588516235,
      "learning_rate": 4.9367823548491734e-05,
      "loss": 3.4544,
      "step": 20500
    },
    {
      "epoch": 6.6774716369529985,
      "grad_norm": 0.9795412421226501,
      "learning_rate": 4.936457995458969e-05,
      "loss": 3.4779,
      "step": 20600
    },
    {
      "epoch": 6.709886547811994,
      "grad_norm": 0.8193641901016235,
      "learning_rate": 4.9361336360687645e-05,
      "loss": 3.4751,
      "step": 20700
    },
    {
      "epoch": 6.742301458670989,
      "grad_norm": 0.7876419425010681,
      "learning_rate": 4.935812520272462e-05,
      "loss": 3.462,
      "step": 20800
    },
    {
      "epoch": 6.774716369529984,
      "grad_norm": 0.8023172616958618,
      "learning_rate": 4.935488160882258e-05,
      "loss": 3.4654,
      "step": 20900
    },
    {
      "epoch": 6.807131280388979,
      "grad_norm": 0.7273173332214355,
      "learning_rate": 4.9351638014920534e-05,
      "loss": 3.4568,
      "step": 21000
    },
    {
      "epoch": 6.839546191247974,
      "grad_norm": 0.9810540676116943,
      "learning_rate": 4.934839442101849e-05,
      "loss": 3.4545,
      "step": 21100
    },
    {
      "epoch": 6.871961102106969,
      "grad_norm": 0.7891786694526672,
      "learning_rate": 4.934515082711645e-05,
      "loss": 3.4496,
      "step": 21200
    },
    {
      "epoch": 6.904376012965964,
      "grad_norm": 0.7615857720375061,
      "learning_rate": 4.93419072332144e-05,
      "loss": 3.4717,
      "step": 21300
    },
    {
      "epoch": 6.93679092382496,
      "grad_norm": 0.7989175319671631,
      "learning_rate": 4.933866363931236e-05,
      "loss": 3.4872,
      "step": 21400
    },
    {
      "epoch": 6.9692058346839545,
      "grad_norm": 0.8268770575523376,
      "learning_rate": 4.933542004541032e-05,
      "loss": 3.4723,
      "step": 21500
    },
    {
      "epoch": 7.0,
      "eval_bleu": 0.8829464028862931,
      "eval_loss": 3.7154741287231445,
      "eval_runtime": 6.2564,
      "eval_samples_per_second": 78.64,
      "eval_steps_per_second": 1.279,
      "step": 21595
    },
    {
      "epoch": 7.00162074554295,
      "grad_norm": 0.7187463641166687,
      "learning_rate": 4.933217645150827e-05,
      "loss": 3.4584,
      "step": 21600
    },
    {
      "epoch": 7.034035656401945,
      "grad_norm": 0.8198065757751465,
      "learning_rate": 4.932893285760623e-05,
      "loss": 3.4526,
      "step": 21700
    },
    {
      "epoch": 7.06645056726094,
      "grad_norm": 0.8343683481216431,
      "learning_rate": 4.932568926370419e-05,
      "loss": 3.4495,
      "step": 21800
    },
    {
      "epoch": 7.098865478119935,
      "grad_norm": 0.8503733277320862,
      "learning_rate": 4.932244566980214e-05,
      "loss": 3.444,
      "step": 21900
    },
    {
      "epoch": 7.13128038897893,
      "grad_norm": 1.1223938465118408,
      "learning_rate": 4.93192020759001e-05,
      "loss": 3.4382,
      "step": 22000
    },
    {
      "epoch": 7.163695299837926,
      "grad_norm": 0.7490034699440002,
      "learning_rate": 4.931595848199805e-05,
      "loss": 3.446,
      "step": 22100
    },
    {
      "epoch": 7.19611021069692,
      "grad_norm": 0.7460349202156067,
      "learning_rate": 4.931271488809601e-05,
      "loss": 3.464,
      "step": 22200
    },
    {
      "epoch": 7.228525121555916,
      "grad_norm": 0.926662027835846,
      "learning_rate": 4.930947129419397e-05,
      "loss": 3.4402,
      "step": 22300
    },
    {
      "epoch": 7.2609400324149105,
      "grad_norm": 0.790433943271637,
      "learning_rate": 4.930622770029192e-05,
      "loss": 3.4433,
      "step": 22400
    },
    {
      "epoch": 7.293354943273906,
      "grad_norm": 0.8982833027839661,
      "learning_rate": 4.930298410638988e-05,
      "loss": 3.4302,
      "step": 22500
    },
    {
      "epoch": 7.3257698541329015,
      "grad_norm": 0.8117097020149231,
      "learning_rate": 4.929974051248784e-05,
      "loss": 3.4557,
      "step": 22600
    },
    {
      "epoch": 7.358184764991896,
      "grad_norm": 0.8021724820137024,
      "learning_rate": 4.929649691858579e-05,
      "loss": 3.4362,
      "step": 22700
    },
    {
      "epoch": 7.390599675850892,
      "grad_norm": 0.7559502124786377,
      "learning_rate": 4.929325332468375e-05,
      "loss": 3.4414,
      "step": 22800
    },
    {
      "epoch": 7.423014586709886,
      "grad_norm": 0.791221559047699,
      "learning_rate": 4.929004216672073e-05,
      "loss": 3.4565,
      "step": 22900
    },
    {
      "epoch": 7.455429497568882,
      "grad_norm": 0.786803662776947,
      "learning_rate": 4.928679857281869e-05,
      "loss": 3.4702,
      "step": 23000
    },
    {
      "epoch": 7.487844408427877,
      "grad_norm": 0.816240131855011,
      "learning_rate": 4.928355497891664e-05,
      "loss": 3.4549,
      "step": 23100
    },
    {
      "epoch": 7.520259319286872,
      "grad_norm": 0.7387259006500244,
      "learning_rate": 4.92803113850146e-05,
      "loss": 3.4418,
      "step": 23200
    },
    {
      "epoch": 7.552674230145867,
      "grad_norm": 0.9476787447929382,
      "learning_rate": 4.9277067791112556e-05,
      "loss": 3.4491,
      "step": 23300
    },
    {
      "epoch": 7.585089141004862,
      "grad_norm": 0.8801339864730835,
      "learning_rate": 4.927382419721051e-05,
      "loss": 3.4376,
      "step": 23400
    },
    {
      "epoch": 7.6175040518638575,
      "grad_norm": 0.9195380806922913,
      "learning_rate": 4.927058060330847e-05,
      "loss": 3.4532,
      "step": 23500
    },
    {
      "epoch": 7.649918962722852,
      "grad_norm": 0.8113082647323608,
      "learning_rate": 4.926733700940642e-05,
      "loss": 3.466,
      "step": 23600
    },
    {
      "epoch": 7.682333873581848,
      "grad_norm": 0.7607602477073669,
      "learning_rate": 4.926409341550438e-05,
      "loss": 3.4379,
      "step": 23700
    },
    {
      "epoch": 7.714748784440843,
      "grad_norm": 0.8079110980033875,
      "learning_rate": 4.9260849821602337e-05,
      "loss": 3.4649,
      "step": 23800
    },
    {
      "epoch": 7.747163695299838,
      "grad_norm": 0.8193179965019226,
      "learning_rate": 4.9257606227700295e-05,
      "loss": 3.4421,
      "step": 23900
    },
    {
      "epoch": 7.779578606158833,
      "grad_norm": 0.9294320344924927,
      "learning_rate": 4.9254362633798254e-05,
      "loss": 3.4465,
      "step": 24000
    },
    {
      "epoch": 7.811993517017828,
      "grad_norm": 0.7508214116096497,
      "learning_rate": 4.925111903989621e-05,
      "loss": 3.4473,
      "step": 24100
    },
    {
      "epoch": 7.844408427876823,
      "grad_norm": 0.9428737163543701,
      "learning_rate": 4.9247875445994165e-05,
      "loss": 3.4572,
      "step": 24200
    },
    {
      "epoch": 7.876823338735819,
      "grad_norm": 1.0330564975738525,
      "learning_rate": 4.9244631852092124e-05,
      "loss": 3.4457,
      "step": 24300
    },
    {
      "epoch": 7.9092382495948135,
      "grad_norm": 0.7217562794685364,
      "learning_rate": 4.9241388258190075e-05,
      "loss": 3.4545,
      "step": 24400
    },
    {
      "epoch": 7.941653160453809,
      "grad_norm": 0.8484567999839783,
      "learning_rate": 4.9238144664288034e-05,
      "loss": 3.4536,
      "step": 24500
    },
    {
      "epoch": 7.974068071312804,
      "grad_norm": 0.7798573970794678,
      "learning_rate": 4.923490107038599e-05,
      "loss": 3.4417,
      "step": 24600
    },
    {
      "epoch": 8.0,
      "eval_bleu": 1.0004787921048046,
      "eval_loss": 3.715090274810791,
      "eval_runtime": 5.2249,
      "eval_samples_per_second": 94.165,
      "eval_steps_per_second": 1.531,
      "step": 24680
    },
    {
      "epoch": 8.006482982171798,
      "grad_norm": 0.7933902740478516,
      "learning_rate": 4.9231657476483945e-05,
      "loss": 3.4374,
      "step": 24700
    },
    {
      "epoch": 8.038897893030795,
      "grad_norm": 0.8368844985961914,
      "learning_rate": 4.9228413882581904e-05,
      "loss": 3.4302,
      "step": 24800
    },
    {
      "epoch": 8.07131280388979,
      "grad_norm": 0.9147778153419495,
      "learning_rate": 4.922520272461888e-05,
      "loss": 3.4412,
      "step": 24900
    },
    {
      "epoch": 8.103727714748784,
      "grad_norm": 0.7827622294425964,
      "learning_rate": 4.922195913071684e-05,
      "loss": 3.4346,
      "step": 25000
    },
    {
      "epoch": 8.13614262560778,
      "grad_norm": 0.8498703837394714,
      "learning_rate": 4.921871553681479e-05,
      "loss": 3.4432,
      "step": 25100
    },
    {
      "epoch": 8.168557536466775,
      "grad_norm": 0.7766798138618469,
      "learning_rate": 4.921547194291275e-05,
      "loss": 3.4229,
      "step": 25200
    },
    {
      "epoch": 8.20097244732577,
      "grad_norm": 0.752259373664856,
      "learning_rate": 4.921222834901071e-05,
      "loss": 3.4388,
      "step": 25300
    },
    {
      "epoch": 8.233387358184766,
      "grad_norm": 0.8153160810470581,
      "learning_rate": 4.920898475510866e-05,
      "loss": 3.4353,
      "step": 25400
    },
    {
      "epoch": 8.26580226904376,
      "grad_norm": 0.8984590172767639,
      "learning_rate": 4.920574116120662e-05,
      "loss": 3.4161,
      "step": 25500
    },
    {
      "epoch": 8.298217179902755,
      "grad_norm": 0.814103364944458,
      "learning_rate": 4.920249756730458e-05,
      "loss": 3.4428,
      "step": 25600
    },
    {
      "epoch": 8.33063209076175,
      "grad_norm": 0.8530371785163879,
      "learning_rate": 4.919925397340253e-05,
      "loss": 3.4367,
      "step": 25700
    },
    {
      "epoch": 8.363047001620746,
      "grad_norm": 0.7768608331680298,
      "learning_rate": 4.919601037950049e-05,
      "loss": 3.4536,
      "step": 25800
    },
    {
      "epoch": 8.39546191247974,
      "grad_norm": 1.1406590938568115,
      "learning_rate": 4.919276678559844e-05,
      "loss": 3.4259,
      "step": 25900
    },
    {
      "epoch": 8.427876823338735,
      "grad_norm": 0.7988888621330261,
      "learning_rate": 4.91895231916964e-05,
      "loss": 3.4382,
      "step": 26000
    },
    {
      "epoch": 8.460291734197732,
      "grad_norm": 0.8671777248382568,
      "learning_rate": 4.918627959779436e-05,
      "loss": 3.4198,
      "step": 26100
    },
    {
      "epoch": 8.492706645056726,
      "grad_norm": 0.8334693908691406,
      "learning_rate": 4.918303600389231e-05,
      "loss": 3.4444,
      "step": 26200
    },
    {
      "epoch": 8.525121555915721,
      "grad_norm": 0.8665046691894531,
      "learning_rate": 4.917979240999027e-05,
      "loss": 3.4436,
      "step": 26300
    },
    {
      "epoch": 8.557536466774716,
      "grad_norm": 0.766768753528595,
      "learning_rate": 4.917654881608823e-05,
      "loss": 3.4249,
      "step": 26400
    },
    {
      "epoch": 8.589951377633712,
      "grad_norm": 0.7333664894104004,
      "learning_rate": 4.917330522218618e-05,
      "loss": 3.4264,
      "step": 26500
    },
    {
      "epoch": 8.622366288492707,
      "grad_norm": 0.8098047375679016,
      "learning_rate": 4.917006162828414e-05,
      "loss": 3.4393,
      "step": 26600
    },
    {
      "epoch": 8.654781199351701,
      "grad_norm": 0.9209089875221252,
      "learning_rate": 4.91668180343821e-05,
      "loss": 3.4346,
      "step": 26700
    },
    {
      "epoch": 8.687196110210698,
      "grad_norm": 0.7632781863212585,
      "learning_rate": 4.916357444048006e-05,
      "loss": 3.4183,
      "step": 26800
    },
    {
      "epoch": 8.719611021069692,
      "grad_norm": 1.0314304828643799,
      "learning_rate": 4.916036328251703e-05,
      "loss": 3.4186,
      "step": 26900
    },
    {
      "epoch": 8.752025931928687,
      "grad_norm": 0.8825028538703918,
      "learning_rate": 4.915711968861499e-05,
      "loss": 3.4412,
      "step": 27000
    },
    {
      "epoch": 8.784440842787681,
      "grad_norm": 0.7875024080276489,
      "learning_rate": 4.915387609471294e-05,
      "loss": 3.4339,
      "step": 27100
    },
    {
      "epoch": 8.816855753646678,
      "grad_norm": 0.7780051231384277,
      "learning_rate": 4.91506325008109e-05,
      "loss": 3.4171,
      "step": 27200
    },
    {
      "epoch": 8.849270664505672,
      "grad_norm": 0.7799684405326843,
      "learning_rate": 4.9147388906908856e-05,
      "loss": 3.4268,
      "step": 27300
    },
    {
      "epoch": 8.881685575364667,
      "grad_norm": 0.8735037446022034,
      "learning_rate": 4.9144145313006815e-05,
      "loss": 3.4239,
      "step": 27400
    },
    {
      "epoch": 8.914100486223663,
      "grad_norm": 0.8465932607650757,
      "learning_rate": 4.9140901719104774e-05,
      "loss": 3.4346,
      "step": 27500
    },
    {
      "epoch": 8.946515397082658,
      "grad_norm": 0.8385270237922668,
      "learning_rate": 4.913765812520273e-05,
      "loss": 3.4337,
      "step": 27600
    },
    {
      "epoch": 8.978930307941653,
      "grad_norm": 0.7858222723007202,
      "learning_rate": 4.9134414531300684e-05,
      "loss": 3.4272,
      "step": 27700
    },
    {
      "epoch": 9.0,
      "eval_bleu": 0.8281434421816143,
      "eval_loss": 3.7080612182617188,
      "eval_runtime": 6.2515,
      "eval_samples_per_second": 78.701,
      "eval_steps_per_second": 1.28,
      "step": 27765
    },
    {
      "epoch": 9.011345218800649,
      "grad_norm": 0.9232629537582397,
      "learning_rate": 4.913117093739864e-05,
      "loss": 3.4231,
      "step": 27800
    },
    {
      "epoch": 9.043760129659644,
      "grad_norm": 0.7361759543418884,
      "learning_rate": 4.91279273434966e-05,
      "loss": 3.4169,
      "step": 27900
    },
    {
      "epoch": 9.076175040518638,
      "grad_norm": 0.9403635263442993,
      "learning_rate": 4.9124683749594554e-05,
      "loss": 3.4145,
      "step": 28000
    },
    {
      "epoch": 9.108589951377633,
      "grad_norm": 0.7525384426116943,
      "learning_rate": 4.912144015569251e-05,
      "loss": 3.4141,
      "step": 28100
    },
    {
      "epoch": 9.14100486223663,
      "grad_norm": 0.7766296863555908,
      "learning_rate": 4.9118196561790464e-05,
      "loss": 3.4354,
      "step": 28200
    },
    {
      "epoch": 9.173419773095624,
      "grad_norm": 0.8282290697097778,
      "learning_rate": 4.911495296788842e-05,
      "loss": 3.4192,
      "step": 28300
    },
    {
      "epoch": 9.205834683954619,
      "grad_norm": 0.907522439956665,
      "learning_rate": 4.911170937398638e-05,
      "loss": 3.4056,
      "step": 28400
    },
    {
      "epoch": 9.238249594813615,
      "grad_norm": 0.8636339902877808,
      "learning_rate": 4.9108465780084334e-05,
      "loss": 3.4223,
      "step": 28500
    },
    {
      "epoch": 9.27066450567261,
      "grad_norm": 0.8065399527549744,
      "learning_rate": 4.910522218618229e-05,
      "loss": 3.4101,
      "step": 28600
    },
    {
      "epoch": 9.303079416531604,
      "grad_norm": 1.034312129020691,
      "learning_rate": 4.910197859228025e-05,
      "loss": 3.4059,
      "step": 28700
    },
    {
      "epoch": 9.335494327390599,
      "grad_norm": 0.903397262096405,
      "learning_rate": 4.9098734998378203e-05,
      "loss": 3.4079,
      "step": 28800
    },
    {
      "epoch": 9.367909238249595,
      "grad_norm": 0.8351680040359497,
      "learning_rate": 4.909552384041518e-05,
      "loss": 3.4078,
      "step": 28900
    },
    {
      "epoch": 9.40032414910859,
      "grad_norm": 0.9417070746421814,
      "learning_rate": 4.909228024651314e-05,
      "loss": 3.4189,
      "step": 29000
    },
    {
      "epoch": 9.432739059967584,
      "grad_norm": 0.939625084400177,
      "learning_rate": 4.90890366526111e-05,
      "loss": 3.425,
      "step": 29100
    },
    {
      "epoch": 9.46515397082658,
      "grad_norm": 0.8781614899635315,
      "learning_rate": 4.908579305870905e-05,
      "loss": 3.4161,
      "step": 29200
    },
    {
      "epoch": 9.497568881685575,
      "grad_norm": 0.9106858968734741,
      "learning_rate": 4.908254946480701e-05,
      "loss": 3.4279,
      "step": 29300
    },
    {
      "epoch": 9.52998379254457,
      "grad_norm": 0.8736319541931152,
      "learning_rate": 4.907930587090496e-05,
      "loss": 3.4155,
      "step": 29400
    },
    {
      "epoch": 9.562398703403566,
      "grad_norm": 0.8725676536560059,
      "learning_rate": 4.907606227700292e-05,
      "loss": 3.4105,
      "step": 29500
    },
    {
      "epoch": 9.594813614262561,
      "grad_norm": 0.7922500371932983,
      "learning_rate": 4.907281868310088e-05,
      "loss": 3.4112,
      "step": 29600
    },
    {
      "epoch": 9.627228525121556,
      "grad_norm": 0.8992781043052673,
      "learning_rate": 4.906957508919883e-05,
      "loss": 3.3912,
      "step": 29700
    },
    {
      "epoch": 9.65964343598055,
      "grad_norm": 0.9855263233184814,
      "learning_rate": 4.906633149529679e-05,
      "loss": 3.4094,
      "step": 29800
    },
    {
      "epoch": 9.692058346839547,
      "grad_norm": 0.8549126386642456,
      "learning_rate": 4.906308790139475e-05,
      "loss": 3.416,
      "step": 29900
    },
    {
      "epoch": 9.724473257698541,
      "grad_norm": 1.1066476106643677,
      "learning_rate": 4.90598443074927e-05,
      "loss": 3.4096,
      "step": 30000
    },
    {
      "epoch": 9.756888168557536,
      "grad_norm": 0.8771908283233643,
      "learning_rate": 4.905660071359066e-05,
      "loss": 3.4134,
      "step": 30100
    },
    {
      "epoch": 9.789303079416532,
      "grad_norm": 0.9264482855796814,
      "learning_rate": 4.905335711968862e-05,
      "loss": 3.4298,
      "step": 30200
    },
    {
      "epoch": 9.821717990275527,
      "grad_norm": 0.8281336426734924,
      "learning_rate": 4.905011352578657e-05,
      "loss": 3.4376,
      "step": 30300
    },
    {
      "epoch": 9.854132901134522,
      "grad_norm": 0.8348588347434998,
      "learning_rate": 4.904686993188453e-05,
      "loss": 3.3983,
      "step": 30400
    },
    {
      "epoch": 9.886547811993516,
      "grad_norm": 0.8369958400726318,
      "learning_rate": 4.904362633798249e-05,
      "loss": 3.4077,
      "step": 30500
    },
    {
      "epoch": 9.918962722852513,
      "grad_norm": 0.7194177508354187,
      "learning_rate": 4.9040382744080446e-05,
      "loss": 3.4204,
      "step": 30600
    },
    {
      "epoch": 9.951377633711507,
      "grad_norm": 0.8229153752326965,
      "learning_rate": 4.9037139150178405e-05,
      "loss": 3.4241,
      "step": 30700
    },
    {
      "epoch": 9.983792544570502,
      "grad_norm": 0.8907066583633423,
      "learning_rate": 4.903389555627636e-05,
      "loss": 3.437,
      "step": 30800
    },
    {
      "epoch": 10.0,
      "eval_bleu": 0.9655614480615706,
      "eval_loss": 3.7094779014587402,
      "eval_runtime": 4.6576,
      "eval_samples_per_second": 105.634,
      "eval_steps_per_second": 1.718,
      "step": 30850
    },
    {
      "epoch": 10.016207455429498,
      "grad_norm": 0.7805720567703247,
      "learning_rate": 4.9030684398313334e-05,
      "loss": 3.3959,
      "step": 30900
    },
    {
      "epoch": 10.048622366288493,
      "grad_norm": 0.886361300945282,
      "learning_rate": 4.902744080441129e-05,
      "loss": 3.3859,
      "step": 31000
    },
    {
      "epoch": 10.081037277147487,
      "grad_norm": 0.8394315242767334,
      "learning_rate": 4.9024197210509245e-05,
      "loss": 3.3799,
      "step": 31100
    },
    {
      "epoch": 10.113452188006484,
      "grad_norm": 0.940592885017395,
      "learning_rate": 4.9020953616607204e-05,
      "loss": 3.3863,
      "step": 31200
    },
    {
      "epoch": 10.145867098865478,
      "grad_norm": 0.7883304953575134,
      "learning_rate": 4.901771002270516e-05,
      "loss": 3.3925,
      "step": 31300
    },
    {
      "epoch": 10.178282009724473,
      "grad_norm": 0.9217875599861145,
      "learning_rate": 4.901446642880312e-05,
      "loss": 3.3942,
      "step": 31400
    },
    {
      "epoch": 10.210696920583468,
      "grad_norm": 0.8611441850662231,
      "learning_rate": 4.901122283490107e-05,
      "loss": 3.3959,
      "step": 31500
    },
    {
      "epoch": 10.243111831442464,
      "grad_norm": 0.8232434988021851,
      "learning_rate": 4.900797924099903e-05,
      "loss": 3.4082,
      "step": 31600
    },
    {
      "epoch": 10.275526742301459,
      "grad_norm": 0.9620345830917358,
      "learning_rate": 4.9004735647096984e-05,
      "loss": 3.4111,
      "step": 31700
    },
    {
      "epoch": 10.307941653160453,
      "grad_norm": 0.7967596650123596,
      "learning_rate": 4.900149205319494e-05,
      "loss": 3.3961,
      "step": 31800
    },
    {
      "epoch": 10.34035656401945,
      "grad_norm": 0.830910325050354,
      "learning_rate": 4.89982484592929e-05,
      "loss": 3.3953,
      "step": 31900
    },
    {
      "epoch": 10.372771474878444,
      "grad_norm": 0.8295581340789795,
      "learning_rate": 4.8995004865390854e-05,
      "loss": 3.4144,
      "step": 32000
    },
    {
      "epoch": 10.405186385737439,
      "grad_norm": 0.737991452217102,
      "learning_rate": 4.899176127148881e-05,
      "loss": 3.3846,
      "step": 32100
    },
    {
      "epoch": 10.437601296596434,
      "grad_norm": 0.7291622161865234,
      "learning_rate": 4.898851767758677e-05,
      "loss": 3.3945,
      "step": 32200
    },
    {
      "epoch": 10.47001620745543,
      "grad_norm": 0.8039703965187073,
      "learning_rate": 4.898527408368472e-05,
      "loss": 3.4027,
      "step": 32300
    },
    {
      "epoch": 10.502431118314425,
      "grad_norm": 0.8011025786399841,
      "learning_rate": 4.898203048978268e-05,
      "loss": 3.3893,
      "step": 32400
    },
    {
      "epoch": 10.53484602917342,
      "grad_norm": 0.7956675291061401,
      "learning_rate": 4.897878689588064e-05,
      "loss": 3.3985,
      "step": 32500
    },
    {
      "epoch": 10.567260940032416,
      "grad_norm": 0.8990044593811035,
      "learning_rate": 4.897554330197859e-05,
      "loss": 3.4053,
      "step": 32600
    },
    {
      "epoch": 10.59967585089141,
      "grad_norm": 0.7837672829627991,
      "learning_rate": 4.897229970807655e-05,
      "loss": 3.4173,
      "step": 32700
    },
    {
      "epoch": 10.632090761750405,
      "grad_norm": 0.8988843560218811,
      "learning_rate": 4.89690561141745e-05,
      "loss": 3.4172,
      "step": 32800
    },
    {
      "epoch": 10.664505672609401,
      "grad_norm": 0.8741223812103271,
      "learning_rate": 4.896581252027246e-05,
      "loss": 3.419,
      "step": 32900
    },
    {
      "epoch": 10.696920583468396,
      "grad_norm": 0.8134416937828064,
      "learning_rate": 4.896260136230944e-05,
      "loss": 3.3937,
      "step": 33000
    },
    {
      "epoch": 10.72933549432739,
      "grad_norm": 0.8315702676773071,
      "learning_rate": 4.89593577684074e-05,
      "loss": 3.4054,
      "step": 33100
    },
    {
      "epoch": 10.761750405186385,
      "grad_norm": 0.8554663062095642,
      "learning_rate": 4.895611417450535e-05,
      "loss": 3.3954,
      "step": 33200
    },
    {
      "epoch": 10.794165316045381,
      "grad_norm": 0.9145713448524475,
      "learning_rate": 4.895287058060331e-05,
      "loss": 3.3993,
      "step": 33300
    },
    {
      "epoch": 10.826580226904376,
      "grad_norm": 0.7831789255142212,
      "learning_rate": 4.894962698670127e-05,
      "loss": 3.3896,
      "step": 33400
    },
    {
      "epoch": 10.85899513776337,
      "grad_norm": 0.8647592067718506,
      "learning_rate": 4.894638339279922e-05,
      "loss": 3.3969,
      "step": 33500
    },
    {
      "epoch": 10.891410048622367,
      "grad_norm": 0.9198357462882996,
      "learning_rate": 4.894313979889718e-05,
      "loss": 3.4223,
      "step": 33600
    },
    {
      "epoch": 10.923824959481362,
      "grad_norm": 0.8358126282691956,
      "learning_rate": 4.893989620499514e-05,
      "loss": 3.4089,
      "step": 33700
    },
    {
      "epoch": 10.956239870340356,
      "grad_norm": 0.8537364602088928,
      "learning_rate": 4.893665261109309e-05,
      "loss": 3.4152,
      "step": 33800
    },
    {
      "epoch": 10.988654781199351,
      "grad_norm": 0.7143152952194214,
      "learning_rate": 4.893340901719105e-05,
      "loss": 3.416,
      "step": 33900
    },
    {
      "epoch": 11.0,
      "eval_bleu": 0.8220910486297761,
      "eval_loss": 3.7041382789611816,
      "eval_runtime": 5.6209,
      "eval_samples_per_second": 87.531,
      "eval_steps_per_second": 1.423,
      "step": 33935
    },
    {
      "epoch": 11.021069692058347,
      "grad_norm": 0.9280109405517578,
      "learning_rate": 4.893016542328901e-05,
      "loss": 3.3761,
      "step": 34000
    },
    {
      "epoch": 11.053484602917342,
      "grad_norm": 0.8342655897140503,
      "learning_rate": 4.8926921829386966e-05,
      "loss": 3.3783,
      "step": 34100
    },
    {
      "epoch": 11.085899513776337,
      "grad_norm": 0.8240007162094116,
      "learning_rate": 4.8923678235484924e-05,
      "loss": 3.3821,
      "step": 34200
    },
    {
      "epoch": 11.118314424635333,
      "grad_norm": 0.9899024963378906,
      "learning_rate": 4.8920434641582876e-05,
      "loss": 3.397,
      "step": 34300
    },
    {
      "epoch": 11.150729335494328,
      "grad_norm": 0.7792001366615295,
      "learning_rate": 4.8917191047680835e-05,
      "loss": 3.3986,
      "step": 34400
    },
    {
      "epoch": 11.183144246353322,
      "grad_norm": 0.7633922696113586,
      "learning_rate": 4.8913947453778794e-05,
      "loss": 3.3747,
      "step": 34500
    },
    {
      "epoch": 11.215559157212319,
      "grad_norm": 0.9633058309555054,
      "learning_rate": 4.8910703859876746e-05,
      "loss": 3.3843,
      "step": 34600
    },
    {
      "epoch": 11.247974068071313,
      "grad_norm": 0.8085567951202393,
      "learning_rate": 4.8907460265974704e-05,
      "loss": 3.3749,
      "step": 34700
    },
    {
      "epoch": 11.280388978930308,
      "grad_norm": 0.8480896949768066,
      "learning_rate": 4.890421667207266e-05,
      "loss": 3.4032,
      "step": 34800
    },
    {
      "epoch": 11.312803889789302,
      "grad_norm": 0.8480569124221802,
      "learning_rate": 4.8900973078170615e-05,
      "loss": 3.4031,
      "step": 34900
    },
    {
      "epoch": 11.345218800648299,
      "grad_norm": 0.9324108958244324,
      "learning_rate": 4.8897729484268574e-05,
      "loss": 3.3864,
      "step": 35000
    },
    {
      "epoch": 11.377633711507293,
      "grad_norm": 0.8135185837745667,
      "learning_rate": 4.889451832630555e-05,
      "loss": 3.3888,
      "step": 35100
    },
    {
      "epoch": 11.410048622366288,
      "grad_norm": 0.8032956123352051,
      "learning_rate": 4.889127473240351e-05,
      "loss": 3.3824,
      "step": 35200
    },
    {
      "epoch": 11.442463533225284,
      "grad_norm": 0.916986882686615,
      "learning_rate": 4.888803113850146e-05,
      "loss": 3.3777,
      "step": 35300
    },
    {
      "epoch": 11.474878444084279,
      "grad_norm": 0.8818184733390808,
      "learning_rate": 4.888478754459942e-05,
      "loss": 3.3833,
      "step": 35400
    },
    {
      "epoch": 11.507293354943274,
      "grad_norm": 0.7291337847709656,
      "learning_rate": 4.888154395069737e-05,
      "loss": 3.3991,
      "step": 35500
    },
    {
      "epoch": 11.539708265802268,
      "grad_norm": 0.8448677062988281,
      "learning_rate": 4.887830035679533e-05,
      "loss": 3.378,
      "step": 35600
    },
    {
      "epoch": 11.572123176661265,
      "grad_norm": 0.8209165930747986,
      "learning_rate": 4.887505676289329e-05,
      "loss": 3.3733,
      "step": 35700
    },
    {
      "epoch": 11.60453808752026,
      "grad_norm": 0.8654229044914246,
      "learning_rate": 4.887181316899124e-05,
      "loss": 3.3982,
      "step": 35800
    },
    {
      "epoch": 11.636952998379254,
      "grad_norm": 0.8399866819381714,
      "learning_rate": 4.88685695750892e-05,
      "loss": 3.3733,
      "step": 35900
    },
    {
      "epoch": 11.66936790923825,
      "grad_norm": 0.7576026916503906,
      "learning_rate": 4.886532598118716e-05,
      "loss": 3.376,
      "step": 36000
    },
    {
      "epoch": 11.701782820097245,
      "grad_norm": 0.7589974999427795,
      "learning_rate": 4.886208238728511e-05,
      "loss": 3.3802,
      "step": 36100
    },
    {
      "epoch": 11.73419773095624,
      "grad_norm": 0.8555134534835815,
      "learning_rate": 4.885883879338307e-05,
      "loss": 3.3968,
      "step": 36200
    },
    {
      "epoch": 11.766612641815236,
      "grad_norm": 0.8349774479866028,
      "learning_rate": 4.885559519948102e-05,
      "loss": 3.3892,
      "step": 36300
    },
    {
      "epoch": 11.79902755267423,
      "grad_norm": 0.8189638257026672,
      "learning_rate": 4.885235160557898e-05,
      "loss": 3.3923,
      "step": 36400
    },
    {
      "epoch": 11.831442463533225,
      "grad_norm": 0.6906423568725586,
      "learning_rate": 4.884910801167694e-05,
      "loss": 3.3937,
      "step": 36500
    },
    {
      "epoch": 11.86385737439222,
      "grad_norm": 0.8934265375137329,
      "learning_rate": 4.884586441777489e-05,
      "loss": 3.3939,
      "step": 36600
    },
    {
      "epoch": 11.896272285251216,
      "grad_norm": 0.7904755473136902,
      "learning_rate": 4.884262082387285e-05,
      "loss": 3.3738,
      "step": 36700
    },
    {
      "epoch": 11.92868719611021,
      "grad_norm": 0.9543753862380981,
      "learning_rate": 4.883937722997081e-05,
      "loss": 3.3948,
      "step": 36800
    },
    {
      "epoch": 11.961102106969205,
      "grad_norm": 0.896672248840332,
      "learning_rate": 4.883613363606876e-05,
      "loss": 3.3878,
      "step": 36900
    },
    {
      "epoch": 11.993517017828202,
      "grad_norm": 0.8005030751228333,
      "learning_rate": 4.883289004216672e-05,
      "loss": 3.3822,
      "step": 37000
    },
    {
      "epoch": 12.0,
      "eval_bleu": 1.0355870735427646,
      "eval_loss": 3.7030842304229736,
      "eval_runtime": 4.696,
      "eval_samples_per_second": 104.771,
      "eval_steps_per_second": 1.704,
      "step": 37020
    },
    {
      "epoch": 12.025931928687196,
      "grad_norm": 0.8065046072006226,
      "learning_rate": 4.88296788842037e-05,
      "loss": 3.3761,
      "step": 37100
    },
    {
      "epoch": 12.058346839546191,
      "grad_norm": 0.7649751901626587,
      "learning_rate": 4.882643529030166e-05,
      "loss": 3.3568,
      "step": 37200
    },
    {
      "epoch": 12.090761750405186,
      "grad_norm": 0.834297776222229,
      "learning_rate": 4.882319169639961e-05,
      "loss": 3.3731,
      "step": 37300
    },
    {
      "epoch": 12.123176661264182,
      "grad_norm": 0.8063680529594421,
      "learning_rate": 4.881994810249757e-05,
      "loss": 3.3624,
      "step": 37400
    },
    {
      "epoch": 12.155591572123177,
      "grad_norm": 0.8803783655166626,
      "learning_rate": 4.8816704508595526e-05,
      "loss": 3.3798,
      "step": 37500
    },
    {
      "epoch": 12.188006482982171,
      "grad_norm": 1.0279874801635742,
      "learning_rate": 4.8813460914693485e-05,
      "loss": 3.3594,
      "step": 37600
    },
    {
      "epoch": 12.220421393841168,
      "grad_norm": 0.8617162108421326,
      "learning_rate": 4.881021732079144e-05,
      "loss": 3.3673,
      "step": 37700
    },
    {
      "epoch": 12.252836304700162,
      "grad_norm": 0.7166876196861267,
      "learning_rate": 4.8806973726889396e-05,
      "loss": 3.3537,
      "step": 37800
    },
    {
      "epoch": 12.285251215559157,
      "grad_norm": 0.8462795615196228,
      "learning_rate": 4.8803730132987355e-05,
      "loss": 3.3847,
      "step": 37900
    },
    {
      "epoch": 12.317666126418152,
      "grad_norm": 0.9206039905548096,
      "learning_rate": 4.880048653908531e-05,
      "loss": 3.373,
      "step": 38000
    },
    {
      "epoch": 12.350081037277148,
      "grad_norm": 0.9789337515830994,
      "learning_rate": 4.8797242945183265e-05,
      "loss": 3.377,
      "step": 38100
    },
    {
      "epoch": 12.382495948136143,
      "grad_norm": 0.8599682450294495,
      "learning_rate": 4.8793999351281224e-05,
      "loss": 3.3582,
      "step": 38200
    },
    {
      "epoch": 12.414910858995137,
      "grad_norm": 0.8834472298622131,
      "learning_rate": 4.879075575737918e-05,
      "loss": 3.3686,
      "step": 38300
    },
    {
      "epoch": 12.447325769854134,
      "grad_norm": 0.7704265713691711,
      "learning_rate": 4.8787512163477135e-05,
      "loss": 3.4032,
      "step": 38400
    },
    {
      "epoch": 12.479740680713128,
      "grad_norm": 0.7884098291397095,
      "learning_rate": 4.8784268569575093e-05,
      "loss": 3.3907,
      "step": 38500
    },
    {
      "epoch": 12.512155591572123,
      "grad_norm": 0.8370823860168457,
      "learning_rate": 4.8781024975673045e-05,
      "loss": 3.4021,
      "step": 38600
    },
    {
      "epoch": 12.544570502431117,
      "grad_norm": 0.8876157402992249,
      "learning_rate": 4.8777781381771004e-05,
      "loss": 3.3774,
      "step": 38700
    },
    {
      "epoch": 12.576985413290114,
      "grad_norm": 0.7321596145629883,
      "learning_rate": 4.877453778786896e-05,
      "loss": 3.3743,
      "step": 38800
    },
    {
      "epoch": 12.609400324149108,
      "grad_norm": 0.796008288860321,
      "learning_rate": 4.8771294193966915e-05,
      "loss": 3.3736,
      "step": 38900
    },
    {
      "epoch": 12.641815235008103,
      "grad_norm": 0.8847756385803223,
      "learning_rate": 4.8768050600064874e-05,
      "loss": 3.376,
      "step": 39000
    },
    {
      "epoch": 12.6742301458671,
      "grad_norm": 0.9310047030448914,
      "learning_rate": 4.876480700616283e-05,
      "loss": 3.3628,
      "step": 39100
    },
    {
      "epoch": 12.706645056726094,
      "grad_norm": 0.896452009677887,
      "learning_rate": 4.876162828413883e-05,
      "loss": 3.3843,
      "step": 39200
    },
    {
      "epoch": 12.739059967585089,
      "grad_norm": 0.9355397820472717,
      "learning_rate": 4.875838469023679e-05,
      "loss": 3.3903,
      "step": 39300
    },
    {
      "epoch": 12.771474878444085,
      "grad_norm": 0.8211333751678467,
      "learning_rate": 4.875514109633474e-05,
      "loss": 3.3779,
      "step": 39400
    },
    {
      "epoch": 12.80388978930308,
      "grad_norm": 0.8220682740211487,
      "learning_rate": 4.87518975024327e-05,
      "loss": 3.3622,
      "step": 39500
    },
    {
      "epoch": 12.836304700162074,
      "grad_norm": 0.8838047981262207,
      "learning_rate": 4.874865390853066e-05,
      "loss": 3.3647,
      "step": 39600
    },
    {
      "epoch": 12.868719611021069,
      "grad_norm": 0.9197787642478943,
      "learning_rate": 4.874541031462861e-05,
      "loss": 3.3738,
      "step": 39700
    },
    {
      "epoch": 12.901134521880065,
      "grad_norm": 0.9371458292007446,
      "learning_rate": 4.874216672072657e-05,
      "loss": 3.352,
      "step": 39800
    },
    {
      "epoch": 12.93354943273906,
      "grad_norm": 1.0395110845565796,
      "learning_rate": 4.873892312682453e-05,
      "loss": 3.3835,
      "step": 39900
    },
    {
      "epoch": 12.965964343598054,
      "grad_norm": 0.7214142084121704,
      "learning_rate": 4.873567953292248e-05,
      "loss": 3.3663,
      "step": 40000
    },
    {
      "epoch": 12.998379254457051,
      "grad_norm": 0.8672387599945068,
      "learning_rate": 4.873243593902044e-05,
      "loss": 3.3846,
      "step": 40100
    },
    {
      "epoch": 13.0,
      "eval_bleu": 1.1511267285676772,
      "eval_loss": 3.7078146934509277,
      "eval_runtime": 4.9055,
      "eval_samples_per_second": 100.296,
      "eval_steps_per_second": 1.631,
      "step": 40105
    },
    {
      "epoch": 13.030794165316046,
      "grad_norm": 0.82475346326828,
      "learning_rate": 4.8729192345118396e-05,
      "loss": 3.3671,
      "step": 40200
    },
    {
      "epoch": 13.06320907617504,
      "grad_norm": 0.7938998341560364,
      "learning_rate": 4.872594875121635e-05,
      "loss": 3.3538,
      "step": 40300
    },
    {
      "epoch": 13.095623987034037,
      "grad_norm": 0.8610314130783081,
      "learning_rate": 4.872270515731431e-05,
      "loss": 3.3625,
      "step": 40400
    },
    {
      "epoch": 13.128038897893031,
      "grad_norm": 0.7855746746063232,
      "learning_rate": 4.871946156341226e-05,
      "loss": 3.3612,
      "step": 40500
    },
    {
      "epoch": 13.160453808752026,
      "grad_norm": 0.7854007482528687,
      "learning_rate": 4.871621796951022e-05,
      "loss": 3.381,
      "step": 40600
    },
    {
      "epoch": 13.19286871961102,
      "grad_norm": 0.8754494786262512,
      "learning_rate": 4.8712974375608176e-05,
      "loss": 3.365,
      "step": 40700
    },
    {
      "epoch": 13.225283630470017,
      "grad_norm": 0.7802200317382812,
      "learning_rate": 4.870973078170613e-05,
      "loss": 3.359,
      "step": 40800
    },
    {
      "epoch": 13.257698541329011,
      "grad_norm": 0.9991410970687866,
      "learning_rate": 4.870648718780409e-05,
      "loss": 3.3673,
      "step": 40900
    },
    {
      "epoch": 13.290113452188006,
      "grad_norm": 0.8323585987091064,
      "learning_rate": 4.8703243593902046e-05,
      "loss": 3.3508,
      "step": 41000
    },
    {
      "epoch": 13.322528363047002,
      "grad_norm": 0.8840786814689636,
      "learning_rate": 4.87e-05,
      "loss": 3.3744,
      "step": 41100
    },
    {
      "epoch": 13.354943273905997,
      "grad_norm": 0.8019024729728699,
      "learning_rate": 4.869675640609796e-05,
      "loss": 3.3801,
      "step": 41200
    },
    {
      "epoch": 13.387358184764992,
      "grad_norm": 0.8235087394714355,
      "learning_rate": 4.8693512812195915e-05,
      "loss": 3.3556,
      "step": 41300
    },
    {
      "epoch": 13.419773095623986,
      "grad_norm": 0.9544911980628967,
      "learning_rate": 4.8690269218293874e-05,
      "loss": 3.3472,
      "step": 41400
    },
    {
      "epoch": 13.452188006482983,
      "grad_norm": 0.8453919291496277,
      "learning_rate": 4.868702562439183e-05,
      "loss": 3.3555,
      "step": 41500
    },
    {
      "epoch": 13.484602917341977,
      "grad_norm": 0.7895106077194214,
      "learning_rate": 4.8683782030489785e-05,
      "loss": 3.3539,
      "step": 41600
    },
    {
      "epoch": 13.517017828200972,
      "grad_norm": 0.8703041672706604,
      "learning_rate": 4.8680538436587744e-05,
      "loss": 3.3631,
      "step": 41700
    },
    {
      "epoch": 13.549432739059968,
      "grad_norm": 0.8142976760864258,
      "learning_rate": 4.86772948426857e-05,
      "loss": 3.3624,
      "step": 41800
    },
    {
      "epoch": 13.581847649918963,
      "grad_norm": 0.8009085059165955,
      "learning_rate": 4.8674051248783654e-05,
      "loss": 3.3558,
      "step": 41900
    },
    {
      "epoch": 13.614262560777957,
      "grad_norm": 0.8004591464996338,
      "learning_rate": 4.867080765488161e-05,
      "loss": 3.3486,
      "step": 42000
    },
    {
      "epoch": 13.646677471636952,
      "grad_norm": 0.978524386882782,
      "learning_rate": 4.866756406097957e-05,
      "loss": 3.3715,
      "step": 42100
    },
    {
      "epoch": 13.679092382495948,
      "grad_norm": 0.8850526213645935,
      "learning_rate": 4.8664320467077524e-05,
      "loss": 3.3646,
      "step": 42200
    },
    {
      "epoch": 13.711507293354943,
      "grad_norm": 0.8361409306526184,
      "learning_rate": 4.866107687317548e-05,
      "loss": 3.3578,
      "step": 42300
    },
    {
      "epoch": 13.743922204213938,
      "grad_norm": 0.8129996061325073,
      "learning_rate": 4.8657833279273434e-05,
      "loss": 3.3673,
      "step": 42400
    },
    {
      "epoch": 13.776337115072934,
      "grad_norm": 0.8515762090682983,
      "learning_rate": 4.865458968537139e-05,
      "loss": 3.3832,
      "step": 42500
    },
    {
      "epoch": 13.808752025931929,
      "grad_norm": 0.8100226521492004,
      "learning_rate": 4.865134609146935e-05,
      "loss": 3.3578,
      "step": 42600
    },
    {
      "epoch": 13.841166936790923,
      "grad_norm": 0.8504425883293152,
      "learning_rate": 4.8648102497567304e-05,
      "loss": 3.3675,
      "step": 42700
    },
    {
      "epoch": 13.87358184764992,
      "grad_norm": 0.8565456867218018,
      "learning_rate": 4.864485890366526e-05,
      "loss": 3.3336,
      "step": 42800
    },
    {
      "epoch": 13.905996758508914,
      "grad_norm": 0.7441787719726562,
      "learning_rate": 4.864161530976322e-05,
      "loss": 3.3771,
      "step": 42900
    },
    {
      "epoch": 13.938411669367909,
      "grad_norm": 0.8569518327713013,
      "learning_rate": 4.863837171586117e-05,
      "loss": 3.3445,
      "step": 43000
    },
    {
      "epoch": 13.970826580226904,
      "grad_norm": 0.9077377319335938,
      "learning_rate": 4.863512812195913e-05,
      "loss": 3.3531,
      "step": 43100
    },
    {
      "epoch": 14.0,
      "eval_bleu": 1.0132972277992491,
      "eval_loss": 3.7006540298461914,
      "eval_runtime": 4.8889,
      "eval_samples_per_second": 100.637,
      "eval_steps_per_second": 1.636,
      "step": 43190
    },
    {
      "epoch": 14.0032414910859,
      "grad_norm": 0.8988618850708008,
      "learning_rate": 4.863191696399611e-05,
      "loss": 3.3517,
      "step": 43200
    },
    {
      "epoch": 14.035656401944895,
      "grad_norm": 0.9054036736488342,
      "learning_rate": 4.862867337009407e-05,
      "loss": 3.3631,
      "step": 43300
    },
    {
      "epoch": 14.06807131280389,
      "grad_norm": 0.7324233651161194,
      "learning_rate": 4.862542977619202e-05,
      "loss": 3.3433,
      "step": 43400
    },
    {
      "epoch": 14.100486223662886,
      "grad_norm": 0.7733725905418396,
      "learning_rate": 4.862218618228998e-05,
      "loss": 3.3461,
      "step": 43500
    },
    {
      "epoch": 14.13290113452188,
      "grad_norm": 0.8425273299217224,
      "learning_rate": 4.861894258838793e-05,
      "loss": 3.3542,
      "step": 43600
    },
    {
      "epoch": 14.165316045380875,
      "grad_norm": 0.8036438226699829,
      "learning_rate": 4.861569899448589e-05,
      "loss": 3.3511,
      "step": 43700
    },
    {
      "epoch": 14.19773095623987,
      "grad_norm": 0.8570528626441956,
      "learning_rate": 4.861245540058385e-05,
      "loss": 3.3503,
      "step": 43800
    },
    {
      "epoch": 14.230145867098866,
      "grad_norm": 0.8350628614425659,
      "learning_rate": 4.86092118066818e-05,
      "loss": 3.3469,
      "step": 43900
    },
    {
      "epoch": 14.26256077795786,
      "grad_norm": 0.8465161919593811,
      "learning_rate": 4.860596821277976e-05,
      "loss": 3.3369,
      "step": 44000
    },
    {
      "epoch": 14.294975688816855,
      "grad_norm": 1.080013394355774,
      "learning_rate": 4.860272461887772e-05,
      "loss": 3.3493,
      "step": 44100
    },
    {
      "epoch": 14.327390599675851,
      "grad_norm": 0.9563872814178467,
      "learning_rate": 4.859948102497568e-05,
      "loss": 3.3461,
      "step": 44200
    },
    {
      "epoch": 14.359805510534846,
      "grad_norm": 0.9281702041625977,
      "learning_rate": 4.8596237431073636e-05,
      "loss": 3.3342,
      "step": 44300
    },
    {
      "epoch": 14.39222042139384,
      "grad_norm": 0.9704748392105103,
      "learning_rate": 4.859299383717159e-05,
      "loss": 3.3419,
      "step": 44400
    },
    {
      "epoch": 14.424635332252837,
      "grad_norm": 0.8847483396530151,
      "learning_rate": 4.8589750243269546e-05,
      "loss": 3.367,
      "step": 44500
    },
    {
      "epoch": 14.457050243111832,
      "grad_norm": 0.869661808013916,
      "learning_rate": 4.8586506649367505e-05,
      "loss": 3.3467,
      "step": 44600
    },
    {
      "epoch": 14.489465153970826,
      "grad_norm": 0.7724795937538147,
      "learning_rate": 4.858326305546546e-05,
      "loss": 3.3668,
      "step": 44700
    },
    {
      "epoch": 14.521880064829821,
      "grad_norm": 0.7809782028198242,
      "learning_rate": 4.8580019461563416e-05,
      "loss": 3.341,
      "step": 44800
    },
    {
      "epoch": 14.554294975688817,
      "grad_norm": 0.7822554707527161,
      "learning_rate": 4.8576775867661375e-05,
      "loss": 3.374,
      "step": 44900
    },
    {
      "epoch": 14.586709886547812,
      "grad_norm": 0.7946828603744507,
      "learning_rate": 4.857353227375933e-05,
      "loss": 3.3275,
      "step": 45000
    },
    {
      "epoch": 14.619124797406807,
      "grad_norm": 0.8408553600311279,
      "learning_rate": 4.8570288679857285e-05,
      "loss": 3.338,
      "step": 45100
    },
    {
      "epoch": 14.651539708265803,
      "grad_norm": 0.859521746635437,
      "learning_rate": 4.856707752189426e-05,
      "loss": 3.3738,
      "step": 45200
    },
    {
      "epoch": 14.683954619124798,
      "grad_norm": 0.8851708769798279,
      "learning_rate": 4.856383392799222e-05,
      "loss": 3.3401,
      "step": 45300
    },
    {
      "epoch": 14.716369529983792,
      "grad_norm": 0.9310758113861084,
      "learning_rate": 4.856062277002919e-05,
      "loss": 3.3401,
      "step": 45400
    },
    {
      "epoch": 14.748784440842787,
      "grad_norm": 0.9620280265808105,
      "learning_rate": 4.855737917612715e-05,
      "loss": 3.3586,
      "step": 45500
    },
    {
      "epoch": 14.781199351701783,
      "grad_norm": 0.9921422600746155,
      "learning_rate": 4.855413558222511e-05,
      "loss": 3.3392,
      "step": 45600
    },
    {
      "epoch": 14.813614262560778,
      "grad_norm": 0.8462003469467163,
      "learning_rate": 4.855089198832307e-05,
      "loss": 3.335,
      "step": 45700
    },
    {
      "epoch": 14.846029173419772,
      "grad_norm": 0.7915138602256775,
      "learning_rate": 4.854764839442102e-05,
      "loss": 3.3378,
      "step": 45800
    },
    {
      "epoch": 14.878444084278769,
      "grad_norm": 0.7702897191047668,
      "learning_rate": 4.854440480051898e-05,
      "loss": 3.3663,
      "step": 45900
    },
    {
      "epoch": 14.910858995137763,
      "grad_norm": 0.8512548804283142,
      "learning_rate": 4.854116120661694e-05,
      "loss": 3.3492,
      "step": 46000
    },
    {
      "epoch": 14.943273905996758,
      "grad_norm": 0.9207866191864014,
      "learning_rate": 4.853791761271489e-05,
      "loss": 3.3616,
      "step": 46100
    },
    {
      "epoch": 14.975688816855754,
      "grad_norm": 0.9523090124130249,
      "learning_rate": 4.853467401881285e-05,
      "loss": 3.3529,
      "step": 46200
    },
    {
      "epoch": 15.0,
      "eval_bleu": 1.1537377872178993,
      "eval_loss": 3.7087063789367676,
      "eval_runtime": 5.1524,
      "eval_samples_per_second": 95.489,
      "eval_steps_per_second": 1.553,
      "step": 46275
    },
    {
      "epoch": 15.008103727714749,
      "grad_norm": 0.9417809247970581,
      "learning_rate": 4.85314304249108e-05,
      "loss": 3.3442,
      "step": 46300
    },
    {
      "epoch": 15.040518638573744,
      "grad_norm": 0.8943811058998108,
      "learning_rate": 4.852818683100876e-05,
      "loss": 3.3184,
      "step": 46400
    },
    {
      "epoch": 15.072933549432738,
      "grad_norm": 0.7843816876411438,
      "learning_rate": 4.852494323710672e-05,
      "loss": 3.3397,
      "step": 46500
    },
    {
      "epoch": 15.105348460291735,
      "grad_norm": 0.9370750188827515,
      "learning_rate": 4.852169964320467e-05,
      "loss": 3.34,
      "step": 46600
    },
    {
      "epoch": 15.13776337115073,
      "grad_norm": 0.9445221424102783,
      "learning_rate": 4.851845604930263e-05,
      "loss": 3.3319,
      "step": 46700
    },
    {
      "epoch": 15.170178282009724,
      "grad_norm": 0.8792291879653931,
      "learning_rate": 4.851521245540059e-05,
      "loss": 3.3427,
      "step": 46800
    },
    {
      "epoch": 15.20259319286872,
      "grad_norm": 0.8570343255996704,
      "learning_rate": 4.851196886149854e-05,
      "loss": 3.339,
      "step": 46900
    },
    {
      "epoch": 15.235008103727715,
      "grad_norm": 0.850071370601654,
      "learning_rate": 4.85087252675965e-05,
      "loss": 3.3269,
      "step": 47000
    },
    {
      "epoch": 15.26742301458671,
      "grad_norm": 0.7588948607444763,
      "learning_rate": 4.850548167369446e-05,
      "loss": 3.3426,
      "step": 47100
    },
    {
      "epoch": 15.299837925445704,
      "grad_norm": 0.9772857427597046,
      "learning_rate": 4.850223807979241e-05,
      "loss": 3.333,
      "step": 47200
    },
    {
      "epoch": 15.3322528363047,
      "grad_norm": 0.8456433415412903,
      "learning_rate": 4.849899448589037e-05,
      "loss": 3.3507,
      "step": 47300
    },
    {
      "epoch": 15.364667747163695,
      "grad_norm": 0.723223090171814,
      "learning_rate": 4.849575089198832e-05,
      "loss": 3.356,
      "step": 47400
    },
    {
      "epoch": 15.39708265802269,
      "grad_norm": 0.8496412634849548,
      "learning_rate": 4.849250729808628e-05,
      "loss": 3.3239,
      "step": 47500
    },
    {
      "epoch": 15.429497568881686,
      "grad_norm": 0.8923414945602417,
      "learning_rate": 4.848926370418424e-05,
      "loss": 3.3335,
      "step": 47600
    },
    {
      "epoch": 15.46191247974068,
      "grad_norm": 0.8233221173286438,
      "learning_rate": 4.848602011028219e-05,
      "loss": 3.3336,
      "step": 47700
    },
    {
      "epoch": 15.494327390599675,
      "grad_norm": 0.7854989171028137,
      "learning_rate": 4.848277651638015e-05,
      "loss": 3.3426,
      "step": 47800
    },
    {
      "epoch": 15.526742301458672,
      "grad_norm": 0.7916027903556824,
      "learning_rate": 4.847953292247811e-05,
      "loss": 3.3319,
      "step": 47900
    },
    {
      "epoch": 15.559157212317666,
      "grad_norm": 0.969714343547821,
      "learning_rate": 4.8476289328576066e-05,
      "loss": 3.3404,
      "step": 48000
    },
    {
      "epoch": 15.591572123176661,
      "grad_norm": 0.8394405245780945,
      "learning_rate": 4.8473045734674025e-05,
      "loss": 3.3418,
      "step": 48100
    },
    {
      "epoch": 15.623987034035656,
      "grad_norm": 0.8447447419166565,
      "learning_rate": 4.846980214077198e-05,
      "loss": 3.3265,
      "step": 48200
    },
    {
      "epoch": 15.656401944894652,
      "grad_norm": 0.6869207620620728,
      "learning_rate": 4.8466558546869935e-05,
      "loss": 3.3485,
      "step": 48300
    },
    {
      "epoch": 15.688816855753647,
      "grad_norm": 0.8515943884849548,
      "learning_rate": 4.8463314952967894e-05,
      "loss": 3.3234,
      "step": 48400
    },
    {
      "epoch": 15.721231766612641,
      "grad_norm": 0.8852052688598633,
      "learning_rate": 4.8460071359065846e-05,
      "loss": 3.3463,
      "step": 48500
    },
    {
      "epoch": 15.753646677471638,
      "grad_norm": 0.9988545179367065,
      "learning_rate": 4.8456827765163805e-05,
      "loss": 3.3462,
      "step": 48600
    },
    {
      "epoch": 15.786061588330632,
      "grad_norm": 0.7237036824226379,
      "learning_rate": 4.8453584171261764e-05,
      "loss": 3.3419,
      "step": 48700
    },
    {
      "epoch": 15.818476499189627,
      "grad_norm": 0.8551185131072998,
      "learning_rate": 4.8450340577359716e-05,
      "loss": 3.3508,
      "step": 48800
    },
    {
      "epoch": 15.850891410048622,
      "grad_norm": 0.817893385887146,
      "learning_rate": 4.8447096983457674e-05,
      "loss": 3.3353,
      "step": 48900
    },
    {
      "epoch": 15.883306320907618,
      "grad_norm": 1.0606361627578735,
      "learning_rate": 4.844385338955563e-05,
      "loss": 3.3246,
      "step": 49000
    },
    {
      "epoch": 15.915721231766613,
      "grad_norm": 0.9245190024375916,
      "learning_rate": 4.844064223159261e-05,
      "loss": 3.3463,
      "step": 49100
    },
    {
      "epoch": 15.948136142625607,
      "grad_norm": 0.8503705263137817,
      "learning_rate": 4.843739863769056e-05,
      "loss": 3.3541,
      "step": 49200
    },
    {
      "epoch": 15.980551053484604,
      "grad_norm": 0.8341427445411682,
      "learning_rate": 4.843415504378852e-05,
      "loss": 3.3379,
      "step": 49300
    },
    {
      "epoch": 16.0,
      "eval_bleu": 1.203098299317556,
      "eval_loss": 3.7052061557769775,
      "eval_runtime": 4.7458,
      "eval_samples_per_second": 103.671,
      "eval_steps_per_second": 1.686,
      "step": 49360
    },
    {
      "epoch": 16.012965964343596,
      "grad_norm": 1.1828787326812744,
      "learning_rate": 4.843091144988648e-05,
      "loss": 3.3108,
      "step": 49400
    },
    {
      "epoch": 16.045380875202593,
      "grad_norm": 0.8910539746284485,
      "learning_rate": 4.842766785598443e-05,
      "loss": 3.3057,
      "step": 49500
    },
    {
      "epoch": 16.07779578606159,
      "grad_norm": 0.7665195465087891,
      "learning_rate": 4.842442426208239e-05,
      "loss": 3.336,
      "step": 49600
    },
    {
      "epoch": 16.110210696920582,
      "grad_norm": 0.7954614758491516,
      "learning_rate": 4.842118066818034e-05,
      "loss": 3.3348,
      "step": 49700
    },
    {
      "epoch": 16.14262560777958,
      "grad_norm": 0.7679864168167114,
      "learning_rate": 4.84179370742783e-05,
      "loss": 3.328,
      "step": 49800
    },
    {
      "epoch": 16.175040518638575,
      "grad_norm": 0.855457067489624,
      "learning_rate": 4.841469348037626e-05,
      "loss": 3.3238,
      "step": 49900
    },
    {
      "epoch": 16.207455429497568,
      "grad_norm": 0.7976498007774353,
      "learning_rate": 4.841144988647421e-05,
      "loss": 3.3274,
      "step": 50000
    },
    {
      "epoch": 16.239870340356564,
      "grad_norm": 0.8051173686981201,
      "learning_rate": 4.840820629257217e-05,
      "loss": 3.3208,
      "step": 50100
    },
    {
      "epoch": 16.27228525121556,
      "grad_norm": 0.8869597911834717,
      "learning_rate": 4.840496269867013e-05,
      "loss": 3.3304,
      "step": 50200
    },
    {
      "epoch": 16.304700162074553,
      "grad_norm": 0.8209633827209473,
      "learning_rate": 4.840171910476808e-05,
      "loss": 3.3184,
      "step": 50300
    },
    {
      "epoch": 16.33711507293355,
      "grad_norm": 0.8491031527519226,
      "learning_rate": 4.839847551086604e-05,
      "loss": 3.3238,
      "step": 50400
    },
    {
      "epoch": 16.369529983792546,
      "grad_norm": 0.965071976184845,
      "learning_rate": 4.8395231916964e-05,
      "loss": 3.3382,
      "step": 50500
    },
    {
      "epoch": 16.40194489465154,
      "grad_norm": 0.790172278881073,
      "learning_rate": 4.839198832306195e-05,
      "loss": 3.3187,
      "step": 50600
    },
    {
      "epoch": 16.434359805510535,
      "grad_norm": 0.7999597787857056,
      "learning_rate": 4.838874472915991e-05,
      "loss": 3.3305,
      "step": 50700
    },
    {
      "epoch": 16.46677471636953,
      "grad_norm": 0.6984388828277588,
      "learning_rate": 4.838550113525787e-05,
      "loss": 3.3406,
      "step": 50800
    },
    {
      "epoch": 16.499189627228525,
      "grad_norm": 0.9485738277435303,
      "learning_rate": 4.838225754135583e-05,
      "loss": 3.3277,
      "step": 50900
    },
    {
      "epoch": 16.53160453808752,
      "grad_norm": 1.1639106273651123,
      "learning_rate": 4.837901394745378e-05,
      "loss": 3.3373,
      "step": 51000
    },
    {
      "epoch": 16.564019448946514,
      "grad_norm": 0.8269292116165161,
      "learning_rate": 4.837577035355174e-05,
      "loss": 3.3266,
      "step": 51100
    },
    {
      "epoch": 16.59643435980551,
      "grad_norm": 0.9172484874725342,
      "learning_rate": 4.83725267596497e-05,
      "loss": 3.3415,
      "step": 51200
    },
    {
      "epoch": 16.628849270664507,
      "grad_norm": 0.8772260546684265,
      "learning_rate": 4.8369283165747656e-05,
      "loss": 3.335,
      "step": 51300
    },
    {
      "epoch": 16.6612641815235,
      "grad_norm": 0.7764860391616821,
      "learning_rate": 4.836603957184561e-05,
      "loss": 3.3266,
      "step": 51400
    },
    {
      "epoch": 16.693679092382496,
      "grad_norm": 0.9977094531059265,
      "learning_rate": 4.8362795977943567e-05,
      "loss": 3.3129,
      "step": 51500
    },
    {
      "epoch": 16.726094003241492,
      "grad_norm": 0.9557291269302368,
      "learning_rate": 4.835955238404152e-05,
      "loss": 3.3239,
      "step": 51600
    },
    {
      "epoch": 16.758508914100485,
      "grad_norm": 0.9720548391342163,
      "learning_rate": 4.835630879013948e-05,
      "loss": 3.3113,
      "step": 51700
    },
    {
      "epoch": 16.79092382495948,
      "grad_norm": 0.8471057415008545,
      "learning_rate": 4.8353065196237436e-05,
      "loss": 3.3469,
      "step": 51800
    },
    {
      "epoch": 16.823338735818478,
      "grad_norm": 0.9792506098747253,
      "learning_rate": 4.834982160233539e-05,
      "loss": 3.3326,
      "step": 51900
    },
    {
      "epoch": 16.85575364667747,
      "grad_norm": 0.8296935558319092,
      "learning_rate": 4.834657800843335e-05,
      "loss": 3.3315,
      "step": 52000
    },
    {
      "epoch": 16.888168557536467,
      "grad_norm": 0.8569448590278625,
      "learning_rate": 4.8343334414531305e-05,
      "loss": 3.3397,
      "step": 52100
    },
    {
      "epoch": 16.920583468395463,
      "grad_norm": 0.880716860294342,
      "learning_rate": 4.834009082062926e-05,
      "loss": 3.3256,
      "step": 52200
    },
    {
      "epoch": 16.952998379254456,
      "grad_norm": 0.8841325044631958,
      "learning_rate": 4.8336847226727216e-05,
      "loss": 3.3444,
      "step": 52300
    },
    {
      "epoch": 16.985413290113453,
      "grad_norm": 0.8976461291313171,
      "learning_rate": 4.8333603632825175e-05,
      "loss": 3.3193,
      "step": 52400
    },
    {
      "epoch": 17.0,
      "eval_bleu": 1.195822658771148,
      "eval_loss": 3.703604221343994,
      "eval_runtime": 4.5689,
      "eval_samples_per_second": 107.685,
      "eval_steps_per_second": 1.751,
      "step": 52445
    },
    {
      "epoch": 17.01782820097245,
      "grad_norm": 0.7072165012359619,
      "learning_rate": 4.833036003892313e-05,
      "loss": 3.3122,
      "step": 52500
    },
    {
      "epoch": 17.050243111831442,
      "grad_norm": 0.8071543574333191,
      "learning_rate": 4.8327116445021086e-05,
      "loss": 3.3092,
      "step": 52600
    },
    {
      "epoch": 17.08265802269044,
      "grad_norm": 0.7598993182182312,
      "learning_rate": 4.832387285111904e-05,
      "loss": 3.3185,
      "step": 52700
    },
    {
      "epoch": 17.11507293354943,
      "grad_norm": 0.8395270705223083,
      "learning_rate": 4.8320629257216996e-05,
      "loss": 3.3079,
      "step": 52800
    },
    {
      "epoch": 17.147487844408428,
      "grad_norm": 1.0392311811447144,
      "learning_rate": 4.8317385663314955e-05,
      "loss": 3.3151,
      "step": 52900
    },
    {
      "epoch": 17.179902755267424,
      "grad_norm": 0.9456632137298584,
      "learning_rate": 4.831414206941291e-05,
      "loss": 3.3176,
      "step": 53000
    },
    {
      "epoch": 17.212317666126417,
      "grad_norm": 0.8059220910072327,
      "learning_rate": 4.8310930911449885e-05,
      "loss": 3.3103,
      "step": 53100
    },
    {
      "epoch": 17.244732576985413,
      "grad_norm": 1.0290120840072632,
      "learning_rate": 4.8307687317547844e-05,
      "loss": 3.3065,
      "step": 53200
    },
    {
      "epoch": 17.27714748784441,
      "grad_norm": 0.8471291661262512,
      "learning_rate": 4.83044437236458e-05,
      "loss": 3.3138,
      "step": 53300
    },
    {
      "epoch": 17.309562398703402,
      "grad_norm": 0.7629430890083313,
      "learning_rate": 4.8301200129743754e-05,
      "loss": 3.322,
      "step": 53400
    },
    {
      "epoch": 17.3419773095624,
      "grad_norm": 0.867941677570343,
      "learning_rate": 4.829798897178073e-05,
      "loss": 3.3355,
      "step": 53500
    },
    {
      "epoch": 17.374392220421395,
      "grad_norm": 0.902538537979126,
      "learning_rate": 4.829474537787869e-05,
      "loss": 3.2908,
      "step": 53600
    },
    {
      "epoch": 17.406807131280388,
      "grad_norm": 0.8464263677597046,
      "learning_rate": 4.829150178397665e-05,
      "loss": 3.3251,
      "step": 53700
    },
    {
      "epoch": 17.439222042139384,
      "grad_norm": 1.0067048072814941,
      "learning_rate": 4.82882581900746e-05,
      "loss": 3.331,
      "step": 53800
    },
    {
      "epoch": 17.47163695299838,
      "grad_norm": 0.9391472935676575,
      "learning_rate": 4.828501459617256e-05,
      "loss": 3.323,
      "step": 53900
    },
    {
      "epoch": 17.504051863857374,
      "grad_norm": 1.079687476158142,
      "learning_rate": 4.828177100227052e-05,
      "loss": 3.3001,
      "step": 54000
    },
    {
      "epoch": 17.53646677471637,
      "grad_norm": 0.8194747567176819,
      "learning_rate": 4.827852740836847e-05,
      "loss": 3.3266,
      "step": 54100
    },
    {
      "epoch": 17.568881685575363,
      "grad_norm": 0.8438014388084412,
      "learning_rate": 4.827528381446643e-05,
      "loss": 3.3145,
      "step": 54200
    },
    {
      "epoch": 17.60129659643436,
      "grad_norm": 0.8633841276168823,
      "learning_rate": 4.827204022056439e-05,
      "loss": 3.3108,
      "step": 54300
    },
    {
      "epoch": 17.633711507293356,
      "grad_norm": 0.8475048542022705,
      "learning_rate": 4.826879662666234e-05,
      "loss": 3.3335,
      "step": 54400
    },
    {
      "epoch": 17.66612641815235,
      "grad_norm": 0.9571916460990906,
      "learning_rate": 4.82655530327603e-05,
      "loss": 3.3182,
      "step": 54500
    },
    {
      "epoch": 17.698541329011345,
      "grad_norm": 0.8556811213493347,
      "learning_rate": 4.826230943885826e-05,
      "loss": 3.3139,
      "step": 54600
    },
    {
      "epoch": 17.73095623987034,
      "grad_norm": 0.8787357211112976,
      "learning_rate": 4.825906584495622e-05,
      "loss": 3.3287,
      "step": 54700
    },
    {
      "epoch": 17.763371150729334,
      "grad_norm": 0.9204088449478149,
      "learning_rate": 4.8255822251054175e-05,
      "loss": 3.3125,
      "step": 54800
    },
    {
      "epoch": 17.79578606158833,
      "grad_norm": 0.7290030121803284,
      "learning_rate": 4.825257865715213e-05,
      "loss": 3.3438,
      "step": 54900
    },
    {
      "epoch": 17.828200972447327,
      "grad_norm": 0.7656348347663879,
      "learning_rate": 4.8249335063250086e-05,
      "loss": 3.3073,
      "step": 55000
    },
    {
      "epoch": 17.86061588330632,
      "grad_norm": 0.8350306153297424,
      "learning_rate": 4.8246091469348045e-05,
      "loss": 3.3236,
      "step": 55100
    },
    {
      "epoch": 17.893030794165316,
      "grad_norm": 0.7929808497428894,
      "learning_rate": 4.8242847875446e-05,
      "loss": 3.3128,
      "step": 55200
    },
    {
      "epoch": 17.925445705024313,
      "grad_norm": 0.8643037676811218,
      "learning_rate": 4.8239604281543956e-05,
      "loss": 3.3097,
      "step": 55300
    },
    {
      "epoch": 17.957860615883305,
      "grad_norm": 0.8506174683570862,
      "learning_rate": 4.823636068764191e-05,
      "loss": 3.3399,
      "step": 55400
    },
    {
      "epoch": 17.990275526742302,
      "grad_norm": 0.8854289650917053,
      "learning_rate": 4.8233117093739866e-05,
      "loss": 3.3094,
      "step": 55500
    },
    {
      "epoch": 18.0,
      "eval_bleu": 0.8687162579092379,
      "eval_loss": 3.7058145999908447,
      "eval_runtime": 4.4928,
      "eval_samples_per_second": 109.509,
      "eval_steps_per_second": 1.781,
      "step": 55530
    },
    {
      "epoch": 18.022690437601298,
      "grad_norm": 0.8606845736503601,
      "learning_rate": 4.8229873499837825e-05,
      "loss": 3.3222,
      "step": 55600
    },
    {
      "epoch": 18.05510534846029,
      "grad_norm": 0.8115867376327515,
      "learning_rate": 4.822662990593578e-05,
      "loss": 3.3019,
      "step": 55700
    },
    {
      "epoch": 18.087520259319287,
      "grad_norm": 0.7867512106895447,
      "learning_rate": 4.8223386312033736e-05,
      "loss": 3.2931,
      "step": 55800
    },
    {
      "epoch": 18.11993517017828,
      "grad_norm": 0.8762028813362122,
      "learning_rate": 4.8220142718131694e-05,
      "loss": 3.2898,
      "step": 55900
    },
    {
      "epoch": 18.152350081037277,
      "grad_norm": 0.9606066346168518,
      "learning_rate": 4.8216899124229646e-05,
      "loss": 3.3274,
      "step": 56000
    },
    {
      "epoch": 18.184764991896273,
      "grad_norm": 1.045259714126587,
      "learning_rate": 4.8213655530327605e-05,
      "loss": 3.3259,
      "step": 56100
    },
    {
      "epoch": 18.217179902755266,
      "grad_norm": 0.8279439806938171,
      "learning_rate": 4.821041193642556e-05,
      "loss": 3.3054,
      "step": 56200
    },
    {
      "epoch": 18.249594813614262,
      "grad_norm": 0.9112544655799866,
      "learning_rate": 4.8207168342523516e-05,
      "loss": 3.299,
      "step": 56300
    },
    {
      "epoch": 18.28200972447326,
      "grad_norm": 0.8794892430305481,
      "learning_rate": 4.8203924748621475e-05,
      "loss": 3.292,
      "step": 56400
    },
    {
      "epoch": 18.31442463533225,
      "grad_norm": 1.036142110824585,
      "learning_rate": 4.8200681154719427e-05,
      "loss": 3.3175,
      "step": 56500
    },
    {
      "epoch": 18.346839546191248,
      "grad_norm": 0.8354015350341797,
      "learning_rate": 4.8197437560817385e-05,
      "loss": 3.2939,
      "step": 56600
    },
    {
      "epoch": 18.379254457050244,
      "grad_norm": 0.764054536819458,
      "learning_rate": 4.8194193966915344e-05,
      "loss": 3.3252,
      "step": 56700
    },
    {
      "epoch": 18.411669367909237,
      "grad_norm": 0.8956862688064575,
      "learning_rate": 4.8190950373013296e-05,
      "loss": 3.2999,
      "step": 56800
    },
    {
      "epoch": 18.444084278768234,
      "grad_norm": 0.8947429656982422,
      "learning_rate": 4.8187706779111255e-05,
      "loss": 3.3158,
      "step": 56900
    },
    {
      "epoch": 18.47649918962723,
      "grad_norm": 1.0378081798553467,
      "learning_rate": 4.8184463185209214e-05,
      "loss": 3.3317,
      "step": 57000
    },
    {
      "epoch": 18.508914100486223,
      "grad_norm": 0.838066577911377,
      "learning_rate": 4.818121959130717e-05,
      "loss": 3.2985,
      "step": 57100
    },
    {
      "epoch": 18.54132901134522,
      "grad_norm": 0.8149452805519104,
      "learning_rate": 4.817797599740513e-05,
      "loss": 3.3282,
      "step": 57200
    },
    {
      "epoch": 18.573743922204216,
      "grad_norm": 0.9831416606903076,
      "learning_rate": 4.817473240350308e-05,
      "loss": 3.3104,
      "step": 57300
    },
    {
      "epoch": 18.60615883306321,
      "grad_norm": 0.7938660383224487,
      "learning_rate": 4.817148880960104e-05,
      "loss": 3.2968,
      "step": 57400
    },
    {
      "epoch": 18.638573743922205,
      "grad_norm": 1.1699098348617554,
      "learning_rate": 4.816827765163802e-05,
      "loss": 3.2999,
      "step": 57500
    },
    {
      "epoch": 18.670988654781198,
      "grad_norm": 0.8718113899230957,
      "learning_rate": 4.816503405773597e-05,
      "loss": 3.3119,
      "step": 57600
    },
    {
      "epoch": 18.703403565640194,
      "grad_norm": 0.868701696395874,
      "learning_rate": 4.816179046383393e-05,
      "loss": 3.3008,
      "step": 57700
    },
    {
      "epoch": 18.73581847649919,
      "grad_norm": 0.8717154860496521,
      "learning_rate": 4.815854686993189e-05,
      "loss": 3.3121,
      "step": 57800
    },
    {
      "epoch": 18.768233387358183,
      "grad_norm": 0.9083405137062073,
      "learning_rate": 4.815530327602985e-05,
      "loss": 3.297,
      "step": 57900
    },
    {
      "epoch": 18.80064829821718,
      "grad_norm": 0.8566670417785645,
      "learning_rate": 4.81520596821278e-05,
      "loss": 3.2977,
      "step": 58000
    },
    {
      "epoch": 18.833063209076176,
      "grad_norm": 0.9121987223625183,
      "learning_rate": 4.814881608822576e-05,
      "loss": 3.3108,
      "step": 58100
    },
    {
      "epoch": 18.86547811993517,
      "grad_norm": 0.7821193337440491,
      "learning_rate": 4.814557249432372e-05,
      "loss": 3.3126,
      "step": 58200
    },
    {
      "epoch": 18.897893030794165,
      "grad_norm": 0.8799347877502441,
      "learning_rate": 4.814232890042167e-05,
      "loss": 3.3102,
      "step": 58300
    },
    {
      "epoch": 18.93030794165316,
      "grad_norm": 0.9495940804481506,
      "learning_rate": 4.813908530651963e-05,
      "loss": 3.3159,
      "step": 58400
    },
    {
      "epoch": 18.962722852512155,
      "grad_norm": 0.925613284111023,
      "learning_rate": 4.813584171261759e-05,
      "loss": 3.3048,
      "step": 58500
    },
    {
      "epoch": 18.99513776337115,
      "grad_norm": 0.8383057713508606,
      "learning_rate": 4.813259811871554e-05,
      "loss": 3.2963,
      "step": 58600
    },
    {
      "epoch": 19.0,
      "eval_bleu": 1.053537543419616,
      "eval_loss": 3.7026805877685547,
      "eval_runtime": 4.3468,
      "eval_samples_per_second": 113.186,
      "eval_steps_per_second": 1.84,
      "step": 58615
    },
    {
      "epoch": 19.027552674230147,
      "grad_norm": 0.8733224272727966,
      "learning_rate": 4.81293545248135e-05,
      "loss": 3.3124,
      "step": 58700
    },
    {
      "epoch": 19.05996758508914,
      "grad_norm": 0.8435048460960388,
      "learning_rate": 4.812611093091145e-05,
      "loss": 3.2836,
      "step": 58800
    },
    {
      "epoch": 19.092382495948137,
      "grad_norm": 0.8296048045158386,
      "learning_rate": 4.812286733700941e-05,
      "loss": 3.3003,
      "step": 58900
    },
    {
      "epoch": 19.124797406807133,
      "grad_norm": 0.8272748589515686,
      "learning_rate": 4.811962374310737e-05,
      "loss": 3.2925,
      "step": 59000
    },
    {
      "epoch": 19.157212317666126,
      "grad_norm": 0.9040014743804932,
      "learning_rate": 4.811638014920532e-05,
      "loss": 3.298,
      "step": 59100
    },
    {
      "epoch": 19.189627228525122,
      "grad_norm": 0.8117141127586365,
      "learning_rate": 4.811313655530328e-05,
      "loss": 3.2969,
      "step": 59200
    },
    {
      "epoch": 19.222042139384115,
      "grad_norm": 1.0276426076889038,
      "learning_rate": 4.8109892961401236e-05,
      "loss": 3.292,
      "step": 59300
    },
    {
      "epoch": 19.25445705024311,
      "grad_norm": 0.9828009009361267,
      "learning_rate": 4.810664936749919e-05,
      "loss": 3.2969,
      "step": 59400
    },
    {
      "epoch": 19.286871961102108,
      "grad_norm": 0.8220027685165405,
      "learning_rate": 4.810340577359715e-05,
      "loss": 3.2806,
      "step": 59500
    },
    {
      "epoch": 19.3192868719611,
      "grad_norm": 0.8581296801567078,
      "learning_rate": 4.8100194615634125e-05,
      "loss": 3.2983,
      "step": 59600
    },
    {
      "epoch": 19.351701782820097,
      "grad_norm": 0.8208685517311096,
      "learning_rate": 4.8096951021732084e-05,
      "loss": 3.295,
      "step": 59700
    },
    {
      "epoch": 19.384116693679093,
      "grad_norm": 0.934032678604126,
      "learning_rate": 4.8093707427830035e-05,
      "loss": 3.3043,
      "step": 59800
    },
    {
      "epoch": 19.416531604538086,
      "grad_norm": 0.7759830951690674,
      "learning_rate": 4.8090463833927994e-05,
      "loss": 3.2706,
      "step": 59900
    },
    {
      "epoch": 19.448946515397083,
      "grad_norm": 0.931555986404419,
      "learning_rate": 4.8087220240025946e-05,
      "loss": 3.3087,
      "step": 60000
    },
    {
      "epoch": 19.48136142625608,
      "grad_norm": 0.8685643076896667,
      "learning_rate": 4.8083976646123905e-05,
      "loss": 3.3153,
      "step": 60100
    },
    {
      "epoch": 19.513776337115072,
      "grad_norm": 0.8379449248313904,
      "learning_rate": 4.8080733052221864e-05,
      "loss": 3.2893,
      "step": 60200
    },
    {
      "epoch": 19.54619124797407,
      "grad_norm": 0.7996879816055298,
      "learning_rate": 4.8077489458319816e-05,
      "loss": 3.3094,
      "step": 60300
    },
    {
      "epoch": 19.578606158833065,
      "grad_norm": 0.8677024245262146,
      "learning_rate": 4.8074245864417774e-05,
      "loss": 3.2891,
      "step": 60400
    },
    {
      "epoch": 19.611021069692057,
      "grad_norm": 0.8134167194366455,
      "learning_rate": 4.807100227051573e-05,
      "loss": 3.2992,
      "step": 60500
    },
    {
      "epoch": 19.643435980551054,
      "grad_norm": 0.8002317547798157,
      "learning_rate": 4.806775867661369e-05,
      "loss": 3.2983,
      "step": 60600
    },
    {
      "epoch": 19.67585089141005,
      "grad_norm": 0.8397817015647888,
      "learning_rate": 4.806451508271165e-05,
      "loss": 3.3276,
      "step": 60700
    },
    {
      "epoch": 19.708265802269043,
      "grad_norm": 0.7835744023323059,
      "learning_rate": 4.806127148880961e-05,
      "loss": 3.2919,
      "step": 60800
    },
    {
      "epoch": 19.74068071312804,
      "grad_norm": 0.9619253277778625,
      "learning_rate": 4.805802789490756e-05,
      "loss": 3.3119,
      "step": 60900
    },
    {
      "epoch": 19.773095623987032,
      "grad_norm": 0.7659015655517578,
      "learning_rate": 4.805478430100552e-05,
      "loss": 3.3119,
      "step": 61000
    },
    {
      "epoch": 19.80551053484603,
      "grad_norm": 1.057233214378357,
      "learning_rate": 4.805154070710347e-05,
      "loss": 3.3071,
      "step": 61100
    },
    {
      "epoch": 19.837925445705025,
      "grad_norm": 0.8695149421691895,
      "learning_rate": 4.804829711320143e-05,
      "loss": 3.291,
      "step": 61200
    },
    {
      "epoch": 19.870340356564018,
      "grad_norm": 0.9096798896789551,
      "learning_rate": 4.804505351929939e-05,
      "loss": 3.3153,
      "step": 61300
    },
    {
      "epoch": 19.902755267423014,
      "grad_norm": 0.7868876457214355,
      "learning_rate": 4.804180992539734e-05,
      "loss": 3.2865,
      "step": 61400
    },
    {
      "epoch": 19.93517017828201,
      "grad_norm": 0.8772745728492737,
      "learning_rate": 4.80385663314953e-05,
      "loss": 3.2781,
      "step": 61500
    },
    {
      "epoch": 19.967585089141004,
      "grad_norm": 0.8301225304603577,
      "learning_rate": 4.803532273759326e-05,
      "loss": 3.294,
      "step": 61600
    },
    {
      "epoch": 20.0,
      "grad_norm": 1.1368858814239502,
      "learning_rate": 4.803211157963024e-05,
      "loss": 3.2997,
      "step": 61700
    },
    {
      "epoch": 20.0,
      "eval_bleu": 0.9826102736216523,
      "eval_loss": 3.7071375846862793,
      "eval_runtime": 4.9268,
      "eval_samples_per_second": 99.861,
      "eval_steps_per_second": 1.624,
      "step": 61700
    },
    {
      "epoch": 20.032414910858996,
      "grad_norm": 1.1134631633758545,
      "learning_rate": 4.802886798572819e-05,
      "loss": 3.2765,
      "step": 61800
    },
    {
      "epoch": 20.06482982171799,
      "grad_norm": 1.1370668411254883,
      "learning_rate": 4.802562439182615e-05,
      "loss": 3.2903,
      "step": 61900
    },
    {
      "epoch": 20.097244732576986,
      "grad_norm": 0.9958800077438354,
      "learning_rate": 4.8022380797924106e-05,
      "loss": 3.2906,
      "step": 62000
    },
    {
      "epoch": 20.129659643435982,
      "grad_norm": 0.8909556269645691,
      "learning_rate": 4.801913720402206e-05,
      "loss": 3.2941,
      "step": 62100
    },
    {
      "epoch": 20.162074554294975,
      "grad_norm": 0.9672849774360657,
      "learning_rate": 4.801589361012002e-05,
      "loss": 3.2968,
      "step": 62200
    },
    {
      "epoch": 20.19448946515397,
      "grad_norm": 0.9129165410995483,
      "learning_rate": 4.801265001621797e-05,
      "loss": 3.2741,
      "step": 62300
    },
    {
      "epoch": 20.226904376012968,
      "grad_norm": 0.9510704874992371,
      "learning_rate": 4.800940642231593e-05,
      "loss": 3.2917,
      "step": 62400
    },
    {
      "epoch": 20.25931928687196,
      "grad_norm": 0.8545668721199036,
      "learning_rate": 4.8006162828413886e-05,
      "loss": 3.3004,
      "step": 62500
    },
    {
      "epoch": 20.291734197730957,
      "grad_norm": 0.9030771255493164,
      "learning_rate": 4.800291923451184e-05,
      "loss": 3.284,
      "step": 62600
    },
    {
      "epoch": 20.32414910858995,
      "grad_norm": 0.806459903717041,
      "learning_rate": 4.79996756406098e-05,
      "loss": 3.2815,
      "step": 62700
    },
    {
      "epoch": 20.356564019448946,
      "grad_norm": 0.7578245997428894,
      "learning_rate": 4.7996432046707756e-05,
      "loss": 3.3051,
      "step": 62800
    },
    {
      "epoch": 20.388978930307943,
      "grad_norm": 0.8351677656173706,
      "learning_rate": 4.799318845280571e-05,
      "loss": 3.295,
      "step": 62900
    },
    {
      "epoch": 20.421393841166935,
      "grad_norm": 0.8181738257408142,
      "learning_rate": 4.7989944858903667e-05,
      "loss": 3.3037,
      "step": 63000
    },
    {
      "epoch": 20.45380875202593,
      "grad_norm": 0.837717592716217,
      "learning_rate": 4.7986701265001625e-05,
      "loss": 3.2854,
      "step": 63100
    },
    {
      "epoch": 20.486223662884928,
      "grad_norm": 0.7934706807136536,
      "learning_rate": 4.798345767109958e-05,
      "loss": 3.2991,
      "step": 63200
    },
    {
      "epoch": 20.51863857374392,
      "grad_norm": 0.9163109064102173,
      "learning_rate": 4.7980214077197536e-05,
      "loss": 3.2921,
      "step": 63300
    },
    {
      "epoch": 20.551053484602917,
      "grad_norm": 0.8181686997413635,
      "learning_rate": 4.7976970483295495e-05,
      "loss": 3.275,
      "step": 63400
    },
    {
      "epoch": 20.583468395461914,
      "grad_norm": 0.7634698748588562,
      "learning_rate": 4.797372688939345e-05,
      "loss": 3.2977,
      "step": 63500
    },
    {
      "epoch": 20.615883306320907,
      "grad_norm": 0.9156355857849121,
      "learning_rate": 4.7970483295491405e-05,
      "loss": 3.3015,
      "step": 63600
    },
    {
      "epoch": 20.648298217179903,
      "grad_norm": 0.8498892784118652,
      "learning_rate": 4.796727213752838e-05,
      "loss": 3.3057,
      "step": 63700
    },
    {
      "epoch": 20.6807131280389,
      "grad_norm": 0.8119916319847107,
      "learning_rate": 4.7964028543626335e-05,
      "loss": 3.286,
      "step": 63800
    },
    {
      "epoch": 20.713128038897892,
      "grad_norm": 0.9372416734695435,
      "learning_rate": 4.7960784949724294e-05,
      "loss": 3.2988,
      "step": 63900
    },
    {
      "epoch": 20.74554294975689,
      "grad_norm": 0.8600747585296631,
      "learning_rate": 4.795754135582225e-05,
      "loss": 3.2839,
      "step": 64000
    },
    {
      "epoch": 20.777957860615885,
      "grad_norm": 0.8918856978416443,
      "learning_rate": 4.795429776192021e-05,
      "loss": 3.2818,
      "step": 64100
    },
    {
      "epoch": 20.810372771474878,
      "grad_norm": 0.9223859310150146,
      "learning_rate": 4.795105416801817e-05,
      "loss": 3.297,
      "step": 64200
    },
    {
      "epoch": 20.842787682333874,
      "grad_norm": 0.8737244009971619,
      "learning_rate": 4.794781057411612e-05,
      "loss": 3.2848,
      "step": 64300
    },
    {
      "epoch": 20.875202593192867,
      "grad_norm": 0.8471442461013794,
      "learning_rate": 4.794456698021408e-05,
      "loss": 3.2863,
      "step": 64400
    },
    {
      "epoch": 20.907617504051863,
      "grad_norm": 0.9783878326416016,
      "learning_rate": 4.794132338631204e-05,
      "loss": 3.3035,
      "step": 64500
    },
    {
      "epoch": 20.94003241491086,
      "grad_norm": 0.8622522950172424,
      "learning_rate": 4.793811222834901e-05,
      "loss": 3.2853,
      "step": 64600
    },
    {
      "epoch": 20.972447325769853,
      "grad_norm": 0.840093731880188,
      "learning_rate": 4.793486863444697e-05,
      "loss": 3.2858,
      "step": 64700
    },
    {
      "epoch": 21.0,
      "eval_bleu": 1.1014286887450142,
      "eval_loss": 3.7099175453186035,
      "eval_runtime": 4.6987,
      "eval_samples_per_second": 104.711,
      "eval_steps_per_second": 1.703,
      "step": 64785
    },
    {
      "epoch": 21.00486223662885,
      "grad_norm": 0.8245484828948975,
      "learning_rate": 4.793162504054493e-05,
      "loss": 3.2706,
      "step": 64800
    },
    {
      "epoch": 21.037277147487845,
      "grad_norm": 0.9257804751396179,
      "learning_rate": 4.792838144664289e-05,
      "loss": 3.2747,
      "step": 64900
    },
    {
      "epoch": 21.06969205834684,
      "grad_norm": 0.7854569554328918,
      "learning_rate": 4.792513785274084e-05,
      "loss": 3.28,
      "step": 65000
    },
    {
      "epoch": 21.102106969205835,
      "grad_norm": 0.8012800216674805,
      "learning_rate": 4.79218942588388e-05,
      "loss": 3.2717,
      "step": 65100
    },
    {
      "epoch": 21.13452188006483,
      "grad_norm": 0.8799124956130981,
      "learning_rate": 4.7918650664936756e-05,
      "loss": 3.2735,
      "step": 65200
    },
    {
      "epoch": 21.166936790923824,
      "grad_norm": 0.849551796913147,
      "learning_rate": 4.791540707103471e-05,
      "loss": 3.2864,
      "step": 65300
    },
    {
      "epoch": 21.19935170178282,
      "grad_norm": 0.8223614692687988,
      "learning_rate": 4.791216347713267e-05,
      "loss": 3.2752,
      "step": 65400
    },
    {
      "epoch": 21.231766612641817,
      "grad_norm": 0.8502179384231567,
      "learning_rate": 4.7908919883230626e-05,
      "loss": 3.2787,
      "step": 65500
    },
    {
      "epoch": 21.26418152350081,
      "grad_norm": 0.8437319397926331,
      "learning_rate": 4.790567628932858e-05,
      "loss": 3.275,
      "step": 65600
    },
    {
      "epoch": 21.296596434359806,
      "grad_norm": 0.7735568284988403,
      "learning_rate": 4.7902432695426537e-05,
      "loss": 3.2817,
      "step": 65700
    },
    {
      "epoch": 21.329011345218802,
      "grad_norm": 1.1938954591751099,
      "learning_rate": 4.789918910152449e-05,
      "loss": 3.2727,
      "step": 65800
    },
    {
      "epoch": 21.361426256077795,
      "grad_norm": 0.7884756326675415,
      "learning_rate": 4.789594550762245e-05,
      "loss": 3.2724,
      "step": 65900
    },
    {
      "epoch": 21.39384116693679,
      "grad_norm": 0.8534108996391296,
      "learning_rate": 4.7892701913720406e-05,
      "loss": 3.2727,
      "step": 66000
    },
    {
      "epoch": 21.426256077795784,
      "grad_norm": 0.9867222905158997,
      "learning_rate": 4.788945831981836e-05,
      "loss": 3.2826,
      "step": 66100
    },
    {
      "epoch": 21.45867098865478,
      "grad_norm": 0.9031102657318115,
      "learning_rate": 4.788621472591632e-05,
      "loss": 3.2772,
      "step": 66200
    },
    {
      "epoch": 21.491085899513777,
      "grad_norm": 0.779308021068573,
      "learning_rate": 4.7882971132014275e-05,
      "loss": 3.3002,
      "step": 66300
    },
    {
      "epoch": 21.52350081037277,
      "grad_norm": 0.9621580839157104,
      "learning_rate": 4.787972753811223e-05,
      "loss": 3.274,
      "step": 66400
    },
    {
      "epoch": 21.555915721231766,
      "grad_norm": 0.7667760252952576,
      "learning_rate": 4.7876483944210186e-05,
      "loss": 3.2978,
      "step": 66500
    },
    {
      "epoch": 21.588330632090763,
      "grad_norm": 0.7862148284912109,
      "learning_rate": 4.7873240350308145e-05,
      "loss": 3.2959,
      "step": 66600
    },
    {
      "epoch": 21.620745542949756,
      "grad_norm": 0.8871970772743225,
      "learning_rate": 4.78699967564061e-05,
      "loss": 3.2739,
      "step": 66700
    },
    {
      "epoch": 21.653160453808752,
      "grad_norm": 0.7562496066093445,
      "learning_rate": 4.7866753162504056e-05,
      "loss": 3.2841,
      "step": 66800
    },
    {
      "epoch": 21.68557536466775,
      "grad_norm": 0.85268634557724,
      "learning_rate": 4.786350956860201e-05,
      "loss": 3.2959,
      "step": 66900
    },
    {
      "epoch": 21.71799027552674,
      "grad_norm": 0.8858005404472351,
      "learning_rate": 4.7860265974699966e-05,
      "loss": 3.2994,
      "step": 67000
    },
    {
      "epoch": 21.750405186385738,
      "grad_norm": 0.8111391663551331,
      "learning_rate": 4.7857022380797925e-05,
      "loss": 3.2861,
      "step": 67100
    },
    {
      "epoch": 21.782820097244734,
      "grad_norm": 0.7381735444068909,
      "learning_rate": 4.7853778786895884e-05,
      "loss": 3.2817,
      "step": 67200
    },
    {
      "epoch": 21.815235008103727,
      "grad_norm": 0.8784536719322205,
      "learning_rate": 4.785053519299384e-05,
      "loss": 3.2789,
      "step": 67300
    },
    {
      "epoch": 21.847649918962723,
      "grad_norm": 0.8383498787879944,
      "learning_rate": 4.78472915990918e-05,
      "loss": 3.2889,
      "step": 67400
    },
    {
      "epoch": 21.88006482982172,
      "grad_norm": 0.9337917566299438,
      "learning_rate": 4.784404800518975e-05,
      "loss": 3.27,
      "step": 67500
    },
    {
      "epoch": 21.912479740680713,
      "grad_norm": 0.9968885779380798,
      "learning_rate": 4.784080441128771e-05,
      "loss": 3.2668,
      "step": 67600
    },
    {
      "epoch": 21.94489465153971,
      "grad_norm": 0.8969876170158386,
      "learning_rate": 4.783756081738567e-05,
      "loss": 3.2851,
      "step": 67700
    },
    {
      "epoch": 21.977309562398702,
      "grad_norm": 0.8953835964202881,
      "learning_rate": 4.783431722348362e-05,
      "loss": 3.2716,
      "step": 67800
    },
    {
      "epoch": 22.0,
      "eval_bleu": 0.9119924154633104,
      "eval_loss": 3.700565814971924,
      "eval_runtime": 5.0067,
      "eval_samples_per_second": 98.267,
      "eval_steps_per_second": 1.598,
      "step": 67870
    },
    {
      "epoch": 22.009724473257698,
      "grad_norm": 0.8443865776062012,
      "learning_rate": 4.783107362958158e-05,
      "loss": 3.2791,
      "step": 67900
    },
    {
      "epoch": 22.042139384116695,
      "grad_norm": 0.9276329874992371,
      "learning_rate": 4.782783003567953e-05,
      "loss": 3.2821,
      "step": 68000
    },
    {
      "epoch": 22.074554294975687,
      "grad_norm": 0.9285898804664612,
      "learning_rate": 4.782458644177749e-05,
      "loss": 3.2617,
      "step": 68100
    },
    {
      "epoch": 22.106969205834684,
      "grad_norm": 0.9778681397438049,
      "learning_rate": 4.782134284787545e-05,
      "loss": 3.2641,
      "step": 68200
    },
    {
      "epoch": 22.13938411669368,
      "grad_norm": 0.8844485282897949,
      "learning_rate": 4.78180992539734e-05,
      "loss": 3.2813,
      "step": 68300
    },
    {
      "epoch": 22.171799027552673,
      "grad_norm": 0.8028387427330017,
      "learning_rate": 4.781485566007136e-05,
      "loss": 3.2677,
      "step": 68400
    },
    {
      "epoch": 22.20421393841167,
      "grad_norm": 0.8116012811660767,
      "learning_rate": 4.781161206616932e-05,
      "loss": 3.2886,
      "step": 68500
    },
    {
      "epoch": 22.236628849270666,
      "grad_norm": 0.9412678480148315,
      "learning_rate": 4.78084009082063e-05,
      "loss": 3.261,
      "step": 68600
    },
    {
      "epoch": 22.26904376012966,
      "grad_norm": 0.9318462014198303,
      "learning_rate": 4.780515731430425e-05,
      "loss": 3.2685,
      "step": 68700
    },
    {
      "epoch": 22.301458670988655,
      "grad_norm": 0.9659602046012878,
      "learning_rate": 4.780191372040221e-05,
      "loss": 3.2638,
      "step": 68800
    },
    {
      "epoch": 22.33387358184765,
      "grad_norm": 0.8653566241264343,
      "learning_rate": 4.779867012650017e-05,
      "loss": 3.282,
      "step": 68900
    },
    {
      "epoch": 22.366288492706644,
      "grad_norm": 0.9683631062507629,
      "learning_rate": 4.779542653259812e-05,
      "loss": 3.2834,
      "step": 69000
    },
    {
      "epoch": 22.39870340356564,
      "grad_norm": 1.147221326828003,
      "learning_rate": 4.779218293869608e-05,
      "loss": 3.2735,
      "step": 69100
    },
    {
      "epoch": 22.431118314424637,
      "grad_norm": 0.8243588209152222,
      "learning_rate": 4.778893934479403e-05,
      "loss": 3.2776,
      "step": 69200
    },
    {
      "epoch": 22.46353322528363,
      "grad_norm": 0.9338653087615967,
      "learning_rate": 4.778569575089199e-05,
      "loss": 3.2752,
      "step": 69300
    },
    {
      "epoch": 22.495948136142626,
      "grad_norm": 0.8621575236320496,
      "learning_rate": 4.778248459292897e-05,
      "loss": 3.2549,
      "step": 69400
    },
    {
      "epoch": 22.52836304700162,
      "grad_norm": 0.874284029006958,
      "learning_rate": 4.7779240999026926e-05,
      "loss": 3.2896,
      "step": 69500
    },
    {
      "epoch": 22.560777957860616,
      "grad_norm": 0.8924210667610168,
      "learning_rate": 4.777599740512488e-05,
      "loss": 3.269,
      "step": 69600
    },
    {
      "epoch": 22.593192868719612,
      "grad_norm": 0.877267599105835,
      "learning_rate": 4.7772753811222836e-05,
      "loss": 3.274,
      "step": 69700
    },
    {
      "epoch": 22.625607779578605,
      "grad_norm": 0.8649918437004089,
      "learning_rate": 4.7769510217320795e-05,
      "loss": 3.2644,
      "step": 69800
    },
    {
      "epoch": 22.6580226904376,
      "grad_norm": 0.7795793414115906,
      "learning_rate": 4.776626662341875e-05,
      "loss": 3.2712,
      "step": 69900
    },
    {
      "epoch": 22.690437601296598,
      "grad_norm": 0.7522197961807251,
      "learning_rate": 4.7763023029516706e-05,
      "loss": 3.2518,
      "step": 70000
    },
    {
      "epoch": 22.72285251215559,
      "grad_norm": 0.8157826066017151,
      "learning_rate": 4.7759779435614664e-05,
      "loss": 3.2726,
      "step": 70100
    },
    {
      "epoch": 22.755267423014587,
      "grad_norm": 0.972424328327179,
      "learning_rate": 4.7756535841712616e-05,
      "loss": 3.2604,
      "step": 70200
    },
    {
      "epoch": 22.787682333873583,
      "grad_norm": 0.9948761463165283,
      "learning_rate": 4.7753292247810575e-05,
      "loss": 3.2781,
      "step": 70300
    },
    {
      "epoch": 22.820097244732576,
      "grad_norm": 0.9745439887046814,
      "learning_rate": 4.7750048653908534e-05,
      "loss": 3.282,
      "step": 70400
    },
    {
      "epoch": 22.852512155591572,
      "grad_norm": 0.8443552851676941,
      "learning_rate": 4.7746805060006486e-05,
      "loss": 3.2611,
      "step": 70500
    },
    {
      "epoch": 22.88492706645057,
      "grad_norm": 0.8128862380981445,
      "learning_rate": 4.7743561466104445e-05,
      "loss": 3.2838,
      "step": 70600
    },
    {
      "epoch": 22.91734197730956,
      "grad_norm": 0.9741435647010803,
      "learning_rate": 4.77403178722024e-05,
      "loss": 3.2748,
      "step": 70700
    },
    {
      "epoch": 22.949756888168558,
      "grad_norm": 0.9337469935417175,
      "learning_rate": 4.773707427830036e-05,
      "loss": 3.2896,
      "step": 70800
    },
    {
      "epoch": 22.982171799027554,
      "grad_norm": 0.897948682308197,
      "learning_rate": 4.7733830684398314e-05,
      "loss": 3.2766,
      "step": 70900
    },
    {
      "epoch": 23.0,
      "eval_bleu": 1.0757074920816854,
      "eval_loss": 3.710995674133301,
      "eval_runtime": 4.6227,
      "eval_samples_per_second": 106.431,
      "eval_steps_per_second": 1.731,
      "step": 70955
    },
    {
      "epoch": 23.014586709886547,
      "grad_norm": 0.8243562579154968,
      "learning_rate": 4.773058709049627e-05,
      "loss": 3.2634,
      "step": 71000
    },
    {
      "epoch": 23.047001620745544,
      "grad_norm": 0.848716676235199,
      "learning_rate": 4.772734349659423e-05,
      "loss": 3.2553,
      "step": 71100
    },
    {
      "epoch": 23.079416531604537,
      "grad_norm": 1.1212663650512695,
      "learning_rate": 4.772409990269219e-05,
      "loss": 3.2634,
      "step": 71200
    },
    {
      "epoch": 23.111831442463533,
      "grad_norm": 0.9614636898040771,
      "learning_rate": 4.772085630879014e-05,
      "loss": 3.2429,
      "step": 71300
    },
    {
      "epoch": 23.14424635332253,
      "grad_norm": 0.8283793330192566,
      "learning_rate": 4.77176127148881e-05,
      "loss": 3.2661,
      "step": 71400
    },
    {
      "epoch": 23.176661264181522,
      "grad_norm": 0.7509739398956299,
      "learning_rate": 4.771436912098605e-05,
      "loss": 3.2677,
      "step": 71500
    },
    {
      "epoch": 23.20907617504052,
      "grad_norm": 0.8966099619865417,
      "learning_rate": 4.771112552708401e-05,
      "loss": 3.2474,
      "step": 71600
    },
    {
      "epoch": 23.241491085899515,
      "grad_norm": 0.818715512752533,
      "learning_rate": 4.770788193318197e-05,
      "loss": 3.2694,
      "step": 71700
    },
    {
      "epoch": 23.273905996758508,
      "grad_norm": 0.9014068841934204,
      "learning_rate": 4.770463833927992e-05,
      "loss": 3.2798,
      "step": 71800
    },
    {
      "epoch": 23.306320907617504,
      "grad_norm": 0.8279420733451843,
      "learning_rate": 4.770139474537788e-05,
      "loss": 3.2661,
      "step": 71900
    },
    {
      "epoch": 23.3387358184765,
      "grad_norm": 0.9249817728996277,
      "learning_rate": 4.769815115147584e-05,
      "loss": 3.2655,
      "step": 72000
    },
    {
      "epoch": 23.371150729335493,
      "grad_norm": 0.9194801449775696,
      "learning_rate": 4.769490755757379e-05,
      "loss": 3.2654,
      "step": 72100
    },
    {
      "epoch": 23.40356564019449,
      "grad_norm": 0.9574617743492126,
      "learning_rate": 4.769166396367175e-05,
      "loss": 3.2685,
      "step": 72200
    },
    {
      "epoch": 23.435980551053486,
      "grad_norm": 0.797116756439209,
      "learning_rate": 4.768842036976971e-05,
      "loss": 3.2618,
      "step": 72300
    },
    {
      "epoch": 23.46839546191248,
      "grad_norm": 0.8915625214576721,
      "learning_rate": 4.768517677586766e-05,
      "loss": 3.2899,
      "step": 72400
    },
    {
      "epoch": 23.500810372771475,
      "grad_norm": 0.921606719493866,
      "learning_rate": 4.768193318196562e-05,
      "loss": 3.2602,
      "step": 72500
    },
    {
      "epoch": 23.533225283630472,
      "grad_norm": 0.8431615233421326,
      "learning_rate": 4.767868958806357e-05,
      "loss": 3.2759,
      "step": 72600
    },
    {
      "epoch": 23.565640194489465,
      "grad_norm": 0.855699360370636,
      "learning_rate": 4.767544599416153e-05,
      "loss": 3.2658,
      "step": 72700
    },
    {
      "epoch": 23.59805510534846,
      "grad_norm": 1.0212626457214355,
      "learning_rate": 4.767220240025949e-05,
      "loss": 3.2523,
      "step": 72800
    },
    {
      "epoch": 23.630470016207454,
      "grad_norm": 0.8608694672584534,
      "learning_rate": 4.766895880635744e-05,
      "loss": 3.2513,
      "step": 72900
    },
    {
      "epoch": 23.66288492706645,
      "grad_norm": 0.9163949489593506,
      "learning_rate": 4.76657152124554e-05,
      "loss": 3.2875,
      "step": 73000
    },
    {
      "epoch": 23.695299837925447,
      "grad_norm": 1.1037729978561401,
      "learning_rate": 4.766247161855336e-05,
      "loss": 3.2567,
      "step": 73100
    },
    {
      "epoch": 23.72771474878444,
      "grad_norm": 0.786390483379364,
      "learning_rate": 4.765922802465132e-05,
      "loss": 3.2938,
      "step": 73200
    },
    {
      "epoch": 23.760129659643436,
      "grad_norm": 0.9198330044746399,
      "learning_rate": 4.7655984430749276e-05,
      "loss": 3.2374,
      "step": 73300
    },
    {
      "epoch": 23.792544570502432,
      "grad_norm": 0.7918649315834045,
      "learning_rate": 4.765274083684723e-05,
      "loss": 3.2679,
      "step": 73400
    },
    {
      "epoch": 23.824959481361425,
      "grad_norm": 0.9288239479064941,
      "learning_rate": 4.7649529678884206e-05,
      "loss": 3.2618,
      "step": 73500
    },
    {
      "epoch": 23.85737439222042,
      "grad_norm": 0.9144817590713501,
      "learning_rate": 4.764628608498216e-05,
      "loss": 3.2662,
      "step": 73600
    },
    {
      "epoch": 23.889789303079418,
      "grad_norm": 1.0328450202941895,
      "learning_rate": 4.764304249108012e-05,
      "loss": 3.2645,
      "step": 73700
    },
    {
      "epoch": 23.92220421393841,
      "grad_norm": 0.927993893623352,
      "learning_rate": 4.7639798897178076e-05,
      "loss": 3.2488,
      "step": 73800
    },
    {
      "epoch": 23.954619124797407,
      "grad_norm": 0.9437822103500366,
      "learning_rate": 4.7636555303276034e-05,
      "loss": 3.2539,
      "step": 73900
    },
    {
      "epoch": 23.987034035656404,
      "grad_norm": 0.8007331490516663,
      "learning_rate": 4.763331170937399e-05,
      "loss": 3.2619,
      "step": 74000
    },
    {
      "epoch": 24.0,
      "eval_bleu": 1.0763185368358918,
      "eval_loss": 3.7178757190704346,
      "eval_runtime": 4.5369,
      "eval_samples_per_second": 108.443,
      "eval_steps_per_second": 1.763,
      "step": 74040
    },
    {
      "epoch": 24.019448946515396,
      "grad_norm": 0.8768735527992249,
      "learning_rate": 4.7630068115471945e-05,
      "loss": 3.255,
      "step": 74100
    },
    {
      "epoch": 24.051863857374393,
      "grad_norm": 0.803268313407898,
      "learning_rate": 4.7626824521569904e-05,
      "loss": 3.2513,
      "step": 74200
    },
    {
      "epoch": 24.084278768233386,
      "grad_norm": 1.003758192062378,
      "learning_rate": 4.762358092766786e-05,
      "loss": 3.25,
      "step": 74300
    },
    {
      "epoch": 24.116693679092382,
      "grad_norm": 0.8264837265014648,
      "learning_rate": 4.7620337333765815e-05,
      "loss": 3.2622,
      "step": 74400
    },
    {
      "epoch": 24.14910858995138,
      "grad_norm": 0.8535533547401428,
      "learning_rate": 4.761709373986377e-05,
      "loss": 3.2412,
      "step": 74500
    },
    {
      "epoch": 24.18152350081037,
      "grad_norm": 1.0274821519851685,
      "learning_rate": 4.761385014596173e-05,
      "loss": 3.2621,
      "step": 74600
    },
    {
      "epoch": 24.213938411669368,
      "grad_norm": 1.239328145980835,
      "learning_rate": 4.7610606552059684e-05,
      "loss": 3.2503,
      "step": 74700
    },
    {
      "epoch": 24.246353322528364,
      "grad_norm": 0.9451021552085876,
      "learning_rate": 4.760736295815764e-05,
      "loss": 3.2542,
      "step": 74800
    },
    {
      "epoch": 24.278768233387357,
      "grad_norm": 0.8727419376373291,
      "learning_rate": 4.7604119364255595e-05,
      "loss": 3.2482,
      "step": 74900
    },
    {
      "epoch": 24.311183144246353,
      "grad_norm": 0.8275892734527588,
      "learning_rate": 4.760090820629258e-05,
      "loss": 3.2577,
      "step": 75000
    },
    {
      "epoch": 24.34359805510535,
      "grad_norm": 0.7916747331619263,
      "learning_rate": 4.759766461239053e-05,
      "loss": 3.2446,
      "step": 75100
    },
    {
      "epoch": 24.376012965964343,
      "grad_norm": 0.8920164704322815,
      "learning_rate": 4.759442101848849e-05,
      "loss": 3.2702,
      "step": 75200
    },
    {
      "epoch": 24.40842787682334,
      "grad_norm": 0.8288471102714539,
      "learning_rate": 4.759117742458644e-05,
      "loss": 3.2693,
      "step": 75300
    },
    {
      "epoch": 24.440842787682335,
      "grad_norm": 0.8027697205543518,
      "learning_rate": 4.75879338306844e-05,
      "loss": 3.2294,
      "step": 75400
    },
    {
      "epoch": 24.473257698541328,
      "grad_norm": 0.8192620277404785,
      "learning_rate": 4.758469023678236e-05,
      "loss": 3.2815,
      "step": 75500
    },
    {
      "epoch": 24.505672609400325,
      "grad_norm": 0.8228100538253784,
      "learning_rate": 4.758144664288031e-05,
      "loss": 3.2714,
      "step": 75600
    },
    {
      "epoch": 24.53808752025932,
      "grad_norm": 1.0634491443634033,
      "learning_rate": 4.757820304897827e-05,
      "loss": 3.2411,
      "step": 75700
    },
    {
      "epoch": 24.570502431118314,
      "grad_norm": 0.8299837708473206,
      "learning_rate": 4.757495945507623e-05,
      "loss": 3.2751,
      "step": 75800
    },
    {
      "epoch": 24.60291734197731,
      "grad_norm": 0.9359004497528076,
      "learning_rate": 4.757171586117418e-05,
      "loss": 3.2453,
      "step": 75900
    },
    {
      "epoch": 24.635332252836303,
      "grad_norm": 0.8783425688743591,
      "learning_rate": 4.756847226727214e-05,
      "loss": 3.2521,
      "step": 76000
    },
    {
      "epoch": 24.6677471636953,
      "grad_norm": 1.055668830871582,
      "learning_rate": 4.756522867337009e-05,
      "loss": 3.2509,
      "step": 76100
    },
    {
      "epoch": 24.700162074554296,
      "grad_norm": 0.928293764591217,
      "learning_rate": 4.756198507946805e-05,
      "loss": 3.2541,
      "step": 76200
    },
    {
      "epoch": 24.73257698541329,
      "grad_norm": 0.8656312823295593,
      "learning_rate": 4.755874148556601e-05,
      "loss": 3.2669,
      "step": 76300
    },
    {
      "epoch": 24.764991896272285,
      "grad_norm": 0.8994680643081665,
      "learning_rate": 4.755549789166396e-05,
      "loss": 3.2656,
      "step": 76400
    },
    {
      "epoch": 24.79740680713128,
      "grad_norm": 0.7852749824523926,
      "learning_rate": 4.755225429776192e-05,
      "loss": 3.272,
      "step": 76500
    },
    {
      "epoch": 24.829821717990274,
      "grad_norm": 0.9707778692245483,
      "learning_rate": 4.754901070385988e-05,
      "loss": 3.2566,
      "step": 76600
    },
    {
      "epoch": 24.86223662884927,
      "grad_norm": 0.8130062818527222,
      "learning_rate": 4.754576710995784e-05,
      "loss": 3.2516,
      "step": 76700
    },
    {
      "epoch": 24.894651539708267,
      "grad_norm": 0.8701446056365967,
      "learning_rate": 4.754252351605579e-05,
      "loss": 3.252,
      "step": 76800
    },
    {
      "epoch": 24.92706645056726,
      "grad_norm": 0.8199581503868103,
      "learning_rate": 4.753927992215375e-05,
      "loss": 3.2603,
      "step": 76900
    },
    {
      "epoch": 24.959481361426256,
      "grad_norm": 0.8691790699958801,
      "learning_rate": 4.753603632825171e-05,
      "loss": 3.2492,
      "step": 77000
    },
    {
      "epoch": 24.991896272285253,
      "grad_norm": 0.8421449065208435,
      "learning_rate": 4.7532792734349665e-05,
      "loss": 3.2671,
      "step": 77100
    },
    {
      "epoch": 25.0,
      "eval_bleu": 1.171312991774984,
      "eval_loss": 3.715087890625,
      "eval_runtime": 4.8164,
      "eval_samples_per_second": 102.152,
      "eval_steps_per_second": 1.661,
      "step": 77125
    },
    {
      "epoch": 25.024311183144246,
      "grad_norm": 0.775773286819458,
      "learning_rate": 4.752954914044762e-05,
      "loss": 3.2364,
      "step": 77200
    },
    {
      "epoch": 25.056726094003242,
      "grad_norm": 0.9894416928291321,
      "learning_rate": 4.7526305546545576e-05,
      "loss": 3.2481,
      "step": 77300
    },
    {
      "epoch": 25.08914100486224,
      "grad_norm": 0.8933448791503906,
      "learning_rate": 4.7523061952643535e-05,
      "loss": 3.2473,
      "step": 77400
    },
    {
      "epoch": 25.12155591572123,
      "grad_norm": 0.8663082718849182,
      "learning_rate": 4.751981835874149e-05,
      "loss": 3.2644,
      "step": 77500
    },
    {
      "epoch": 25.153970826580228,
      "grad_norm": 0.8531897068023682,
      "learning_rate": 4.7516574764839446e-05,
      "loss": 3.2443,
      "step": 77600
    },
    {
      "epoch": 25.18638573743922,
      "grad_norm": 0.7778231501579285,
      "learning_rate": 4.7513331170937404e-05,
      "loss": 3.2436,
      "step": 77700
    },
    {
      "epoch": 25.218800648298217,
      "grad_norm": 0.8978093266487122,
      "learning_rate": 4.7510087577035356e-05,
      "loss": 3.2491,
      "step": 77800
    },
    {
      "epoch": 25.251215559157213,
      "grad_norm": 0.8269281983375549,
      "learning_rate": 4.7506843983133315e-05,
      "loss": 3.263,
      "step": 77900
    },
    {
      "epoch": 25.283630470016206,
      "grad_norm": 0.8854743242263794,
      "learning_rate": 4.7503600389231274e-05,
      "loss": 3.2633,
      "step": 78000
    },
    {
      "epoch": 25.316045380875202,
      "grad_norm": 0.7297553420066833,
      "learning_rate": 4.7500356795329226e-05,
      "loss": 3.2311,
      "step": 78100
    },
    {
      "epoch": 25.3484602917342,
      "grad_norm": 0.9313138723373413,
      "learning_rate": 4.7497113201427185e-05,
      "loss": 3.2383,
      "step": 78200
    },
    {
      "epoch": 25.38087520259319,
      "grad_norm": 0.8120205402374268,
      "learning_rate": 4.7493869607525137e-05,
      "loss": 3.2377,
      "step": 78300
    },
    {
      "epoch": 25.413290113452188,
      "grad_norm": 0.9985283017158508,
      "learning_rate": 4.7490626013623095e-05,
      "loss": 3.2651,
      "step": 78400
    },
    {
      "epoch": 25.445705024311184,
      "grad_norm": 0.8506268262863159,
      "learning_rate": 4.7487382419721054e-05,
      "loss": 3.2578,
      "step": 78500
    },
    {
      "epoch": 25.478119935170177,
      "grad_norm": 0.8653962016105652,
      "learning_rate": 4.7484138825819006e-05,
      "loss": 3.2404,
      "step": 78600
    },
    {
      "epoch": 25.510534846029174,
      "grad_norm": 0.838596522808075,
      "learning_rate": 4.7480895231916965e-05,
      "loss": 3.2751,
      "step": 78700
    },
    {
      "epoch": 25.54294975688817,
      "grad_norm": 1.0048962831497192,
      "learning_rate": 4.7477651638014923e-05,
      "loss": 3.2456,
      "step": 78800
    },
    {
      "epoch": 25.575364667747163,
      "grad_norm": 0.9100828766822815,
      "learning_rate": 4.7474408044112875e-05,
      "loss": 3.2443,
      "step": 78900
    },
    {
      "epoch": 25.60777957860616,
      "grad_norm": 0.8006051778793335,
      "learning_rate": 4.747119688614985e-05,
      "loss": 3.2588,
      "step": 79000
    },
    {
      "epoch": 25.640194489465156,
      "grad_norm": 0.7676663398742676,
      "learning_rate": 4.746795329224781e-05,
      "loss": 3.2519,
      "step": 79100
    },
    {
      "epoch": 25.67260940032415,
      "grad_norm": 0.9863914251327515,
      "learning_rate": 4.746470969834577e-05,
      "loss": 3.2667,
      "step": 79200
    },
    {
      "epoch": 25.705024311183145,
      "grad_norm": 0.8919060826301575,
      "learning_rate": 4.746146610444372e-05,
      "loss": 3.2336,
      "step": 79300
    },
    {
      "epoch": 25.737439222042138,
      "grad_norm": 0.9381189942359924,
      "learning_rate": 4.745822251054168e-05,
      "loss": 3.2536,
      "step": 79400
    },
    {
      "epoch": 25.769854132901134,
      "grad_norm": 0.978266179561615,
      "learning_rate": 4.745497891663963e-05,
      "loss": 3.2499,
      "step": 79500
    },
    {
      "epoch": 25.80226904376013,
      "grad_norm": 0.8508621454238892,
      "learning_rate": 4.745173532273759e-05,
      "loss": 3.248,
      "step": 79600
    },
    {
      "epoch": 25.834683954619123,
      "grad_norm": 1.161535620689392,
      "learning_rate": 4.744849172883555e-05,
      "loss": 3.2378,
      "step": 79700
    },
    {
      "epoch": 25.86709886547812,
      "grad_norm": 0.8118118643760681,
      "learning_rate": 4.744524813493351e-05,
      "loss": 3.2516,
      "step": 79800
    },
    {
      "epoch": 25.899513776337116,
      "grad_norm": 1.0001522302627563,
      "learning_rate": 4.744200454103147e-05,
      "loss": 3.2461,
      "step": 79900
    },
    {
      "epoch": 25.93192868719611,
      "grad_norm": 0.7949409484863281,
      "learning_rate": 4.743876094712942e-05,
      "loss": 3.2341,
      "step": 80000
    },
    {
      "epoch": 25.964343598055105,
      "grad_norm": 1.130479335784912,
      "learning_rate": 4.743551735322738e-05,
      "loss": 3.2442,
      "step": 80100
    },
    {
      "epoch": 25.996758508914102,
      "grad_norm": 0.869892954826355,
      "learning_rate": 4.743227375932534e-05,
      "loss": 3.2633,
      "step": 80200
    },
    {
      "epoch": 26.0,
      "eval_bleu": 1.1047696811784966,
      "eval_loss": 3.7164268493652344,
      "eval_runtime": 4.5687,
      "eval_samples_per_second": 107.69,
      "eval_steps_per_second": 1.751,
      "step": 80210
    },
    {
      "epoch": 26.029173419773095,
      "grad_norm": 0.7359393835067749,
      "learning_rate": 4.7429030165423297e-05,
      "loss": 3.2383,
      "step": 80300
    },
    {
      "epoch": 26.06158833063209,
      "grad_norm": 1.0057424306869507,
      "learning_rate": 4.742578657152125e-05,
      "loss": 3.2378,
      "step": 80400
    },
    {
      "epoch": 26.094003241491087,
      "grad_norm": 0.9581093192100525,
      "learning_rate": 4.742254297761921e-05,
      "loss": 3.2183,
      "step": 80500
    },
    {
      "epoch": 26.12641815235008,
      "grad_norm": 0.9035683870315552,
      "learning_rate": 4.741929938371716e-05,
      "loss": 3.2446,
      "step": 80600
    },
    {
      "epoch": 26.158833063209077,
      "grad_norm": 0.9605050683021545,
      "learning_rate": 4.741605578981512e-05,
      "loss": 3.2499,
      "step": 80700
    },
    {
      "epoch": 26.191247974068073,
      "grad_norm": 0.7987914085388184,
      "learning_rate": 4.741281219591308e-05,
      "loss": 3.2557,
      "step": 80800
    },
    {
      "epoch": 26.223662884927066,
      "grad_norm": 0.8493878841400146,
      "learning_rate": 4.740956860201103e-05,
      "loss": 3.2413,
      "step": 80900
    },
    {
      "epoch": 26.256077795786062,
      "grad_norm": 1.0518476963043213,
      "learning_rate": 4.7406357444048006e-05,
      "loss": 3.2423,
      "step": 81000
    },
    {
      "epoch": 26.288492706645055,
      "grad_norm": 0.8379240036010742,
      "learning_rate": 4.7403113850145965e-05,
      "loss": 3.259,
      "step": 81100
    },
    {
      "epoch": 26.32090761750405,
      "grad_norm": 0.7951277494430542,
      "learning_rate": 4.7399870256243924e-05,
      "loss": 3.2446,
      "step": 81200
    },
    {
      "epoch": 26.353322528363048,
      "grad_norm": 1.02846097946167,
      "learning_rate": 4.7396626662341876e-05,
      "loss": 3.2401,
      "step": 81300
    },
    {
      "epoch": 26.38573743922204,
      "grad_norm": 0.8962801098823547,
      "learning_rate": 4.7393383068439835e-05,
      "loss": 3.2395,
      "step": 81400
    },
    {
      "epoch": 26.418152350081037,
      "grad_norm": 0.8636525273323059,
      "learning_rate": 4.7390139474537793e-05,
      "loss": 3.2409,
      "step": 81500
    },
    {
      "epoch": 26.450567260940034,
      "grad_norm": 1.0217043161392212,
      "learning_rate": 4.7386895880635745e-05,
      "loss": 3.2415,
      "step": 81600
    },
    {
      "epoch": 26.482982171799026,
      "grad_norm": 0.8765149116516113,
      "learning_rate": 4.7383652286733704e-05,
      "loss": 3.2378,
      "step": 81700
    },
    {
      "epoch": 26.515397082658023,
      "grad_norm": 0.9284408092498779,
      "learning_rate": 4.7380408692831656e-05,
      "loss": 3.2301,
      "step": 81800
    },
    {
      "epoch": 26.54781199351702,
      "grad_norm": 0.9525735974311829,
      "learning_rate": 4.7377165098929615e-05,
      "loss": 3.2397,
      "step": 81900
    },
    {
      "epoch": 26.580226904376012,
      "grad_norm": 0.8759743571281433,
      "learning_rate": 4.7373921505027574e-05,
      "loss": 3.2462,
      "step": 82000
    },
    {
      "epoch": 26.61264181523501,
      "grad_norm": 0.8841198086738586,
      "learning_rate": 4.7370677911125526e-05,
      "loss": 3.2496,
      "step": 82100
    },
    {
      "epoch": 26.645056726094005,
      "grad_norm": 0.8010789155960083,
      "learning_rate": 4.7367434317223484e-05,
      "loss": 3.2418,
      "step": 82200
    },
    {
      "epoch": 26.677471636952998,
      "grad_norm": 0.9061064720153809,
      "learning_rate": 4.736419072332144e-05,
      "loss": 3.2486,
      "step": 82300
    },
    {
      "epoch": 26.709886547811994,
      "grad_norm": 0.9909858703613281,
      "learning_rate": 4.7360947129419395e-05,
      "loss": 3.2495,
      "step": 82400
    },
    {
      "epoch": 26.742301458670987,
      "grad_norm": 0.8410025835037231,
      "learning_rate": 4.7357703535517354e-05,
      "loss": 3.2491,
      "step": 82500
    },
    {
      "epoch": 26.774716369529983,
      "grad_norm": 1.1023732423782349,
      "learning_rate": 4.735445994161531e-05,
      "loss": 3.2434,
      "step": 82600
    },
    {
      "epoch": 26.80713128038898,
      "grad_norm": 0.969001054763794,
      "learning_rate": 4.7351216347713264e-05,
      "loss": 3.2344,
      "step": 82700
    },
    {
      "epoch": 26.839546191247972,
      "grad_norm": 0.8901473879814148,
      "learning_rate": 4.734797275381122e-05,
      "loss": 3.2526,
      "step": 82800
    },
    {
      "epoch": 26.87196110210697,
      "grad_norm": 0.9235983490943909,
      "learning_rate": 4.734472915990918e-05,
      "loss": 3.2354,
      "step": 82900
    },
    {
      "epoch": 26.904376012965965,
      "grad_norm": 0.9452083110809326,
      "learning_rate": 4.734151800194616e-05,
      "loss": 3.2288,
      "step": 83000
    },
    {
      "epoch": 26.936790923824958,
      "grad_norm": 0.8909045457839966,
      "learning_rate": 4.733827440804411e-05,
      "loss": 3.263,
      "step": 83100
    },
    {
      "epoch": 26.969205834683954,
      "grad_norm": 1.0283564329147339,
      "learning_rate": 4.733503081414207e-05,
      "loss": 3.2436,
      "step": 83200
    },
    {
      "epoch": 27.0,
      "eval_bleu": 1.1564511397397266,
      "eval_loss": 3.715927839279175,
      "eval_runtime": 4.6031,
      "eval_samples_per_second": 106.885,
      "eval_steps_per_second": 1.738,
      "step": 83295
    },
    {
      "epoch": 27.00162074554295,
      "grad_norm": 0.8801915049552917,
      "learning_rate": 4.733178722024003e-05,
      "loss": 3.24,
      "step": 83300
    },
    {
      "epoch": 27.034035656401944,
      "grad_norm": 0.8015052080154419,
      "learning_rate": 4.732854362633798e-05,
      "loss": 3.216,
      "step": 83400
    },
    {
      "epoch": 27.06645056726094,
      "grad_norm": 0.8208436965942383,
      "learning_rate": 4.732530003243594e-05,
      "loss": 3.2113,
      "step": 83500
    },
    {
      "epoch": 27.098865478119937,
      "grad_norm": 0.7837216258049011,
      "learning_rate": 4.73220564385339e-05,
      "loss": 3.23,
      "step": 83600
    },
    {
      "epoch": 27.13128038897893,
      "grad_norm": 0.8142829537391663,
      "learning_rate": 4.731881284463186e-05,
      "loss": 3.2334,
      "step": 83700
    },
    {
      "epoch": 27.163695299837926,
      "grad_norm": 1.0355675220489502,
      "learning_rate": 4.7315569250729816e-05,
      "loss": 3.2297,
      "step": 83800
    },
    {
      "epoch": 27.196110210696922,
      "grad_norm": 0.9307448863983154,
      "learning_rate": 4.731232565682777e-05,
      "loss": 3.214,
      "step": 83900
    },
    {
      "epoch": 27.228525121555915,
      "grad_norm": 0.9059737324714661,
      "learning_rate": 4.730908206292573e-05,
      "loss": 3.2345,
      "step": 84000
    },
    {
      "epoch": 27.26094003241491,
      "grad_norm": 0.9997667670249939,
      "learning_rate": 4.730583846902368e-05,
      "loss": 3.2181,
      "step": 84100
    },
    {
      "epoch": 27.293354943273904,
      "grad_norm": 0.8966726064682007,
      "learning_rate": 4.730259487512164e-05,
      "loss": 3.2458,
      "step": 84200
    },
    {
      "epoch": 27.3257698541329,
      "grad_norm": 0.8832192420959473,
      "learning_rate": 4.7299351281219596e-05,
      "loss": 3.2356,
      "step": 84300
    },
    {
      "epoch": 27.358184764991897,
      "grad_norm": 0.8023368716239929,
      "learning_rate": 4.729610768731755e-05,
      "loss": 3.246,
      "step": 84400
    },
    {
      "epoch": 27.39059967585089,
      "grad_norm": 0.9183096885681152,
      "learning_rate": 4.729286409341551e-05,
      "loss": 3.2391,
      "step": 84500
    },
    {
      "epoch": 27.423014586709886,
      "grad_norm": 0.9081838726997375,
      "learning_rate": 4.7289620499513466e-05,
      "loss": 3.2195,
      "step": 84600
    },
    {
      "epoch": 27.455429497568883,
      "grad_norm": 0.7576412558555603,
      "learning_rate": 4.728637690561142e-05,
      "loss": 3.2488,
      "step": 84700
    },
    {
      "epoch": 27.487844408427875,
      "grad_norm": 0.8169103860855103,
      "learning_rate": 4.7283133311709376e-05,
      "loss": 3.2495,
      "step": 84800
    },
    {
      "epoch": 27.520259319286872,
      "grad_norm": 0.8990625143051147,
      "learning_rate": 4.7279889717807335e-05,
      "loss": 3.2271,
      "step": 84900
    },
    {
      "epoch": 27.55267423014587,
      "grad_norm": 1.0501149892807007,
      "learning_rate": 4.727667855984431e-05,
      "loss": 3.2428,
      "step": 85000
    },
    {
      "epoch": 27.58508914100486,
      "grad_norm": 0.8705491423606873,
      "learning_rate": 4.7273434965942265e-05,
      "loss": 3.243,
      "step": 85100
    },
    {
      "epoch": 27.617504051863857,
      "grad_norm": 0.9048194289207458,
      "learning_rate": 4.7270191372040224e-05,
      "loss": 3.2198,
      "step": 85200
    },
    {
      "epoch": 27.649918962722854,
      "grad_norm": 0.9554929137229919,
      "learning_rate": 4.726694777813818e-05,
      "loss": 3.2332,
      "step": 85300
    },
    {
      "epoch": 27.682333873581847,
      "grad_norm": 0.8119944930076599,
      "learning_rate": 4.7263704184236134e-05,
      "loss": 3.2364,
      "step": 85400
    },
    {
      "epoch": 27.714748784440843,
      "grad_norm": 0.9195199608802795,
      "learning_rate": 4.726046059033409e-05,
      "loss": 3.2321,
      "step": 85500
    },
    {
      "epoch": 27.74716369529984,
      "grad_norm": 0.8603640198707581,
      "learning_rate": 4.7257216996432045e-05,
      "loss": 3.2276,
      "step": 85600
    },
    {
      "epoch": 27.779578606158832,
      "grad_norm": 0.9460744857788086,
      "learning_rate": 4.7253973402530004e-05,
      "loss": 3.2259,
      "step": 85700
    },
    {
      "epoch": 27.81199351701783,
      "grad_norm": 0.9148878455162048,
      "learning_rate": 4.725072980862796e-05,
      "loss": 3.2588,
      "step": 85800
    },
    {
      "epoch": 27.84440842787682,
      "grad_norm": 0.9529094099998474,
      "learning_rate": 4.7247486214725915e-05,
      "loss": 3.2448,
      "step": 85900
    },
    {
      "epoch": 27.876823338735818,
      "grad_norm": 0.8275254368782043,
      "learning_rate": 4.724424262082387e-05,
      "loss": 3.2496,
      "step": 86000
    },
    {
      "epoch": 27.909238249594814,
      "grad_norm": 0.9331241250038147,
      "learning_rate": 4.724099902692183e-05,
      "loss": 3.2446,
      "step": 86100
    },
    {
      "epoch": 27.941653160453807,
      "grad_norm": 0.8560378551483154,
      "learning_rate": 4.7237755433019784e-05,
      "loss": 3.2306,
      "step": 86200
    },
    {
      "epoch": 27.974068071312804,
      "grad_norm": 0.9922318458557129,
      "learning_rate": 4.723451183911774e-05,
      "loss": 3.2429,
      "step": 86300
    },
    {
      "epoch": 28.0,
      "eval_bleu": 1.016993195089588,
      "eval_loss": 3.71298885345459,
      "eval_runtime": 5.1454,
      "eval_samples_per_second": 95.619,
      "eval_steps_per_second": 1.555,
      "step": 86380
    },
    {
      "epoch": 28.0064829821718,
      "grad_norm": 0.7811623215675354,
      "learning_rate": 4.72312682452157e-05,
      "loss": 3.2411,
      "step": 86400
    },
    {
      "epoch": 28.038897893030793,
      "grad_norm": 0.9034388661384583,
      "learning_rate": 4.722802465131366e-05,
      "loss": 3.2331,
      "step": 86500
    },
    {
      "epoch": 28.07131280388979,
      "grad_norm": 0.8952441215515137,
      "learning_rate": 4.722478105741162e-05,
      "loss": 3.2252,
      "step": 86600
    },
    {
      "epoch": 28.103727714748786,
      "grad_norm": 0.9215511679649353,
      "learning_rate": 4.722153746350957e-05,
      "loss": 3.2347,
      "step": 86700
    },
    {
      "epoch": 28.13614262560778,
      "grad_norm": 1.0312906503677368,
      "learning_rate": 4.721829386960753e-05,
      "loss": 3.2296,
      "step": 86800
    },
    {
      "epoch": 28.168557536466775,
      "grad_norm": 0.9444610476493835,
      "learning_rate": 4.721505027570549e-05,
      "loss": 3.2212,
      "step": 86900
    },
    {
      "epoch": 28.20097244732577,
      "grad_norm": 0.7815536856651306,
      "learning_rate": 4.721183911774246e-05,
      "loss": 3.2357,
      "step": 87000
    },
    {
      "epoch": 28.233387358184764,
      "grad_norm": 0.8535163402557373,
      "learning_rate": 4.720859552384042e-05,
      "loss": 3.215,
      "step": 87100
    },
    {
      "epoch": 28.26580226904376,
      "grad_norm": 1.0644131898880005,
      "learning_rate": 4.720535192993838e-05,
      "loss": 3.2183,
      "step": 87200
    },
    {
      "epoch": 28.298217179902757,
      "grad_norm": 0.9090868234634399,
      "learning_rate": 4.7202108336036336e-05,
      "loss": 3.2183,
      "step": 87300
    },
    {
      "epoch": 28.33063209076175,
      "grad_norm": 0.858262300491333,
      "learning_rate": 4.719886474213429e-05,
      "loss": 3.2306,
      "step": 87400
    },
    {
      "epoch": 28.363047001620746,
      "grad_norm": 0.842239499092102,
      "learning_rate": 4.7195621148232246e-05,
      "loss": 3.2276,
      "step": 87500
    },
    {
      "epoch": 28.39546191247974,
      "grad_norm": 0.7752278447151184,
      "learning_rate": 4.7192377554330205e-05,
      "loss": 3.2371,
      "step": 87600
    },
    {
      "epoch": 28.427876823338735,
      "grad_norm": 1.0732678174972534,
      "learning_rate": 4.718913396042816e-05,
      "loss": 3.2213,
      "step": 87700
    },
    {
      "epoch": 28.46029173419773,
      "grad_norm": 0.80622398853302,
      "learning_rate": 4.7185890366526116e-05,
      "loss": 3.2155,
      "step": 87800
    },
    {
      "epoch": 28.492706645056725,
      "grad_norm": 0.9782589673995972,
      "learning_rate": 4.718264677262407e-05,
      "loss": 3.203,
      "step": 87900
    },
    {
      "epoch": 28.52512155591572,
      "grad_norm": 0.736560583114624,
      "learning_rate": 4.7179403178722027e-05,
      "loss": 3.2334,
      "step": 88000
    },
    {
      "epoch": 28.557536466774717,
      "grad_norm": 0.8794552087783813,
      "learning_rate": 4.7176159584819985e-05,
      "loss": 3.2235,
      "step": 88100
    },
    {
      "epoch": 28.58995137763371,
      "grad_norm": 0.7932811975479126,
      "learning_rate": 4.717291599091794e-05,
      "loss": 3.2219,
      "step": 88200
    },
    {
      "epoch": 28.622366288492707,
      "grad_norm": 0.7872946262359619,
      "learning_rate": 4.7169672397015896e-05,
      "loss": 3.2247,
      "step": 88300
    },
    {
      "epoch": 28.654781199351703,
      "grad_norm": 0.8411557674407959,
      "learning_rate": 4.7166428803113855e-05,
      "loss": 3.2225,
      "step": 88400
    },
    {
      "epoch": 28.687196110210696,
      "grad_norm": 0.8057785034179688,
      "learning_rate": 4.716318520921181e-05,
      "loss": 3.232,
      "step": 88500
    },
    {
      "epoch": 28.719611021069692,
      "grad_norm": 0.992669939994812,
      "learning_rate": 4.7159941615309765e-05,
      "loss": 3.2227,
      "step": 88600
    },
    {
      "epoch": 28.75202593192869,
      "grad_norm": 0.9484904408454895,
      "learning_rate": 4.7156698021407724e-05,
      "loss": 3.2604,
      "step": 88700
    },
    {
      "epoch": 28.78444084278768,
      "grad_norm": 0.807748019695282,
      "learning_rate": 4.7153454427505676e-05,
      "loss": 3.235,
      "step": 88800
    },
    {
      "epoch": 28.816855753646678,
      "grad_norm": 0.9000910520553589,
      "learning_rate": 4.7150210833603635e-05,
      "loss": 3.2303,
      "step": 88900
    },
    {
      "epoch": 28.849270664505674,
      "grad_norm": 0.977117657661438,
      "learning_rate": 4.714699967564061e-05,
      "loss": 3.2174,
      "step": 89000
    },
    {
      "epoch": 28.881685575364667,
      "grad_norm": 0.9425219297409058,
      "learning_rate": 4.7143756081738565e-05,
      "loss": 3.2345,
      "step": 89100
    },
    {
      "epoch": 28.914100486223663,
      "grad_norm": 0.9931532144546509,
      "learning_rate": 4.7140512487836523e-05,
      "loss": 3.2348,
      "step": 89200
    },
    {
      "epoch": 28.946515397082656,
      "grad_norm": 0.9346868991851807,
      "learning_rate": 4.713726889393448e-05,
      "loss": 3.2379,
      "step": 89300
    },
    {
      "epoch": 28.978930307941653,
      "grad_norm": 1.0380154848098755,
      "learning_rate": 4.7134025300032434e-05,
      "loss": 3.2344,
      "step": 89400
    },
    {
      "epoch": 29.0,
      "eval_bleu": 0.8620958450774207,
      "eval_loss": 3.7156527042388916,
      "eval_runtime": 4.903,
      "eval_samples_per_second": 100.346,
      "eval_steps_per_second": 1.632,
      "step": 89465
    },
    {
      "epoch": 29.01134521880065,
      "grad_norm": 0.9693220257759094,
      "learning_rate": 4.713078170613039e-05,
      "loss": 3.2472,
      "step": 89500
    },
    {
      "epoch": 29.043760129659642,
      "grad_norm": 0.836755633354187,
      "learning_rate": 4.712753811222835e-05,
      "loss": 3.2071,
      "step": 89600
    },
    {
      "epoch": 29.07617504051864,
      "grad_norm": 0.963408887386322,
      "learning_rate": 4.7124294518326304e-05,
      "loss": 3.2228,
      "step": 89700
    },
    {
      "epoch": 29.108589951377635,
      "grad_norm": 0.8612934350967407,
      "learning_rate": 4.712105092442426e-05,
      "loss": 3.2113,
      "step": 89800
    },
    {
      "epoch": 29.141004862236628,
      "grad_norm": 0.9327705502510071,
      "learning_rate": 4.711780733052222e-05,
      "loss": 3.2174,
      "step": 89900
    },
    {
      "epoch": 29.173419773095624,
      "grad_norm": 0.9117677211761475,
      "learning_rate": 4.711456373662017e-05,
      "loss": 3.2027,
      "step": 90000
    },
    {
      "epoch": 29.20583468395462,
      "grad_norm": 0.8437981605529785,
      "learning_rate": 4.711132014271813e-05,
      "loss": 3.2345,
      "step": 90100
    },
    {
      "epoch": 29.238249594813613,
      "grad_norm": 0.8107649683952332,
      "learning_rate": 4.710807654881609e-05,
      "loss": 3.2353,
      "step": 90200
    },
    {
      "epoch": 29.27066450567261,
      "grad_norm": 1.035867691040039,
      "learning_rate": 4.710483295491405e-05,
      "loss": 3.2256,
      "step": 90300
    },
    {
      "epoch": 29.303079416531606,
      "grad_norm": 0.8371264338493347,
      "learning_rate": 4.710158936101201e-05,
      "loss": 3.2348,
      "step": 90400
    },
    {
      "epoch": 29.3354943273906,
      "grad_norm": 0.8924967646598816,
      "learning_rate": 4.709834576710996e-05,
      "loss": 3.2159,
      "step": 90500
    },
    {
      "epoch": 29.367909238249595,
      "grad_norm": 0.8506584167480469,
      "learning_rate": 4.709510217320792e-05,
      "loss": 3.1907,
      "step": 90600
    },
    {
      "epoch": 29.40032414910859,
      "grad_norm": 0.9373581409454346,
      "learning_rate": 4.709185857930588e-05,
      "loss": 3.2246,
      "step": 90700
    },
    {
      "epoch": 29.432739059967584,
      "grad_norm": 0.9310516119003296,
      "learning_rate": 4.708861498540383e-05,
      "loss": 3.2302,
      "step": 90800
    },
    {
      "epoch": 29.46515397082658,
      "grad_norm": 1.0228257179260254,
      "learning_rate": 4.708537139150179e-05,
      "loss": 3.2273,
      "step": 90900
    },
    {
      "epoch": 29.497568881685574,
      "grad_norm": 0.8112310767173767,
      "learning_rate": 4.708212779759975e-05,
      "loss": 3.2216,
      "step": 91000
    },
    {
      "epoch": 29.52998379254457,
      "grad_norm": 0.8605124354362488,
      "learning_rate": 4.7078916639636725e-05,
      "loss": 3.2431,
      "step": 91100
    },
    {
      "epoch": 29.562398703403566,
      "grad_norm": 0.9410679340362549,
      "learning_rate": 4.707567304573468e-05,
      "loss": 3.2272,
      "step": 91200
    },
    {
      "epoch": 29.59481361426256,
      "grad_norm": 0.8497024178504944,
      "learning_rate": 4.7072429451832635e-05,
      "loss": 3.2274,
      "step": 91300
    },
    {
      "epoch": 29.627228525121556,
      "grad_norm": 0.9948247075080872,
      "learning_rate": 4.706918585793059e-05,
      "loss": 3.2056,
      "step": 91400
    },
    {
      "epoch": 29.659643435980552,
      "grad_norm": 0.9449138045310974,
      "learning_rate": 4.7065942264028546e-05,
      "loss": 3.2203,
      "step": 91500
    },
    {
      "epoch": 29.692058346839545,
      "grad_norm": 1.0411323308944702,
      "learning_rate": 4.7062698670126505e-05,
      "loss": 3.2096,
      "step": 91600
    },
    {
      "epoch": 29.72447325769854,
      "grad_norm": 1.0237008333206177,
      "learning_rate": 4.705945507622446e-05,
      "loss": 3.2025,
      "step": 91700
    },
    {
      "epoch": 29.756888168557538,
      "grad_norm": 0.852400004863739,
      "learning_rate": 4.7056211482322416e-05,
      "loss": 3.2229,
      "step": 91800
    },
    {
      "epoch": 29.78930307941653,
      "grad_norm": 0.9233277440071106,
      "learning_rate": 4.7052967888420374e-05,
      "loss": 3.2206,
      "step": 91900
    },
    {
      "epoch": 29.821717990275527,
      "grad_norm": 0.963147759437561,
      "learning_rate": 4.7049724294518326e-05,
      "loss": 3.2371,
      "step": 92000
    },
    {
      "epoch": 29.854132901134523,
      "grad_norm": 0.9863738417625427,
      "learning_rate": 4.7046480700616285e-05,
      "loss": 3.2193,
      "step": 92100
    },
    {
      "epoch": 29.886547811993516,
      "grad_norm": 0.9980084300041199,
      "learning_rate": 4.7043237106714244e-05,
      "loss": 3.2183,
      "step": 92200
    },
    {
      "epoch": 29.918962722852513,
      "grad_norm": 0.9427039623260498,
      "learning_rate": 4.7039993512812196e-05,
      "loss": 3.2205,
      "step": 92300
    },
    {
      "epoch": 29.95137763371151,
      "grad_norm": 0.744028627872467,
      "learning_rate": 4.7036749918910154e-05,
      "loss": 3.2197,
      "step": 92400
    },
    {
      "epoch": 29.983792544570502,
      "grad_norm": 0.8347026109695435,
      "learning_rate": 4.7033506325008106e-05,
      "loss": 3.2394,
      "step": 92500
    },
    {
      "epoch": 30.0,
      "eval_bleu": 1.0578432419301722,
      "eval_loss": 3.7217772006988525,
      "eval_runtime": 4.4621,
      "eval_samples_per_second": 110.262,
      "eval_steps_per_second": 1.793,
      "step": 92550
    },
    {
      "epoch": 30.016207455429498,
      "grad_norm": 0.9188928008079529,
      "learning_rate": 4.7030262731106065e-05,
      "loss": 3.211,
      "step": 92600
    },
    {
      "epoch": 30.04862236628849,
      "grad_norm": 0.822765588760376,
      "learning_rate": 4.7027019137204024e-05,
      "loss": 3.2024,
      "step": 92700
    },
    {
      "epoch": 30.081037277147487,
      "grad_norm": 0.896817147731781,
      "learning_rate": 4.7023775543301976e-05,
      "loss": 3.2004,
      "step": 92800
    },
    {
      "epoch": 30.113452188006484,
      "grad_norm": 0.8816471099853516,
      "learning_rate": 4.7020531949399935e-05,
      "loss": 3.2167,
      "step": 92900
    },
    {
      "epoch": 30.145867098865477,
      "grad_norm": 0.8790517449378967,
      "learning_rate": 4.7017288355497893e-05,
      "loss": 3.2176,
      "step": 93000
    },
    {
      "epoch": 30.178282009724473,
      "grad_norm": 0.8545476794242859,
      "learning_rate": 4.701407719753487e-05,
      "loss": 3.2229,
      "step": 93100
    },
    {
      "epoch": 30.21069692058347,
      "grad_norm": 1.0187726020812988,
      "learning_rate": 4.701083360363282e-05,
      "loss": 3.1946,
      "step": 93200
    },
    {
      "epoch": 30.243111831442462,
      "grad_norm": 0.9758113622665405,
      "learning_rate": 4.700759000973078e-05,
      "loss": 3.2237,
      "step": 93300
    },
    {
      "epoch": 30.27552674230146,
      "grad_norm": 0.8630130290985107,
      "learning_rate": 4.700434641582874e-05,
      "loss": 3.2132,
      "step": 93400
    },
    {
      "epoch": 30.307941653160455,
      "grad_norm": 0.9997749328613281,
      "learning_rate": 4.700110282192669e-05,
      "loss": 3.2152,
      "step": 93500
    },
    {
      "epoch": 30.340356564019448,
      "grad_norm": 1.017067790031433,
      "learning_rate": 4.699785922802465e-05,
      "loss": 3.2145,
      "step": 93600
    },
    {
      "epoch": 30.372771474878444,
      "grad_norm": 0.9030765295028687,
      "learning_rate": 4.699461563412261e-05,
      "loss": 3.2011,
      "step": 93700
    },
    {
      "epoch": 30.40518638573744,
      "grad_norm": 0.8409193754196167,
      "learning_rate": 4.699137204022057e-05,
      "loss": 3.2183,
      "step": 93800
    },
    {
      "epoch": 30.437601296596434,
      "grad_norm": 0.8864888548851013,
      "learning_rate": 4.698812844631853e-05,
      "loss": 3.2209,
      "step": 93900
    },
    {
      "epoch": 30.47001620745543,
      "grad_norm": 0.9642324447631836,
      "learning_rate": 4.698488485241648e-05,
      "loss": 3.2024,
      "step": 94000
    },
    {
      "epoch": 30.502431118314426,
      "grad_norm": 0.8766580820083618,
      "learning_rate": 4.698164125851444e-05,
      "loss": 3.2137,
      "step": 94100
    },
    {
      "epoch": 30.53484602917342,
      "grad_norm": 0.8159382343292236,
      "learning_rate": 4.69783976646124e-05,
      "loss": 3.227,
      "step": 94200
    },
    {
      "epoch": 30.567260940032416,
      "grad_norm": 0.9068295359611511,
      "learning_rate": 4.697515407071035e-05,
      "loss": 3.1983,
      "step": 94300
    },
    {
      "epoch": 30.59967585089141,
      "grad_norm": 0.9001442790031433,
      "learning_rate": 4.697191047680831e-05,
      "loss": 3.2331,
      "step": 94400
    },
    {
      "epoch": 30.632090761750405,
      "grad_norm": 0.9198023676872253,
      "learning_rate": 4.6968666882906266e-05,
      "loss": 3.1917,
      "step": 94500
    },
    {
      "epoch": 30.6645056726094,
      "grad_norm": 0.8896318674087524,
      "learning_rate": 4.696542328900422e-05,
      "loss": 3.2129,
      "step": 94600
    },
    {
      "epoch": 30.696920583468394,
      "grad_norm": 0.8808829188346863,
      "learning_rate": 4.696217969510218e-05,
      "loss": 3.2129,
      "step": 94700
    },
    {
      "epoch": 30.72933549432739,
      "grad_norm": 0.8463298678398132,
      "learning_rate": 4.695893610120013e-05,
      "loss": 3.2468,
      "step": 94800
    },
    {
      "epoch": 30.761750405186387,
      "grad_norm": 0.8516310453414917,
      "learning_rate": 4.6955724943237114e-05,
      "loss": 3.2171,
      "step": 94900
    },
    {
      "epoch": 30.79416531604538,
      "grad_norm": 0.8366274833679199,
      "learning_rate": 4.6952481349335066e-05,
      "loss": 3.2047,
      "step": 95000
    },
    {
      "epoch": 30.826580226904376,
      "grad_norm": 1.2102687358856201,
      "learning_rate": 4.6949237755433024e-05,
      "loss": 3.2134,
      "step": 95100
    },
    {
      "epoch": 30.858995137763372,
      "grad_norm": 0.798808753490448,
      "learning_rate": 4.6945994161530976e-05,
      "loss": 3.2321,
      "step": 95200
    },
    {
      "epoch": 30.891410048622365,
      "grad_norm": 0.8087403774261475,
      "learning_rate": 4.6942750567628935e-05,
      "loss": 3.2198,
      "step": 95300
    },
    {
      "epoch": 30.92382495948136,
      "grad_norm": 1.0594292879104614,
      "learning_rate": 4.6939506973726894e-05,
      "loss": 3.2213,
      "step": 95400
    },
    {
      "epoch": 30.956239870340358,
      "grad_norm": 1.0351459980010986,
      "learning_rate": 4.6936263379824846e-05,
      "loss": 3.2076,
      "step": 95500
    },
    {
      "epoch": 30.98865478119935,
      "grad_norm": 0.8211233615875244,
      "learning_rate": 4.6933019785922805e-05,
      "loss": 3.2204,
      "step": 95600
    },
    {
      "epoch": 31.0,
      "eval_bleu": 0.9521607640525667,
      "eval_loss": 3.7175779342651367,
      "eval_runtime": 4.702,
      "eval_samples_per_second": 104.635,
      "eval_steps_per_second": 1.701,
      "step": 95635
    },
    {
      "epoch": 31.021069692058347,
      "grad_norm": 0.8903776407241821,
      "learning_rate": 4.692977619202076e-05,
      "loss": 3.209,
      "step": 95700
    },
    {
      "epoch": 31.053484602917344,
      "grad_norm": 0.7941554188728333,
      "learning_rate": 4.6926532598118715e-05,
      "loss": 3.2002,
      "step": 95800
    },
    {
      "epoch": 31.085899513776337,
      "grad_norm": 1.108073353767395,
      "learning_rate": 4.6923289004216674e-05,
      "loss": 3.2104,
      "step": 95900
    },
    {
      "epoch": 31.118314424635333,
      "grad_norm": 0.9436637163162231,
      "learning_rate": 4.6920045410314626e-05,
      "loss": 3.2159,
      "step": 96000
    },
    {
      "epoch": 31.150729335494326,
      "grad_norm": 0.8878848552703857,
      "learning_rate": 4.6916801816412585e-05,
      "loss": 3.1952,
      "step": 96100
    },
    {
      "epoch": 31.183144246353322,
      "grad_norm": 0.84607994556427,
      "learning_rate": 4.6913558222510544e-05,
      "loss": 3.196,
      "step": 96200
    },
    {
      "epoch": 31.21555915721232,
      "grad_norm": 0.9180102944374084,
      "learning_rate": 4.6910314628608495e-05,
      "loss": 3.215,
      "step": 96300
    },
    {
      "epoch": 31.24797406807131,
      "grad_norm": 0.9465857148170471,
      "learning_rate": 4.6907071034706454e-05,
      "loss": 3.2246,
      "step": 96400
    },
    {
      "epoch": 31.280388978930308,
      "grad_norm": 0.9359623789787292,
      "learning_rate": 4.690382744080441e-05,
      "loss": 3.2047,
      "step": 96500
    },
    {
      "epoch": 31.312803889789304,
      "grad_norm": 0.9237015247344971,
      "learning_rate": 4.690058384690237e-05,
      "loss": 3.1847,
      "step": 96600
    },
    {
      "epoch": 31.345218800648297,
      "grad_norm": 0.9187662601470947,
      "learning_rate": 4.6897340253000324e-05,
      "loss": 3.198,
      "step": 96700
    },
    {
      "epoch": 31.377633711507293,
      "grad_norm": 0.8106099367141724,
      "learning_rate": 4.689409665909828e-05,
      "loss": 3.2106,
      "step": 96800
    },
    {
      "epoch": 31.41004862236629,
      "grad_norm": 0.8186467885971069,
      "learning_rate": 4.689085306519624e-05,
      "loss": 3.2212,
      "step": 96900
    },
    {
      "epoch": 31.442463533225283,
      "grad_norm": 0.8866676092147827,
      "learning_rate": 4.68876094712942e-05,
      "loss": 3.2205,
      "step": 97000
    },
    {
      "epoch": 31.47487844408428,
      "grad_norm": 0.8730195760726929,
      "learning_rate": 4.688436587739215e-05,
      "loss": 3.2141,
      "step": 97100
    },
    {
      "epoch": 31.507293354943275,
      "grad_norm": 1.1999715566635132,
      "learning_rate": 4.688112228349011e-05,
      "loss": 3.2132,
      "step": 97200
    },
    {
      "epoch": 31.53970826580227,
      "grad_norm": 0.9076035022735596,
      "learning_rate": 4.687787868958807e-05,
      "loss": 3.2142,
      "step": 97300
    },
    {
      "epoch": 31.572123176661265,
      "grad_norm": 0.8119577765464783,
      "learning_rate": 4.687463509568602e-05,
      "loss": 3.1928,
      "step": 97400
    },
    {
      "epoch": 31.60453808752026,
      "grad_norm": 0.884177565574646,
      "learning_rate": 4.687139150178398e-05,
      "loss": 3.2124,
      "step": 97500
    },
    {
      "epoch": 31.636952998379254,
      "grad_norm": 0.9078370928764343,
      "learning_rate": 4.686814790788194e-05,
      "loss": 3.1957,
      "step": 97600
    },
    {
      "epoch": 31.66936790923825,
      "grad_norm": 0.9091132879257202,
      "learning_rate": 4.686490431397989e-05,
      "loss": 3.2223,
      "step": 97700
    },
    {
      "epoch": 31.701782820097243,
      "grad_norm": 0.8677605390548706,
      "learning_rate": 4.686169315601687e-05,
      "loss": 3.1858,
      "step": 97800
    },
    {
      "epoch": 31.73419773095624,
      "grad_norm": 0.7816935181617737,
      "learning_rate": 4.685844956211483e-05,
      "loss": 3.215,
      "step": 97900
    },
    {
      "epoch": 31.766612641815236,
      "grad_norm": 0.8640094995498657,
      "learning_rate": 4.6855205968212786e-05,
      "loss": 3.2076,
      "step": 98000
    },
    {
      "epoch": 31.79902755267423,
      "grad_norm": 0.8532865047454834,
      "learning_rate": 4.685196237431074e-05,
      "loss": 3.2148,
      "step": 98100
    },
    {
      "epoch": 31.831442463533225,
      "grad_norm": 0.7971879243850708,
      "learning_rate": 4.68487187804087e-05,
      "loss": 3.2133,
      "step": 98200
    },
    {
      "epoch": 31.86385737439222,
      "grad_norm": 0.965357780456543,
      "learning_rate": 4.6845475186506656e-05,
      "loss": 3.1915,
      "step": 98300
    },
    {
      "epoch": 31.896272285251214,
      "grad_norm": 0.9404030442237854,
      "learning_rate": 4.684223159260461e-05,
      "loss": 3.2366,
      "step": 98400
    },
    {
      "epoch": 31.92868719611021,
      "grad_norm": 0.8669416904449463,
      "learning_rate": 4.6838987998702566e-05,
      "loss": 3.1912,
      "step": 98500
    },
    {
      "epoch": 31.961102106969207,
      "grad_norm": 0.9696821570396423,
      "learning_rate": 4.683574440480052e-05,
      "loss": 3.2218,
      "step": 98600
    },
    {
      "epoch": 31.9935170178282,
      "grad_norm": 0.9548887014389038,
      "learning_rate": 4.683250081089848e-05,
      "loss": 3.192,
      "step": 98700
    },
    {
      "epoch": 32.0,
      "eval_bleu": 1.039920174071523,
      "eval_loss": 3.718536853790283,
      "eval_runtime": 4.5374,
      "eval_samples_per_second": 108.433,
      "eval_steps_per_second": 1.763,
      "step": 98720
    },
    {
      "epoch": 32.02593192868719,
      "grad_norm": 0.9219508767127991,
      "learning_rate": 4.6829257216996436e-05,
      "loss": 3.23,
      "step": 98800
    },
    {
      "epoch": 32.05834683954619,
      "grad_norm": 0.7699658274650574,
      "learning_rate": 4.682601362309439e-05,
      "loss": 3.1939,
      "step": 98900
    },
    {
      "epoch": 32.090761750405186,
      "grad_norm": 1.0319527387619019,
      "learning_rate": 4.6822770029192346e-05,
      "loss": 3.2038,
      "step": 99000
    },
    {
      "epoch": 32.12317666126418,
      "grad_norm": 0.8969290852546692,
      "learning_rate": 4.6819526435290305e-05,
      "loss": 3.1965,
      "step": 99100
    },
    {
      "epoch": 32.15559157212318,
      "grad_norm": 0.9121366143226624,
      "learning_rate": 4.681628284138826e-05,
      "loss": 3.1915,
      "step": 99200
    },
    {
      "epoch": 32.188006482982175,
      "grad_norm": 0.8702734112739563,
      "learning_rate": 4.6813039247486216e-05,
      "loss": 3.1807,
      "step": 99300
    },
    {
      "epoch": 32.220421393841164,
      "grad_norm": 0.8458595871925354,
      "learning_rate": 4.680979565358417e-05,
      "loss": 3.2061,
      "step": 99400
    },
    {
      "epoch": 32.25283630470016,
      "grad_norm": 0.9202053546905518,
      "learning_rate": 4.6806552059682127e-05,
      "loss": 3.202,
      "step": 99500
    },
    {
      "epoch": 32.28525121555916,
      "grad_norm": 0.8985304832458496,
      "learning_rate": 4.6803308465780085e-05,
      "loss": 3.1888,
      "step": 99600
    },
    {
      "epoch": 32.31766612641815,
      "grad_norm": 0.9207409620285034,
      "learning_rate": 4.6800064871878044e-05,
      "loss": 3.1935,
      "step": 99700
    },
    {
      "epoch": 32.35008103727715,
      "grad_norm": 0.9339014291763306,
      "learning_rate": 4.6796821277976e-05,
      "loss": 3.1897,
      "step": 99800
    },
    {
      "epoch": 32.382495948136146,
      "grad_norm": 0.8218439817428589,
      "learning_rate": 4.679357768407396e-05,
      "loss": 3.1946,
      "step": 99900
    },
    {
      "epoch": 32.414910858995135,
      "grad_norm": 0.8805781006813049,
      "learning_rate": 4.6790334090171913e-05,
      "loss": 3.1876,
      "step": 100000
    },
    {
      "epoch": 32.44732576985413,
      "grad_norm": 0.8465056419372559,
      "learning_rate": 4.678709049626987e-05,
      "loss": 3.2054,
      "step": 100100
    },
    {
      "epoch": 32.47974068071313,
      "grad_norm": 0.8700856566429138,
      "learning_rate": 4.678384690236783e-05,
      "loss": 3.1968,
      "step": 100200
    },
    {
      "epoch": 32.512155591572125,
      "grad_norm": 0.8809384107589722,
      "learning_rate": 4.678060330846578e-05,
      "loss": 3.2232,
      "step": 100300
    },
    {
      "epoch": 32.54457050243112,
      "grad_norm": 0.9054728746414185,
      "learning_rate": 4.677735971456374e-05,
      "loss": 3.2031,
      "step": 100400
    },
    {
      "epoch": 32.57698541329011,
      "grad_norm": 0.980101466178894,
      "learning_rate": 4.6774116120661694e-05,
      "loss": 3.1988,
      "step": 100500
    },
    {
      "epoch": 32.60940032414911,
      "grad_norm": 0.9070025682449341,
      "learning_rate": 4.677087252675965e-05,
      "loss": 3.2061,
      "step": 100600
    },
    {
      "epoch": 32.6418152350081,
      "grad_norm": 0.9876022338867188,
      "learning_rate": 4.676762893285761e-05,
      "loss": 3.1936,
      "step": 100700
    },
    {
      "epoch": 32.6742301458671,
      "grad_norm": 0.9292892813682556,
      "learning_rate": 4.676438533895556e-05,
      "loss": 3.1939,
      "step": 100800
    },
    {
      "epoch": 32.706645056726096,
      "grad_norm": 1.0896451473236084,
      "learning_rate": 4.676114174505352e-05,
      "loss": 3.2067,
      "step": 100900
    },
    {
      "epoch": 32.73905996758509,
      "grad_norm": 0.8370418548583984,
      "learning_rate": 4.675789815115148e-05,
      "loss": 3.1983,
      "step": 101000
    },
    {
      "epoch": 32.77147487844408,
      "grad_norm": 0.9945325255393982,
      "learning_rate": 4.675465455724943e-05,
      "loss": 3.2005,
      "step": 101100
    },
    {
      "epoch": 32.80388978930308,
      "grad_norm": 0.8277724385261536,
      "learning_rate": 4.675141096334739e-05,
      "loss": 3.2054,
      "step": 101200
    },
    {
      "epoch": 32.836304700162074,
      "grad_norm": 0.8415814638137817,
      "learning_rate": 4.674816736944535e-05,
      "loss": 3.2016,
      "step": 101300
    },
    {
      "epoch": 32.86871961102107,
      "grad_norm": 0.7820727825164795,
      "learning_rate": 4.67449237755433e-05,
      "loss": 3.2141,
      "step": 101400
    },
    {
      "epoch": 32.90113452188007,
      "grad_norm": 0.8547714948654175,
      "learning_rate": 4.674168018164126e-05,
      "loss": 3.2348,
      "step": 101500
    },
    {
      "epoch": 32.93354943273906,
      "grad_norm": 0.8712201714515686,
      "learning_rate": 4.673843658773921e-05,
      "loss": 3.2147,
      "step": 101600
    },
    {
      "epoch": 32.96596434359805,
      "grad_norm": 0.8755193948745728,
      "learning_rate": 4.673519299383717e-05,
      "loss": 3.2031,
      "step": 101700
    },
    {
      "epoch": 32.99837925445705,
      "grad_norm": 1.0285942554473877,
      "learning_rate": 4.673198183587415e-05,
      "loss": 3.2101,
      "step": 101800
    },
    {
      "epoch": 33.0,
      "eval_bleu": 1.0968360674256468,
      "eval_loss": 3.722261428833008,
      "eval_runtime": 4.7657,
      "eval_samples_per_second": 103.238,
      "eval_steps_per_second": 1.679,
      "step": 101805
    },
    {
      "epoch": 33.030794165316046,
      "grad_norm": 0.7800317406654358,
      "learning_rate": 4.672873824197211e-05,
      "loss": 3.1935,
      "step": 101900
    },
    {
      "epoch": 33.06320907617504,
      "grad_norm": 0.8956158757209778,
      "learning_rate": 4.672549464807006e-05,
      "loss": 3.2043,
      "step": 102000
    },
    {
      "epoch": 33.09562398703404,
      "grad_norm": 0.8137602806091309,
      "learning_rate": 4.672225105416802e-05,
      "loss": 3.1623,
      "step": 102100
    },
    {
      "epoch": 33.12803889789303,
      "grad_norm": 0.9699781537055969,
      "learning_rate": 4.671900746026598e-05,
      "loss": 3.1906,
      "step": 102200
    },
    {
      "epoch": 33.160453808752024,
      "grad_norm": 1.0317703485488892,
      "learning_rate": 4.671576386636393e-05,
      "loss": 3.2022,
      "step": 102300
    },
    {
      "epoch": 33.19286871961102,
      "grad_norm": 1.0089045763015747,
      "learning_rate": 4.671252027246189e-05,
      "loss": 3.1933,
      "step": 102400
    },
    {
      "epoch": 33.22528363047002,
      "grad_norm": 0.843778133392334,
      "learning_rate": 4.670927667855985e-05,
      "loss": 3.1981,
      "step": 102500
    },
    {
      "epoch": 33.25769854132901,
      "grad_norm": 0.8670610189437866,
      "learning_rate": 4.67060330846578e-05,
      "loss": 3.1848,
      "step": 102600
    },
    {
      "epoch": 33.29011345218801,
      "grad_norm": 0.8825877904891968,
      "learning_rate": 4.670278949075576e-05,
      "loss": 3.1863,
      "step": 102700
    },
    {
      "epoch": 33.322528363047,
      "grad_norm": 1.0116809606552124,
      "learning_rate": 4.6699545896853716e-05,
      "loss": 3.1995,
      "step": 102800
    },
    {
      "epoch": 33.354943273905995,
      "grad_norm": 0.8166382312774658,
      "learning_rate": 4.6696302302951675e-05,
      "loss": 3.194,
      "step": 102900
    },
    {
      "epoch": 33.38735818476499,
      "grad_norm": 0.876072347164154,
      "learning_rate": 4.6693058709049634e-05,
      "loss": 3.1959,
      "step": 103000
    },
    {
      "epoch": 33.41977309562399,
      "grad_norm": 0.9637598991394043,
      "learning_rate": 4.6689815115147586e-05,
      "loss": 3.2152,
      "step": 103100
    },
    {
      "epoch": 33.452188006482984,
      "grad_norm": 0.9068995118141174,
      "learning_rate": 4.668663639312358e-05,
      "loss": 3.1907,
      "step": 103200
    },
    {
      "epoch": 33.48460291734198,
      "grad_norm": 0.9221773147583008,
      "learning_rate": 4.668342523516056e-05,
      "loss": 3.2087,
      "step": 103300
    },
    {
      "epoch": 33.51701782820097,
      "grad_norm": 0.9367987513542175,
      "learning_rate": 4.668018164125852e-05,
      "loss": 3.2072,
      "step": 103400
    },
    {
      "epoch": 33.54943273905997,
      "grad_norm": 0.8639792203903198,
      "learning_rate": 4.667693804735647e-05,
      "loss": 3.1885,
      "step": 103500
    },
    {
      "epoch": 33.58184764991896,
      "grad_norm": 0.9860755205154419,
      "learning_rate": 4.667369445345443e-05,
      "loss": 3.1888,
      "step": 103600
    },
    {
      "epoch": 33.61426256077796,
      "grad_norm": 0.8923941850662231,
      "learning_rate": 4.667045085955238e-05,
      "loss": 3.198,
      "step": 103700
    },
    {
      "epoch": 33.646677471636956,
      "grad_norm": 0.905695915222168,
      "learning_rate": 4.666720726565034e-05,
      "loss": 3.1742,
      "step": 103800
    },
    {
      "epoch": 33.679092382495945,
      "grad_norm": 0.8533588647842407,
      "learning_rate": 4.66639636717483e-05,
      "loss": 3.2165,
      "step": 103900
    },
    {
      "epoch": 33.71150729335494,
      "grad_norm": 1.045547366142273,
      "learning_rate": 4.666072007784625e-05,
      "loss": 3.1787,
      "step": 104000
    },
    {
      "epoch": 33.74392220421394,
      "grad_norm": 0.9620909690856934,
      "learning_rate": 4.665747648394421e-05,
      "loss": 3.1869,
      "step": 104100
    },
    {
      "epoch": 33.776337115072934,
      "grad_norm": 0.9421836733818054,
      "learning_rate": 4.665423289004217e-05,
      "loss": 3.2012,
      "step": 104200
    },
    {
      "epoch": 33.80875202593193,
      "grad_norm": 0.8454378247261047,
      "learning_rate": 4.665098929614012e-05,
      "loss": 3.2151,
      "step": 104300
    },
    {
      "epoch": 33.84116693679093,
      "grad_norm": 0.8711632490158081,
      "learning_rate": 4.664774570223808e-05,
      "loss": 3.1989,
      "step": 104400
    },
    {
      "epoch": 33.873581847649916,
      "grad_norm": 1.0074613094329834,
      "learning_rate": 4.664450210833604e-05,
      "loss": 3.182,
      "step": 104500
    },
    {
      "epoch": 33.90599675850891,
      "grad_norm": 0.9877121448516846,
      "learning_rate": 4.6641258514434e-05,
      "loss": 3.1884,
      "step": 104600
    },
    {
      "epoch": 33.93841166936791,
      "grad_norm": 0.8752614259719849,
      "learning_rate": 4.6638014920531956e-05,
      "loss": 3.1957,
      "step": 104700
    },
    {
      "epoch": 33.970826580226905,
      "grad_norm": 0.9652555584907532,
      "learning_rate": 4.663477132662991e-05,
      "loss": 3.2178,
      "step": 104800
    },
    {
      "epoch": 34.0,
      "eval_bleu": 0.9388188905439638,
      "eval_loss": 3.7274720668792725,
      "eval_runtime": 4.5169,
      "eval_samples_per_second": 108.924,
      "eval_steps_per_second": 1.771,
      "step": 104890
    },
    {
      "epoch": 34.0032414910859,
      "grad_norm": 0.8019259572029114,
      "learning_rate": 4.6631527732727866e-05,
      "loss": 3.1975,
      "step": 104900
    },
    {
      "epoch": 34.0356564019449,
      "grad_norm": 0.9212105870246887,
      "learning_rate": 4.6628284138825825e-05,
      "loss": 3.1752,
      "step": 105000
    },
    {
      "epoch": 34.06807131280389,
      "grad_norm": 0.9145721793174744,
      "learning_rate": 4.662504054492378e-05,
      "loss": 3.1795,
      "step": 105100
    },
    {
      "epoch": 34.100486223662884,
      "grad_norm": 1.1005901098251343,
      "learning_rate": 4.6621796951021736e-05,
      "loss": 3.1863,
      "step": 105200
    },
    {
      "epoch": 34.13290113452188,
      "grad_norm": 0.9118809700012207,
      "learning_rate": 4.6618585793058714e-05,
      "loss": 3.1949,
      "step": 105300
    },
    {
      "epoch": 34.16531604538088,
      "grad_norm": 1.0129868984222412,
      "learning_rate": 4.661534219915667e-05,
      "loss": 3.1917,
      "step": 105400
    },
    {
      "epoch": 34.19773095623987,
      "grad_norm": 0.9981558918952942,
      "learning_rate": 4.6612098605254624e-05,
      "loss": 3.1851,
      "step": 105500
    },
    {
      "epoch": 34.23014586709886,
      "grad_norm": 1.012673020362854,
      "learning_rate": 4.660885501135258e-05,
      "loss": 3.1679,
      "step": 105600
    },
    {
      "epoch": 34.26256077795786,
      "grad_norm": 0.9064047932624817,
      "learning_rate": 4.660561141745054e-05,
      "loss": 3.1959,
      "step": 105700
    },
    {
      "epoch": 34.294975688816855,
      "grad_norm": 0.873271107673645,
      "learning_rate": 4.6602367823548494e-05,
      "loss": 3.1854,
      "step": 105800
    },
    {
      "epoch": 34.32739059967585,
      "grad_norm": 0.8083904385566711,
      "learning_rate": 4.659912422964645e-05,
      "loss": 3.1858,
      "step": 105900
    },
    {
      "epoch": 34.35980551053485,
      "grad_norm": 0.8278627395629883,
      "learning_rate": 4.6595880635744405e-05,
      "loss": 3.1847,
      "step": 106000
    },
    {
      "epoch": 34.392220421393844,
      "grad_norm": 0.9548860192298889,
      "learning_rate": 4.659266947778139e-05,
      "loss": 3.2063,
      "step": 106100
    },
    {
      "epoch": 34.424635332252834,
      "grad_norm": 1.058962345123291,
      "learning_rate": 4.658945831981836e-05,
      "loss": 3.2031,
      "step": 106200
    },
    {
      "epoch": 34.45705024311183,
      "grad_norm": 0.9175042510032654,
      "learning_rate": 4.658621472591632e-05,
      "loss": 3.2078,
      "step": 106300
    },
    {
      "epoch": 34.489465153970826,
      "grad_norm": 1.0057713985443115,
      "learning_rate": 4.658297113201428e-05,
      "loss": 3.1923,
      "step": 106400
    },
    {
      "epoch": 34.52188006482982,
      "grad_norm": 0.8823800086975098,
      "learning_rate": 4.6579727538112236e-05,
      "loss": 3.1924,
      "step": 106500
    },
    {
      "epoch": 34.55429497568882,
      "grad_norm": 0.7858763337135315,
      "learning_rate": 4.657648394421019e-05,
      "loss": 3.1802,
      "step": 106600
    },
    {
      "epoch": 34.58670988654781,
      "grad_norm": 1.0265415906906128,
      "learning_rate": 4.657324035030815e-05,
      "loss": 3.1954,
      "step": 106700
    },
    {
      "epoch": 34.619124797406805,
      "grad_norm": 0.895444929599762,
      "learning_rate": 4.65699967564061e-05,
      "loss": 3.1942,
      "step": 106800
    },
    {
      "epoch": 34.6515397082658,
      "grad_norm": 0.942039430141449,
      "learning_rate": 4.656675316250406e-05,
      "loss": 3.1834,
      "step": 106900
    },
    {
      "epoch": 34.6839546191248,
      "grad_norm": 0.8262958526611328,
      "learning_rate": 4.6563509568602017e-05,
      "loss": 3.19,
      "step": 107000
    },
    {
      "epoch": 34.716369529983794,
      "grad_norm": 0.8327507376670837,
      "learning_rate": 4.656026597469997e-05,
      "loss": 3.2038,
      "step": 107100
    },
    {
      "epoch": 34.74878444084279,
      "grad_norm": 1.1137975454330444,
      "learning_rate": 4.655702238079793e-05,
      "loss": 3.1742,
      "step": 107200
    },
    {
      "epoch": 34.78119935170178,
      "grad_norm": 0.8229698538780212,
      "learning_rate": 4.6553778786895886e-05,
      "loss": 3.1955,
      "step": 107300
    },
    {
      "epoch": 34.813614262560776,
      "grad_norm": 0.878894567489624,
      "learning_rate": 4.655053519299384e-05,
      "loss": 3.1913,
      "step": 107400
    },
    {
      "epoch": 34.84602917341977,
      "grad_norm": 1.0076828002929688,
      "learning_rate": 4.65472915990918e-05,
      "loss": 3.1719,
      "step": 107500
    },
    {
      "epoch": 34.87844408427877,
      "grad_norm": 0.851231575012207,
      "learning_rate": 4.654404800518975e-05,
      "loss": 3.2083,
      "step": 107600
    },
    {
      "epoch": 34.910858995137765,
      "grad_norm": 0.9445328116416931,
      "learning_rate": 4.654080441128771e-05,
      "loss": 3.2006,
      "step": 107700
    },
    {
      "epoch": 34.94327390599676,
      "grad_norm": 0.9421026706695557,
      "learning_rate": 4.6537560817385666e-05,
      "loss": 3.1965,
      "step": 107800
    },
    {
      "epoch": 34.97568881685575,
      "grad_norm": 0.9222015142440796,
      "learning_rate": 4.653431722348362e-05,
      "loss": 3.1888,
      "step": 107900
    },
    {
      "epoch": 35.0,
      "eval_bleu": 0.9453184352973777,
      "eval_loss": 3.725027322769165,
      "eval_runtime": 4.6813,
      "eval_samples_per_second": 105.1,
      "eval_steps_per_second": 1.709,
      "step": 107975
    },
    {
      "epoch": 35.00810372771475,
      "grad_norm": 1.0478609800338745,
      "learning_rate": 4.653107362958158e-05,
      "loss": 3.2016,
      "step": 108000
    },
    {
      "epoch": 35.040518638573744,
      "grad_norm": 0.897853434085846,
      "learning_rate": 4.6527830035679536e-05,
      "loss": 3.1917,
      "step": 108100
    },
    {
      "epoch": 35.07293354943274,
      "grad_norm": 0.8081399202346802,
      "learning_rate": 4.652458644177749e-05,
      "loss": 3.1762,
      "step": 108200
    },
    {
      "epoch": 35.10534846029174,
      "grad_norm": 0.9911915063858032,
      "learning_rate": 4.6521342847875446e-05,
      "loss": 3.1802,
      "step": 108300
    },
    {
      "epoch": 35.137763371150726,
      "grad_norm": 0.9403888583183289,
      "learning_rate": 4.6518099253973405e-05,
      "loss": 3.1769,
      "step": 108400
    },
    {
      "epoch": 35.17017828200972,
      "grad_norm": 1.0647908449172974,
      "learning_rate": 4.651485566007136e-05,
      "loss": 3.1891,
      "step": 108500
    },
    {
      "epoch": 35.20259319286872,
      "grad_norm": 1.1439307928085327,
      "learning_rate": 4.6511612066169316e-05,
      "loss": 3.1984,
      "step": 108600
    },
    {
      "epoch": 35.235008103727715,
      "grad_norm": 0.8809279799461365,
      "learning_rate": 4.6508368472267275e-05,
      "loss": 3.1917,
      "step": 108700
    },
    {
      "epoch": 35.26742301458671,
      "grad_norm": 0.8126108050346375,
      "learning_rate": 4.650512487836523e-05,
      "loss": 3.1806,
      "step": 108800
    },
    {
      "epoch": 35.29983792544571,
      "grad_norm": 0.8341014981269836,
      "learning_rate": 4.650188128446319e-05,
      "loss": 3.1875,
      "step": 108900
    },
    {
      "epoch": 35.3322528363047,
      "grad_norm": 0.8766995072364807,
      "learning_rate": 4.6498637690561144e-05,
      "loss": 3.1847,
      "step": 109000
    },
    {
      "epoch": 35.36466774716369,
      "grad_norm": 0.83847975730896,
      "learning_rate": 4.64953940966591e-05,
      "loss": 3.1926,
      "step": 109100
    },
    {
      "epoch": 35.39708265802269,
      "grad_norm": 0.9519503712654114,
      "learning_rate": 4.649215050275706e-05,
      "loss": 3.1719,
      "step": 109200
    },
    {
      "epoch": 35.429497568881686,
      "grad_norm": 0.8990920186042786,
      "learning_rate": 4.6488906908855013e-05,
      "loss": 3.2037,
      "step": 109300
    },
    {
      "epoch": 35.46191247974068,
      "grad_norm": 0.9153710603713989,
      "learning_rate": 4.648566331495297e-05,
      "loss": 3.1885,
      "step": 109400
    },
    {
      "epoch": 35.49432739059968,
      "grad_norm": 0.9574485421180725,
      "learning_rate": 4.648241972105093e-05,
      "loss": 3.182,
      "step": 109500
    },
    {
      "epoch": 35.52674230145867,
      "grad_norm": 0.9595448970794678,
      "learning_rate": 4.647917612714888e-05,
      "loss": 3.1912,
      "step": 109600
    },
    {
      "epoch": 35.559157212317665,
      "grad_norm": 0.9367234110832214,
      "learning_rate": 4.647593253324684e-05,
      "loss": 3.1783,
      "step": 109700
    },
    {
      "epoch": 35.59157212317666,
      "grad_norm": 0.8334375023841858,
      "learning_rate": 4.6472688939344794e-05,
      "loss": 3.1527,
      "step": 109800
    },
    {
      "epoch": 35.62398703403566,
      "grad_norm": 0.9186452031135559,
      "learning_rate": 4.646944534544275e-05,
      "loss": 3.181,
      "step": 109900
    },
    {
      "epoch": 35.656401944894654,
      "grad_norm": 0.9090968370437622,
      "learning_rate": 4.646620175154071e-05,
      "loss": 3.1897,
      "step": 110000
    },
    {
      "epoch": 35.68881685575364,
      "grad_norm": 0.934284508228302,
      "learning_rate": 4.646295815763866e-05,
      "loss": 3.1905,
      "step": 110100
    },
    {
      "epoch": 35.72123176661264,
      "grad_norm": 0.8335790634155273,
      "learning_rate": 4.645971456373662e-05,
      "loss": 3.1872,
      "step": 110200
    },
    {
      "epoch": 35.753646677471636,
      "grad_norm": 0.9974843859672546,
      "learning_rate": 4.645647096983458e-05,
      "loss": 3.1825,
      "step": 110300
    },
    {
      "epoch": 35.78606158833063,
      "grad_norm": 0.9354469180107117,
      "learning_rate": 4.645322737593253e-05,
      "loss": 3.1645,
      "step": 110400
    },
    {
      "epoch": 35.81847649918963,
      "grad_norm": 0.9747728109359741,
      "learning_rate": 4.644998378203049e-05,
      "loss": 3.1787,
      "step": 110500
    },
    {
      "epoch": 35.850891410048625,
      "grad_norm": 0.7990772724151611,
      "learning_rate": 4.644674018812844e-05,
      "loss": 3.1839,
      "step": 110600
    },
    {
      "epoch": 35.883306320907614,
      "grad_norm": 0.8656535744667053,
      "learning_rate": 4.64434965942264e-05,
      "loss": 3.1914,
      "step": 110700
    },
    {
      "epoch": 35.91572123176661,
      "grad_norm": 0.9005745649337769,
      "learning_rate": 4.644025300032436e-05,
      "loss": 3.1845,
      "step": 110800
    },
    {
      "epoch": 35.94813614262561,
      "grad_norm": 0.8498059511184692,
      "learning_rate": 4.643700940642231e-05,
      "loss": 3.1928,
      "step": 110900
    },
    {
      "epoch": 35.980551053484604,
      "grad_norm": 1.0529240369796753,
      "learning_rate": 4.643376581252027e-05,
      "loss": 3.1835,
      "step": 111000
    },
    {
      "epoch": 36.0,
      "eval_bleu": 1.106127292779125,
      "eval_loss": 3.7246158123016357,
      "eval_runtime": 4.5746,
      "eval_samples_per_second": 107.551,
      "eval_steps_per_second": 1.749,
      "step": 111060
    },
    {
      "epoch": 36.0129659643436,
      "grad_norm": 0.9090566039085388,
      "learning_rate": 4.643052221861823e-05,
      "loss": 3.1735,
      "step": 111100
    },
    {
      "epoch": 36.045380875202596,
      "grad_norm": 1.0108298063278198,
      "learning_rate": 4.642727862471619e-05,
      "loss": 3.1741,
      "step": 111200
    },
    {
      "epoch": 36.077795786061586,
      "grad_norm": 1.2153499126434326,
      "learning_rate": 4.642403503081415e-05,
      "loss": 3.1561,
      "step": 111300
    },
    {
      "epoch": 36.11021069692058,
      "grad_norm": 0.9361422061920166,
      "learning_rate": 4.6420791436912106e-05,
      "loss": 3.1659,
      "step": 111400
    },
    {
      "epoch": 36.14262560777958,
      "grad_norm": 0.8187971711158752,
      "learning_rate": 4.641754784301006e-05,
      "loss": 3.1731,
      "step": 111500
    },
    {
      "epoch": 36.175040518638575,
      "grad_norm": 0.860126256942749,
      "learning_rate": 4.641430424910802e-05,
      "loss": 3.164,
      "step": 111600
    },
    {
      "epoch": 36.20745542949757,
      "grad_norm": 0.8220388293266296,
      "learning_rate": 4.641106065520597e-05,
      "loss": 3.1725,
      "step": 111700
    },
    {
      "epoch": 36.23987034035656,
      "grad_norm": 0.9482901096343994,
      "learning_rate": 4.640781706130393e-05,
      "loss": 3.1784,
      "step": 111800
    },
    {
      "epoch": 36.27228525121556,
      "grad_norm": 0.9371810555458069,
      "learning_rate": 4.6404573467401887e-05,
      "loss": 3.1986,
      "step": 111900
    },
    {
      "epoch": 36.30470016207455,
      "grad_norm": 0.9338274598121643,
      "learning_rate": 4.640132987349984e-05,
      "loss": 3.194,
      "step": 112000
    },
    {
      "epoch": 36.33711507293355,
      "grad_norm": 0.9595882892608643,
      "learning_rate": 4.63980862795978e-05,
      "loss": 3.1734,
      "step": 112100
    },
    {
      "epoch": 36.369529983792546,
      "grad_norm": 1.0485495328903198,
      "learning_rate": 4.6394842685695756e-05,
      "loss": 3.1719,
      "step": 112200
    },
    {
      "epoch": 36.40194489465154,
      "grad_norm": 0.9546681642532349,
      "learning_rate": 4.639159909179371e-05,
      "loss": 3.1788,
      "step": 112300
    },
    {
      "epoch": 36.43435980551053,
      "grad_norm": 0.8361537456512451,
      "learning_rate": 4.638835549789167e-05,
      "loss": 3.1829,
      "step": 112400
    },
    {
      "epoch": 36.46677471636953,
      "grad_norm": 0.9260414242744446,
      "learning_rate": 4.6385111903989625e-05,
      "loss": 3.1757,
      "step": 112500
    },
    {
      "epoch": 36.499189627228525,
      "grad_norm": 0.8211867213249207,
      "learning_rate": 4.638186831008758e-05,
      "loss": 3.1658,
      "step": 112600
    },
    {
      "epoch": 36.53160453808752,
      "grad_norm": 0.7724260687828064,
      "learning_rate": 4.6378657152124555e-05,
      "loss": 3.1636,
      "step": 112700
    },
    {
      "epoch": 36.56401944894652,
      "grad_norm": 0.9775712490081787,
      "learning_rate": 4.6375413558222514e-05,
      "loss": 3.1657,
      "step": 112800
    },
    {
      "epoch": 36.596434359805514,
      "grad_norm": 0.8928866386413574,
      "learning_rate": 4.6372169964320466e-05,
      "loss": 3.1788,
      "step": 112900
    },
    {
      "epoch": 36.6288492706645,
      "grad_norm": 0.8869342803955078,
      "learning_rate": 4.6368926370418425e-05,
      "loss": 3.1806,
      "step": 113000
    },
    {
      "epoch": 36.6612641815235,
      "grad_norm": 0.9338503479957581,
      "learning_rate": 4.6365682776516383e-05,
      "loss": 3.1947,
      "step": 113100
    },
    {
      "epoch": 36.693679092382496,
      "grad_norm": 1.1796499490737915,
      "learning_rate": 4.6362439182614335e-05,
      "loss": 3.182,
      "step": 113200
    },
    {
      "epoch": 36.72609400324149,
      "grad_norm": 0.8488435745239258,
      "learning_rate": 4.6359195588712294e-05,
      "loss": 3.1655,
      "step": 113300
    },
    {
      "epoch": 36.75850891410049,
      "grad_norm": 0.7764959931373596,
      "learning_rate": 4.635595199481025e-05,
      "loss": 3.1869,
      "step": 113400
    },
    {
      "epoch": 36.79092382495948,
      "grad_norm": 0.8976495265960693,
      "learning_rate": 4.6352708400908205e-05,
      "loss": 3.1751,
      "step": 113500
    },
    {
      "epoch": 36.823338735818474,
      "grad_norm": 0.9037179350852966,
      "learning_rate": 4.6349464807006164e-05,
      "loss": 3.1873,
      "step": 113600
    },
    {
      "epoch": 36.85575364667747,
      "grad_norm": 0.8857465386390686,
      "learning_rate": 4.634622121310412e-05,
      "loss": 3.1946,
      "step": 113700
    },
    {
      "epoch": 36.88816855753647,
      "grad_norm": 1.0443952083587646,
      "learning_rate": 4.6342977619202074e-05,
      "loss": 3.1912,
      "step": 113800
    },
    {
      "epoch": 36.92058346839546,
      "grad_norm": 0.8300617337226868,
      "learning_rate": 4.633973402530003e-05,
      "loss": 3.177,
      "step": 113900
    },
    {
      "epoch": 36.95299837925446,
      "grad_norm": 1.0912394523620605,
      "learning_rate": 4.633649043139799e-05,
      "loss": 3.196,
      "step": 114000
    },
    {
      "epoch": 36.98541329011345,
      "grad_norm": 0.9284029006958008,
      "learning_rate": 4.633324683749595e-05,
      "loss": 3.181,
      "step": 114100
    },
    {
      "epoch": 37.0,
      "eval_bleu": 1.1194100051281957,
      "eval_loss": 3.7273097038269043,
      "eval_runtime": 4.8956,
      "eval_samples_per_second": 100.499,
      "eval_steps_per_second": 1.634,
      "step": 114145
    },
    {
      "epoch": 37.017828200972446,
      "grad_norm": 0.9788601994514465,
      "learning_rate": 4.63300032435939e-05,
      "loss": 3.1866,
      "step": 114200
    },
    {
      "epoch": 37.05024311183144,
      "grad_norm": 0.8127401471138,
      "learning_rate": 4.632675964969186e-05,
      "loss": 3.1734,
      "step": 114300
    },
    {
      "epoch": 37.08265802269044,
      "grad_norm": 1.0080010890960693,
      "learning_rate": 4.632351605578982e-05,
      "loss": 3.1631,
      "step": 114400
    },
    {
      "epoch": 37.115072933549435,
      "grad_norm": 1.0841151475906372,
      "learning_rate": 4.632027246188778e-05,
      "loss": 3.1801,
      "step": 114500
    },
    {
      "epoch": 37.14748784440843,
      "grad_norm": 0.9313936233520508,
      "learning_rate": 4.631706130392475e-05,
      "loss": 3.1671,
      "step": 114600
    },
    {
      "epoch": 37.17990275526742,
      "grad_norm": 1.028465986251831,
      "learning_rate": 4.631381771002271e-05,
      "loss": 3.1712,
      "step": 114700
    },
    {
      "epoch": 37.21231766612642,
      "grad_norm": 0.9576243758201599,
      "learning_rate": 4.631057411612067e-05,
      "loss": 3.1629,
      "step": 114800
    },
    {
      "epoch": 37.24473257698541,
      "grad_norm": 0.834165096282959,
      "learning_rate": 4.6307330522218626e-05,
      "loss": 3.1558,
      "step": 114900
    },
    {
      "epoch": 37.27714748784441,
      "grad_norm": 0.8748229742050171,
      "learning_rate": 4.630408692831658e-05,
      "loss": 3.1547,
      "step": 115000
    },
    {
      "epoch": 37.309562398703406,
      "grad_norm": 0.812533438205719,
      "learning_rate": 4.630084333441454e-05,
      "loss": 3.173,
      "step": 115100
    },
    {
      "epoch": 37.341977309562395,
      "grad_norm": 0.8992028832435608,
      "learning_rate": 4.6297599740512495e-05,
      "loss": 3.1745,
      "step": 115200
    },
    {
      "epoch": 37.37439222042139,
      "grad_norm": 0.9124386310577393,
      "learning_rate": 4.629435614661045e-05,
      "loss": 3.1832,
      "step": 115300
    },
    {
      "epoch": 37.40680713128039,
      "grad_norm": 0.9429936408996582,
      "learning_rate": 4.6291112552708406e-05,
      "loss": 3.1899,
      "step": 115400
    },
    {
      "epoch": 37.439222042139384,
      "grad_norm": 0.8015742301940918,
      "learning_rate": 4.628786895880636e-05,
      "loss": 3.1919,
      "step": 115500
    },
    {
      "epoch": 37.47163695299838,
      "grad_norm": 0.9371631741523743,
      "learning_rate": 4.628462536490432e-05,
      "loss": 3.1737,
      "step": 115600
    },
    {
      "epoch": 37.50405186385738,
      "grad_norm": 0.8266986608505249,
      "learning_rate": 4.6281381771002276e-05,
      "loss": 3.156,
      "step": 115700
    },
    {
      "epoch": 37.53646677471637,
      "grad_norm": 1.0008533000946045,
      "learning_rate": 4.627813817710023e-05,
      "loss": 3.1735,
      "step": 115800
    },
    {
      "epoch": 37.56888168557536,
      "grad_norm": 0.9163750410079956,
      "learning_rate": 4.6274894583198186e-05,
      "loss": 3.1726,
      "step": 115900
    },
    {
      "epoch": 37.60129659643436,
      "grad_norm": 0.8883119225502014,
      "learning_rate": 4.6271650989296145e-05,
      "loss": 3.1727,
      "step": 116000
    },
    {
      "epoch": 37.633711507293356,
      "grad_norm": 0.9525693655014038,
      "learning_rate": 4.62684073953941e-05,
      "loss": 3.1743,
      "step": 116100
    },
    {
      "epoch": 37.66612641815235,
      "grad_norm": 1.0676506757736206,
      "learning_rate": 4.6265163801492056e-05,
      "loss": 3.1668,
      "step": 116200
    },
    {
      "epoch": 37.69854132901135,
      "grad_norm": 0.8236426115036011,
      "learning_rate": 4.626192020759001e-05,
      "loss": 3.171,
      "step": 116300
    },
    {
      "epoch": 37.73095623987034,
      "grad_norm": 0.9773117899894714,
      "learning_rate": 4.6258676613687966e-05,
      "loss": 3.1785,
      "step": 116400
    },
    {
      "epoch": 37.763371150729334,
      "grad_norm": 1.1350595951080322,
      "learning_rate": 4.6255433019785925e-05,
      "loss": 3.1882,
      "step": 116500
    },
    {
      "epoch": 37.79578606158833,
      "grad_norm": 1.0719459056854248,
      "learning_rate": 4.625218942588388e-05,
      "loss": 3.1705,
      "step": 116600
    },
    {
      "epoch": 37.82820097244733,
      "grad_norm": 0.9174105525016785,
      "learning_rate": 4.6248945831981836e-05,
      "loss": 3.1848,
      "step": 116700
    },
    {
      "epoch": 37.86061588330632,
      "grad_norm": 0.867039144039154,
      "learning_rate": 4.6245702238079795e-05,
      "loss": 3.1713,
      "step": 116800
    },
    {
      "epoch": 37.89303079416531,
      "grad_norm": 0.9754366278648376,
      "learning_rate": 4.624245864417775e-05,
      "loss": 3.1845,
      "step": 116900
    },
    {
      "epoch": 37.92544570502431,
      "grad_norm": 0.8129899501800537,
      "learning_rate": 4.6239215050275705e-05,
      "loss": 3.1558,
      "step": 117000
    },
    {
      "epoch": 37.957860615883305,
      "grad_norm": 0.9454236626625061,
      "learning_rate": 4.6235971456373664e-05,
      "loss": 3.1751,
      "step": 117100
    },
    {
      "epoch": 37.9902755267423,
      "grad_norm": 0.8291586637496948,
      "learning_rate": 4.623272786247162e-05,
      "loss": 3.1591,
      "step": 117200
    },
    {
      "epoch": 38.0,
      "eval_bleu": 1.1870226064188687,
      "eval_loss": 3.72916316986084,
      "eval_runtime": 4.5162,
      "eval_samples_per_second": 108.94,
      "eval_steps_per_second": 1.771,
      "step": 117230
    },
    {
      "epoch": 38.0226904376013,
      "grad_norm": 0.8337608575820923,
      "learning_rate": 4.622948426856958e-05,
      "loss": 3.1641,
      "step": 117300
    },
    {
      "epoch": 38.055105348460295,
      "grad_norm": 1.0819714069366455,
      "learning_rate": 4.6226240674667534e-05,
      "loss": 3.1667,
      "step": 117400
    },
    {
      "epoch": 38.087520259319284,
      "grad_norm": 0.9340637922286987,
      "learning_rate": 4.622299708076549e-05,
      "loss": 3.1447,
      "step": 117500
    },
    {
      "epoch": 38.11993517017828,
      "grad_norm": 0.7909200191497803,
      "learning_rate": 4.621975348686345e-05,
      "loss": 3.165,
      "step": 117600
    },
    {
      "epoch": 38.15235008103728,
      "grad_norm": 0.8327709436416626,
      "learning_rate": 4.62165098929614e-05,
      "loss": 3.1535,
      "step": 117700
    },
    {
      "epoch": 38.18476499189627,
      "grad_norm": 0.8214728236198425,
      "learning_rate": 4.621326629905936e-05,
      "loss": 3.1666,
      "step": 117800
    },
    {
      "epoch": 38.21717990275527,
      "grad_norm": 1.0969563722610474,
      "learning_rate": 4.621002270515732e-05,
      "loss": 3.161,
      "step": 117900
    },
    {
      "epoch": 38.249594813614266,
      "grad_norm": 0.8333291411399841,
      "learning_rate": 4.620677911125527e-05,
      "loss": 3.1499,
      "step": 118000
    },
    {
      "epoch": 38.282009724473255,
      "grad_norm": 0.9735608100891113,
      "learning_rate": 4.620353551735323e-05,
      "loss": 3.1773,
      "step": 118100
    },
    {
      "epoch": 38.31442463533225,
      "grad_norm": 0.8974720239639282,
      "learning_rate": 4.620029192345119e-05,
      "loss": 3.1605,
      "step": 118200
    },
    {
      "epoch": 38.34683954619125,
      "grad_norm": 1.0591739416122437,
      "learning_rate": 4.619704832954914e-05,
      "loss": 3.1901,
      "step": 118300
    },
    {
      "epoch": 38.379254457050244,
      "grad_norm": 1.033583164215088,
      "learning_rate": 4.61938047356471e-05,
      "loss": 3.1543,
      "step": 118400
    },
    {
      "epoch": 38.41166936790924,
      "grad_norm": 0.9921106696128845,
      "learning_rate": 4.619056114174505e-05,
      "loss": 3.1648,
      "step": 118500
    },
    {
      "epoch": 38.44408427876823,
      "grad_norm": 1.053819179534912,
      "learning_rate": 4.618731754784301e-05,
      "loss": 3.1751,
      "step": 118600
    },
    {
      "epoch": 38.476499189627226,
      "grad_norm": 0.8557673096656799,
      "learning_rate": 4.618407395394097e-05,
      "loss": 3.1792,
      "step": 118700
    },
    {
      "epoch": 38.50891410048622,
      "grad_norm": 0.7896551489830017,
      "learning_rate": 4.618083036003892e-05,
      "loss": 3.1631,
      "step": 118800
    },
    {
      "epoch": 38.54132901134522,
      "grad_norm": 0.8116888999938965,
      "learning_rate": 4.617758676613688e-05,
      "loss": 3.1552,
      "step": 118900
    },
    {
      "epoch": 38.573743922204216,
      "grad_norm": 0.9483992457389832,
      "learning_rate": 4.617434317223484e-05,
      "loss": 3.1461,
      "step": 119000
    },
    {
      "epoch": 38.60615883306321,
      "grad_norm": 0.8973764777183533,
      "learning_rate": 4.617109957833279e-05,
      "loss": 3.1594,
      "step": 119100
    },
    {
      "epoch": 38.6385737439222,
      "grad_norm": 0.8667300939559937,
      "learning_rate": 4.616785598443075e-05,
      "loss": 3.1883,
      "step": 119200
    },
    {
      "epoch": 38.6709886547812,
      "grad_norm": 1.0398048162460327,
      "learning_rate": 4.61646123905287e-05,
      "loss": 3.1773,
      "step": 119300
    },
    {
      "epoch": 38.703403565640194,
      "grad_norm": 0.9880317449569702,
      "learning_rate": 4.616136879662666e-05,
      "loss": 3.1759,
      "step": 119400
    },
    {
      "epoch": 38.73581847649919,
      "grad_norm": 0.8303514719009399,
      "learning_rate": 4.615812520272462e-05,
      "loss": 3.1551,
      "step": 119500
    },
    {
      "epoch": 38.76823338735819,
      "grad_norm": 0.9851863384246826,
      "learning_rate": 4.615488160882258e-05,
      "loss": 3.1803,
      "step": 119600
    },
    {
      "epoch": 38.80064829821718,
      "grad_norm": 0.8830580115318298,
      "learning_rate": 4.615163801492054e-05,
      "loss": 3.1589,
      "step": 119700
    },
    {
      "epoch": 38.83306320907617,
      "grad_norm": 0.8159946799278259,
      "learning_rate": 4.6148394421018496e-05,
      "loss": 3.1826,
      "step": 119800
    },
    {
      "epoch": 38.86547811993517,
      "grad_norm": 0.8236784934997559,
      "learning_rate": 4.614515082711645e-05,
      "loss": 3.1786,
      "step": 119900
    },
    {
      "epoch": 38.897893030794165,
      "grad_norm": 0.8387981653213501,
      "learning_rate": 4.614190723321441e-05,
      "loss": 3.1616,
      "step": 120000
    },
    {
      "epoch": 38.93030794165316,
      "grad_norm": 0.9352803230285645,
      "learning_rate": 4.6138663639312365e-05,
      "loss": 3.18,
      "step": 120100
    },
    {
      "epoch": 38.96272285251216,
      "grad_norm": 0.8862873911857605,
      "learning_rate": 4.613542004541032e-05,
      "loss": 3.166,
      "step": 120200
    },
    {
      "epoch": 38.99513776337115,
      "grad_norm": 1.0821279287338257,
      "learning_rate": 4.6132176451508276e-05,
      "loss": 3.1756,
      "step": 120300
    },
    {
      "epoch": 39.0,
      "eval_bleu": 1.033884315853997,
      "eval_loss": 3.7330241203308105,
      "eval_runtime": 4.6897,
      "eval_samples_per_second": 104.911,
      "eval_steps_per_second": 1.706,
      "step": 120315
    },
    {
      "epoch": 39.027552674230144,
      "grad_norm": 1.0518709421157837,
      "learning_rate": 4.612893285760623e-05,
      "loss": 3.1539,
      "step": 120400
    },
    {
      "epoch": 39.05996758508914,
      "grad_norm": 0.9576952457427979,
      "learning_rate": 4.612568926370419e-05,
      "loss": 3.144,
      "step": 120500
    },
    {
      "epoch": 39.09238249594814,
      "grad_norm": 0.9993543028831482,
      "learning_rate": 4.6122445669802146e-05,
      "loss": 3.1532,
      "step": 120600
    },
    {
      "epoch": 39.12479740680713,
      "grad_norm": 0.8721500039100647,
      "learning_rate": 4.61192020759001e-05,
      "loss": 3.1547,
      "step": 120700
    },
    {
      "epoch": 39.15721231766613,
      "grad_norm": 1.0198324918746948,
      "learning_rate": 4.6115958481998056e-05,
      "loss": 3.1666,
      "step": 120800
    },
    {
      "epoch": 39.18962722852512,
      "grad_norm": 0.9878070950508118,
      "learning_rate": 4.6112714888096015e-05,
      "loss": 3.1616,
      "step": 120900
    },
    {
      "epoch": 39.222042139384115,
      "grad_norm": 0.8289362192153931,
      "learning_rate": 4.610947129419397e-05,
      "loss": 3.1573,
      "step": 121000
    },
    {
      "epoch": 39.25445705024311,
      "grad_norm": 0.7403767108917236,
      "learning_rate": 4.6106227700291926e-05,
      "loss": 3.1657,
      "step": 121100
    },
    {
      "epoch": 39.28687196110211,
      "grad_norm": 0.8969583511352539,
      "learning_rate": 4.6102984106389884e-05,
      "loss": 3.1764,
      "step": 121200
    },
    {
      "epoch": 39.319286871961104,
      "grad_norm": 0.9757165312767029,
      "learning_rate": 4.6099740512487836e-05,
      "loss": 3.1786,
      "step": 121300
    },
    {
      "epoch": 39.3517017828201,
      "grad_norm": 0.8385777473449707,
      "learning_rate": 4.6096496918585795e-05,
      "loss": 3.1428,
      "step": 121400
    },
    {
      "epoch": 39.38411669367909,
      "grad_norm": 0.8289440274238586,
      "learning_rate": 4.609325332468375e-05,
      "loss": 3.1564,
      "step": 121500
    },
    {
      "epoch": 39.416531604538086,
      "grad_norm": 0.7768101096153259,
      "learning_rate": 4.6090009730781706e-05,
      "loss": 3.1627,
      "step": 121600
    },
    {
      "epoch": 39.44894651539708,
      "grad_norm": 0.9249829053878784,
      "learning_rate": 4.6086766136879665e-05,
      "loss": 3.1698,
      "step": 121700
    },
    {
      "epoch": 39.48136142625608,
      "grad_norm": 0.9315572381019592,
      "learning_rate": 4.608352254297762e-05,
      "loss": 3.1444,
      "step": 121800
    },
    {
      "epoch": 39.513776337115075,
      "grad_norm": 0.7677350044250488,
      "learning_rate": 4.6080278949075575e-05,
      "loss": 3.1714,
      "step": 121900
    },
    {
      "epoch": 39.546191247974065,
      "grad_norm": 0.9034419655799866,
      "learning_rate": 4.6077035355173534e-05,
      "loss": 3.1417,
      "step": 122000
    },
    {
      "epoch": 39.57860615883306,
      "grad_norm": 0.8502383232116699,
      "learning_rate": 4.607379176127149e-05,
      "loss": 3.1781,
      "step": 122100
    },
    {
      "epoch": 39.61102106969206,
      "grad_norm": 0.821972668170929,
      "learning_rate": 4.607054816736945e-05,
      "loss": 3.1483,
      "step": 122200
    },
    {
      "epoch": 39.643435980551054,
      "grad_norm": 1.0969666242599487,
      "learning_rate": 4.606733700940642e-05,
      "loss": 3.167,
      "step": 122300
    },
    {
      "epoch": 39.67585089141005,
      "grad_norm": 1.0272279977798462,
      "learning_rate": 4.606409341550438e-05,
      "loss": 3.1674,
      "step": 122400
    },
    {
      "epoch": 39.70826580226905,
      "grad_norm": 0.9367011189460754,
      "learning_rate": 4.606084982160233e-05,
      "loss": 3.1528,
      "step": 122500
    },
    {
      "epoch": 39.740680713128036,
      "grad_norm": 0.8662858605384827,
      "learning_rate": 4.605760622770029e-05,
      "loss": 3.1767,
      "step": 122600
    },
    {
      "epoch": 39.77309562398703,
      "grad_norm": 0.9076191782951355,
      "learning_rate": 4.605439506973727e-05,
      "loss": 3.1584,
      "step": 122700
    },
    {
      "epoch": 39.80551053484603,
      "grad_norm": 0.8437454700469971,
      "learning_rate": 4.605115147583523e-05,
      "loss": 3.1674,
      "step": 122800
    },
    {
      "epoch": 39.837925445705025,
      "grad_norm": 1.0088030099868774,
      "learning_rate": 4.604790788193318e-05,
      "loss": 3.1672,
      "step": 122900
    },
    {
      "epoch": 39.87034035656402,
      "grad_norm": 0.84013831615448,
      "learning_rate": 4.604466428803114e-05,
      "loss": 3.1706,
      "step": 123000
    },
    {
      "epoch": 39.90275526742302,
      "grad_norm": 0.7918412089347839,
      "learning_rate": 4.60414206941291e-05,
      "loss": 3.1663,
      "step": 123100
    },
    {
      "epoch": 39.93517017828201,
      "grad_norm": 0.8901945948600769,
      "learning_rate": 4.603817710022706e-05,
      "loss": 3.1705,
      "step": 123200
    },
    {
      "epoch": 39.967585089141004,
      "grad_norm": 0.9327324032783508,
      "learning_rate": 4.603493350632501e-05,
      "loss": 3.1637,
      "step": 123300
    },
    {
      "epoch": 40.0,
      "grad_norm": 0.9496197700500488,
      "learning_rate": 4.603168991242297e-05,
      "loss": 3.1537,
      "step": 123400
    },
    {
      "epoch": 40.0,
      "eval_bleu": 1.11621957322237,
      "eval_loss": 3.7393763065338135,
      "eval_runtime": 4.605,
      "eval_samples_per_second": 106.84,
      "eval_steps_per_second": 1.737,
      "step": 123400
    },
    {
      "epoch": 40.032414910858996,
      "grad_norm": 0.9844576120376587,
      "learning_rate": 4.6028446318520926e-05,
      "loss": 3.1476,
      "step": 123500
    },
    {
      "epoch": 40.06482982171799,
      "grad_norm": 0.9136404395103455,
      "learning_rate": 4.6025202724618885e-05,
      "loss": 3.1604,
      "step": 123600
    },
    {
      "epoch": 40.09724473257698,
      "grad_norm": 0.9235369563102722,
      "learning_rate": 4.602195913071684e-05,
      "loss": 3.1497,
      "step": 123700
    },
    {
      "epoch": 40.12965964343598,
      "grad_norm": 0.8370351791381836,
      "learning_rate": 4.6018715536814796e-05,
      "loss": 3.1323,
      "step": 123800
    },
    {
      "epoch": 40.162074554294975,
      "grad_norm": 1.2283134460449219,
      "learning_rate": 4.601547194291275e-05,
      "loss": 3.1568,
      "step": 123900
    },
    {
      "epoch": 40.19448946515397,
      "grad_norm": 0.8979170322418213,
      "learning_rate": 4.6012228349010706e-05,
      "loss": 3.1619,
      "step": 124000
    },
    {
      "epoch": 40.22690437601297,
      "grad_norm": 0.9249616861343384,
      "learning_rate": 4.6008984755108665e-05,
      "loss": 3.1527,
      "step": 124100
    },
    {
      "epoch": 40.259319286871964,
      "grad_norm": 0.8156850934028625,
      "learning_rate": 4.600574116120662e-05,
      "loss": 3.162,
      "step": 124200
    },
    {
      "epoch": 40.29173419773095,
      "grad_norm": 0.9723141193389893,
      "learning_rate": 4.6002497567304576e-05,
      "loss": 3.1551,
      "step": 124300
    },
    {
      "epoch": 40.32414910858995,
      "grad_norm": 0.8535184860229492,
      "learning_rate": 4.5999253973402535e-05,
      "loss": 3.1648,
      "step": 124400
    },
    {
      "epoch": 40.356564019448946,
      "grad_norm": 0.9572745561599731,
      "learning_rate": 4.5996010379500487e-05,
      "loss": 3.1609,
      "step": 124500
    },
    {
      "epoch": 40.38897893030794,
      "grad_norm": 1.0384132862091064,
      "learning_rate": 4.5992766785598445e-05,
      "loss": 3.1579,
      "step": 124600
    },
    {
      "epoch": 40.42139384116694,
      "grad_norm": 0.9565693140029907,
      "learning_rate": 4.5989523191696404e-05,
      "loss": 3.1589,
      "step": 124700
    },
    {
      "epoch": 40.453808752025935,
      "grad_norm": 0.9028939008712769,
      "learning_rate": 4.5986279597794356e-05,
      "loss": 3.149,
      "step": 124800
    },
    {
      "epoch": 40.486223662884925,
      "grad_norm": 0.9528378248214722,
      "learning_rate": 4.5983036003892315e-05,
      "loss": 3.1646,
      "step": 124900
    },
    {
      "epoch": 40.51863857374392,
      "grad_norm": 0.8544490933418274,
      "learning_rate": 4.597979240999027e-05,
      "loss": 3.1662,
      "step": 125000
    },
    {
      "epoch": 40.55105348460292,
      "grad_norm": 0.8833342790603638,
      "learning_rate": 4.5976548816088225e-05,
      "loss": 3.1709,
      "step": 125100
    },
    {
      "epoch": 40.583468395461914,
      "grad_norm": 1.0169634819030762,
      "learning_rate": 4.5973305222186184e-05,
      "loss": 3.1361,
      "step": 125200
    },
    {
      "epoch": 40.61588330632091,
      "grad_norm": 0.9381358623504639,
      "learning_rate": 4.5970061628284136e-05,
      "loss": 3.1569,
      "step": 125300
    },
    {
      "epoch": 40.6482982171799,
      "grad_norm": 1.085260272026062,
      "learning_rate": 4.5966818034382095e-05,
      "loss": 3.1694,
      "step": 125400
    },
    {
      "epoch": 40.680713128038896,
      "grad_norm": 0.903556227684021,
      "learning_rate": 4.5963574440480054e-05,
      "loss": 3.1631,
      "step": 125500
    },
    {
      "epoch": 40.71312803889789,
      "grad_norm": 1.0021151304244995,
      "learning_rate": 4.596033084657801e-05,
      "loss": 3.1377,
      "step": 125600
    },
    {
      "epoch": 40.74554294975689,
      "grad_norm": 0.832374095916748,
      "learning_rate": 4.5957087252675964e-05,
      "loss": 3.1673,
      "step": 125700
    },
    {
      "epoch": 40.777957860615885,
      "grad_norm": 0.8500869870185852,
      "learning_rate": 4.595384365877392e-05,
      "loss": 3.15,
      "step": 125800
    },
    {
      "epoch": 40.81037277147488,
      "grad_norm": 1.198673963546753,
      "learning_rate": 4.595060006487188e-05,
      "loss": 3.1695,
      "step": 125900
    },
    {
      "epoch": 40.84278768233387,
      "grad_norm": 0.8970091938972473,
      "learning_rate": 4.594735647096984e-05,
      "loss": 3.1737,
      "step": 126000
    },
    {
      "epoch": 40.87520259319287,
      "grad_norm": 0.8494666218757629,
      "learning_rate": 4.594411287706779e-05,
      "loss": 3.1521,
      "step": 126100
    },
    {
      "epoch": 40.90761750405186,
      "grad_norm": 1.0136501789093018,
      "learning_rate": 4.594086928316575e-05,
      "loss": 3.1653,
      "step": 126200
    },
    {
      "epoch": 40.94003241491086,
      "grad_norm": 0.8595350980758667,
      "learning_rate": 4.593762568926371e-05,
      "loss": 3.1367,
      "step": 126300
    },
    {
      "epoch": 40.972447325769856,
      "grad_norm": 0.8823848366737366,
      "learning_rate": 4.593438209536166e-05,
      "loss": 3.1384,
      "step": 126400
    },
    {
      "epoch": 41.0,
      "eval_bleu": 1.0345636036565988,
      "eval_loss": 3.7360057830810547,
      "eval_runtime": 4.3603,
      "eval_samples_per_second": 112.835,
      "eval_steps_per_second": 1.835,
      "step": 126485
    },
    {
      "epoch": 41.00486223662885,
      "grad_norm": 0.8960253000259399,
      "learning_rate": 4.593113850145962e-05,
      "loss": 3.158,
      "step": 126500
    },
    {
      "epoch": 41.03727714748784,
      "grad_norm": 0.8626531362533569,
      "learning_rate": 4.592789490755758e-05,
      "loss": 3.1418,
      "step": 126600
    },
    {
      "epoch": 41.06969205834684,
      "grad_norm": 1.1586333513259888,
      "learning_rate": 4.592465131365553e-05,
      "loss": 3.1414,
      "step": 126700
    },
    {
      "epoch": 41.102106969205835,
      "grad_norm": 0.9867116808891296,
      "learning_rate": 4.592140771975349e-05,
      "loss": 3.1354,
      "step": 126800
    },
    {
      "epoch": 41.13452188006483,
      "grad_norm": 0.851769208908081,
      "learning_rate": 4.591816412585145e-05,
      "loss": 3.1436,
      "step": 126900
    },
    {
      "epoch": 41.16693679092383,
      "grad_norm": 0.9926910400390625,
      "learning_rate": 4.59149205319494e-05,
      "loss": 3.139,
      "step": 127000
    },
    {
      "epoch": 41.19935170178282,
      "grad_norm": 0.7866466045379639,
      "learning_rate": 4.591167693804736e-05,
      "loss": 3.1601,
      "step": 127100
    },
    {
      "epoch": 41.23176661264181,
      "grad_norm": 0.9389259815216064,
      "learning_rate": 4.590843334414531e-05,
      "loss": 3.1513,
      "step": 127200
    },
    {
      "epoch": 41.26418152350081,
      "grad_norm": 0.9913707971572876,
      "learning_rate": 4.590518975024327e-05,
      "loss": 3.1431,
      "step": 127300
    },
    {
      "epoch": 41.296596434359806,
      "grad_norm": 0.9526100754737854,
      "learning_rate": 4.590194615634123e-05,
      "loss": 3.1783,
      "step": 127400
    },
    {
      "epoch": 41.3290113452188,
      "grad_norm": 1.092116355895996,
      "learning_rate": 4.589870256243918e-05,
      "loss": 3.1446,
      "step": 127500
    },
    {
      "epoch": 41.3614262560778,
      "grad_norm": 0.9582242965698242,
      "learning_rate": 4.589545896853714e-05,
      "loss": 3.1627,
      "step": 127600
    },
    {
      "epoch": 41.39384116693679,
      "grad_norm": 0.8177607655525208,
      "learning_rate": 4.58922153746351e-05,
      "loss": 3.1379,
      "step": 127700
    },
    {
      "epoch": 41.426256077795784,
      "grad_norm": 0.9119894504547119,
      "learning_rate": 4.588897178073305e-05,
      "loss": 3.1705,
      "step": 127800
    },
    {
      "epoch": 41.45867098865478,
      "grad_norm": 1.0474365949630737,
      "learning_rate": 4.588572818683101e-05,
      "loss": 3.1679,
      "step": 127900
    },
    {
      "epoch": 41.49108589951378,
      "grad_norm": 0.8267908096313477,
      "learning_rate": 4.588248459292897e-05,
      "loss": 3.1512,
      "step": 128000
    },
    {
      "epoch": 41.523500810372774,
      "grad_norm": 0.9483926296234131,
      "learning_rate": 4.587924099902693e-05,
      "loss": 3.1387,
      "step": 128100
    },
    {
      "epoch": 41.55591572123177,
      "grad_norm": 0.9541379809379578,
      "learning_rate": 4.587599740512488e-05,
      "loss": 3.1509,
      "step": 128200
    },
    {
      "epoch": 41.58833063209076,
      "grad_norm": 0.9950642585754395,
      "learning_rate": 4.587275381122284e-05,
      "loss": 3.1576,
      "step": 128300
    },
    {
      "epoch": 41.620745542949756,
      "grad_norm": 1.0187331438064575,
      "learning_rate": 4.5869510217320796e-05,
      "loss": 3.1426,
      "step": 128400
    },
    {
      "epoch": 41.65316045380875,
      "grad_norm": 0.8352580070495605,
      "learning_rate": 4.5866266623418755e-05,
      "loss": 3.1469,
      "step": 128500
    },
    {
      "epoch": 41.68557536466775,
      "grad_norm": 0.9997631907463074,
      "learning_rate": 4.586302302951671e-05,
      "loss": 3.1464,
      "step": 128600
    },
    {
      "epoch": 41.717990275526745,
      "grad_norm": 0.9713588953018188,
      "learning_rate": 4.5859779435614666e-05,
      "loss": 3.1519,
      "step": 128700
    },
    {
      "epoch": 41.750405186385734,
      "grad_norm": 0.8959241509437561,
      "learning_rate": 4.5856535841712624e-05,
      "loss": 3.1357,
      "step": 128800
    },
    {
      "epoch": 41.78282009724473,
      "grad_norm": 0.9071714282035828,
      "learning_rate": 4.5853292247810576e-05,
      "loss": 3.1528,
      "step": 128900
    },
    {
      "epoch": 41.81523500810373,
      "grad_norm": 0.8834446668624878,
      "learning_rate": 4.5850048653908535e-05,
      "loss": 3.1465,
      "step": 129000
    },
    {
      "epoch": 41.84764991896272,
      "grad_norm": 0.8347866535186768,
      "learning_rate": 4.584680506000649e-05,
      "loss": 3.1368,
      "step": 129100
    },
    {
      "epoch": 41.88006482982172,
      "grad_norm": 0.9023616909980774,
      "learning_rate": 4.5843561466104446e-05,
      "loss": 3.164,
      "step": 129200
    },
    {
      "epoch": 41.912479740680716,
      "grad_norm": 0.9065122008323669,
      "learning_rate": 4.5840317872202405e-05,
      "loss": 3.1676,
      "step": 129300
    },
    {
      "epoch": 41.944894651539705,
      "grad_norm": 0.9135403037071228,
      "learning_rate": 4.583710671423938e-05,
      "loss": 3.1559,
      "step": 129400
    },
    {
      "epoch": 41.9773095623987,
      "grad_norm": 0.7869455814361572,
      "learning_rate": 4.5833863120337334e-05,
      "loss": 3.1589,
      "step": 129500
    },
    {
      "epoch": 42.0,
      "eval_bleu": 1.1218241837245695,
      "eval_loss": 3.7363414764404297,
      "eval_runtime": 5.3371,
      "eval_samples_per_second": 92.185,
      "eval_steps_per_second": 1.499,
      "step": 129570
    },
    {
      "epoch": 42.0097244732577,
      "grad_norm": 0.8866503834724426,
      "learning_rate": 4.583061952643529e-05,
      "loss": 3.1418,
      "step": 129600
    },
    {
      "epoch": 42.042139384116695,
      "grad_norm": 0.7875815629959106,
      "learning_rate": 4.582737593253325e-05,
      "loss": 3.112,
      "step": 129700
    },
    {
      "epoch": 42.07455429497569,
      "grad_norm": 1.0215955972671509,
      "learning_rate": 4.5824132338631204e-05,
      "loss": 3.1528,
      "step": 129800
    },
    {
      "epoch": 42.10696920583469,
      "grad_norm": 1.0371382236480713,
      "learning_rate": 4.582088874472916e-05,
      "loss": 3.1602,
      "step": 129900
    },
    {
      "epoch": 42.13938411669368,
      "grad_norm": 0.9932175278663635,
      "learning_rate": 4.581764515082712e-05,
      "loss": 3.1435,
      "step": 130000
    },
    {
      "epoch": 42.17179902755267,
      "grad_norm": 0.8902833461761475,
      "learning_rate": 4.581440155692507e-05,
      "loss": 3.1398,
      "step": 130100
    },
    {
      "epoch": 42.20421393841167,
      "grad_norm": 0.9435009360313416,
      "learning_rate": 4.581115796302303e-05,
      "loss": 3.145,
      "step": 130200
    },
    {
      "epoch": 42.236628849270666,
      "grad_norm": 0.8556351065635681,
      "learning_rate": 4.5807914369120984e-05,
      "loss": 3.1582,
      "step": 130300
    },
    {
      "epoch": 42.26904376012966,
      "grad_norm": 0.9137068390846252,
      "learning_rate": 4.580467077521894e-05,
      "loss": 3.1527,
      "step": 130400
    },
    {
      "epoch": 42.30145867098865,
      "grad_norm": 1.0301969051361084,
      "learning_rate": 4.58014271813169e-05,
      "loss": 3.1487,
      "step": 130500
    },
    {
      "epoch": 42.33387358184765,
      "grad_norm": 0.9426267147064209,
      "learning_rate": 4.5798183587414853e-05,
      "loss": 3.1342,
      "step": 130600
    },
    {
      "epoch": 42.366288492706644,
      "grad_norm": 1.0616579055786133,
      "learning_rate": 4.579493999351281e-05,
      "loss": 3.1371,
      "step": 130700
    },
    {
      "epoch": 42.39870340356564,
      "grad_norm": 0.8589192628860474,
      "learning_rate": 4.579169639961077e-05,
      "loss": 3.1395,
      "step": 130800
    },
    {
      "epoch": 42.43111831442464,
      "grad_norm": 0.9385725259780884,
      "learning_rate": 4.578845280570872e-05,
      "loss": 3.1442,
      "step": 130900
    },
    {
      "epoch": 42.46353322528363,
      "grad_norm": 0.9380668997764587,
      "learning_rate": 4.578520921180668e-05,
      "loss": 3.1502,
      "step": 131000
    },
    {
      "epoch": 42.49594813614262,
      "grad_norm": 0.8761782646179199,
      "learning_rate": 4.578196561790464e-05,
      "loss": 3.142,
      "step": 131100
    },
    {
      "epoch": 42.52836304700162,
      "grad_norm": 0.898663341999054,
      "learning_rate": 4.57787220240026e-05,
      "loss": 3.1564,
      "step": 131200
    },
    {
      "epoch": 42.560777957860616,
      "grad_norm": 0.9238415360450745,
      "learning_rate": 4.577547843010056e-05,
      "loss": 3.1291,
      "step": 131300
    },
    {
      "epoch": 42.59319286871961,
      "grad_norm": 1.1190303564071655,
      "learning_rate": 4.577223483619851e-05,
      "loss": 3.1483,
      "step": 131400
    },
    {
      "epoch": 42.62560777957861,
      "grad_norm": 0.8780283331871033,
      "learning_rate": 4.576899124229647e-05,
      "loss": 3.1581,
      "step": 131500
    },
    {
      "epoch": 42.658022690437605,
      "grad_norm": 1.037509560585022,
      "learning_rate": 4.576574764839443e-05,
      "loss": 3.1424,
      "step": 131600
    },
    {
      "epoch": 42.690437601296594,
      "grad_norm": 0.8719333410263062,
      "learning_rate": 4.576250405449238e-05,
      "loss": 3.1294,
      "step": 131700
    },
    {
      "epoch": 42.72285251215559,
      "grad_norm": 1.2072104215621948,
      "learning_rate": 4.575926046059034e-05,
      "loss": 3.1269,
      "step": 131800
    },
    {
      "epoch": 42.75526742301459,
      "grad_norm": 0.8842899203300476,
      "learning_rate": 4.5756049302627316e-05,
      "loss": 3.1354,
      "step": 131900
    },
    {
      "epoch": 42.78768233387358,
      "grad_norm": 1.0913312435150146,
      "learning_rate": 4.5752805708725275e-05,
      "loss": 3.1493,
      "step": 132000
    },
    {
      "epoch": 42.82009724473258,
      "grad_norm": 0.8736574053764343,
      "learning_rate": 4.5749562114823227e-05,
      "loss": 3.1568,
      "step": 132100
    },
    {
      "epoch": 42.85251215559157,
      "grad_norm": 0.9154927730560303,
      "learning_rate": 4.5746318520921185e-05,
      "loss": 3.1645,
      "step": 132200
    },
    {
      "epoch": 42.884927066450565,
      "grad_norm": 1.0082712173461914,
      "learning_rate": 4.5743074927019144e-05,
      "loss": 3.1535,
      "step": 132300
    },
    {
      "epoch": 42.91734197730956,
      "grad_norm": 1.2893333435058594,
      "learning_rate": 4.5739831333117096e-05,
      "loss": 3.148,
      "step": 132400
    },
    {
      "epoch": 42.94975688816856,
      "grad_norm": 0.8718233108520508,
      "learning_rate": 4.5736587739215055e-05,
      "loss": 3.1404,
      "step": 132500
    },
    {
      "epoch": 42.982171799027554,
      "grad_norm": 1.044385313987732,
      "learning_rate": 4.573334414531301e-05,
      "loss": 3.173,
      "step": 132600
    },
    {
      "epoch": 43.0,
      "eval_bleu": 0.9481399823638387,
      "eval_loss": 3.7404379844665527,
      "eval_runtime": 4.7337,
      "eval_samples_per_second": 103.935,
      "eval_steps_per_second": 1.69,
      "step": 132655
    },
    {
      "epoch": 43.01458670988655,
      "grad_norm": 0.8583739995956421,
      "learning_rate": 4.5730100551410965e-05,
      "loss": 3.1475,
      "step": 132700
    },
    {
      "epoch": 43.04700162074554,
      "grad_norm": 0.9419816732406616,
      "learning_rate": 4.5726856957508924e-05,
      "loss": 3.1569,
      "step": 132800
    },
    {
      "epoch": 43.07941653160454,
      "grad_norm": 0.8981895446777344,
      "learning_rate": 4.5723613363606876e-05,
      "loss": 3.1424,
      "step": 132900
    },
    {
      "epoch": 43.11183144246353,
      "grad_norm": 0.8793110847473145,
      "learning_rate": 4.5720369769704835e-05,
      "loss": 3.1536,
      "step": 133000
    },
    {
      "epoch": 43.14424635332253,
      "grad_norm": 0.9396337270736694,
      "learning_rate": 4.5717126175802794e-05,
      "loss": 3.1611,
      "step": 133100
    },
    {
      "epoch": 43.176661264181526,
      "grad_norm": 1.0077484846115112,
      "learning_rate": 4.5713882581900746e-05,
      "loss": 3.1358,
      "step": 133200
    },
    {
      "epoch": 43.20907617504052,
      "grad_norm": 1.1445286273956299,
      "learning_rate": 4.5710638987998704e-05,
      "loss": 3.1516,
      "step": 133300
    },
    {
      "epoch": 43.24149108589951,
      "grad_norm": 1.014430284500122,
      "learning_rate": 4.570739539409666e-05,
      "loss": 3.1437,
      "step": 133400
    },
    {
      "epoch": 43.27390599675851,
      "grad_norm": 0.9071393609046936,
      "learning_rate": 4.5704151800194615e-05,
      "loss": 3.1173,
      "step": 133500
    },
    {
      "epoch": 43.306320907617504,
      "grad_norm": 0.9009402990341187,
      "learning_rate": 4.5700908206292574e-05,
      "loss": 3.1287,
      "step": 133600
    },
    {
      "epoch": 43.3387358184765,
      "grad_norm": 1.1579945087432861,
      "learning_rate": 4.5697664612390526e-05,
      "loss": 3.1463,
      "step": 133700
    },
    {
      "epoch": 43.3711507293355,
      "grad_norm": 0.9555280208587646,
      "learning_rate": 4.5694421018488484e-05,
      "loss": 3.1479,
      "step": 133800
    },
    {
      "epoch": 43.403565640194486,
      "grad_norm": 0.8718819618225098,
      "learning_rate": 4.569117742458644e-05,
      "loss": 3.1525,
      "step": 133900
    },
    {
      "epoch": 43.43598055105348,
      "grad_norm": 0.9767548441886902,
      "learning_rate": 4.5687933830684395e-05,
      "loss": 3.1435,
      "step": 134000
    },
    {
      "epoch": 43.46839546191248,
      "grad_norm": 0.9193469882011414,
      "learning_rate": 4.5684690236782354e-05,
      "loss": 3.1422,
      "step": 134100
    },
    {
      "epoch": 43.500810372771475,
      "grad_norm": 0.9274998903274536,
      "learning_rate": 4.568144664288031e-05,
      "loss": 3.1326,
      "step": 134200
    },
    {
      "epoch": 43.53322528363047,
      "grad_norm": 0.920428454875946,
      "learning_rate": 4.567820304897827e-05,
      "loss": 3.1352,
      "step": 134300
    },
    {
      "epoch": 43.56564019448947,
      "grad_norm": 0.8940868377685547,
      "learning_rate": 4.567495945507623e-05,
      "loss": 3.1376,
      "step": 134400
    },
    {
      "epoch": 43.59805510534846,
      "grad_norm": 0.9079470038414001,
      "learning_rate": 4.567171586117419e-05,
      "loss": 3.1282,
      "step": 134500
    },
    {
      "epoch": 43.630470016207454,
      "grad_norm": 0.8140437602996826,
      "learning_rate": 4.566847226727214e-05,
      "loss": 3.1444,
      "step": 134600
    },
    {
      "epoch": 43.66288492706645,
      "grad_norm": 0.9193212985992432,
      "learning_rate": 4.56652286733701e-05,
      "loss": 3.1347,
      "step": 134700
    },
    {
      "epoch": 43.69529983792545,
      "grad_norm": 0.7931756377220154,
      "learning_rate": 4.566198507946805e-05,
      "loss": 3.142,
      "step": 134800
    },
    {
      "epoch": 43.72771474878444,
      "grad_norm": 0.9885871410369873,
      "learning_rate": 4.565874148556601e-05,
      "loss": 3.1544,
      "step": 134900
    },
    {
      "epoch": 43.76012965964344,
      "grad_norm": 0.9460179805755615,
      "learning_rate": 4.565549789166397e-05,
      "loss": 3.1067,
      "step": 135000
    },
    {
      "epoch": 43.79254457050243,
      "grad_norm": 0.855376660823822,
      "learning_rate": 4.565225429776192e-05,
      "loss": 3.1422,
      "step": 135100
    },
    {
      "epoch": 43.824959481361425,
      "grad_norm": 1.0726900100708008,
      "learning_rate": 4.564901070385988e-05,
      "loss": 3.1323,
      "step": 135200
    },
    {
      "epoch": 43.85737439222042,
      "grad_norm": 0.9215888977050781,
      "learning_rate": 4.564576710995784e-05,
      "loss": 3.1299,
      "step": 135300
    },
    {
      "epoch": 43.88978930307942,
      "grad_norm": 0.9134015440940857,
      "learning_rate": 4.564252351605579e-05,
      "loss": 3.1314,
      "step": 135400
    },
    {
      "epoch": 43.922204213938414,
      "grad_norm": 0.8680312037467957,
      "learning_rate": 4.563927992215375e-05,
      "loss": 3.1362,
      "step": 135500
    },
    {
      "epoch": 43.954619124797404,
      "grad_norm": 1.0454591512680054,
      "learning_rate": 4.563603632825171e-05,
      "loss": 3.1362,
      "step": 135600
    },
    {
      "epoch": 43.9870340356564,
      "grad_norm": 0.9308097958564758,
      "learning_rate": 4.563279273434966e-05,
      "loss": 3.1554,
      "step": 135700
    },
    {
      "epoch": 44.0,
      "eval_bleu": 0.927797414892225,
      "eval_loss": 3.741178035736084,
      "eval_runtime": 4.3717,
      "eval_samples_per_second": 112.541,
      "eval_steps_per_second": 1.83,
      "step": 135740
    },
    {
      "epoch": 44.019448946515396,
      "grad_norm": 1.161232590675354,
      "learning_rate": 4.562954914044762e-05,
      "loss": 3.1355,
      "step": 135800
    },
    {
      "epoch": 44.05186385737439,
      "grad_norm": 0.9266771078109741,
      "learning_rate": 4.5626337982484596e-05,
      "loss": 3.1275,
      "step": 135900
    },
    {
      "epoch": 44.08427876823339,
      "grad_norm": 0.9338603615760803,
      "learning_rate": 4.562309438858255e-05,
      "loss": 3.1327,
      "step": 136000
    },
    {
      "epoch": 44.116693679092386,
      "grad_norm": 1.0332984924316406,
      "learning_rate": 4.561985079468051e-05,
      "loss": 3.1549,
      "step": 136100
    },
    {
      "epoch": 44.149108589951375,
      "grad_norm": 0.8439421653747559,
      "learning_rate": 4.5616607200778466e-05,
      "loss": 3.1272,
      "step": 136200
    },
    {
      "epoch": 44.18152350081037,
      "grad_norm": 0.8472484946250916,
      "learning_rate": 4.561336360687642e-05,
      "loss": 3.1436,
      "step": 136300
    },
    {
      "epoch": 44.21393841166937,
      "grad_norm": 0.9709866046905518,
      "learning_rate": 4.561012001297438e-05,
      "loss": 3.1243,
      "step": 136400
    },
    {
      "epoch": 44.246353322528364,
      "grad_norm": 1.0164843797683716,
      "learning_rate": 4.5606876419072335e-05,
      "loss": 3.1325,
      "step": 136500
    },
    {
      "epoch": 44.27876823338736,
      "grad_norm": 0.842199444770813,
      "learning_rate": 4.560363282517029e-05,
      "loss": 3.1414,
      "step": 136600
    },
    {
      "epoch": 44.31118314424636,
      "grad_norm": 0.8822439312934875,
      "learning_rate": 4.5600389231268246e-05,
      "loss": 3.1361,
      "step": 136700
    },
    {
      "epoch": 44.343598055105346,
      "grad_norm": 0.8919244408607483,
      "learning_rate": 4.5597145637366205e-05,
      "loss": 3.1225,
      "step": 136800
    },
    {
      "epoch": 44.37601296596434,
      "grad_norm": 0.9336486458778381,
      "learning_rate": 4.559390204346416e-05,
      "loss": 3.1304,
      "step": 136900
    },
    {
      "epoch": 44.40842787682334,
      "grad_norm": 0.9652215838432312,
      "learning_rate": 4.5590690885501135e-05,
      "loss": 3.1472,
      "step": 137000
    },
    {
      "epoch": 44.440842787682335,
      "grad_norm": 0.9500108361244202,
      "learning_rate": 4.558744729159909e-05,
      "loss": 3.1301,
      "step": 137100
    },
    {
      "epoch": 44.47325769854133,
      "grad_norm": 1.0220675468444824,
      "learning_rate": 4.558420369769705e-05,
      "loss": 3.144,
      "step": 137200
    },
    {
      "epoch": 44.50567260940032,
      "grad_norm": 0.9276295900344849,
      "learning_rate": 4.5580960103795004e-05,
      "loss": 3.143,
      "step": 137300
    },
    {
      "epoch": 44.53808752025932,
      "grad_norm": 0.9407332539558411,
      "learning_rate": 4.557774894583198e-05,
      "loss": 3.1414,
      "step": 137400
    },
    {
      "epoch": 44.570502431118314,
      "grad_norm": 1.0043888092041016,
      "learning_rate": 4.557450535192994e-05,
      "loss": 3.1438,
      "step": 137500
    },
    {
      "epoch": 44.60291734197731,
      "grad_norm": 0.8339277505874634,
      "learning_rate": 4.557126175802789e-05,
      "loss": 3.131,
      "step": 137600
    },
    {
      "epoch": 44.63533225283631,
      "grad_norm": 1.0543043613433838,
      "learning_rate": 4.556801816412585e-05,
      "loss": 3.1312,
      "step": 137700
    },
    {
      "epoch": 44.6677471636953,
      "grad_norm": 1.0692118406295776,
      "learning_rate": 4.556477457022381e-05,
      "loss": 3.1357,
      "step": 137800
    },
    {
      "epoch": 44.70016207455429,
      "grad_norm": 1.0194711685180664,
      "learning_rate": 4.556153097632176e-05,
      "loss": 3.1408,
      "step": 137900
    },
    {
      "epoch": 44.73257698541329,
      "grad_norm": 0.9197695851325989,
      "learning_rate": 4.555828738241972e-05,
      "loss": 3.1357,
      "step": 138000
    },
    {
      "epoch": 44.764991896272285,
      "grad_norm": 0.9247215390205383,
      "learning_rate": 4.555504378851768e-05,
      "loss": 3.1384,
      "step": 138100
    },
    {
      "epoch": 44.79740680713128,
      "grad_norm": 0.9304542541503906,
      "learning_rate": 4.555180019461563e-05,
      "loss": 3.1414,
      "step": 138200
    },
    {
      "epoch": 44.82982171799028,
      "grad_norm": 1.1118457317352295,
      "learning_rate": 4.554855660071359e-05,
      "loss": 3.1316,
      "step": 138300
    },
    {
      "epoch": 44.862236628849274,
      "grad_norm": 0.9765797853469849,
      "learning_rate": 4.554531300681155e-05,
      "loss": 3.1551,
      "step": 138400
    },
    {
      "epoch": 44.89465153970826,
      "grad_norm": 0.8029811382293701,
      "learning_rate": 4.554206941290951e-05,
      "loss": 3.1246,
      "step": 138500
    },
    {
      "epoch": 44.92706645056726,
      "grad_norm": 0.9488136172294617,
      "learning_rate": 4.5538825819007466e-05,
      "loss": 3.1228,
      "step": 138600
    },
    {
      "epoch": 44.959481361426256,
      "grad_norm": 1.005617618560791,
      "learning_rate": 4.553558222510542e-05,
      "loss": 3.13,
      "step": 138700
    },
    {
      "epoch": 44.99189627228525,
      "grad_norm": 0.9994165301322937,
      "learning_rate": 4.5532371067142396e-05,
      "loss": 3.1187,
      "step": 138800
    },
    {
      "epoch": 45.0,
      "eval_bleu": 0.9941232593031739,
      "eval_loss": 3.7480878829956055,
      "eval_runtime": 4.3048,
      "eval_samples_per_second": 114.292,
      "eval_steps_per_second": 1.858,
      "step": 138825
    },
    {
      "epoch": 45.02431118314425,
      "grad_norm": 0.9487804174423218,
      "learning_rate": 4.5529127473240355e-05,
      "loss": 3.1024,
      "step": 138900
    },
    {
      "epoch": 45.05672609400324,
      "grad_norm": 0.8566412925720215,
      "learning_rate": 4.552588387933831e-05,
      "loss": 3.1266,
      "step": 139000
    },
    {
      "epoch": 45.089141004862235,
      "grad_norm": 1.1083990335464478,
      "learning_rate": 4.5522640285436266e-05,
      "loss": 3.1213,
      "step": 139100
    },
    {
      "epoch": 45.12155591572123,
      "grad_norm": 0.9115878939628601,
      "learning_rate": 4.5519396691534224e-05,
      "loss": 3.1362,
      "step": 139200
    },
    {
      "epoch": 45.15397082658023,
      "grad_norm": 0.894212543964386,
      "learning_rate": 4.551615309763218e-05,
      "loss": 3.1232,
      "step": 139300
    },
    {
      "epoch": 45.186385737439224,
      "grad_norm": 0.9627618193626404,
      "learning_rate": 4.5512909503730135e-05,
      "loss": 3.1294,
      "step": 139400
    },
    {
      "epoch": 45.21880064829822,
      "grad_norm": 0.9603878855705261,
      "learning_rate": 4.5509665909828094e-05,
      "loss": 3.132,
      "step": 139500
    },
    {
      "epoch": 45.25121555915721,
      "grad_norm": 1.0172016620635986,
      "learning_rate": 4.550642231592605e-05,
      "loss": 3.1335,
      "step": 139600
    },
    {
      "epoch": 45.283630470016206,
      "grad_norm": 1.0706373453140259,
      "learning_rate": 4.5503178722024005e-05,
      "loss": 3.1221,
      "step": 139700
    },
    {
      "epoch": 45.3160453808752,
      "grad_norm": 0.9187244176864624,
      "learning_rate": 4.549993512812196e-05,
      "loss": 3.1189,
      "step": 139800
    },
    {
      "epoch": 45.3484602917342,
      "grad_norm": 1.0507376194000244,
      "learning_rate": 4.5496691534219915e-05,
      "loss": 3.1249,
      "step": 139900
    },
    {
      "epoch": 45.380875202593195,
      "grad_norm": 0.9996633529663086,
      "learning_rate": 4.5493447940317874e-05,
      "loss": 3.1377,
      "step": 140000
    },
    {
      "epoch": 45.41329011345219,
      "grad_norm": 1.0380547046661377,
      "learning_rate": 4.549020434641583e-05,
      "loss": 3.1178,
      "step": 140100
    },
    {
      "epoch": 45.44570502431118,
      "grad_norm": 0.920059084892273,
      "learning_rate": 4.5486960752513785e-05,
      "loss": 3.1304,
      "step": 140200
    },
    {
      "epoch": 45.47811993517018,
      "grad_norm": 0.9161081910133362,
      "learning_rate": 4.5483717158611743e-05,
      "loss": 3.1234,
      "step": 140300
    },
    {
      "epoch": 45.510534846029174,
      "grad_norm": 0.8582990169525146,
      "learning_rate": 4.54804735647097e-05,
      "loss": 3.1475,
      "step": 140400
    },
    {
      "epoch": 45.54294975688817,
      "grad_norm": 0.8895837068557739,
      "learning_rate": 4.5477229970807654e-05,
      "loss": 3.1277,
      "step": 140500
    },
    {
      "epoch": 45.575364667747166,
      "grad_norm": 0.9232614636421204,
      "learning_rate": 4.547398637690561e-05,
      "loss": 3.12,
      "step": 140600
    },
    {
      "epoch": 45.607779578606156,
      "grad_norm": 0.9658222198486328,
      "learning_rate": 4.547074278300357e-05,
      "loss": 3.1174,
      "step": 140700
    },
    {
      "epoch": 45.64019448946515,
      "grad_norm": 1.0038875341415405,
      "learning_rate": 4.5467499189101524e-05,
      "loss": 3.1242,
      "step": 140800
    },
    {
      "epoch": 45.67260940032415,
      "grad_norm": 0.7769636511802673,
      "learning_rate": 4.546425559519948e-05,
      "loss": 3.1499,
      "step": 140900
    },
    {
      "epoch": 45.705024311183145,
      "grad_norm": 1.023628830909729,
      "learning_rate": 4.5461012001297434e-05,
      "loss": 3.1594,
      "step": 141000
    },
    {
      "epoch": 45.73743922204214,
      "grad_norm": 0.9918527007102966,
      "learning_rate": 4.545776840739539e-05,
      "loss": 3.1412,
      "step": 141100
    },
    {
      "epoch": 45.76985413290114,
      "grad_norm": 1.061635971069336,
      "learning_rate": 4.545452481349335e-05,
      "loss": 3.1264,
      "step": 141200
    },
    {
      "epoch": 45.80226904376013,
      "grad_norm": 1.0102208852767944,
      "learning_rate": 4.545128121959131e-05,
      "loss": 3.1212,
      "step": 141300
    },
    {
      "epoch": 45.83468395461912,
      "grad_norm": 0.9276434779167175,
      "learning_rate": 4.544803762568926e-05,
      "loss": 3.1197,
      "step": 141400
    },
    {
      "epoch": 45.86709886547812,
      "grad_norm": 1.130599856376648,
      "learning_rate": 4.544479403178722e-05,
      "loss": 3.1444,
      "step": 141500
    },
    {
      "epoch": 45.899513776337116,
      "grad_norm": 0.907529890537262,
      "learning_rate": 4.544155043788518e-05,
      "loss": 3.1364,
      "step": 141600
    },
    {
      "epoch": 45.93192868719611,
      "grad_norm": 0.85237717628479,
      "learning_rate": 4.543830684398314e-05,
      "loss": 3.1277,
      "step": 141700
    },
    {
      "epoch": 45.96434359805511,
      "grad_norm": 0.9248133897781372,
      "learning_rate": 4.543509568602011e-05,
      "loss": 3.1483,
      "step": 141800
    },
    {
      "epoch": 45.9967585089141,
      "grad_norm": 0.9887186288833618,
      "learning_rate": 4.543185209211807e-05,
      "loss": 3.1149,
      "step": 141900
    },
    {
      "epoch": 46.0,
      "eval_bleu": 0.9432120466224351,
      "eval_loss": 3.7476415634155273,
      "eval_runtime": 3.9767,
      "eval_samples_per_second": 123.72,
      "eval_steps_per_second": 2.012,
      "step": 141910
    },
    {
      "epoch": 46.029173419773095,
      "grad_norm": 1.017760157585144,
      "learning_rate": 4.542860849821603e-05,
      "loss": 3.128,
      "step": 142000
    },
    {
      "epoch": 46.06158833063209,
      "grad_norm": 0.9456163644790649,
      "learning_rate": 4.5425364904313986e-05,
      "loss": 3.1086,
      "step": 142100
    },
    {
      "epoch": 46.09400324149109,
      "grad_norm": 1.1550788879394531,
      "learning_rate": 4.542212131041194e-05,
      "loss": 3.1311,
      "step": 142200
    },
    {
      "epoch": 46.126418152350084,
      "grad_norm": 1.026902198791504,
      "learning_rate": 4.54188777165099e-05,
      "loss": 3.1282,
      "step": 142300
    },
    {
      "epoch": 46.15883306320907,
      "grad_norm": 1.0089647769927979,
      "learning_rate": 4.5415634122607855e-05,
      "loss": 3.1263,
      "step": 142400
    },
    {
      "epoch": 46.19124797406807,
      "grad_norm": 0.8608373999595642,
      "learning_rate": 4.541239052870581e-05,
      "loss": 3.1194,
      "step": 142500
    },
    {
      "epoch": 46.223662884927066,
      "grad_norm": 0.8565713167190552,
      "learning_rate": 4.5409146934803766e-05,
      "loss": 3.0989,
      "step": 142600
    },
    {
      "epoch": 46.25607779578606,
      "grad_norm": 1.0183147192001343,
      "learning_rate": 4.5405903340901725e-05,
      "loss": 3.1237,
      "step": 142700
    },
    {
      "epoch": 46.28849270664506,
      "grad_norm": 0.8250812292098999,
      "learning_rate": 4.54026921829387e-05,
      "loss": 3.1225,
      "step": 142800
    },
    {
      "epoch": 46.320907617504055,
      "grad_norm": 0.8660604953765869,
      "learning_rate": 4.5399448589036655e-05,
      "loss": 3.1207,
      "step": 142900
    },
    {
      "epoch": 46.353322528363044,
      "grad_norm": 0.8320249319076538,
      "learning_rate": 4.5396204995134613e-05,
      "loss": 3.1262,
      "step": 143000
    },
    {
      "epoch": 46.38573743922204,
      "grad_norm": 0.9543217420578003,
      "learning_rate": 4.539296140123257e-05,
      "loss": 3.129,
      "step": 143100
    },
    {
      "epoch": 46.41815235008104,
      "grad_norm": 0.8417069911956787,
      "learning_rate": 4.5389717807330524e-05,
      "loss": 3.1332,
      "step": 143200
    },
    {
      "epoch": 46.45056726094003,
      "grad_norm": 0.9806358814239502,
      "learning_rate": 4.538647421342848e-05,
      "loss": 3.1239,
      "step": 143300
    },
    {
      "epoch": 46.48298217179903,
      "grad_norm": 1.0434421300888062,
      "learning_rate": 4.538323061952644e-05,
      "loss": 3.143,
      "step": 143400
    },
    {
      "epoch": 46.51539708265802,
      "grad_norm": 0.9713369011878967,
      "learning_rate": 4.5379987025624394e-05,
      "loss": 3.1246,
      "step": 143500
    },
    {
      "epoch": 46.547811993517016,
      "grad_norm": 1.0660046339035034,
      "learning_rate": 4.537674343172235e-05,
      "loss": 3.1373,
      "step": 143600
    },
    {
      "epoch": 46.58022690437601,
      "grad_norm": 1.0193437337875366,
      "learning_rate": 4.5373499837820304e-05,
      "loss": 3.1263,
      "step": 143700
    },
    {
      "epoch": 46.61264181523501,
      "grad_norm": 1.0791493654251099,
      "learning_rate": 4.537025624391826e-05,
      "loss": 3.1219,
      "step": 143800
    },
    {
      "epoch": 46.645056726094005,
      "grad_norm": 0.9801159501075745,
      "learning_rate": 4.536701265001622e-05,
      "loss": 3.1393,
      "step": 143900
    },
    {
      "epoch": 46.677471636953,
      "grad_norm": 0.9358268976211548,
      "learning_rate": 4.5363769056114174e-05,
      "loss": 3.1136,
      "step": 144000
    },
    {
      "epoch": 46.70988654781199,
      "grad_norm": 0.9196190237998962,
      "learning_rate": 4.536052546221213e-05,
      "loss": 3.1201,
      "step": 144100
    },
    {
      "epoch": 46.74230145867099,
      "grad_norm": 0.8364621996879578,
      "learning_rate": 4.535728186831009e-05,
      "loss": 3.1335,
      "step": 144200
    },
    {
      "epoch": 46.77471636952998,
      "grad_norm": 0.949940025806427,
      "learning_rate": 4.535403827440804e-05,
      "loss": 3.116,
      "step": 144300
    },
    {
      "epoch": 46.80713128038898,
      "grad_norm": 1.0605628490447998,
      "learning_rate": 4.5350794680506e-05,
      "loss": 3.1358,
      "step": 144400
    },
    {
      "epoch": 46.839546191247976,
      "grad_norm": 0.9642239212989807,
      "learning_rate": 4.534755108660396e-05,
      "loss": 3.1341,
      "step": 144500
    },
    {
      "epoch": 46.87196110210697,
      "grad_norm": 1.0234025716781616,
      "learning_rate": 4.534430749270191e-05,
      "loss": 3.1186,
      "step": 144600
    },
    {
      "epoch": 46.90437601296596,
      "grad_norm": 0.8895108699798584,
      "learning_rate": 4.534106389879987e-05,
      "loss": 3.1261,
      "step": 144700
    },
    {
      "epoch": 46.93679092382496,
      "grad_norm": 0.8197807669639587,
      "learning_rate": 4.533782030489782e-05,
      "loss": 3.1312,
      "step": 144800
    },
    {
      "epoch": 46.969205834683954,
      "grad_norm": 0.9053052663803101,
      "learning_rate": 4.533457671099578e-05,
      "loss": 3.1229,
      "step": 144900
    },
    {
      "epoch": 47.0,
      "eval_bleu": 1.165935607253103,
      "eval_loss": 3.7525534629821777,
      "eval_runtime": 4.4276,
      "eval_samples_per_second": 111.12,
      "eval_steps_per_second": 1.807,
      "step": 144995
    },
    {
      "epoch": 47.00162074554295,
      "grad_norm": 0.8532252907752991,
      "learning_rate": 4.533133311709374e-05,
      "loss": 3.1149,
      "step": 145000
    },
    {
      "epoch": 47.03403565640195,
      "grad_norm": 0.8322739005088806,
      "learning_rate": 4.53280895231917e-05,
      "loss": 3.1116,
      "step": 145100
    },
    {
      "epoch": 47.06645056726094,
      "grad_norm": 1.0524837970733643,
      "learning_rate": 4.532484592928966e-05,
      "loss": 3.1076,
      "step": 145200
    },
    {
      "epoch": 47.09886547811993,
      "grad_norm": 0.7654354572296143,
      "learning_rate": 4.532160233538762e-05,
      "loss": 3.0974,
      "step": 145300
    },
    {
      "epoch": 47.13128038897893,
      "grad_norm": 0.9560398459434509,
      "learning_rate": 4.531835874148557e-05,
      "loss": 3.1218,
      "step": 145400
    },
    {
      "epoch": 47.163695299837926,
      "grad_norm": 0.9176743626594543,
      "learning_rate": 4.531511514758353e-05,
      "loss": 3.1092,
      "step": 145500
    },
    {
      "epoch": 47.19611021069692,
      "grad_norm": 0.7933011651039124,
      "learning_rate": 4.531187155368148e-05,
      "loss": 3.1252,
      "step": 145600
    },
    {
      "epoch": 47.22852512155592,
      "grad_norm": 0.9406899213790894,
      "learning_rate": 4.530862795977944e-05,
      "loss": 3.1324,
      "step": 145700
    },
    {
      "epoch": 47.26094003241491,
      "grad_norm": 0.8715283274650574,
      "learning_rate": 4.53053843658774e-05,
      "loss": 3.1335,
      "step": 145800
    },
    {
      "epoch": 47.293354943273904,
      "grad_norm": 1.0689418315887451,
      "learning_rate": 4.530214077197535e-05,
      "loss": 3.1152,
      "step": 145900
    },
    {
      "epoch": 47.3257698541329,
      "grad_norm": 0.8630914092063904,
      "learning_rate": 4.529889717807331e-05,
      "loss": 3.1394,
      "step": 146000
    },
    {
      "epoch": 47.3581847649919,
      "grad_norm": 0.7609529495239258,
      "learning_rate": 4.529565358417127e-05,
      "loss": 3.127,
      "step": 146100
    },
    {
      "epoch": 47.39059967585089,
      "grad_norm": 1.000158667564392,
      "learning_rate": 4.529240999026922e-05,
      "loss": 3.1193,
      "step": 146200
    },
    {
      "epoch": 47.42301458670989,
      "grad_norm": 0.8744961619377136,
      "learning_rate": 4.528916639636718e-05,
      "loss": 3.0952,
      "step": 146300
    },
    {
      "epoch": 47.45542949756888,
      "grad_norm": 0.8220177888870239,
      "learning_rate": 4.5285922802465136e-05,
      "loss": 3.1318,
      "step": 146400
    },
    {
      "epoch": 47.487844408427875,
      "grad_norm": 0.9729081988334656,
      "learning_rate": 4.528267920856309e-05,
      "loss": 3.1127,
      "step": 146500
    },
    {
      "epoch": 47.52025931928687,
      "grad_norm": 0.8328625559806824,
      "learning_rate": 4.527943561466105e-05,
      "loss": 3.1158,
      "step": 146600
    },
    {
      "epoch": 47.55267423014587,
      "grad_norm": 0.9399292469024658,
      "learning_rate": 4.5276192020759e-05,
      "loss": 3.1369,
      "step": 146700
    },
    {
      "epoch": 47.585089141004865,
      "grad_norm": 0.8707639575004578,
      "learning_rate": 4.527294842685696e-05,
      "loss": 3.1197,
      "step": 146800
    },
    {
      "epoch": 47.617504051863854,
      "grad_norm": 0.9639185667037964,
      "learning_rate": 4.5269704832954916e-05,
      "loss": 3.1119,
      "step": 146900
    },
    {
      "epoch": 47.64991896272285,
      "grad_norm": 0.9133157134056091,
      "learning_rate": 4.526646123905287e-05,
      "loss": 3.1232,
      "step": 147000
    },
    {
      "epoch": 47.68233387358185,
      "grad_norm": 0.9493808150291443,
      "learning_rate": 4.526321764515083e-05,
      "loss": 3.122,
      "step": 147100
    },
    {
      "epoch": 47.71474878444084,
      "grad_norm": 0.93483966588974,
      "learning_rate": 4.5259974051248786e-05,
      "loss": 3.112,
      "step": 147200
    },
    {
      "epoch": 47.74716369529984,
      "grad_norm": 0.9953950643539429,
      "learning_rate": 4.525673045734674e-05,
      "loss": 3.1165,
      "step": 147300
    },
    {
      "epoch": 47.779578606158836,
      "grad_norm": 0.8445706963539124,
      "learning_rate": 4.5253486863444696e-05,
      "loss": 3.1358,
      "step": 147400
    },
    {
      "epoch": 47.811993517017825,
      "grad_norm": 0.9090886116027832,
      "learning_rate": 4.5250243269542655e-05,
      "loss": 3.1264,
      "step": 147500
    },
    {
      "epoch": 47.84440842787682,
      "grad_norm": 0.9368017911911011,
      "learning_rate": 4.5246999675640614e-05,
      "loss": 3.1401,
      "step": 147600
    },
    {
      "epoch": 47.87682333873582,
      "grad_norm": 1.0287601947784424,
      "learning_rate": 4.524375608173857e-05,
      "loss": 3.1109,
      "step": 147700
    },
    {
      "epoch": 47.909238249594814,
      "grad_norm": 0.8257893919944763,
      "learning_rate": 4.5240512487836525e-05,
      "loss": 3.1238,
      "step": 147800
    },
    {
      "epoch": 47.94165316045381,
      "grad_norm": 1.0042285919189453,
      "learning_rate": 4.5237268893934483e-05,
      "loss": 3.1255,
      "step": 147900
    },
    {
      "epoch": 47.97406807131281,
      "grad_norm": 0.8824120759963989,
      "learning_rate": 4.523402530003244e-05,
      "loss": 3.1168,
      "step": 148000
    },
    {
      "epoch": 48.0,
      "eval_bleu": 1.0739338508100271,
      "eval_loss": 3.750614881515503,
      "eval_runtime": 4.1137,
      "eval_samples_per_second": 119.601,
      "eval_steps_per_second": 1.945,
      "step": 148080
    },
    {
      "epoch": 48.006482982171796,
      "grad_norm": 0.8159235715866089,
      "learning_rate": 4.5230781706130394e-05,
      "loss": 3.1394,
      "step": 148100
    },
    {
      "epoch": 48.03889789303079,
      "grad_norm": 0.8922426104545593,
      "learning_rate": 4.522753811222835e-05,
      "loss": 3.0961,
      "step": 148200
    },
    {
      "epoch": 48.07131280388979,
      "grad_norm": 0.9675890207290649,
      "learning_rate": 4.522429451832631e-05,
      "loss": 3.1103,
      "step": 148300
    },
    {
      "epoch": 48.103727714748786,
      "grad_norm": 0.8389149904251099,
      "learning_rate": 4.5221050924424264e-05,
      "loss": 3.1008,
      "step": 148400
    },
    {
      "epoch": 48.13614262560778,
      "grad_norm": 0.8369548320770264,
      "learning_rate": 4.521780733052222e-05,
      "loss": 3.1158,
      "step": 148500
    },
    {
      "epoch": 48.16855753646677,
      "grad_norm": 0.8855810761451721,
      "learning_rate": 4.5214563736620174e-05,
      "loss": 3.1132,
      "step": 148600
    },
    {
      "epoch": 48.20097244732577,
      "grad_norm": 0.9860530495643616,
      "learning_rate": 4.521132014271813e-05,
      "loss": 3.1184,
      "step": 148700
    },
    {
      "epoch": 48.233387358184764,
      "grad_norm": 0.9256541132926941,
      "learning_rate": 4.520807654881609e-05,
      "loss": 3.1163,
      "step": 148800
    },
    {
      "epoch": 48.26580226904376,
      "grad_norm": 1.0195196866989136,
      "learning_rate": 4.5204832954914044e-05,
      "loss": 3.1047,
      "step": 148900
    },
    {
      "epoch": 48.29821717990276,
      "grad_norm": 0.8926693201065063,
      "learning_rate": 4.5201589361012e-05,
      "loss": 3.1243,
      "step": 149000
    },
    {
      "epoch": 48.33063209076175,
      "grad_norm": 0.8778586387634277,
      "learning_rate": 4.519834576710996e-05,
      "loss": 3.089,
      "step": 149100
    },
    {
      "epoch": 48.36304700162074,
      "grad_norm": 0.794789731502533,
      "learning_rate": 4.519510217320791e-05,
      "loss": 3.1159,
      "step": 149200
    },
    {
      "epoch": 48.39546191247974,
      "grad_norm": 0.9336301684379578,
      "learning_rate": 4.519185857930587e-05,
      "loss": 3.118,
      "step": 149300
    },
    {
      "epoch": 48.427876823338735,
      "grad_norm": 0.8335862159729004,
      "learning_rate": 4.518861498540383e-05,
      "loss": 3.1138,
      "step": 149400
    },
    {
      "epoch": 48.46029173419773,
      "grad_norm": 1.0472791194915771,
      "learning_rate": 4.518537139150178e-05,
      "loss": 3.1154,
      "step": 149500
    },
    {
      "epoch": 48.49270664505673,
      "grad_norm": 0.9193012118339539,
      "learning_rate": 4.518212779759974e-05,
      "loss": 3.1021,
      "step": 149600
    },
    {
      "epoch": 48.525121555915725,
      "grad_norm": 0.8649392127990723,
      "learning_rate": 4.517888420369769e-05,
      "loss": 3.1191,
      "step": 149700
    },
    {
      "epoch": 48.557536466774714,
      "grad_norm": 1.0279219150543213,
      "learning_rate": 4.517564060979565e-05,
      "loss": 3.1016,
      "step": 149800
    },
    {
      "epoch": 48.58995137763371,
      "grad_norm": 0.8999248147010803,
      "learning_rate": 4.517239701589361e-05,
      "loss": 3.1009,
      "step": 149900
    },
    {
      "epoch": 48.62236628849271,
      "grad_norm": 0.9124566316604614,
      "learning_rate": 4.516915342199157e-05,
      "loss": 3.1127,
      "step": 150000
    },
    {
      "epoch": 48.6547811993517,
      "grad_norm": 0.9918031692504883,
      "learning_rate": 4.516590982808953e-05,
      "loss": 3.1213,
      "step": 150100
    },
    {
      "epoch": 48.6871961102107,
      "grad_norm": 1.092551827430725,
      "learning_rate": 4.516266623418749e-05,
      "loss": 3.1237,
      "step": 150200
    },
    {
      "epoch": 48.71961102106969,
      "grad_norm": 1.0144835710525513,
      "learning_rate": 4.515942264028544e-05,
      "loss": 3.1318,
      "step": 150300
    },
    {
      "epoch": 48.752025931928685,
      "grad_norm": 1.1565043926239014,
      "learning_rate": 4.51561790463834e-05,
      "loss": 3.1163,
      "step": 150400
    },
    {
      "epoch": 48.78444084278768,
      "grad_norm": 0.8706660270690918,
      "learning_rate": 4.5152935452481357e-05,
      "loss": 3.14,
      "step": 150500
    },
    {
      "epoch": 48.81685575364668,
      "grad_norm": 0.8656037449836731,
      "learning_rate": 4.514972429451833e-05,
      "loss": 3.1181,
      "step": 150600
    },
    {
      "epoch": 48.849270664505674,
      "grad_norm": 1.071699857711792,
      "learning_rate": 4.5146480700616286e-05,
      "loss": 3.1083,
      "step": 150700
    },
    {
      "epoch": 48.88168557536467,
      "grad_norm": 1.0093107223510742,
      "learning_rate": 4.5143237106714245e-05,
      "loss": 3.1111,
      "step": 150800
    },
    {
      "epoch": 48.91410048622366,
      "grad_norm": 0.9132304787635803,
      "learning_rate": 4.51399935128122e-05,
      "loss": 3.1293,
      "step": 150900
    },
    {
      "epoch": 48.946515397082656,
      "grad_norm": 1.0481152534484863,
      "learning_rate": 4.5136749918910156e-05,
      "loss": 3.1225,
      "step": 151000
    },
    {
      "epoch": 48.97893030794165,
      "grad_norm": 0.8678814768791199,
      "learning_rate": 4.5133506325008114e-05,
      "loss": 3.1343,
      "step": 151100
    },
    {
      "epoch": 49.0,
      "eval_bleu": 1.1070525667943056,
      "eval_loss": 3.7557830810546875,
      "eval_runtime": 4.5072,
      "eval_samples_per_second": 109.159,
      "eval_steps_per_second": 1.775,
      "step": 151165
    },
    {
      "epoch": 49.01134521880065,
      "grad_norm": 0.8396968245506287,
      "learning_rate": 4.5130262731106066e-05,
      "loss": 3.1177,
      "step": 151200
    },
    {
      "epoch": 49.043760129659645,
      "grad_norm": 1.082525610923767,
      "learning_rate": 4.5127019137204025e-05,
      "loss": 3.103,
      "step": 151300
    },
    {
      "epoch": 49.07617504051864,
      "grad_norm": 0.8964900374412537,
      "learning_rate": 4.5123775543301984e-05,
      "loss": 3.0919,
      "step": 151400
    },
    {
      "epoch": 49.10858995137763,
      "grad_norm": 0.9359110593795776,
      "learning_rate": 4.5120531949399936e-05,
      "loss": 3.1033,
      "step": 151500
    },
    {
      "epoch": 49.14100486223663,
      "grad_norm": 0.9962767958641052,
      "learning_rate": 4.5117288355497895e-05,
      "loss": 3.1152,
      "step": 151600
    },
    {
      "epoch": 49.173419773095624,
      "grad_norm": 0.8432102799415588,
      "learning_rate": 4.5114044761595853e-05,
      "loss": 3.1214,
      "step": 151700
    },
    {
      "epoch": 49.20583468395462,
      "grad_norm": 0.8410413861274719,
      "learning_rate": 4.5110801167693805e-05,
      "loss": 3.1119,
      "step": 151800
    },
    {
      "epoch": 49.23824959481362,
      "grad_norm": 0.8476511240005493,
      "learning_rate": 4.5107557573791764e-05,
      "loss": 3.1066,
      "step": 151900
    },
    {
      "epoch": 49.270664505672606,
      "grad_norm": 0.9156368374824524,
      "learning_rate": 4.5104313979889716e-05,
      "loss": 3.1201,
      "step": 152000
    },
    {
      "epoch": 49.3030794165316,
      "grad_norm": 0.9691908359527588,
      "learning_rate": 4.5101070385987675e-05,
      "loss": 3.0979,
      "step": 152100
    },
    {
      "epoch": 49.3354943273906,
      "grad_norm": 0.8192843794822693,
      "learning_rate": 4.5097826792085634e-05,
      "loss": 3.0964,
      "step": 152200
    },
    {
      "epoch": 49.367909238249595,
      "grad_norm": 0.9507142305374146,
      "learning_rate": 4.5094583198183586e-05,
      "loss": 3.1096,
      "step": 152300
    },
    {
      "epoch": 49.40032414910859,
      "grad_norm": 0.898819625377655,
      "learning_rate": 4.5091339604281544e-05,
      "loss": 3.1102,
      "step": 152400
    },
    {
      "epoch": 49.43273905996759,
      "grad_norm": 0.9769629240036011,
      "learning_rate": 4.50880960103795e-05,
      "loss": 3.1047,
      "step": 152500
    },
    {
      "epoch": 49.46515397082658,
      "grad_norm": 1.0441793203353882,
      "learning_rate": 4.5084852416477455e-05,
      "loss": 3.1159,
      "step": 152600
    },
    {
      "epoch": 49.497568881685574,
      "grad_norm": 1.0027556419372559,
      "learning_rate": 4.5081608822575414e-05,
      "loss": 3.0913,
      "step": 152700
    },
    {
      "epoch": 49.52998379254457,
      "grad_norm": 0.8098018765449524,
      "learning_rate": 4.507836522867337e-05,
      "loss": 3.1238,
      "step": 152800
    },
    {
      "epoch": 49.56239870340357,
      "grad_norm": 1.3674529790878296,
      "learning_rate": 4.507512163477133e-05,
      "loss": 3.1124,
      "step": 152900
    },
    {
      "epoch": 49.59481361426256,
      "grad_norm": 0.9630165696144104,
      "learning_rate": 4.507187804086928e-05,
      "loss": 3.1084,
      "step": 153000
    },
    {
      "epoch": 49.62722852512156,
      "grad_norm": 1.055614709854126,
      "learning_rate": 4.506866688290626e-05,
      "loss": 3.1229,
      "step": 153100
    },
    {
      "epoch": 49.65964343598055,
      "grad_norm": 0.9126559495925903,
      "learning_rate": 4.506542328900421e-05,
      "loss": 3.1191,
      "step": 153200
    },
    {
      "epoch": 49.692058346839545,
      "grad_norm": 0.9396699070930481,
      "learning_rate": 4.506217969510217e-05,
      "loss": 3.1119,
      "step": 153300
    },
    {
      "epoch": 49.72447325769854,
      "grad_norm": 0.8181762099266052,
      "learning_rate": 4.505893610120013e-05,
      "loss": 3.1139,
      "step": 153400
    },
    {
      "epoch": 49.75688816855754,
      "grad_norm": 0.8413995504379272,
      "learning_rate": 4.505569250729809e-05,
      "loss": 3.1205,
      "step": 153500
    },
    {
      "epoch": 49.789303079416534,
      "grad_norm": 1.0115749835968018,
      "learning_rate": 4.505244891339605e-05,
      "loss": 3.1294,
      "step": 153600
    },
    {
      "epoch": 49.82171799027552,
      "grad_norm": 1.0071079730987549,
      "learning_rate": 4.504920531949401e-05,
      "loss": 3.0987,
      "step": 153700
    },
    {
      "epoch": 49.85413290113452,
      "grad_norm": 0.9735620617866516,
      "learning_rate": 4.504596172559196e-05,
      "loss": 3.1166,
      "step": 153800
    },
    {
      "epoch": 49.886547811993516,
      "grad_norm": 0.9350029826164246,
      "learning_rate": 4.504271813168992e-05,
      "loss": 3.0976,
      "step": 153900
    },
    {
      "epoch": 49.91896272285251,
      "grad_norm": 0.9160934090614319,
      "learning_rate": 4.5039474537787876e-05,
      "loss": 3.1148,
      "step": 154000
    },
    {
      "epoch": 49.95137763371151,
      "grad_norm": 0.9517815709114075,
      "learning_rate": 4.503623094388583e-05,
      "loss": 3.1225,
      "step": 154100
    },
    {
      "epoch": 49.983792544570505,
      "grad_norm": 0.8653106689453125,
      "learning_rate": 4.503298734998379e-05,
      "loss": 3.1102,
      "step": 154200
    },
    {
      "epoch": 50.0,
      "eval_bleu": 1.151145423420661,
      "eval_loss": 3.7576873302459717,
      "eval_runtime": 4.7736,
      "eval_samples_per_second": 103.066,
      "eval_steps_per_second": 1.676,
      "step": 154250
    },
    {
      "epoch": 50.016207455429495,
      "grad_norm": 0.9743425250053406,
      "learning_rate": 4.502974375608174e-05,
      "loss": 3.0765,
      "step": 154300
    },
    {
      "epoch": 50.04862236628849,
      "grad_norm": 0.9787591695785522,
      "learning_rate": 4.50265001621797e-05,
      "loss": 3.0961,
      "step": 154400
    },
    {
      "epoch": 50.08103727714749,
      "grad_norm": 1.0276235342025757,
      "learning_rate": 4.5023256568277656e-05,
      "loss": 3.088,
      "step": 154500
    },
    {
      "epoch": 50.113452188006484,
      "grad_norm": 1.0611143112182617,
      "learning_rate": 4.502001297437561e-05,
      "loss": 3.1196,
      "step": 154600
    },
    {
      "epoch": 50.14586709886548,
      "grad_norm": 0.9761902689933777,
      "learning_rate": 4.501676938047357e-05,
      "loss": 3.0945,
      "step": 154700
    },
    {
      "epoch": 50.17828200972448,
      "grad_norm": 1.027915120124817,
      "learning_rate": 4.5013525786571526e-05,
      "loss": 3.0989,
      "step": 154800
    },
    {
      "epoch": 50.210696920583466,
      "grad_norm": 1.0402449369430542,
      "learning_rate": 4.501028219266948e-05,
      "loss": 3.1152,
      "step": 154900
    },
    {
      "epoch": 50.24311183144246,
      "grad_norm": 1.0243490934371948,
      "learning_rate": 4.5007038598767436e-05,
      "loss": 3.0908,
      "step": 155000
    },
    {
      "epoch": 50.27552674230146,
      "grad_norm": 1.0416210889816284,
      "learning_rate": 4.5003795004865395e-05,
      "loss": 3.106,
      "step": 155100
    },
    {
      "epoch": 50.307941653160455,
      "grad_norm": 0.8598913550376892,
      "learning_rate": 4.500055141096335e-05,
      "loss": 3.1109,
      "step": 155200
    },
    {
      "epoch": 50.34035656401945,
      "grad_norm": 1.0525538921356201,
      "learning_rate": 4.4997307817061306e-05,
      "loss": 3.0902,
      "step": 155300
    },
    {
      "epoch": 50.37277147487844,
      "grad_norm": 0.9820732474327087,
      "learning_rate": 4.499406422315926e-05,
      "loss": 3.1129,
      "step": 155400
    },
    {
      "epoch": 50.40518638573744,
      "grad_norm": 1.0121688842773438,
      "learning_rate": 4.4990820629257217e-05,
      "loss": 3.0994,
      "step": 155500
    },
    {
      "epoch": 50.43760129659643,
      "grad_norm": 1.0766725540161133,
      "learning_rate": 4.4987577035355175e-05,
      "loss": 3.1057,
      "step": 155600
    },
    {
      "epoch": 50.47001620745543,
      "grad_norm": 0.9170451760292053,
      "learning_rate": 4.498433344145313e-05,
      "loss": 3.1154,
      "step": 155700
    },
    {
      "epoch": 50.502431118314426,
      "grad_norm": 0.7724522352218628,
      "learning_rate": 4.4981089847551086e-05,
      "loss": 3.1143,
      "step": 155800
    },
    {
      "epoch": 50.53484602917342,
      "grad_norm": 0.9767469763755798,
      "learning_rate": 4.4977846253649045e-05,
      "loss": 3.1113,
      "step": 155900
    },
    {
      "epoch": 50.56726094003241,
      "grad_norm": 0.8646152019500732,
      "learning_rate": 4.4974602659747004e-05,
      "loss": 3.114,
      "step": 156000
    },
    {
      "epoch": 50.59967585089141,
      "grad_norm": 1.0001672506332397,
      "learning_rate": 4.497135906584496e-05,
      "loss": 3.1226,
      "step": 156100
    },
    {
      "epoch": 50.632090761750405,
      "grad_norm": 1.0434032678604126,
      "learning_rate": 4.496811547194292e-05,
      "loss": 3.0943,
      "step": 156200
    },
    {
      "epoch": 50.6645056726094,
      "grad_norm": 0.795707643032074,
      "learning_rate": 4.496487187804087e-05,
      "loss": 3.1091,
      "step": 156300
    },
    {
      "epoch": 50.6969205834684,
      "grad_norm": 0.8914735913276672,
      "learning_rate": 4.496162828413883e-05,
      "loss": 3.1071,
      "step": 156400
    },
    {
      "epoch": 50.729335494327394,
      "grad_norm": 0.9482558965682983,
      "learning_rate": 4.4958384690236784e-05,
      "loss": 3.1289,
      "step": 156500
    },
    {
      "epoch": 50.76175040518638,
      "grad_norm": 0.9654344320297241,
      "learning_rate": 4.495514109633474e-05,
      "loss": 3.1056,
      "step": 156600
    },
    {
      "epoch": 50.79416531604538,
      "grad_norm": 1.178837776184082,
      "learning_rate": 4.49518975024327e-05,
      "loss": 3.0988,
      "step": 156700
    },
    {
      "epoch": 50.826580226904376,
      "grad_norm": 0.9621936082839966,
      "learning_rate": 4.494865390853065e-05,
      "loss": 3.1323,
      "step": 156800
    },
    {
      "epoch": 50.85899513776337,
      "grad_norm": 0.872199296951294,
      "learning_rate": 4.494541031462861e-05,
      "loss": 3.0888,
      "step": 156900
    },
    {
      "epoch": 50.89141004862237,
      "grad_norm": 1.0443679094314575,
      "learning_rate": 4.494216672072657e-05,
      "loss": 3.1046,
      "step": 157000
    },
    {
      "epoch": 50.92382495948136,
      "grad_norm": 0.9157425165176392,
      "learning_rate": 4.493892312682452e-05,
      "loss": 3.1032,
      "step": 157100
    },
    {
      "epoch": 50.956239870340355,
      "grad_norm": 0.915890634059906,
      "learning_rate": 4.493567953292248e-05,
      "loss": 3.1181,
      "step": 157200
    },
    {
      "epoch": 50.98865478119935,
      "grad_norm": 0.9522885084152222,
      "learning_rate": 4.493243593902043e-05,
      "loss": 3.098,
      "step": 157300
    },
    {
      "epoch": 51.0,
      "eval_bleu": 1.2205828035850521,
      "eval_loss": 3.7503809928894043,
      "eval_runtime": 5.3163,
      "eval_samples_per_second": 92.546,
      "eval_steps_per_second": 1.505,
      "step": 157335
    },
    {
      "epoch": 51.02106969205835,
      "grad_norm": 1.0643036365509033,
      "learning_rate": 4.492919234511839e-05,
      "loss": 3.0723,
      "step": 157400
    },
    {
      "epoch": 51.053484602917344,
      "grad_norm": 1.051707148551941,
      "learning_rate": 4.492594875121635e-05,
      "loss": 3.1002,
      "step": 157500
    },
    {
      "epoch": 51.08589951377634,
      "grad_norm": 0.9452869892120361,
      "learning_rate": 4.49227051573143e-05,
      "loss": 3.0817,
      "step": 157600
    },
    {
      "epoch": 51.11831442463533,
      "grad_norm": 0.8823714852333069,
      "learning_rate": 4.491946156341226e-05,
      "loss": 3.108,
      "step": 157700
    },
    {
      "epoch": 51.150729335494326,
      "grad_norm": 0.8517045378684998,
      "learning_rate": 4.491621796951022e-05,
      "loss": 3.0932,
      "step": 157800
    },
    {
      "epoch": 51.18314424635332,
      "grad_norm": 0.9068583250045776,
      "learning_rate": 4.491297437560817e-05,
      "loss": 3.0999,
      "step": 157900
    },
    {
      "epoch": 51.21555915721232,
      "grad_norm": 1.0560989379882812,
      "learning_rate": 4.490973078170613e-05,
      "loss": 3.1212,
      "step": 158000
    },
    {
      "epoch": 51.247974068071315,
      "grad_norm": 0.855201780796051,
      "learning_rate": 4.490648718780409e-05,
      "loss": 3.0948,
      "step": 158100
    },
    {
      "epoch": 51.28038897893031,
      "grad_norm": 0.9869187474250793,
      "learning_rate": 4.490324359390204e-05,
      "loss": 3.1053,
      "step": 158200
    },
    {
      "epoch": 51.3128038897893,
      "grad_norm": 1.0160491466522217,
      "learning_rate": 4.49e-05,
      "loss": 3.0873,
      "step": 158300
    },
    {
      "epoch": 51.3452188006483,
      "grad_norm": 0.8936554193496704,
      "learning_rate": 4.489675640609796e-05,
      "loss": 3.1123,
      "step": 158400
    },
    {
      "epoch": 51.37763371150729,
      "grad_norm": 0.8206405639648438,
      "learning_rate": 4.489351281219592e-05,
      "loss": 3.1013,
      "step": 158500
    },
    {
      "epoch": 51.41004862236629,
      "grad_norm": 1.0304954051971436,
      "learning_rate": 4.489026921829388e-05,
      "loss": 3.1137,
      "step": 158600
    },
    {
      "epoch": 51.442463533225286,
      "grad_norm": 0.9344852566719055,
      "learning_rate": 4.488705806033085e-05,
      "loss": 3.1094,
      "step": 158700
    },
    {
      "epoch": 51.474878444084275,
      "grad_norm": 0.9291937947273254,
      "learning_rate": 4.48838144664288e-05,
      "loss": 3.0685,
      "step": 158800
    },
    {
      "epoch": 51.50729335494327,
      "grad_norm": 1.0356419086456299,
      "learning_rate": 4.488057087252676e-05,
      "loss": 3.1302,
      "step": 158900
    },
    {
      "epoch": 51.53970826580227,
      "grad_norm": 0.9128406643867493,
      "learning_rate": 4.487732727862472e-05,
      "loss": 3.0881,
      "step": 159000
    },
    {
      "epoch": 51.572123176661265,
      "grad_norm": 0.988945722579956,
      "learning_rate": 4.4874083684722676e-05,
      "loss": 3.0647,
      "step": 159100
    },
    {
      "epoch": 51.60453808752026,
      "grad_norm": 1.0346429347991943,
      "learning_rate": 4.4870840090820635e-05,
      "loss": 3.1076,
      "step": 159200
    },
    {
      "epoch": 51.63695299837926,
      "grad_norm": 0.9595087170600891,
      "learning_rate": 4.486759649691859e-05,
      "loss": 3.1202,
      "step": 159300
    },
    {
      "epoch": 51.66936790923825,
      "grad_norm": 0.8875600099563599,
      "learning_rate": 4.4864352903016545e-05,
      "loss": 3.1216,
      "step": 159400
    },
    {
      "epoch": 51.70178282009724,
      "grad_norm": 0.9138034582138062,
      "learning_rate": 4.4861109309114504e-05,
      "loss": 3.1032,
      "step": 159500
    },
    {
      "epoch": 51.73419773095624,
      "grad_norm": 0.8980088829994202,
      "learning_rate": 4.4857865715212456e-05,
      "loss": 3.1101,
      "step": 159600
    },
    {
      "epoch": 51.766612641815236,
      "grad_norm": 1.0449392795562744,
      "learning_rate": 4.4854622121310415e-05,
      "loss": 3.1249,
      "step": 159700
    },
    {
      "epoch": 51.79902755267423,
      "grad_norm": 1.0568054914474487,
      "learning_rate": 4.4851378527408373e-05,
      "loss": 3.0834,
      "step": 159800
    },
    {
      "epoch": 51.83144246353323,
      "grad_norm": 0.8995320200920105,
      "learning_rate": 4.4848134933506325e-05,
      "loss": 3.0938,
      "step": 159900
    },
    {
      "epoch": 51.86385737439222,
      "grad_norm": 0.9072110056877136,
      "learning_rate": 4.4844891339604284e-05,
      "loss": 3.1207,
      "step": 160000
    },
    {
      "epoch": 51.896272285251214,
      "grad_norm": 1.0906175374984741,
      "learning_rate": 4.484164774570224e-05,
      "loss": 3.1037,
      "step": 160100
    },
    {
      "epoch": 51.92868719611021,
      "grad_norm": 0.8867667317390442,
      "learning_rate": 4.4838404151800195e-05,
      "loss": 3.1102,
      "step": 160200
    },
    {
      "epoch": 51.96110210696921,
      "grad_norm": 0.8926135301589966,
      "learning_rate": 4.4835160557898154e-05,
      "loss": 3.1065,
      "step": 160300
    },
    {
      "epoch": 51.993517017828204,
      "grad_norm": 0.9557908177375793,
      "learning_rate": 4.483191696399611e-05,
      "loss": 3.0838,
      "step": 160400
    },
    {
      "epoch": 52.0,
      "eval_bleu": 1.2410698298872123,
      "eval_loss": 3.762298107147217,
      "eval_runtime": 4.2085,
      "eval_samples_per_second": 116.906,
      "eval_steps_per_second": 1.901,
      "step": 160420
    },
    {
      "epoch": 52.02593192868719,
      "grad_norm": 1.0619851350784302,
      "learning_rate": 4.4828673370094064e-05,
      "loss": 3.0921,
      "step": 160500
    },
    {
      "epoch": 52.05834683954619,
      "grad_norm": 0.9836370944976807,
      "learning_rate": 4.482542977619202e-05,
      "loss": 3.1041,
      "step": 160600
    },
    {
      "epoch": 52.090761750405186,
      "grad_norm": 0.9678823351860046,
      "learning_rate": 4.4822218618229e-05,
      "loss": 3.0783,
      "step": 160700
    },
    {
      "epoch": 52.12317666126418,
      "grad_norm": 0.8701555728912354,
      "learning_rate": 4.481897502432696e-05,
      "loss": 3.1107,
      "step": 160800
    },
    {
      "epoch": 52.15559157212318,
      "grad_norm": 0.818109929561615,
      "learning_rate": 4.481573143042491e-05,
      "loss": 3.0953,
      "step": 160900
    },
    {
      "epoch": 52.188006482982175,
      "grad_norm": 0.9042512774467468,
      "learning_rate": 4.481248783652287e-05,
      "loss": 3.0954,
      "step": 161000
    },
    {
      "epoch": 52.220421393841164,
      "grad_norm": 0.876621663570404,
      "learning_rate": 4.480924424262082e-05,
      "loss": 3.0897,
      "step": 161100
    },
    {
      "epoch": 52.25283630470016,
      "grad_norm": 0.9201112985610962,
      "learning_rate": 4.480600064871878e-05,
      "loss": 3.088,
      "step": 161200
    },
    {
      "epoch": 52.28525121555916,
      "grad_norm": 0.9242194294929504,
      "learning_rate": 4.480275705481674e-05,
      "loss": 3.1033,
      "step": 161300
    },
    {
      "epoch": 52.31766612641815,
      "grad_norm": 1.023519515991211,
      "learning_rate": 4.479951346091469e-05,
      "loss": 3.0914,
      "step": 161400
    },
    {
      "epoch": 52.35008103727715,
      "grad_norm": 1.0146260261535645,
      "learning_rate": 4.479626986701265e-05,
      "loss": 3.0855,
      "step": 161500
    },
    {
      "epoch": 52.382495948136146,
      "grad_norm": 0.8553302884101868,
      "learning_rate": 4.479302627311061e-05,
      "loss": 3.1127,
      "step": 161600
    },
    {
      "epoch": 52.414910858995135,
      "grad_norm": 0.859031617641449,
      "learning_rate": 4.478978267920856e-05,
      "loss": 3.0913,
      "step": 161700
    },
    {
      "epoch": 52.44732576985413,
      "grad_norm": 1.03914475440979,
      "learning_rate": 4.478653908530652e-05,
      "loss": 3.0839,
      "step": 161800
    },
    {
      "epoch": 52.47974068071313,
      "grad_norm": 1.0289028882980347,
      "learning_rate": 4.478329549140448e-05,
      "loss": 3.1066,
      "step": 161900
    },
    {
      "epoch": 52.512155591572125,
      "grad_norm": 0.9043976068496704,
      "learning_rate": 4.478005189750244e-05,
      "loss": 3.086,
      "step": 162000
    },
    {
      "epoch": 52.54457050243112,
      "grad_norm": 1.1944093704223633,
      "learning_rate": 4.477680830360039e-05,
      "loss": 3.0913,
      "step": 162100
    },
    {
      "epoch": 52.57698541329011,
      "grad_norm": 0.9290409684181213,
      "learning_rate": 4.477356470969835e-05,
      "loss": 3.1126,
      "step": 162200
    },
    {
      "epoch": 52.60940032414911,
      "grad_norm": 1.0535515546798706,
      "learning_rate": 4.477032111579631e-05,
      "loss": 3.0916,
      "step": 162300
    },
    {
      "epoch": 52.6418152350081,
      "grad_norm": 0.8809988498687744,
      "learning_rate": 4.4767077521894266e-05,
      "loss": 3.0959,
      "step": 162400
    },
    {
      "epoch": 52.6742301458671,
      "grad_norm": 1.0527112483978271,
      "learning_rate": 4.476383392799222e-05,
      "loss": 3.0897,
      "step": 162500
    },
    {
      "epoch": 52.706645056726096,
      "grad_norm": 0.9187372326850891,
      "learning_rate": 4.4760590334090176e-05,
      "loss": 3.0941,
      "step": 162600
    },
    {
      "epoch": 52.73905996758509,
      "grad_norm": 0.9578344225883484,
      "learning_rate": 4.4757346740188135e-05,
      "loss": 3.1121,
      "step": 162700
    },
    {
      "epoch": 52.77147487844408,
      "grad_norm": 1.0419291257858276,
      "learning_rate": 4.475410314628609e-05,
      "loss": 3.1084,
      "step": 162800
    },
    {
      "epoch": 52.80388978930308,
      "grad_norm": 0.8757657408714294,
      "learning_rate": 4.4750859552384046e-05,
      "loss": 3.111,
      "step": 162900
    },
    {
      "epoch": 52.836304700162074,
      "grad_norm": 0.8772410750389099,
      "learning_rate": 4.4747615958482e-05,
      "loss": 3.104,
      "step": 163000
    },
    {
      "epoch": 52.86871961102107,
      "grad_norm": 0.808211088180542,
      "learning_rate": 4.4744372364579957e-05,
      "loss": 3.1025,
      "step": 163100
    },
    {
      "epoch": 52.90113452188007,
      "grad_norm": 0.944987952709198,
      "learning_rate": 4.4741128770677915e-05,
      "loss": 3.0956,
      "step": 163200
    },
    {
      "epoch": 52.93354943273906,
      "grad_norm": 0.8905817270278931,
      "learning_rate": 4.473788517677587e-05,
      "loss": 3.0855,
      "step": 163300
    },
    {
      "epoch": 52.96596434359805,
      "grad_norm": 1.0829766988754272,
      "learning_rate": 4.4734674018812845e-05,
      "loss": 3.0984,
      "step": 163400
    },
    {
      "epoch": 52.99837925445705,
      "grad_norm": 0.8204167485237122,
      "learning_rate": 4.4731430424910804e-05,
      "loss": 3.1057,
      "step": 163500
    },
    {
      "epoch": 53.0,
      "eval_bleu": 1.213402287289117,
      "eval_loss": 3.761035442352295,
      "eval_runtime": 4.4275,
      "eval_samples_per_second": 111.124,
      "eval_steps_per_second": 1.807,
      "step": 163505
    },
    {
      "epoch": 53.030794165316046,
      "grad_norm": 1.1334736347198486,
      "learning_rate": 4.472818683100876e-05,
      "loss": 3.0768,
      "step": 163600
    },
    {
      "epoch": 53.06320907617504,
      "grad_norm": 0.883377194404602,
      "learning_rate": 4.4724943237106714e-05,
      "loss": 3.087,
      "step": 163700
    },
    {
      "epoch": 53.09562398703404,
      "grad_norm": 1.0072332620620728,
      "learning_rate": 4.472169964320467e-05,
      "loss": 3.0932,
      "step": 163800
    },
    {
      "epoch": 53.12803889789303,
      "grad_norm": 0.8506906032562256,
      "learning_rate": 4.471845604930263e-05,
      "loss": 3.0956,
      "step": 163900
    },
    {
      "epoch": 53.160453808752024,
      "grad_norm": 1.008259654045105,
      "learning_rate": 4.4715212455400584e-05,
      "loss": 3.0802,
      "step": 164000
    },
    {
      "epoch": 53.19286871961102,
      "grad_norm": 1.0742640495300293,
      "learning_rate": 4.471196886149854e-05,
      "loss": 3.0779,
      "step": 164100
    },
    {
      "epoch": 53.22528363047002,
      "grad_norm": 0.8969210982322693,
      "learning_rate": 4.47087252675965e-05,
      "loss": 3.0921,
      "step": 164200
    },
    {
      "epoch": 53.25769854132901,
      "grad_norm": 1.0013072490692139,
      "learning_rate": 4.4705481673694453e-05,
      "loss": 3.0926,
      "step": 164300
    },
    {
      "epoch": 53.29011345218801,
      "grad_norm": 0.9881411194801331,
      "learning_rate": 4.470223807979241e-05,
      "loss": 3.1044,
      "step": 164400
    },
    {
      "epoch": 53.322528363047,
      "grad_norm": 1.044658899307251,
      "learning_rate": 4.4698994485890364e-05,
      "loss": 3.0998,
      "step": 164500
    },
    {
      "epoch": 53.354943273905995,
      "grad_norm": 0.8106368184089661,
      "learning_rate": 4.469575089198832e-05,
      "loss": 3.0894,
      "step": 164600
    },
    {
      "epoch": 53.38735818476499,
      "grad_norm": 0.8657239079475403,
      "learning_rate": 4.469250729808628e-05,
      "loss": 3.0815,
      "step": 164700
    },
    {
      "epoch": 53.41977309562399,
      "grad_norm": 0.8520392179489136,
      "learning_rate": 4.4689263704184234e-05,
      "loss": 3.0821,
      "step": 164800
    },
    {
      "epoch": 53.452188006482984,
      "grad_norm": 0.8988003730773926,
      "learning_rate": 4.468602011028219e-05,
      "loss": 3.0654,
      "step": 164900
    },
    {
      "epoch": 53.48460291734198,
      "grad_norm": 1.1772228479385376,
      "learning_rate": 4.468277651638015e-05,
      "loss": 3.1071,
      "step": 165000
    },
    {
      "epoch": 53.51701782820097,
      "grad_norm": 0.9131725430488586,
      "learning_rate": 4.467953292247811e-05,
      "loss": 3.0937,
      "step": 165100
    },
    {
      "epoch": 53.54943273905997,
      "grad_norm": 0.9171448349952698,
      "learning_rate": 4.467628932857607e-05,
      "loss": 3.0814,
      "step": 165200
    },
    {
      "epoch": 53.58184764991896,
      "grad_norm": 1.0460872650146484,
      "learning_rate": 4.467304573467402e-05,
      "loss": 3.0874,
      "step": 165300
    },
    {
      "epoch": 53.61426256077796,
      "grad_norm": 0.9420486092567444,
      "learning_rate": 4.466980214077198e-05,
      "loss": 3.1181,
      "step": 165400
    },
    {
      "epoch": 53.646677471636956,
      "grad_norm": 1.0744729042053223,
      "learning_rate": 4.466655854686994e-05,
      "loss": 3.0912,
      "step": 165500
    },
    {
      "epoch": 53.679092382495945,
      "grad_norm": 0.9328302145004272,
      "learning_rate": 4.466331495296789e-05,
      "loss": 3.1102,
      "step": 165600
    },
    {
      "epoch": 53.71150729335494,
      "grad_norm": 0.9722738265991211,
      "learning_rate": 4.466007135906585e-05,
      "loss": 3.1147,
      "step": 165700
    },
    {
      "epoch": 53.74392220421394,
      "grad_norm": 0.9706699252128601,
      "learning_rate": 4.465682776516381e-05,
      "loss": 3.113,
      "step": 165800
    },
    {
      "epoch": 53.776337115072934,
      "grad_norm": 0.8359207510948181,
      "learning_rate": 4.465358417126176e-05,
      "loss": 3.0815,
      "step": 165900
    },
    {
      "epoch": 53.80875202593193,
      "grad_norm": 0.9553876519203186,
      "learning_rate": 4.465034057735972e-05,
      "loss": 3.0988,
      "step": 166000
    },
    {
      "epoch": 53.84116693679093,
      "grad_norm": 0.957941472530365,
      "learning_rate": 4.464709698345768e-05,
      "loss": 3.0876,
      "step": 166100
    },
    {
      "epoch": 53.873581847649916,
      "grad_norm": 0.783126950263977,
      "learning_rate": 4.464385338955563e-05,
      "loss": 3.0967,
      "step": 166200
    },
    {
      "epoch": 53.90599675850891,
      "grad_norm": 0.9657486081123352,
      "learning_rate": 4.464060979565359e-05,
      "loss": 3.0855,
      "step": 166300
    },
    {
      "epoch": 53.93841166936791,
      "grad_norm": 0.8427544832229614,
      "learning_rate": 4.463736620175154e-05,
      "loss": 3.1013,
      "step": 166400
    },
    {
      "epoch": 53.970826580226905,
      "grad_norm": 0.9030826091766357,
      "learning_rate": 4.46341226078495e-05,
      "loss": 3.0989,
      "step": 166500
    },
    {
      "epoch": 54.0,
      "eval_bleu": 1.3060552815292787,
      "eval_loss": 3.7637887001037598,
      "eval_runtime": 4.3334,
      "eval_samples_per_second": 113.538,
      "eval_steps_per_second": 1.846,
      "step": 166590
    },
    {
      "epoch": 54.0032414910859,
      "grad_norm": 0.8096458911895752,
      "learning_rate": 4.463087901394746e-05,
      "loss": 3.0887,
      "step": 166600
    },
    {
      "epoch": 54.0356564019449,
      "grad_norm": 1.0502824783325195,
      "learning_rate": 4.462763542004541e-05,
      "loss": 3.0928,
      "step": 166700
    },
    {
      "epoch": 54.06807131280389,
      "grad_norm": 1.063646674156189,
      "learning_rate": 4.462439182614337e-05,
      "loss": 3.0826,
      "step": 166800
    },
    {
      "epoch": 54.100486223662884,
      "grad_norm": 0.8209788799285889,
      "learning_rate": 4.4621148232241327e-05,
      "loss": 3.0912,
      "step": 166900
    },
    {
      "epoch": 54.13290113452188,
      "grad_norm": 1.0074101686477661,
      "learning_rate": 4.461790463833928e-05,
      "loss": 3.09,
      "step": 167000
    },
    {
      "epoch": 54.16531604538088,
      "grad_norm": 0.9278899431228638,
      "learning_rate": 4.461466104443724e-05,
      "loss": 3.099,
      "step": 167100
    },
    {
      "epoch": 54.19773095623987,
      "grad_norm": 1.001463770866394,
      "learning_rate": 4.4611417450535196e-05,
      "loss": 3.1051,
      "step": 167200
    },
    {
      "epoch": 54.23014586709886,
      "grad_norm": 0.9740964770317078,
      "learning_rate": 4.460817385663315e-05,
      "loss": 3.0672,
      "step": 167300
    },
    {
      "epoch": 54.26256077795786,
      "grad_norm": 0.9629859328269958,
      "learning_rate": 4.4604962698670126e-05,
      "loss": 3.1002,
      "step": 167400
    },
    {
      "epoch": 54.294975688816855,
      "grad_norm": 0.8557560443878174,
      "learning_rate": 4.4601719104768084e-05,
      "loss": 3.088,
      "step": 167500
    },
    {
      "epoch": 54.32739059967585,
      "grad_norm": 0.9163900017738342,
      "learning_rate": 4.4598475510866036e-05,
      "loss": 3.0795,
      "step": 167600
    },
    {
      "epoch": 54.35980551053485,
      "grad_norm": 0.91778564453125,
      "learning_rate": 4.4595231916963995e-05,
      "loss": 3.1132,
      "step": 167700
    },
    {
      "epoch": 54.392220421393844,
      "grad_norm": 0.9968768358230591,
      "learning_rate": 4.4591988323061954e-05,
      "loss": 3.0952,
      "step": 167800
    },
    {
      "epoch": 54.424635332252834,
      "grad_norm": 1.096392273902893,
      "learning_rate": 4.4588744729159906e-05,
      "loss": 3.076,
      "step": 167900
    },
    {
      "epoch": 54.45705024311183,
      "grad_norm": 0.9892162084579468,
      "learning_rate": 4.4585533571196884e-05,
      "loss": 3.095,
      "step": 168000
    },
    {
      "epoch": 54.489465153970826,
      "grad_norm": 0.9461535811424255,
      "learning_rate": 4.458228997729484e-05,
      "loss": 3.0794,
      "step": 168100
    },
    {
      "epoch": 54.52188006482982,
      "grad_norm": 1.0456233024597168,
      "learning_rate": 4.45790463833928e-05,
      "loss": 3.0698,
      "step": 168200
    },
    {
      "epoch": 54.55429497568882,
      "grad_norm": 0.9592326879501343,
      "learning_rate": 4.457580278949075e-05,
      "loss": 3.0982,
      "step": 168300
    },
    {
      "epoch": 54.58670988654781,
      "grad_norm": 1.0210853815078735,
      "learning_rate": 4.457255919558871e-05,
      "loss": 3.0776,
      "step": 168400
    },
    {
      "epoch": 54.619124797406805,
      "grad_norm": 1.0267373323440552,
      "learning_rate": 4.456931560168667e-05,
      "loss": 3.0927,
      "step": 168500
    },
    {
      "epoch": 54.6515397082658,
      "grad_norm": 0.9276992678642273,
      "learning_rate": 4.456607200778463e-05,
      "loss": 3.0811,
      "step": 168600
    },
    {
      "epoch": 54.6839546191248,
      "grad_norm": 1.0178595781326294,
      "learning_rate": 4.456282841388258e-05,
      "loss": 3.0852,
      "step": 168700
    },
    {
      "epoch": 54.716369529983794,
      "grad_norm": 0.904073178768158,
      "learning_rate": 4.455961725591956e-05,
      "loss": 3.0707,
      "step": 168800
    },
    {
      "epoch": 54.74878444084279,
      "grad_norm": 0.9151212573051453,
      "learning_rate": 4.455637366201752e-05,
      "loss": 3.091,
      "step": 168900
    },
    {
      "epoch": 54.78119935170178,
      "grad_norm": 0.8964971303939819,
      "learning_rate": 4.455313006811547e-05,
      "loss": 3.0967,
      "step": 169000
    },
    {
      "epoch": 54.813614262560776,
      "grad_norm": 1.0192793607711792,
      "learning_rate": 4.454988647421343e-05,
      "loss": 3.0785,
      "step": 169100
    },
    {
      "epoch": 54.84602917341977,
      "grad_norm": 0.9625380635261536,
      "learning_rate": 4.454664288031139e-05,
      "loss": 3.0903,
      "step": 169200
    },
    {
      "epoch": 54.87844408427877,
      "grad_norm": 0.8561363220214844,
      "learning_rate": 4.4543399286409346e-05,
      "loss": 3.0963,
      "step": 169300
    },
    {
      "epoch": 54.910858995137765,
      "grad_norm": 0.859481692314148,
      "learning_rate": 4.4540155692507305e-05,
      "loss": 3.0785,
      "step": 169400
    },
    {
      "epoch": 54.94327390599676,
      "grad_norm": 1.1770014762878418,
      "learning_rate": 4.453691209860526e-05,
      "loss": 3.082,
      "step": 169500
    },
    {
      "epoch": 54.97568881685575,
      "grad_norm": 1.099898099899292,
      "learning_rate": 4.4533668504703216e-05,
      "loss": 3.1063,
      "step": 169600
    },
    {
      "epoch": 55.0,
      "eval_bleu": 1.2105644711835633,
      "eval_loss": 3.7647452354431152,
      "eval_runtime": 4.0576,
      "eval_samples_per_second": 121.255,
      "eval_steps_per_second": 1.972,
      "step": 169675
    },
    {
      "epoch": 55.00810372771475,
      "grad_norm": 0.9697479605674744,
      "learning_rate": 4.4530424910801174e-05,
      "loss": 3.0619,
      "step": 169700
    },
    {
      "epoch": 55.040518638573744,
      "grad_norm": 1.151084065437317,
      "learning_rate": 4.4527181316899126e-05,
      "loss": 3.0925,
      "step": 169800
    },
    {
      "epoch": 55.07293354943274,
      "grad_norm": 0.9920884370803833,
      "learning_rate": 4.4523937722997085e-05,
      "loss": 3.0907,
      "step": 169900
    },
    {
      "epoch": 55.10534846029174,
      "grad_norm": 0.9352954030036926,
      "learning_rate": 4.4520694129095044e-05,
      "loss": 3.0802,
      "step": 170000
    },
    {
      "epoch": 55.137763371150726,
      "grad_norm": 0.9317305088043213,
      "learning_rate": 4.4517450535192996e-05,
      "loss": 3.0523,
      "step": 170100
    },
    {
      "epoch": 55.17017828200972,
      "grad_norm": 1.0697829723358154,
      "learning_rate": 4.4514206941290954e-05,
      "loss": 3.0765,
      "step": 170200
    },
    {
      "epoch": 55.20259319286872,
      "grad_norm": 1.2021167278289795,
      "learning_rate": 4.4510963347388906e-05,
      "loss": 3.0809,
      "step": 170300
    },
    {
      "epoch": 55.235008103727715,
      "grad_norm": 1.0012785196304321,
      "learning_rate": 4.4507719753486865e-05,
      "loss": 3.0631,
      "step": 170400
    },
    {
      "epoch": 55.26742301458671,
      "grad_norm": 0.9638460278511047,
      "learning_rate": 4.4504476159584824e-05,
      "loss": 3.1021,
      "step": 170500
    },
    {
      "epoch": 55.29983792544571,
      "grad_norm": 0.9687654376029968,
      "learning_rate": 4.4501232565682776e-05,
      "loss": 3.0824,
      "step": 170600
    },
    {
      "epoch": 55.3322528363047,
      "grad_norm": 1.2248437404632568,
      "learning_rate": 4.4497988971780735e-05,
      "loss": 3.1048,
      "step": 170700
    },
    {
      "epoch": 55.36466774716369,
      "grad_norm": 0.9530860185623169,
      "learning_rate": 4.449474537787869e-05,
      "loss": 3.0808,
      "step": 170800
    },
    {
      "epoch": 55.39708265802269,
      "grad_norm": 0.9905874729156494,
      "learning_rate": 4.4491501783976645e-05,
      "loss": 3.0766,
      "step": 170900
    },
    {
      "epoch": 55.429497568881686,
      "grad_norm": 0.9625462889671326,
      "learning_rate": 4.4488258190074604e-05,
      "loss": 3.0647,
      "step": 171000
    },
    {
      "epoch": 55.46191247974068,
      "grad_norm": 0.9172755479812622,
      "learning_rate": 4.448501459617256e-05,
      "loss": 3.0836,
      "step": 171100
    },
    {
      "epoch": 55.49432739059968,
      "grad_norm": 0.9569387435913086,
      "learning_rate": 4.4481771002270515e-05,
      "loss": 3.0938,
      "step": 171200
    },
    {
      "epoch": 55.52674230145867,
      "grad_norm": 1.1024770736694336,
      "learning_rate": 4.4478527408368473e-05,
      "loss": 3.0852,
      "step": 171300
    },
    {
      "epoch": 55.559157212317665,
      "grad_norm": 1.0434993505477905,
      "learning_rate": 4.4475283814466425e-05,
      "loss": 3.1032,
      "step": 171400
    },
    {
      "epoch": 55.59157212317666,
      "grad_norm": 0.8440393209457397,
      "learning_rate": 4.4472040220564384e-05,
      "loss": 3.1046,
      "step": 171500
    },
    {
      "epoch": 55.62398703403566,
      "grad_norm": 0.9475730061531067,
      "learning_rate": 4.446879662666234e-05,
      "loss": 3.0975,
      "step": 171600
    },
    {
      "epoch": 55.656401944894654,
      "grad_norm": 1.1551638841629028,
      "learning_rate": 4.44655530327603e-05,
      "loss": 3.0728,
      "step": 171700
    },
    {
      "epoch": 55.68881685575364,
      "grad_norm": 1.0345134735107422,
      "learning_rate": 4.446230943885826e-05,
      "loss": 3.0675,
      "step": 171800
    },
    {
      "epoch": 55.72123176661264,
      "grad_norm": 1.1572818756103516,
      "learning_rate": 4.445906584495622e-05,
      "loss": 3.0823,
      "step": 171900
    },
    {
      "epoch": 55.753646677471636,
      "grad_norm": 1.1069427728652954,
      "learning_rate": 4.445582225105417e-05,
      "loss": 3.0942,
      "step": 172000
    },
    {
      "epoch": 55.78606158833063,
      "grad_norm": 1.0463335514068604,
      "learning_rate": 4.445257865715213e-05,
      "loss": 3.0908,
      "step": 172100
    },
    {
      "epoch": 55.81847649918963,
      "grad_norm": 0.9392539858818054,
      "learning_rate": 4.444933506325008e-05,
      "loss": 3.0734,
      "step": 172200
    },
    {
      "epoch": 55.850891410048625,
      "grad_norm": 0.8115541934967041,
      "learning_rate": 4.444609146934804e-05,
      "loss": 3.0818,
      "step": 172300
    },
    {
      "epoch": 55.883306320907614,
      "grad_norm": 0.9366373419761658,
      "learning_rate": 4.4442847875446e-05,
      "loss": 3.097,
      "step": 172400
    },
    {
      "epoch": 55.91572123176661,
      "grad_norm": 0.8653913140296936,
      "learning_rate": 4.443960428154395e-05,
      "loss": 3.0923,
      "step": 172500
    },
    {
      "epoch": 55.94813614262561,
      "grad_norm": 1.118883490562439,
      "learning_rate": 4.443636068764191e-05,
      "loss": 3.0945,
      "step": 172600
    },
    {
      "epoch": 55.980551053484604,
      "grad_norm": 0.8975049257278442,
      "learning_rate": 4.443311709373987e-05,
      "loss": 3.0723,
      "step": 172700
    },
    {
      "epoch": 56.0,
      "eval_bleu": 1.2919208120993049,
      "eval_loss": 3.7712948322296143,
      "eval_runtime": 4.8405,
      "eval_samples_per_second": 101.641,
      "eval_steps_per_second": 1.653,
      "step": 172760
    },
    {
      "epoch": 56.0129659643436,
      "grad_norm": 0.8876650333404541,
      "learning_rate": 4.442987349983782e-05,
      "loss": 3.0626,
      "step": 172800
    },
    {
      "epoch": 56.045380875202596,
      "grad_norm": 0.8401455879211426,
      "learning_rate": 4.442662990593578e-05,
      "loss": 3.0801,
      "step": 172900
    },
    {
      "epoch": 56.077795786061586,
      "grad_norm": 0.8640024662017822,
      "learning_rate": 4.442338631203374e-05,
      "loss": 3.0729,
      "step": 173000
    },
    {
      "epoch": 56.11021069692058,
      "grad_norm": 0.8933972120285034,
      "learning_rate": 4.442014271813169e-05,
      "loss": 3.0717,
      "step": 173100
    },
    {
      "epoch": 56.14262560777958,
      "grad_norm": 0.9339763522148132,
      "learning_rate": 4.441689912422965e-05,
      "loss": 3.0875,
      "step": 173200
    },
    {
      "epoch": 56.175040518638575,
      "grad_norm": 0.9704414010047913,
      "learning_rate": 4.44136555303276e-05,
      "loss": 3.078,
      "step": 173300
    },
    {
      "epoch": 56.20745542949757,
      "grad_norm": 0.8429357409477234,
      "learning_rate": 4.441041193642556e-05,
      "loss": 3.0801,
      "step": 173400
    },
    {
      "epoch": 56.23987034035656,
      "grad_norm": 0.8911164402961731,
      "learning_rate": 4.440716834252352e-05,
      "loss": 3.0851,
      "step": 173500
    },
    {
      "epoch": 56.27228525121556,
      "grad_norm": 0.8735036253929138,
      "learning_rate": 4.440392474862147e-05,
      "loss": 3.066,
      "step": 173600
    },
    {
      "epoch": 56.30470016207455,
      "grad_norm": 0.8166907429695129,
      "learning_rate": 4.440068115471943e-05,
      "loss": 3.0703,
      "step": 173700
    },
    {
      "epoch": 56.33711507293355,
      "grad_norm": 0.9348018765449524,
      "learning_rate": 4.439743756081739e-05,
      "loss": 3.0692,
      "step": 173800
    },
    {
      "epoch": 56.369529983792546,
      "grad_norm": 1.129471778869629,
      "learning_rate": 4.439419396691534e-05,
      "loss": 3.0846,
      "step": 173900
    },
    {
      "epoch": 56.40194489465154,
      "grad_norm": 0.8134592175483704,
      "learning_rate": 4.43909503730133e-05,
      "loss": 3.0957,
      "step": 174000
    },
    {
      "epoch": 56.43435980551053,
      "grad_norm": 0.9984643459320068,
      "learning_rate": 4.438770677911126e-05,
      "loss": 3.08,
      "step": 174100
    },
    {
      "epoch": 56.46677471636953,
      "grad_norm": 1.1238782405853271,
      "learning_rate": 4.4384463185209216e-05,
      "loss": 3.0834,
      "step": 174200
    },
    {
      "epoch": 56.499189627228525,
      "grad_norm": 0.9437011480331421,
      "learning_rate": 4.4381219591307175e-05,
      "loss": 3.0858,
      "step": 174300
    },
    {
      "epoch": 56.53160453808752,
      "grad_norm": 0.9418139457702637,
      "learning_rate": 4.437797599740513e-05,
      "loss": 3.0734,
      "step": 174400
    },
    {
      "epoch": 56.56401944894652,
      "grad_norm": 0.9815011024475098,
      "learning_rate": 4.4374732403503086e-05,
      "loss": 3.0809,
      "step": 174500
    },
    {
      "epoch": 56.596434359805514,
      "grad_norm": 1.0073981285095215,
      "learning_rate": 4.4371488809601044e-05,
      "loss": 3.0709,
      "step": 174600
    },
    {
      "epoch": 56.6288492706645,
      "grad_norm": 0.8952256441116333,
      "learning_rate": 4.4368245215698996e-05,
      "loss": 3.0831,
      "step": 174700
    },
    {
      "epoch": 56.6612641815235,
      "grad_norm": 1.0122112035751343,
      "learning_rate": 4.4365034057735974e-05,
      "loss": 3.095,
      "step": 174800
    },
    {
      "epoch": 56.693679092382496,
      "grad_norm": 1.1019912958145142,
      "learning_rate": 4.436179046383393e-05,
      "loss": 3.0921,
      "step": 174900
    },
    {
      "epoch": 56.72609400324149,
      "grad_norm": 1.0311636924743652,
      "learning_rate": 4.435854686993189e-05,
      "loss": 3.0615,
      "step": 175000
    },
    {
      "epoch": 56.75850891410049,
      "grad_norm": 0.920150876045227,
      "learning_rate": 4.4355303276029843e-05,
      "loss": 3.069,
      "step": 175100
    },
    {
      "epoch": 56.79092382495948,
      "grad_norm": 1.040536880493164,
      "learning_rate": 4.43520596821278e-05,
      "loss": 3.0791,
      "step": 175200
    },
    {
      "epoch": 56.823338735818474,
      "grad_norm": 0.8571653366088867,
      "learning_rate": 4.434881608822576e-05,
      "loss": 3.081,
      "step": 175300
    },
    {
      "epoch": 56.85575364667747,
      "grad_norm": 1.0282868146896362,
      "learning_rate": 4.434557249432371e-05,
      "loss": 3.0663,
      "step": 175400
    },
    {
      "epoch": 56.88816855753647,
      "grad_norm": 0.915986955165863,
      "learning_rate": 4.434232890042167e-05,
      "loss": 3.0762,
      "step": 175500
    },
    {
      "epoch": 56.92058346839546,
      "grad_norm": 0.9570252895355225,
      "learning_rate": 4.4339085306519624e-05,
      "loss": 3.0924,
      "step": 175600
    },
    {
      "epoch": 56.95299837925446,
      "grad_norm": 0.9513145685195923,
      "learning_rate": 4.433584171261758e-05,
      "loss": 3.0804,
      "step": 175700
    },
    {
      "epoch": 56.98541329011345,
      "grad_norm": 0.9313979744911194,
      "learning_rate": 4.433259811871554e-05,
      "loss": 3.0809,
      "step": 175800
    },
    {
      "epoch": 57.0,
      "eval_bleu": 1.0905839461629254,
      "eval_loss": 3.7697227001190186,
      "eval_runtime": 4.7383,
      "eval_samples_per_second": 103.835,
      "eval_steps_per_second": 1.688,
      "step": 175845
    },
    {
      "epoch": 57.017828200972446,
      "grad_norm": 0.9048677682876587,
      "learning_rate": 4.432935452481349e-05,
      "loss": 3.0576,
      "step": 175900
    },
    {
      "epoch": 57.05024311183144,
      "grad_norm": 0.897935688495636,
      "learning_rate": 4.432611093091145e-05,
      "loss": 3.0718,
      "step": 176000
    },
    {
      "epoch": 57.08265802269044,
      "grad_norm": 0.9602908492088318,
      "learning_rate": 4.432286733700941e-05,
      "loss": 3.0746,
      "step": 176100
    },
    {
      "epoch": 57.115072933549435,
      "grad_norm": 0.9436280727386475,
      "learning_rate": 4.431962374310736e-05,
      "loss": 3.0611,
      "step": 176200
    },
    {
      "epoch": 57.14748784440843,
      "grad_norm": 1.329330563545227,
      "learning_rate": 4.431638014920532e-05,
      "loss": 3.0811,
      "step": 176300
    },
    {
      "epoch": 57.17990275526742,
      "grad_norm": 0.8829637765884399,
      "learning_rate": 4.431313655530328e-05,
      "loss": 3.0748,
      "step": 176400
    },
    {
      "epoch": 57.21231766612642,
      "grad_norm": 1.1503254175186157,
      "learning_rate": 4.430989296140123e-05,
      "loss": 3.0562,
      "step": 176500
    },
    {
      "epoch": 57.24473257698541,
      "grad_norm": 0.9482676386833191,
      "learning_rate": 4.430664936749919e-05,
      "loss": 3.081,
      "step": 176600
    },
    {
      "epoch": 57.27714748784441,
      "grad_norm": 0.8969950675964355,
      "learning_rate": 4.430340577359714e-05,
      "loss": 3.0754,
      "step": 176700
    },
    {
      "epoch": 57.309562398703406,
      "grad_norm": 1.0222902297973633,
      "learning_rate": 4.430019461563413e-05,
      "loss": 3.0495,
      "step": 176800
    },
    {
      "epoch": 57.341977309562395,
      "grad_norm": 0.9790005087852478,
      "learning_rate": 4.429695102173208e-05,
      "loss": 3.078,
      "step": 176900
    },
    {
      "epoch": 57.37439222042139,
      "grad_norm": 1.0091127157211304,
      "learning_rate": 4.429370742783004e-05,
      "loss": 3.052,
      "step": 177000
    },
    {
      "epoch": 57.40680713128039,
      "grad_norm": 0.9778331518173218,
      "learning_rate": 4.429046383392799e-05,
      "loss": 3.0702,
      "step": 177100
    },
    {
      "epoch": 57.439222042139384,
      "grad_norm": 0.7851403951644897,
      "learning_rate": 4.428722024002595e-05,
      "loss": 3.0939,
      "step": 177200
    },
    {
      "epoch": 57.47163695299838,
      "grad_norm": 0.9741658568382263,
      "learning_rate": 4.428397664612391e-05,
      "loss": 3.0816,
      "step": 177300
    },
    {
      "epoch": 57.50405186385738,
      "grad_norm": 0.8399521708488464,
      "learning_rate": 4.428073305222186e-05,
      "loss": 3.0872,
      "step": 177400
    },
    {
      "epoch": 57.53646677471637,
      "grad_norm": 0.9239978790283203,
      "learning_rate": 4.427748945831982e-05,
      "loss": 3.0647,
      "step": 177500
    },
    {
      "epoch": 57.56888168557536,
      "grad_norm": 0.8352873921394348,
      "learning_rate": 4.427424586441778e-05,
      "loss": 3.0993,
      "step": 177600
    },
    {
      "epoch": 57.60129659643436,
      "grad_norm": 1.0507447719573975,
      "learning_rate": 4.4271002270515736e-05,
      "loss": 3.0738,
      "step": 177700
    },
    {
      "epoch": 57.633711507293356,
      "grad_norm": 0.8412164449691772,
      "learning_rate": 4.426775867661369e-05,
      "loss": 3.0629,
      "step": 177800
    },
    {
      "epoch": 57.66612641815235,
      "grad_norm": 0.9211402535438538,
      "learning_rate": 4.4264515082711646e-05,
      "loss": 3.0784,
      "step": 177900
    },
    {
      "epoch": 57.69854132901135,
      "grad_norm": 0.9064056277275085,
      "learning_rate": 4.4261271488809605e-05,
      "loss": 3.0827,
      "step": 178000
    },
    {
      "epoch": 57.73095623987034,
      "grad_norm": 0.9838758111000061,
      "learning_rate": 4.4258027894907564e-05,
      "loss": 3.0975,
      "step": 178100
    },
    {
      "epoch": 57.763371150729334,
      "grad_norm": 1.0438121557235718,
      "learning_rate": 4.4254784301005516e-05,
      "loss": 3.085,
      "step": 178200
    },
    {
      "epoch": 57.79578606158833,
      "grad_norm": 1.1364717483520508,
      "learning_rate": 4.4251540707103475e-05,
      "loss": 3.0736,
      "step": 178300
    },
    {
      "epoch": 57.82820097244733,
      "grad_norm": 0.8668408989906311,
      "learning_rate": 4.424829711320143e-05,
      "loss": 3.0676,
      "step": 178400
    },
    {
      "epoch": 57.86061588330632,
      "grad_norm": 0.9514926671981812,
      "learning_rate": 4.4245053519299385e-05,
      "loss": 3.0893,
      "step": 178500
    },
    {
      "epoch": 57.89303079416531,
      "grad_norm": 0.8557425737380981,
      "learning_rate": 4.4241809925397344e-05,
      "loss": 3.0838,
      "step": 178600
    },
    {
      "epoch": 57.92544570502431,
      "grad_norm": 0.9923211932182312,
      "learning_rate": 4.42385663314953e-05,
      "loss": 3.0638,
      "step": 178700
    },
    {
      "epoch": 57.957860615883305,
      "grad_norm": 0.8923498392105103,
      "learning_rate": 4.423535517353228e-05,
      "loss": 3.0919,
      "step": 178800
    },
    {
      "epoch": 57.9902755267423,
      "grad_norm": 0.9183527231216431,
      "learning_rate": 4.423214401556925e-05,
      "loss": 3.0716,
      "step": 178900
    },
    {
      "epoch": 58.0,
      "eval_bleu": 1.218304024487825,
      "eval_loss": 3.7746798992156982,
      "eval_runtime": 4.6522,
      "eval_samples_per_second": 105.757,
      "eval_steps_per_second": 1.72,
      "step": 178930
    },
    {
      "epoch": 58.0226904376013,
      "grad_norm": 0.9891571402549744,
      "learning_rate": 4.422890042166721e-05,
      "loss": 3.0686,
      "step": 179000
    },
    {
      "epoch": 58.055105348460295,
      "grad_norm": 1.034783124923706,
      "learning_rate": 4.422565682776517e-05,
      "loss": 3.0712,
      "step": 179100
    },
    {
      "epoch": 58.087520259319284,
      "grad_norm": 1.1537972688674927,
      "learning_rate": 4.422241323386313e-05,
      "loss": 3.0625,
      "step": 179200
    },
    {
      "epoch": 58.11993517017828,
      "grad_norm": 1.0361486673355103,
      "learning_rate": 4.421916963996108e-05,
      "loss": 3.0676,
      "step": 179300
    },
    {
      "epoch": 58.15235008103728,
      "grad_norm": 1.105081558227539,
      "learning_rate": 4.421592604605904e-05,
      "loss": 3.0897,
      "step": 179400
    },
    {
      "epoch": 58.18476499189627,
      "grad_norm": 1.1052169799804688,
      "learning_rate": 4.421268245215699e-05,
      "loss": 3.0568,
      "step": 179500
    },
    {
      "epoch": 58.21717990275527,
      "grad_norm": 0.8295475840568542,
      "learning_rate": 4.420943885825495e-05,
      "loss": 3.0565,
      "step": 179600
    },
    {
      "epoch": 58.249594813614266,
      "grad_norm": 0.8754131197929382,
      "learning_rate": 4.420619526435291e-05,
      "loss": 3.0649,
      "step": 179700
    },
    {
      "epoch": 58.282009724473255,
      "grad_norm": 1.1506290435791016,
      "learning_rate": 4.420295167045086e-05,
      "loss": 3.068,
      "step": 179800
    },
    {
      "epoch": 58.31442463533225,
      "grad_norm": 0.9906895756721497,
      "learning_rate": 4.419970807654882e-05,
      "loss": 3.0873,
      "step": 179900
    },
    {
      "epoch": 58.34683954619125,
      "grad_norm": 1.1105040311813354,
      "learning_rate": 4.419646448264678e-05,
      "loss": 3.0453,
      "step": 180000
    },
    {
      "epoch": 58.379254457050244,
      "grad_norm": 0.8596630096435547,
      "learning_rate": 4.419322088874473e-05,
      "loss": 3.0676,
      "step": 180100
    },
    {
      "epoch": 58.41166936790924,
      "grad_norm": 0.9051358103752136,
      "learning_rate": 4.418997729484269e-05,
      "loss": 3.0785,
      "step": 180200
    },
    {
      "epoch": 58.44408427876823,
      "grad_norm": 1.0401694774627686,
      "learning_rate": 4.418673370094065e-05,
      "loss": 3.0814,
      "step": 180300
    },
    {
      "epoch": 58.476499189627226,
      "grad_norm": 0.9706991314888,
      "learning_rate": 4.41834901070386e-05,
      "loss": 3.0666,
      "step": 180400
    },
    {
      "epoch": 58.50891410048622,
      "grad_norm": 0.9607043266296387,
      "learning_rate": 4.418024651313656e-05,
      "loss": 3.0686,
      "step": 180500
    },
    {
      "epoch": 58.54132901134522,
      "grad_norm": 1.1464266777038574,
      "learning_rate": 4.417700291923451e-05,
      "loss": 3.0679,
      "step": 180600
    },
    {
      "epoch": 58.573743922204216,
      "grad_norm": 0.9490071535110474,
      "learning_rate": 4.417375932533247e-05,
      "loss": 3.0705,
      "step": 180700
    },
    {
      "epoch": 58.60615883306321,
      "grad_norm": 0.99520343542099,
      "learning_rate": 4.417051573143043e-05,
      "loss": 3.0637,
      "step": 180800
    },
    {
      "epoch": 58.6385737439222,
      "grad_norm": 0.9586635828018188,
      "learning_rate": 4.416727213752838e-05,
      "loss": 3.0771,
      "step": 180900
    },
    {
      "epoch": 58.6709886547812,
      "grad_norm": 0.9579364657402039,
      "learning_rate": 4.416402854362634e-05,
      "loss": 3.066,
      "step": 181000
    },
    {
      "epoch": 58.703403565640194,
      "grad_norm": 1.0461047887802124,
      "learning_rate": 4.4160784949724296e-05,
      "loss": 3.0545,
      "step": 181100
    },
    {
      "epoch": 58.73581847649919,
      "grad_norm": 1.0280810594558716,
      "learning_rate": 4.415754135582225e-05,
      "loss": 3.0635,
      "step": 181200
    },
    {
      "epoch": 58.76823338735819,
      "grad_norm": 0.9722675085067749,
      "learning_rate": 4.415429776192021e-05,
      "loss": 3.0761,
      "step": 181300
    },
    {
      "epoch": 58.80064829821718,
      "grad_norm": 1.2333132028579712,
      "learning_rate": 4.4151054168018166e-05,
      "loss": 3.0773,
      "step": 181400
    },
    {
      "epoch": 58.83306320907617,
      "grad_norm": 1.0046762228012085,
      "learning_rate": 4.4147810574116125e-05,
      "loss": 3.0862,
      "step": 181500
    },
    {
      "epoch": 58.86547811993517,
      "grad_norm": 0.9400038123130798,
      "learning_rate": 4.4144566980214083e-05,
      "loss": 3.0838,
      "step": 181600
    },
    {
      "epoch": 58.897893030794165,
      "grad_norm": 1.117648959159851,
      "learning_rate": 4.4141323386312035e-05,
      "loss": 3.0705,
      "step": 181700
    },
    {
      "epoch": 58.93030794165316,
      "grad_norm": 0.8535289168357849,
      "learning_rate": 4.4138079792409994e-05,
      "loss": 3.0982,
      "step": 181800
    },
    {
      "epoch": 58.96272285251216,
      "grad_norm": 0.8865091800689697,
      "learning_rate": 4.413483619850795e-05,
      "loss": 3.0826,
      "step": 181900
    },
    {
      "epoch": 58.99513776337115,
      "grad_norm": 1.030324101448059,
      "learning_rate": 4.4131625040544924e-05,
      "loss": 3.0857,
      "step": 182000
    },
    {
      "epoch": 59.0,
      "eval_bleu": 1.3125946033464335,
      "eval_loss": 3.771334171295166,
      "eval_runtime": 4.9336,
      "eval_samples_per_second": 99.724,
      "eval_steps_per_second": 1.622,
      "step": 182015
    },
    {
      "epoch": 59.027552674230144,
      "grad_norm": 1.0162546634674072,
      "learning_rate": 4.412838144664288e-05,
      "loss": 3.0592,
      "step": 182100
    },
    {
      "epoch": 59.05996758508914,
      "grad_norm": 0.9468590021133423,
      "learning_rate": 4.412513785274084e-05,
      "loss": 3.0768,
      "step": 182200
    },
    {
      "epoch": 59.09238249594814,
      "grad_norm": 1.2393648624420166,
      "learning_rate": 4.41218942588388e-05,
      "loss": 3.0602,
      "step": 182300
    },
    {
      "epoch": 59.12479740680713,
      "grad_norm": 0.9997361898422241,
      "learning_rate": 4.411865066493675e-05,
      "loss": 3.0596,
      "step": 182400
    },
    {
      "epoch": 59.15721231766613,
      "grad_norm": 1.0541558265686035,
      "learning_rate": 4.411540707103471e-05,
      "loss": 3.0497,
      "step": 182500
    },
    {
      "epoch": 59.18962722852512,
      "grad_norm": 0.8438422679901123,
      "learning_rate": 4.411216347713267e-05,
      "loss": 3.0735,
      "step": 182600
    },
    {
      "epoch": 59.222042139384115,
      "grad_norm": 0.9710532426834106,
      "learning_rate": 4.410891988323062e-05,
      "loss": 3.0717,
      "step": 182700
    },
    {
      "epoch": 59.25445705024311,
      "grad_norm": 0.9883425235748291,
      "learning_rate": 4.410567628932858e-05,
      "loss": 3.0516,
      "step": 182800
    },
    {
      "epoch": 59.28687196110211,
      "grad_norm": 1.0859140157699585,
      "learning_rate": 4.410243269542653e-05,
      "loss": 3.067,
      "step": 182900
    },
    {
      "epoch": 59.319286871961104,
      "grad_norm": 0.9958247542381287,
      "learning_rate": 4.409918910152449e-05,
      "loss": 3.0697,
      "step": 183000
    },
    {
      "epoch": 59.3517017828201,
      "grad_norm": 1.1476421356201172,
      "learning_rate": 4.409594550762245e-05,
      "loss": 3.0758,
      "step": 183100
    },
    {
      "epoch": 59.38411669367909,
      "grad_norm": 1.0573232173919678,
      "learning_rate": 4.40927019137204e-05,
      "loss": 3.0792,
      "step": 183200
    },
    {
      "epoch": 59.416531604538086,
      "grad_norm": 1.0666143894195557,
      "learning_rate": 4.408949075575738e-05,
      "loss": 3.0739,
      "step": 183300
    },
    {
      "epoch": 59.44894651539708,
      "grad_norm": 0.8798184990882874,
      "learning_rate": 4.408624716185534e-05,
      "loss": 3.0409,
      "step": 183400
    },
    {
      "epoch": 59.48136142625608,
      "grad_norm": 0.9080670475959778,
      "learning_rate": 4.40830035679533e-05,
      "loss": 3.068,
      "step": 183500
    },
    {
      "epoch": 59.513776337115075,
      "grad_norm": 1.0992388725280762,
      "learning_rate": 4.407975997405125e-05,
      "loss": 3.0803,
      "step": 183600
    },
    {
      "epoch": 59.546191247974065,
      "grad_norm": 1.0189086198806763,
      "learning_rate": 4.407651638014921e-05,
      "loss": 3.0737,
      "step": 183700
    },
    {
      "epoch": 59.57860615883306,
      "grad_norm": 0.9760293364524841,
      "learning_rate": 4.4073272786247166e-05,
      "loss": 3.0659,
      "step": 183800
    },
    {
      "epoch": 59.61102106969206,
      "grad_norm": 1.0488795042037964,
      "learning_rate": 4.407002919234512e-05,
      "loss": 3.0511,
      "step": 183900
    },
    {
      "epoch": 59.643435980551054,
      "grad_norm": 0.8668627142906189,
      "learning_rate": 4.406678559844308e-05,
      "loss": 3.0973,
      "step": 184000
    },
    {
      "epoch": 59.67585089141005,
      "grad_norm": 0.9437422752380371,
      "learning_rate": 4.4063542004541036e-05,
      "loss": 3.0608,
      "step": 184100
    },
    {
      "epoch": 59.70826580226905,
      "grad_norm": 1.1421325206756592,
      "learning_rate": 4.406029841063899e-05,
      "loss": 3.0483,
      "step": 184200
    },
    {
      "epoch": 59.740680713128036,
      "grad_norm": 1.2751518487930298,
      "learning_rate": 4.4057054816736947e-05,
      "loss": 3.0669,
      "step": 184300
    },
    {
      "epoch": 59.77309562398703,
      "grad_norm": 0.9740843176841736,
      "learning_rate": 4.40538112228349e-05,
      "loss": 3.0476,
      "step": 184400
    },
    {
      "epoch": 59.80551053484603,
      "grad_norm": 0.8945028781890869,
      "learning_rate": 4.405056762893286e-05,
      "loss": 3.0732,
      "step": 184500
    },
    {
      "epoch": 59.837925445705025,
      "grad_norm": 0.9093775749206543,
      "learning_rate": 4.4047324035030816e-05,
      "loss": 3.0808,
      "step": 184600
    },
    {
      "epoch": 59.87034035656402,
      "grad_norm": 0.958479106426239,
      "learning_rate": 4.404408044112877e-05,
      "loss": 3.0865,
      "step": 184700
    },
    {
      "epoch": 59.90275526742302,
      "grad_norm": 0.9988131523132324,
      "learning_rate": 4.404083684722673e-05,
      "loss": 3.0767,
      "step": 184800
    },
    {
      "epoch": 59.93517017828201,
      "grad_norm": 0.9560626149177551,
      "learning_rate": 4.4037593253324685e-05,
      "loss": 3.0575,
      "step": 184900
    },
    {
      "epoch": 59.967585089141004,
      "grad_norm": 0.9048149585723877,
      "learning_rate": 4.4034349659422644e-05,
      "loss": 3.0515,
      "step": 185000
    },
    {
      "epoch": 60.0,
      "grad_norm": 0.9394872784614563,
      "learning_rate": 4.40311060655206e-05,
      "loss": 3.0761,
      "step": 185100
    },
    {
      "epoch": 60.0,
      "eval_bleu": 1.1745726010883217,
      "eval_loss": 3.771141767501831,
      "eval_runtime": 4.2558,
      "eval_samples_per_second": 115.607,
      "eval_steps_per_second": 1.88,
      "step": 185100
    },
    {
      "epoch": 60.032414910858996,
      "grad_norm": 0.8226368427276611,
      "learning_rate": 4.4027862471618555e-05,
      "loss": 3.0483,
      "step": 185200
    },
    {
      "epoch": 60.06482982171799,
      "grad_norm": 1.102583408355713,
      "learning_rate": 4.4024618877716514e-05,
      "loss": 3.033,
      "step": 185300
    },
    {
      "epoch": 60.09724473257698,
      "grad_norm": 1.00700843334198,
      "learning_rate": 4.4021407719753485e-05,
      "loss": 3.0708,
      "step": 185400
    },
    {
      "epoch": 60.12965964343598,
      "grad_norm": 1.1970701217651367,
      "learning_rate": 4.4018164125851443e-05,
      "loss": 3.0613,
      "step": 185500
    },
    {
      "epoch": 60.162074554294975,
      "grad_norm": 0.8917521834373474,
      "learning_rate": 4.40149205319494e-05,
      "loss": 3.0599,
      "step": 185600
    },
    {
      "epoch": 60.19448946515397,
      "grad_norm": 0.9084467887878418,
      "learning_rate": 4.401167693804736e-05,
      "loss": 3.0574,
      "step": 185700
    },
    {
      "epoch": 60.22690437601297,
      "grad_norm": 0.9764646887779236,
      "learning_rate": 4.400843334414532e-05,
      "loss": 3.0497,
      "step": 185800
    },
    {
      "epoch": 60.259319286871964,
      "grad_norm": 1.176646113395691,
      "learning_rate": 4.400518975024327e-05,
      "loss": 3.0653,
      "step": 185900
    },
    {
      "epoch": 60.29173419773095,
      "grad_norm": 0.9441313743591309,
      "learning_rate": 4.400194615634123e-05,
      "loss": 3.0587,
      "step": 186000
    },
    {
      "epoch": 60.32414910858995,
      "grad_norm": 1.0887404680252075,
      "learning_rate": 4.399870256243919e-05,
      "loss": 3.0532,
      "step": 186100
    },
    {
      "epoch": 60.356564019448946,
      "grad_norm": 0.8637223839759827,
      "learning_rate": 4.399545896853714e-05,
      "loss": 3.0559,
      "step": 186200
    },
    {
      "epoch": 60.38897893030794,
      "grad_norm": 1.1171196699142456,
      "learning_rate": 4.39922153746351e-05,
      "loss": 3.0678,
      "step": 186300
    },
    {
      "epoch": 60.42139384116694,
      "grad_norm": 0.8717026710510254,
      "learning_rate": 4.398897178073306e-05,
      "loss": 3.0423,
      "step": 186400
    },
    {
      "epoch": 60.453808752025935,
      "grad_norm": 0.8688693642616272,
      "learning_rate": 4.398572818683101e-05,
      "loss": 3.0486,
      "step": 186500
    },
    {
      "epoch": 60.486223662884925,
      "grad_norm": 0.9162328243255615,
      "learning_rate": 4.398248459292897e-05,
      "loss": 3.089,
      "step": 186600
    },
    {
      "epoch": 60.51863857374392,
      "grad_norm": 0.9879987835884094,
      "learning_rate": 4.397924099902692e-05,
      "loss": 3.0615,
      "step": 186700
    },
    {
      "epoch": 60.55105348460292,
      "grad_norm": 0.9252604246139526,
      "learning_rate": 4.397599740512488e-05,
      "loss": 3.0629,
      "step": 186800
    },
    {
      "epoch": 60.583468395461914,
      "grad_norm": 0.8941739201545715,
      "learning_rate": 4.397275381122284e-05,
      "loss": 3.0883,
      "step": 186900
    },
    {
      "epoch": 60.61588330632091,
      "grad_norm": 0.8336390256881714,
      "learning_rate": 4.396951021732079e-05,
      "loss": 3.0489,
      "step": 187000
    },
    {
      "epoch": 60.6482982171799,
      "grad_norm": 0.8716663122177124,
      "learning_rate": 4.396626662341875e-05,
      "loss": 3.073,
      "step": 187100
    },
    {
      "epoch": 60.680713128038896,
      "grad_norm": 1.0754162073135376,
      "learning_rate": 4.396302302951671e-05,
      "loss": 3.0498,
      "step": 187200
    },
    {
      "epoch": 60.71312803889789,
      "grad_norm": 1.0052999258041382,
      "learning_rate": 4.395977943561466e-05,
      "loss": 3.0716,
      "step": 187300
    },
    {
      "epoch": 60.74554294975689,
      "grad_norm": 0.9895509481430054,
      "learning_rate": 4.395653584171262e-05,
      "loss": 3.059,
      "step": 187400
    },
    {
      "epoch": 60.777957860615885,
      "grad_norm": 0.9713923335075378,
      "learning_rate": 4.395329224781057e-05,
      "loss": 3.0612,
      "step": 187500
    },
    {
      "epoch": 60.81037277147488,
      "grad_norm": 0.7779287099838257,
      "learning_rate": 4.395004865390853e-05,
      "loss": 3.0732,
      "step": 187600
    },
    {
      "epoch": 60.84278768233387,
      "grad_norm": 1.0104321241378784,
      "learning_rate": 4.394680506000649e-05,
      "loss": 3.0694,
      "step": 187700
    },
    {
      "epoch": 60.87520259319287,
      "grad_norm": 1.1676678657531738,
      "learning_rate": 4.394356146610444e-05,
      "loss": 3.0632,
      "step": 187800
    },
    {
      "epoch": 60.90761750405186,
      "grad_norm": 1.0543310642242432,
      "learning_rate": 4.39403178722024e-05,
      "loss": 3.0591,
      "step": 187900
    },
    {
      "epoch": 60.94003241491086,
      "grad_norm": 0.8929482698440552,
      "learning_rate": 4.393707427830036e-05,
      "loss": 3.0739,
      "step": 188000
    },
    {
      "epoch": 60.972447325769856,
      "grad_norm": 0.9342983961105347,
      "learning_rate": 4.3933830684398317e-05,
      "loss": 3.0719,
      "step": 188100
    },
    {
      "epoch": 61.0,
      "eval_bleu": 1.2771470495050232,
      "eval_loss": 3.770902156829834,
      "eval_runtime": 4.5149,
      "eval_samples_per_second": 108.974,
      "eval_steps_per_second": 1.772,
      "step": 188185
    },
    {
      "epoch": 61.00486223662885,
      "grad_norm": 1.0028547048568726,
      "learning_rate": 4.3930587090496275e-05,
      "loss": 3.0755,
      "step": 188200
    },
    {
      "epoch": 61.03727714748784,
      "grad_norm": 0.9024468660354614,
      "learning_rate": 4.3927343496594234e-05,
      "loss": 3.0713,
      "step": 188300
    },
    {
      "epoch": 61.06969205834684,
      "grad_norm": 0.9333835244178772,
      "learning_rate": 4.3924099902692186e-05,
      "loss": 3.0559,
      "step": 188400
    },
    {
      "epoch": 61.102106969205835,
      "grad_norm": 0.9430490136146545,
      "learning_rate": 4.3920856308790145e-05,
      "loss": 3.0534,
      "step": 188500
    },
    {
      "epoch": 61.13452188006483,
      "grad_norm": 0.9487960338592529,
      "learning_rate": 4.39176127148881e-05,
      "loss": 3.0513,
      "step": 188600
    },
    {
      "epoch": 61.16693679092383,
      "grad_norm": 0.957923412322998,
      "learning_rate": 4.3914369120986055e-05,
      "loss": 3.0542,
      "step": 188700
    },
    {
      "epoch": 61.19935170178282,
      "grad_norm": 0.9974782466888428,
      "learning_rate": 4.3911125527084014e-05,
      "loss": 3.0627,
      "step": 188800
    },
    {
      "epoch": 61.23176661264181,
      "grad_norm": 0.8664445281028748,
      "learning_rate": 4.3907881933181966e-05,
      "loss": 3.0594,
      "step": 188900
    },
    {
      "epoch": 61.26418152350081,
      "grad_norm": 1.0057573318481445,
      "learning_rate": 4.3904638339279925e-05,
      "loss": 3.0594,
      "step": 189000
    },
    {
      "epoch": 61.296596434359806,
      "grad_norm": 0.9491165280342102,
      "learning_rate": 4.3901394745377884e-05,
      "loss": 3.0544,
      "step": 189100
    },
    {
      "epoch": 61.3290113452188,
      "grad_norm": 1.1215214729309082,
      "learning_rate": 4.3898151151475836e-05,
      "loss": 3.0474,
      "step": 189200
    },
    {
      "epoch": 61.3614262560778,
      "grad_norm": 0.8649635314941406,
      "learning_rate": 4.3894907557573794e-05,
      "loss": 3.0616,
      "step": 189300
    },
    {
      "epoch": 61.39384116693679,
      "grad_norm": 1.1284451484680176,
      "learning_rate": 4.389166396367175e-05,
      "loss": 3.0563,
      "step": 189400
    },
    {
      "epoch": 61.426256077795784,
      "grad_norm": 1.0374317169189453,
      "learning_rate": 4.3888420369769705e-05,
      "loss": 3.0471,
      "step": 189500
    },
    {
      "epoch": 61.45867098865478,
      "grad_norm": 0.9241070747375488,
      "learning_rate": 4.3885176775867664e-05,
      "loss": 3.0656,
      "step": 189600
    },
    {
      "epoch": 61.49108589951378,
      "grad_norm": 0.9279916882514954,
      "learning_rate": 4.3881933181965616e-05,
      "loss": 3.0598,
      "step": 189700
    },
    {
      "epoch": 61.523500810372774,
      "grad_norm": 0.8678724765777588,
      "learning_rate": 4.3878689588063575e-05,
      "loss": 3.0584,
      "step": 189800
    },
    {
      "epoch": 61.55591572123177,
      "grad_norm": 0.8811473846435547,
      "learning_rate": 4.387547843010055e-05,
      "loss": 3.0662,
      "step": 189900
    },
    {
      "epoch": 61.58833063209076,
      "grad_norm": 0.9932101964950562,
      "learning_rate": 4.387223483619851e-05,
      "loss": 3.0573,
      "step": 190000
    },
    {
      "epoch": 61.620745542949756,
      "grad_norm": 0.8941240906715393,
      "learning_rate": 4.386899124229646e-05,
      "loss": 3.0543,
      "step": 190100
    },
    {
      "epoch": 61.65316045380875,
      "grad_norm": 0.8203635811805725,
      "learning_rate": 4.386574764839442e-05,
      "loss": 3.0671,
      "step": 190200
    },
    {
      "epoch": 61.68557536466775,
      "grad_norm": 1.3489412069320679,
      "learning_rate": 4.386250405449238e-05,
      "loss": 3.0537,
      "step": 190300
    },
    {
      "epoch": 61.717990275526745,
      "grad_norm": 0.8611129522323608,
      "learning_rate": 4.385926046059033e-05,
      "loss": 3.0517,
      "step": 190400
    },
    {
      "epoch": 61.750405186385734,
      "grad_norm": 1.0961743593215942,
      "learning_rate": 4.385601686668829e-05,
      "loss": 3.0718,
      "step": 190500
    },
    {
      "epoch": 61.78282009724473,
      "grad_norm": 1.045393705368042,
      "learning_rate": 4.385277327278625e-05,
      "loss": 3.0449,
      "step": 190600
    },
    {
      "epoch": 61.81523500810373,
      "grad_norm": 0.8827307224273682,
      "learning_rate": 4.38495296788842e-05,
      "loss": 3.0662,
      "step": 190700
    },
    {
      "epoch": 61.84764991896272,
      "grad_norm": 1.0554745197296143,
      "learning_rate": 4.384628608498216e-05,
      "loss": 3.0646,
      "step": 190800
    },
    {
      "epoch": 61.88006482982172,
      "grad_norm": 0.9538054466247559,
      "learning_rate": 4.384304249108012e-05,
      "loss": 3.0613,
      "step": 190900
    },
    {
      "epoch": 61.912479740680716,
      "grad_norm": 0.8263408541679382,
      "learning_rate": 4.383979889717808e-05,
      "loss": 3.0577,
      "step": 191000
    },
    {
      "epoch": 61.944894651539705,
      "grad_norm": 0.9366182088851929,
      "learning_rate": 4.383655530327603e-05,
      "loss": 3.0663,
      "step": 191100
    },
    {
      "epoch": 61.9773095623987,
      "grad_norm": 0.9944483637809753,
      "learning_rate": 4.383331170937399e-05,
      "loss": 3.0584,
      "step": 191200
    },
    {
      "epoch": 62.0,
      "eval_bleu": 1.2649112635978008,
      "eval_loss": 3.780653953552246,
      "eval_runtime": 4.3353,
      "eval_samples_per_second": 113.486,
      "eval_steps_per_second": 1.845,
      "step": 191270
    },
    {
      "epoch": 62.0097244732577,
      "grad_norm": 1.1764910221099854,
      "learning_rate": 4.383006811547195e-05,
      "loss": 3.0551,
      "step": 191300
    },
    {
      "epoch": 62.042139384116695,
      "grad_norm": 0.9573681354522705,
      "learning_rate": 4.3826824521569906e-05,
      "loss": 3.0488,
      "step": 191400
    },
    {
      "epoch": 62.07455429497569,
      "grad_norm": 1.127432942390442,
      "learning_rate": 4.382358092766786e-05,
      "loss": 3.0442,
      "step": 191500
    },
    {
      "epoch": 62.10696920583469,
      "grad_norm": 0.995548665523529,
      "learning_rate": 4.382033733376582e-05,
      "loss": 3.0632,
      "step": 191600
    },
    {
      "epoch": 62.13938411669368,
      "grad_norm": 1.067821979522705,
      "learning_rate": 4.3817093739863776e-05,
      "loss": 3.0468,
      "step": 191700
    },
    {
      "epoch": 62.17179902755267,
      "grad_norm": 1.134963870048523,
      "learning_rate": 4.381385014596173e-05,
      "loss": 3.0564,
      "step": 191800
    },
    {
      "epoch": 62.20421393841167,
      "grad_norm": 1.0378031730651855,
      "learning_rate": 4.3810606552059687e-05,
      "loss": 3.0478,
      "step": 191900
    },
    {
      "epoch": 62.236628849270666,
      "grad_norm": 0.9893219470977783,
      "learning_rate": 4.380736295815764e-05,
      "loss": 3.0635,
      "step": 192000
    },
    {
      "epoch": 62.26904376012966,
      "grad_norm": 0.8239917755126953,
      "learning_rate": 4.38041193642556e-05,
      "loss": 3.0492,
      "step": 192100
    },
    {
      "epoch": 62.30145867098865,
      "grad_norm": 0.8279983997344971,
      "learning_rate": 4.3800875770353556e-05,
      "loss": 3.0571,
      "step": 192200
    },
    {
      "epoch": 62.33387358184765,
      "grad_norm": 0.9591814279556274,
      "learning_rate": 4.379763217645151e-05,
      "loss": 3.058,
      "step": 192300
    },
    {
      "epoch": 62.366288492706644,
      "grad_norm": 1.0036828517913818,
      "learning_rate": 4.379438858254947e-05,
      "loss": 3.0675,
      "step": 192400
    },
    {
      "epoch": 62.39870340356564,
      "grad_norm": 1.0044517517089844,
      "learning_rate": 4.3791144988647425e-05,
      "loss": 3.0466,
      "step": 192500
    },
    {
      "epoch": 62.43111831442464,
      "grad_norm": 0.9899506568908691,
      "learning_rate": 4.378790139474538e-05,
      "loss": 3.0419,
      "step": 192600
    },
    {
      "epoch": 62.46353322528363,
      "grad_norm": 0.9122377634048462,
      "learning_rate": 4.3784657800843336e-05,
      "loss": 3.0709,
      "step": 192700
    },
    {
      "epoch": 62.49594813614262,
      "grad_norm": 0.8799233436584473,
      "learning_rate": 4.3781414206941295e-05,
      "loss": 3.0591,
      "step": 192800
    },
    {
      "epoch": 62.52836304700162,
      "grad_norm": 1.065356731414795,
      "learning_rate": 4.377817061303925e-05,
      "loss": 3.0632,
      "step": 192900
    },
    {
      "epoch": 62.560777957860616,
      "grad_norm": 1.002855896949768,
      "learning_rate": 4.3774927019137206e-05,
      "loss": 3.0546,
      "step": 193000
    },
    {
      "epoch": 62.59319286871961,
      "grad_norm": 0.9872782826423645,
      "learning_rate": 4.377168342523516e-05,
      "loss": 3.051,
      "step": 193100
    },
    {
      "epoch": 62.62560777957861,
      "grad_norm": 1.1321728229522705,
      "learning_rate": 4.3768439831333116e-05,
      "loss": 3.0648,
      "step": 193200
    },
    {
      "epoch": 62.658022690437605,
      "grad_norm": 1.1143913269042969,
      "learning_rate": 4.3765196237431075e-05,
      "loss": 3.0634,
      "step": 193300
    },
    {
      "epoch": 62.690437601296594,
      "grad_norm": 1.0388262271881104,
      "learning_rate": 4.3761952643529034e-05,
      "loss": 3.0639,
      "step": 193400
    },
    {
      "epoch": 62.72285251215559,
      "grad_norm": 1.0367424488067627,
      "learning_rate": 4.3758709049626986e-05,
      "loss": 3.045,
      "step": 193500
    },
    {
      "epoch": 62.75526742301459,
      "grad_norm": 0.8622238039970398,
      "learning_rate": 4.3755465455724944e-05,
      "loss": 3.0473,
      "step": 193600
    },
    {
      "epoch": 62.78768233387358,
      "grad_norm": 0.9953689575195312,
      "learning_rate": 4.37522218618229e-05,
      "loss": 3.0421,
      "step": 193700
    },
    {
      "epoch": 62.82009724473258,
      "grad_norm": 0.9002699255943298,
      "learning_rate": 4.374897826792086e-05,
      "loss": 3.042,
      "step": 193800
    },
    {
      "epoch": 62.85251215559157,
      "grad_norm": 1.1570343971252441,
      "learning_rate": 4.374576710995783e-05,
      "loss": 3.0577,
      "step": 193900
    },
    {
      "epoch": 62.884927066450565,
      "grad_norm": 0.996429979801178,
      "learning_rate": 4.374252351605579e-05,
      "loss": 3.0547,
      "step": 194000
    },
    {
      "epoch": 62.91734197730956,
      "grad_norm": 0.9824897646903992,
      "learning_rate": 4.373927992215375e-05,
      "loss": 3.0589,
      "step": 194100
    },
    {
      "epoch": 62.94975688816856,
      "grad_norm": 0.937312126159668,
      "learning_rate": 4.373603632825171e-05,
      "loss": 3.0687,
      "step": 194200
    },
    {
      "epoch": 62.982171799027554,
      "grad_norm": 1.00648033618927,
      "learning_rate": 4.373279273434966e-05,
      "loss": 3.0554,
      "step": 194300
    },
    {
      "epoch": 63.0,
      "eval_bleu": 1.1137214574588679,
      "eval_loss": 3.7799429893493652,
      "eval_runtime": 4.3569,
      "eval_samples_per_second": 112.924,
      "eval_steps_per_second": 1.836,
      "step": 194355
    },
    {
      "epoch": 63.01458670988655,
      "grad_norm": 0.8836382627487183,
      "learning_rate": 4.372954914044762e-05,
      "loss": 3.0423,
      "step": 194400
    },
    {
      "epoch": 63.04700162074554,
      "grad_norm": 1.0779087543487549,
      "learning_rate": 4.372630554654558e-05,
      "loss": 3.0483,
      "step": 194500
    },
    {
      "epoch": 63.07941653160454,
      "grad_norm": 0.9877982139587402,
      "learning_rate": 4.372306195264353e-05,
      "loss": 3.0431,
      "step": 194600
    },
    {
      "epoch": 63.11183144246353,
      "grad_norm": 0.8537886142730713,
      "learning_rate": 4.371985079468051e-05,
      "loss": 3.0402,
      "step": 194700
    },
    {
      "epoch": 63.14424635332253,
      "grad_norm": 0.8825653791427612,
      "learning_rate": 4.371660720077847e-05,
      "loss": 3.0589,
      "step": 194800
    },
    {
      "epoch": 63.176661264181526,
      "grad_norm": 1.0236682891845703,
      "learning_rate": 4.3713363606876426e-05,
      "loss": 3.0582,
      "step": 194900
    },
    {
      "epoch": 63.20907617504052,
      "grad_norm": 1.00654137134552,
      "learning_rate": 4.37101524489134e-05,
      "loss": 3.0507,
      "step": 195000
    },
    {
      "epoch": 63.24149108589951,
      "grad_norm": 1.160554051399231,
      "learning_rate": 4.3706908855011356e-05,
      "loss": 3.0509,
      "step": 195100
    },
    {
      "epoch": 63.27390599675851,
      "grad_norm": 0.9959549903869629,
      "learning_rate": 4.3703665261109314e-05,
      "loss": 3.0182,
      "step": 195200
    },
    {
      "epoch": 63.306320907617504,
      "grad_norm": Infinity,
      "learning_rate": 4.3700421667207266e-05,
      "loss": 3.0499,
      "step": 195300
    },
    {
      "epoch": 63.3387358184765,
      "grad_norm": 0.9056329131126404,
      "learning_rate": 4.3697210509244244e-05,
      "loss": 3.0219,
      "step": 195400
    },
    {
      "epoch": 63.3711507293355,
      "grad_norm": 0.9769321084022522,
      "learning_rate": 4.3693966915342196e-05,
      "loss": 3.0388,
      "step": 195500
    },
    {
      "epoch": 63.403565640194486,
      "grad_norm": 0.894415020942688,
      "learning_rate": 4.3690723321440155e-05,
      "loss": 3.026,
      "step": 195600
    },
    {
      "epoch": 63.43598055105348,
      "grad_norm": 1.0139333009719849,
      "learning_rate": 4.3687479727538114e-05,
      "loss": 3.0596,
      "step": 195700
    },
    {
      "epoch": 63.46839546191248,
      "grad_norm": 1.0630230903625488,
      "learning_rate": 4.368423613363607e-05,
      "loss": 3.0809,
      "step": 195800
    },
    {
      "epoch": 63.500810372771475,
      "grad_norm": 0.8489808440208435,
      "learning_rate": 4.368099253973403e-05,
      "loss": 3.0604,
      "step": 195900
    },
    {
      "epoch": 63.53322528363047,
      "grad_norm": 0.9256314039230347,
      "learning_rate": 4.367774894583199e-05,
      "loss": 3.0763,
      "step": 196000
    },
    {
      "epoch": 63.56564019448947,
      "grad_norm": 0.9444587230682373,
      "learning_rate": 4.367450535192994e-05,
      "loss": 3.0554,
      "step": 196100
    },
    {
      "epoch": 63.59805510534846,
      "grad_norm": 0.8484342098236084,
      "learning_rate": 4.36712617580279e-05,
      "loss": 3.0554,
      "step": 196200
    },
    {
      "epoch": 63.630470016207454,
      "grad_norm": 1.0074366331100464,
      "learning_rate": 4.366801816412585e-05,
      "loss": 3.0363,
      "step": 196300
    },
    {
      "epoch": 63.66288492706645,
      "grad_norm": 1.222711443901062,
      "learning_rate": 4.366477457022381e-05,
      "loss": 3.0453,
      "step": 196400
    },
    {
      "epoch": 63.69529983792545,
      "grad_norm": 0.9307461380958557,
      "learning_rate": 4.366153097632177e-05,
      "loss": 3.0585,
      "step": 196500
    },
    {
      "epoch": 63.72771474878444,
      "grad_norm": 1.1002087593078613,
      "learning_rate": 4.365828738241972e-05,
      "loss": 3.052,
      "step": 196600
    },
    {
      "epoch": 63.76012965964344,
      "grad_norm": 0.9807111024856567,
      "learning_rate": 4.365504378851768e-05,
      "loss": 3.0441,
      "step": 196700
    },
    {
      "epoch": 63.79254457050243,
      "grad_norm": 0.9719679951667786,
      "learning_rate": 4.365180019461564e-05,
      "loss": 3.0644,
      "step": 196800
    },
    {
      "epoch": 63.824959481361425,
      "grad_norm": 0.9792425036430359,
      "learning_rate": 4.364855660071359e-05,
      "loss": 3.07,
      "step": 196900
    },
    {
      "epoch": 63.85737439222042,
      "grad_norm": 0.9182358980178833,
      "learning_rate": 4.364531300681155e-05,
      "loss": 3.0524,
      "step": 197000
    },
    {
      "epoch": 63.88978930307942,
      "grad_norm": 0.9443842768669128,
      "learning_rate": 4.36420694129095e-05,
      "loss": 3.0403,
      "step": 197100
    },
    {
      "epoch": 63.922204213938414,
      "grad_norm": 0.9240365028381348,
      "learning_rate": 4.363882581900746e-05,
      "loss": 3.0597,
      "step": 197200
    },
    {
      "epoch": 63.954619124797404,
      "grad_norm": 0.934362530708313,
      "learning_rate": 4.363558222510542e-05,
      "loss": 3.0525,
      "step": 197300
    },
    {
      "epoch": 63.9870340356564,
      "grad_norm": 0.7999933362007141,
      "learning_rate": 4.363233863120337e-05,
      "loss": 3.0532,
      "step": 197400
    },
    {
      "epoch": 64.0,
      "eval_bleu": 1.1755134202342348,
      "eval_loss": 3.7782771587371826,
      "eval_runtime": 4.6327,
      "eval_samples_per_second": 106.202,
      "eval_steps_per_second": 1.727,
      "step": 197440
    },
    {
      "epoch": 64.0194489465154,
      "grad_norm": 0.9670348167419434,
      "learning_rate": 4.362909503730133e-05,
      "loss": 3.0626,
      "step": 197500
    },
    {
      "epoch": 64.05186385737439,
      "grad_norm": 0.9270984530448914,
      "learning_rate": 4.362585144339929e-05,
      "loss": 3.0215,
      "step": 197600
    },
    {
      "epoch": 64.08427876823339,
      "grad_norm": 0.9270864725112915,
      "learning_rate": 4.362260784949724e-05,
      "loss": 3.0506,
      "step": 197700
    },
    {
      "epoch": 64.11669367909238,
      "grad_norm": 1.0777394771575928,
      "learning_rate": 4.36193642555952e-05,
      "loss": 3.0286,
      "step": 197800
    },
    {
      "epoch": 64.14910858995138,
      "grad_norm": 0.9762635231018066,
      "learning_rate": 4.361612066169316e-05,
      "loss": 3.0405,
      "step": 197900
    },
    {
      "epoch": 64.18152350081037,
      "grad_norm": 0.8856582641601562,
      "learning_rate": 4.361287706779111e-05,
      "loss": 3.0424,
      "step": 198000
    },
    {
      "epoch": 64.21393841166937,
      "grad_norm": 0.995111882686615,
      "learning_rate": 4.360963347388907e-05,
      "loss": 3.0648,
      "step": 198100
    },
    {
      "epoch": 64.24635332252836,
      "grad_norm": 0.9787915945053101,
      "learning_rate": 4.360638987998703e-05,
      "loss": 3.0477,
      "step": 198200
    },
    {
      "epoch": 64.27876823338735,
      "grad_norm": 1.0421338081359863,
      "learning_rate": 4.360314628608499e-05,
      "loss": 3.029,
      "step": 198300
    },
    {
      "epoch": 64.31118314424636,
      "grad_norm": 1.049673080444336,
      "learning_rate": 4.3599902692182946e-05,
      "loss": 3.0566,
      "step": 198400
    },
    {
      "epoch": 64.34359805510535,
      "grad_norm": 1.0479387044906616,
      "learning_rate": 4.35966590982809e-05,
      "loss": 3.0574,
      "step": 198500
    },
    {
      "epoch": 64.37601296596435,
      "grad_norm": 1.015287160873413,
      "learning_rate": 4.3593415504378856e-05,
      "loss": 3.0419,
      "step": 198600
    },
    {
      "epoch": 64.40842787682334,
      "grad_norm": 0.8631727695465088,
      "learning_rate": 4.3590171910476815e-05,
      "loss": 3.0385,
      "step": 198700
    },
    {
      "epoch": 64.44084278768233,
      "grad_norm": 1.1546121835708618,
      "learning_rate": 4.358692831657477e-05,
      "loss": 3.0364,
      "step": 198800
    },
    {
      "epoch": 64.47325769854133,
      "grad_norm": 1.0086637735366821,
      "learning_rate": 4.3583684722672726e-05,
      "loss": 3.0422,
      "step": 198900
    },
    {
      "epoch": 64.50567260940032,
      "grad_norm": 0.9907424449920654,
      "learning_rate": 4.3580441128770684e-05,
      "loss": 3.0484,
      "step": 199000
    },
    {
      "epoch": 64.53808752025932,
      "grad_norm": 1.0428730249404907,
      "learning_rate": 4.3577197534868636e-05,
      "loss": 3.0574,
      "step": 199100
    },
    {
      "epoch": 64.57050243111831,
      "grad_norm": 0.934903621673584,
      "learning_rate": 4.3573953940966595e-05,
      "loss": 3.0273,
      "step": 199200
    },
    {
      "epoch": 64.6029173419773,
      "grad_norm": 1.0910265445709229,
      "learning_rate": 4.357071034706455e-05,
      "loss": 3.0428,
      "step": 199300
    },
    {
      "epoch": 64.6353322528363,
      "grad_norm": 0.9274861812591553,
      "learning_rate": 4.3567466753162506e-05,
      "loss": 3.0326,
      "step": 199400
    },
    {
      "epoch": 64.6677471636953,
      "grad_norm": 1.1793371438980103,
      "learning_rate": 4.3564223159260465e-05,
      "loss": 3.0593,
      "step": 199500
    },
    {
      "epoch": 64.7001620745543,
      "grad_norm": 1.0921708345413208,
      "learning_rate": 4.3560979565358417e-05,
      "loss": 3.0407,
      "step": 199600
    },
    {
      "epoch": 64.73257698541329,
      "grad_norm": 0.9744971394538879,
      "learning_rate": 4.3557735971456375e-05,
      "loss": 3.0461,
      "step": 199700
    },
    {
      "epoch": 64.76499189627229,
      "grad_norm": 0.9142928123474121,
      "learning_rate": 4.3554492377554334e-05,
      "loss": 3.0497,
      "step": 199800
    },
    {
      "epoch": 64.79740680713128,
      "grad_norm": 0.9186406135559082,
      "learning_rate": 4.3551248783652286e-05,
      "loss": 3.0488,
      "step": 199900
    },
    {
      "epoch": 64.82982171799027,
      "grad_norm": 0.9940789341926575,
      "learning_rate": 4.3548005189750245e-05,
      "loss": 3.0484,
      "step": 200000
    },
    {
      "epoch": 64.86223662884927,
      "grad_norm": 1.054619550704956,
      "learning_rate": 4.35447615958482e-05,
      "loss": 3.0502,
      "step": 200100
    },
    {
      "epoch": 64.89465153970826,
      "grad_norm": 0.9971835613250732,
      "learning_rate": 4.3541518001946155e-05,
      "loss": 3.0571,
      "step": 200200
    },
    {
      "epoch": 64.92706645056727,
      "grad_norm": 1.1745442152023315,
      "learning_rate": 4.3538274408044114e-05,
      "loss": 3.0492,
      "step": 200300
    },
    {
      "epoch": 64.95948136142626,
      "grad_norm": 1.0438578128814697,
      "learning_rate": 4.3535030814142066e-05,
      "loss": 3.051,
      "step": 200400
    },
    {
      "epoch": 64.99189627228525,
      "grad_norm": 1.006197452545166,
      "learning_rate": 4.3531787220240025e-05,
      "loss": 3.0711,
      "step": 200500
    },
    {
      "epoch": 65.0,
      "eval_bleu": 1.1254436470330103,
      "eval_loss": 3.780184268951416,
      "eval_runtime": 4.8496,
      "eval_samples_per_second": 101.453,
      "eval_steps_per_second": 1.65,
      "step": 200525
    },
    {
      "epoch": 65.02431118314425,
      "grad_norm": 1.1703344583511353,
      "learning_rate": 4.3528543626337984e-05,
      "loss": 3.0549,
      "step": 200600
    },
    {
      "epoch": 65.05672609400324,
      "grad_norm": 0.8846217393875122,
      "learning_rate": 4.352530003243594e-05,
      "loss": 3.0183,
      "step": 200700
    },
    {
      "epoch": 65.08914100486224,
      "grad_norm": 1.1856942176818848,
      "learning_rate": 4.35220564385339e-05,
      "loss": 3.0464,
      "step": 200800
    },
    {
      "epoch": 65.12155591572123,
      "grad_norm": 1.0172741413116455,
      "learning_rate": 4.351881284463186e-05,
      "loss": 3.035,
      "step": 200900
    },
    {
      "epoch": 65.15397082658022,
      "grad_norm": 1.0093344449996948,
      "learning_rate": 4.351556925072981e-05,
      "loss": 3.0404,
      "step": 201000
    },
    {
      "epoch": 65.18638573743922,
      "grad_norm": 0.8948166966438293,
      "learning_rate": 4.351232565682777e-05,
      "loss": 3.0395,
      "step": 201100
    },
    {
      "epoch": 65.21880064829821,
      "grad_norm": 0.9195936322212219,
      "learning_rate": 4.350908206292572e-05,
      "loss": 3.0467,
      "step": 201200
    },
    {
      "epoch": 65.25121555915722,
      "grad_norm": 0.9950392246246338,
      "learning_rate": 4.350583846902368e-05,
      "loss": 3.0566,
      "step": 201300
    },
    {
      "epoch": 65.2836304700162,
      "grad_norm": 0.9310985803604126,
      "learning_rate": 4.350259487512164e-05,
      "loss": 3.0639,
      "step": 201400
    },
    {
      "epoch": 65.31604538087521,
      "grad_norm": 0.9778191447257996,
      "learning_rate": 4.349935128121959e-05,
      "loss": 3.047,
      "step": 201500
    },
    {
      "epoch": 65.3484602917342,
      "grad_norm": 1.0078799724578857,
      "learning_rate": 4.349610768731755e-05,
      "loss": 3.0207,
      "step": 201600
    },
    {
      "epoch": 65.38087520259319,
      "grad_norm": 1.0354281663894653,
      "learning_rate": 4.349286409341551e-05,
      "loss": 3.0475,
      "step": 201700
    },
    {
      "epoch": 65.41329011345219,
      "grad_norm": 1.084295630455017,
      "learning_rate": 4.348965293545249e-05,
      "loss": 3.0419,
      "step": 201800
    },
    {
      "epoch": 65.44570502431118,
      "grad_norm": 0.8872249126434326,
      "learning_rate": 4.348640934155044e-05,
      "loss": 3.0301,
      "step": 201900
    },
    {
      "epoch": 65.47811993517018,
      "grad_norm": 1.1794567108154297,
      "learning_rate": 4.34831657476484e-05,
      "loss": 3.0187,
      "step": 202000
    },
    {
      "epoch": 65.51053484602917,
      "grad_norm": 0.8854323029518127,
      "learning_rate": 4.347992215374636e-05,
      "loss": 3.0428,
      "step": 202100
    },
    {
      "epoch": 65.54294975688816,
      "grad_norm": 0.9921071529388428,
      "learning_rate": 4.347667855984431e-05,
      "loss": 3.0428,
      "step": 202200
    },
    {
      "epoch": 65.57536466774717,
      "grad_norm": 1.0995851755142212,
      "learning_rate": 4.347343496594227e-05,
      "loss": 3.0527,
      "step": 202300
    },
    {
      "epoch": 65.60777957860616,
      "grad_norm": 0.998117208480835,
      "learning_rate": 4.3470191372040226e-05,
      "loss": 3.0543,
      "step": 202400
    },
    {
      "epoch": 65.64019448946516,
      "grad_norm": 1.1028380393981934,
      "learning_rate": 4.346694777813818e-05,
      "loss": 3.0457,
      "step": 202500
    },
    {
      "epoch": 65.67260940032415,
      "grad_norm": 1.0331721305847168,
      "learning_rate": 4.346370418423614e-05,
      "loss": 3.0399,
      "step": 202600
    },
    {
      "epoch": 65.70502431118314,
      "grad_norm": 1.0302934646606445,
      "learning_rate": 4.346046059033409e-05,
      "loss": 3.046,
      "step": 202700
    },
    {
      "epoch": 65.73743922204214,
      "grad_norm": 0.9718247652053833,
      "learning_rate": 4.345721699643205e-05,
      "loss": 3.0522,
      "step": 202800
    },
    {
      "epoch": 65.76985413290113,
      "grad_norm": 1.0012487173080444,
      "learning_rate": 4.3453973402530006e-05,
      "loss": 3.0521,
      "step": 202900
    },
    {
      "epoch": 65.80226904376013,
      "grad_norm": 0.9215603470802307,
      "learning_rate": 4.345072980862796e-05,
      "loss": 3.0503,
      "step": 203000
    },
    {
      "epoch": 65.83468395461912,
      "grad_norm": 1.1252351999282837,
      "learning_rate": 4.344748621472592e-05,
      "loss": 3.0411,
      "step": 203100
    },
    {
      "epoch": 65.86709886547813,
      "grad_norm": 0.8905367851257324,
      "learning_rate": 4.3444242620823876e-05,
      "loss": 3.0476,
      "step": 203200
    },
    {
      "epoch": 65.89951377633712,
      "grad_norm": 1.1351019144058228,
      "learning_rate": 4.344099902692183e-05,
      "loss": 3.0438,
      "step": 203300
    },
    {
      "epoch": 65.9319286871961,
      "grad_norm": 0.8879687786102295,
      "learning_rate": 4.3437755433019787e-05,
      "loss": 3.0629,
      "step": 203400
    },
    {
      "epoch": 65.96434359805511,
      "grad_norm": 0.8141821622848511,
      "learning_rate": 4.3434511839117745e-05,
      "loss": 3.0376,
      "step": 203500
    },
    {
      "epoch": 65.9967585089141,
      "grad_norm": 0.9039072394371033,
      "learning_rate": 4.34312682452157e-05,
      "loss": 3.03,
      "step": 203600
    },
    {
      "epoch": 66.0,
      "eval_bleu": 1.1631872084580528,
      "eval_loss": 3.780597686767578,
      "eval_runtime": 4.1385,
      "eval_samples_per_second": 118.882,
      "eval_steps_per_second": 1.933,
      "step": 203610
    },
    {
      "epoch": 66.0291734197731,
      "grad_norm": 1.0182334184646606,
      "learning_rate": 4.3428024651313656e-05,
      "loss": 3.0221,
      "step": 203700
    },
    {
      "epoch": 66.06158833063209,
      "grad_norm": 1.0733033418655396,
      "learning_rate": 4.3424781057411615e-05,
      "loss": 3.0342,
      "step": 203800
    },
    {
      "epoch": 66.09400324149108,
      "grad_norm": 0.9387120604515076,
      "learning_rate": 4.3421537463509573e-05,
      "loss": 3.0391,
      "step": 203900
    },
    {
      "epoch": 66.12641815235008,
      "grad_norm": 0.9388461709022522,
      "learning_rate": 4.341829386960753e-05,
      "loss": 3.0376,
      "step": 204000
    },
    {
      "epoch": 66.15883306320907,
      "grad_norm": 0.9730757474899292,
      "learning_rate": 4.3415050275705484e-05,
      "loss": 3.0401,
      "step": 204100
    },
    {
      "epoch": 66.19124797406808,
      "grad_norm": 0.9859522581100464,
      "learning_rate": 4.341180668180344e-05,
      "loss": 3.0343,
      "step": 204200
    },
    {
      "epoch": 66.22366288492707,
      "grad_norm": 0.9018638730049133,
      "learning_rate": 4.34085630879014e-05,
      "loss": 3.0381,
      "step": 204300
    },
    {
      "epoch": 66.25607779578606,
      "grad_norm": 0.9052261114120483,
      "learning_rate": 4.3405319493999354e-05,
      "loss": 3.0473,
      "step": 204400
    },
    {
      "epoch": 66.28849270664506,
      "grad_norm": 1.137751579284668,
      "learning_rate": 4.340207590009731e-05,
      "loss": 3.0141,
      "step": 204500
    },
    {
      "epoch": 66.32090761750405,
      "grad_norm": 0.8870409727096558,
      "learning_rate": 4.3398832306195264e-05,
      "loss": 3.0427,
      "step": 204600
    },
    {
      "epoch": 66.35332252836305,
      "grad_norm": 1.0963623523712158,
      "learning_rate": 4.339558871229322e-05,
      "loss": 3.029,
      "step": 204700
    },
    {
      "epoch": 66.38573743922204,
      "grad_norm": 0.9300582408905029,
      "learning_rate": 4.33923775543302e-05,
      "loss": 3.0254,
      "step": 204800
    },
    {
      "epoch": 66.41815235008104,
      "grad_norm": 0.8602294921875,
      "learning_rate": 4.338913396042816e-05,
      "loss": 3.0307,
      "step": 204900
    },
    {
      "epoch": 66.45056726094003,
      "grad_norm": 1.0046895742416382,
      "learning_rate": 4.338589036652611e-05,
      "loss": 3.056,
      "step": 205000
    },
    {
      "epoch": 66.48298217179902,
      "grad_norm": 0.9586430191993713,
      "learning_rate": 4.338264677262407e-05,
      "loss": 3.0301,
      "step": 205100
    },
    {
      "epoch": 66.51539708265803,
      "grad_norm": 1.1178067922592163,
      "learning_rate": 4.337940317872203e-05,
      "loss": 3.0394,
      "step": 205200
    },
    {
      "epoch": 66.54781199351702,
      "grad_norm": 0.8983089923858643,
      "learning_rate": 4.337615958481998e-05,
      "loss": 3.0492,
      "step": 205300
    },
    {
      "epoch": 66.58022690437602,
      "grad_norm": 0.8471813201904297,
      "learning_rate": 4.337291599091794e-05,
      "loss": 3.0508,
      "step": 205400
    },
    {
      "epoch": 66.61264181523501,
      "grad_norm": 0.9718757271766663,
      "learning_rate": 4.33696723970159e-05,
      "loss": 3.0443,
      "step": 205500
    },
    {
      "epoch": 66.645056726094,
      "grad_norm": 0.9958570003509521,
      "learning_rate": 4.336642880311385e-05,
      "loss": 3.0196,
      "step": 205600
    },
    {
      "epoch": 66.677471636953,
      "grad_norm": 1.0056426525115967,
      "learning_rate": 4.336318520921181e-05,
      "loss": 3.0505,
      "step": 205700
    },
    {
      "epoch": 66.70988654781199,
      "grad_norm": 0.9996539950370789,
      "learning_rate": 4.335994161530976e-05,
      "loss": 3.0322,
      "step": 205800
    },
    {
      "epoch": 66.742301458671,
      "grad_norm": 1.2080883979797363,
      "learning_rate": 4.335669802140772e-05,
      "loss": 3.033,
      "step": 205900
    },
    {
      "epoch": 66.77471636952998,
      "grad_norm": 0.8555617928504944,
      "learning_rate": 4.335345442750568e-05,
      "loss": 3.0445,
      "step": 206000
    },
    {
      "epoch": 66.80713128038897,
      "grad_norm": 1.0349833965301514,
      "learning_rate": 4.335021083360363e-05,
      "loss": 3.0359,
      "step": 206100
    },
    {
      "epoch": 66.83954619124798,
      "grad_norm": 1.106604814529419,
      "learning_rate": 4.334696723970159e-05,
      "loss": 3.0335,
      "step": 206200
    },
    {
      "epoch": 66.87196110210697,
      "grad_norm": 0.9045965671539307,
      "learning_rate": 4.334372364579955e-05,
      "loss": 3.0482,
      "step": 206300
    },
    {
      "epoch": 66.90437601296597,
      "grad_norm": 1.0949839353561401,
      "learning_rate": 4.33404800518975e-05,
      "loss": 3.0344,
      "step": 206400
    },
    {
      "epoch": 66.93679092382496,
      "grad_norm": 0.9657273888587952,
      "learning_rate": 4.333723645799546e-05,
      "loss": 3.0468,
      "step": 206500
    },
    {
      "epoch": 66.96920583468396,
      "grad_norm": 1.0171905755996704,
      "learning_rate": 4.333399286409342e-05,
      "loss": 3.0468,
      "step": 206600
    },
    {
      "epoch": 67.0,
      "eval_bleu": 1.1639881616521353,
      "eval_loss": 3.7845418453216553,
      "eval_runtime": 4.3897,
      "eval_samples_per_second": 112.08,
      "eval_steps_per_second": 1.822,
      "step": 206695
    },
    {
      "epoch": 67.00162074554295,
      "grad_norm": 0.8773579597473145,
      "learning_rate": 4.3330749270191376e-05,
      "loss": 3.0432,
      "step": 206700
    },
    {
      "epoch": 67.03403565640194,
      "grad_norm": 0.8395187258720398,
      "learning_rate": 4.332750567628933e-05,
      "loss": 3.0305,
      "step": 206800
    },
    {
      "epoch": 67.06645056726094,
      "grad_norm": 1.0187628269195557,
      "learning_rate": 4.332426208238729e-05,
      "loss": 3.0444,
      "step": 206900
    },
    {
      "epoch": 67.09886547811993,
      "grad_norm": 0.9841111898422241,
      "learning_rate": 4.3321018488485246e-05,
      "loss": 3.0263,
      "step": 207000
    },
    {
      "epoch": 67.13128038897894,
      "grad_norm": 0.9976674914360046,
      "learning_rate": 4.3317774894583205e-05,
      "loss": 3.0285,
      "step": 207100
    },
    {
      "epoch": 67.16369529983793,
      "grad_norm": 0.9882503747940063,
      "learning_rate": 4.3314531300681156e-05,
      "loss": 3.0254,
      "step": 207200
    },
    {
      "epoch": 67.19611021069692,
      "grad_norm": 1.1670362949371338,
      "learning_rate": 4.3311320142718134e-05,
      "loss": 3.0224,
      "step": 207300
    },
    {
      "epoch": 67.22852512155592,
      "grad_norm": 1.1286765336990356,
      "learning_rate": 4.330807654881609e-05,
      "loss": 3.0173,
      "step": 207400
    },
    {
      "epoch": 67.26094003241491,
      "grad_norm": 1.1004703044891357,
      "learning_rate": 4.330483295491405e-05,
      "loss": 3.0278,
      "step": 207500
    },
    {
      "epoch": 67.29335494327391,
      "grad_norm": 1.075177550315857,
      "learning_rate": 4.3301589361012004e-05,
      "loss": 3.0555,
      "step": 207600
    },
    {
      "epoch": 67.3257698541329,
      "grad_norm": 1.0277799367904663,
      "learning_rate": 4.329834576710996e-05,
      "loss": 3.0382,
      "step": 207700
    },
    {
      "epoch": 67.35818476499189,
      "grad_norm": 1.0440131425857544,
      "learning_rate": 4.329510217320792e-05,
      "loss": 3.0338,
      "step": 207800
    },
    {
      "epoch": 67.3905996758509,
      "grad_norm": 1.0381040573120117,
      "learning_rate": 4.329185857930587e-05,
      "loss": 3.0433,
      "step": 207900
    },
    {
      "epoch": 67.42301458670988,
      "grad_norm": 0.9683030843734741,
      "learning_rate": 4.328861498540383e-05,
      "loss": 3.0525,
      "step": 208000
    },
    {
      "epoch": 67.45542949756889,
      "grad_norm": 1.0808918476104736,
      "learning_rate": 4.3285371391501784e-05,
      "loss": 3.037,
      "step": 208100
    },
    {
      "epoch": 67.48784440842788,
      "grad_norm": 1.0482556819915771,
      "learning_rate": 4.328212779759974e-05,
      "loss": 3.0249,
      "step": 208200
    },
    {
      "epoch": 67.52025931928688,
      "grad_norm": 1.020625352859497,
      "learning_rate": 4.32788842036977e-05,
      "loss": 3.0244,
      "step": 208300
    },
    {
      "epoch": 67.55267423014587,
      "grad_norm": 1.09775710105896,
      "learning_rate": 4.327564060979565e-05,
      "loss": 3.0291,
      "step": 208400
    },
    {
      "epoch": 67.58508914100486,
      "grad_norm": 1.2418582439422607,
      "learning_rate": 4.327239701589361e-05,
      "loss": 3.0254,
      "step": 208500
    },
    {
      "epoch": 67.61750405186386,
      "grad_norm": 0.9640125632286072,
      "learning_rate": 4.326915342199157e-05,
      "loss": 3.0328,
      "step": 208600
    },
    {
      "epoch": 67.64991896272285,
      "grad_norm": 0.8825747966766357,
      "learning_rate": 4.326590982808952e-05,
      "loss": 3.0479,
      "step": 208700
    },
    {
      "epoch": 67.68233387358185,
      "grad_norm": 1.1742922067642212,
      "learning_rate": 4.326266623418748e-05,
      "loss": 3.0169,
      "step": 208800
    },
    {
      "epoch": 67.71474878444084,
      "grad_norm": 0.8814970850944519,
      "learning_rate": 4.325942264028544e-05,
      "loss": 3.0236,
      "step": 208900
    },
    {
      "epoch": 67.74716369529983,
      "grad_norm": 1.1444956064224243,
      "learning_rate": 4.325617904638339e-05,
      "loss": 3.037,
      "step": 209000
    },
    {
      "epoch": 67.77957860615884,
      "grad_norm": 0.9761528968811035,
      "learning_rate": 4.325293545248135e-05,
      "loss": 3.0462,
      "step": 209100
    },
    {
      "epoch": 67.81199351701783,
      "grad_norm": 0.8449925184249878,
      "learning_rate": 4.32496918585793e-05,
      "loss": 3.0416,
      "step": 209200
    },
    {
      "epoch": 67.84440842787683,
      "grad_norm": 1.0891598463058472,
      "learning_rate": 4.324644826467726e-05,
      "loss": 3.0318,
      "step": 209300
    },
    {
      "epoch": 67.87682333873582,
      "grad_norm": 0.8694517612457275,
      "learning_rate": 4.324320467077522e-05,
      "loss": 3.0256,
      "step": 209400
    },
    {
      "epoch": 67.90923824959481,
      "grad_norm": 1.0017534494400024,
      "learning_rate": 4.323996107687317e-05,
      "loss": 3.0455,
      "step": 209500
    },
    {
      "epoch": 67.94165316045381,
      "grad_norm": 0.9121502041816711,
      "learning_rate": 4.323671748297113e-05,
      "loss": 3.0609,
      "step": 209600
    },
    {
      "epoch": 67.9740680713128,
      "grad_norm": 0.931928813457489,
      "learning_rate": 4.323347388906909e-05,
      "loss": 3.0473,
      "step": 209700
    },
    {
      "epoch": 68.0,
      "eval_bleu": 1.2699031740351658,
      "eval_loss": 3.783064842224121,
      "eval_runtime": 4.5067,
      "eval_samples_per_second": 109.17,
      "eval_steps_per_second": 1.775,
      "step": 209780
    },
    {
      "epoch": 68.0064829821718,
      "grad_norm": 1.0818344354629517,
      "learning_rate": 4.323023029516705e-05,
      "loss": 3.0378,
      "step": 209800
    },
    {
      "epoch": 68.03889789303079,
      "grad_norm": 0.8633217215538025,
      "learning_rate": 4.322698670126501e-05,
      "loss": 3.0349,
      "step": 209900
    },
    {
      "epoch": 68.0713128038898,
      "grad_norm": 1.1368777751922607,
      "learning_rate": 4.3223743107362966e-05,
      "loss": 3.0245,
      "step": 210000
    },
    {
      "epoch": 68.10372771474879,
      "grad_norm": 0.9364138245582581,
      "learning_rate": 4.322049951346092e-05,
      "loss": 3.0212,
      "step": 210100
    },
    {
      "epoch": 68.13614262560777,
      "grad_norm": 0.885876476764679,
      "learning_rate": 4.321725591955888e-05,
      "loss": 3.014,
      "step": 210200
    },
    {
      "epoch": 68.16855753646678,
      "grad_norm": 0.8717752695083618,
      "learning_rate": 4.321401232565683e-05,
      "loss": 3.0371,
      "step": 210300
    },
    {
      "epoch": 68.20097244732577,
      "grad_norm": 1.102931261062622,
      "learning_rate": 4.321076873175479e-05,
      "loss": 3.0147,
      "step": 210400
    },
    {
      "epoch": 68.23338735818477,
      "grad_norm": 0.8952922821044922,
      "learning_rate": 4.3207525137852746e-05,
      "loss": 3.0496,
      "step": 210500
    },
    {
      "epoch": 68.26580226904376,
      "grad_norm": 1.0068033933639526,
      "learning_rate": 4.32042815439507e-05,
      "loss": 3.0346,
      "step": 210600
    },
    {
      "epoch": 68.29821717990275,
      "grad_norm": 1.0138667821884155,
      "learning_rate": 4.320103795004866e-05,
      "loss": 3.0334,
      "step": 210700
    },
    {
      "epoch": 68.33063209076175,
      "grad_norm": 1.0862351655960083,
      "learning_rate": 4.3197794356146616e-05,
      "loss": 3.0199,
      "step": 210800
    },
    {
      "epoch": 68.36304700162074,
      "grad_norm": 1.0013558864593506,
      "learning_rate": 4.319455076224457e-05,
      "loss": 3.0274,
      "step": 210900
    },
    {
      "epoch": 68.39546191247975,
      "grad_norm": 1.0750834941864014,
      "learning_rate": 4.3191307168342526e-05,
      "loss": 3.0354,
      "step": 211000
    },
    {
      "epoch": 68.42787682333874,
      "grad_norm": 1.1346707344055176,
      "learning_rate": 4.318806357444048e-05,
      "loss": 3.0444,
      "step": 211100
    },
    {
      "epoch": 68.46029173419772,
      "grad_norm": 1.0610458850860596,
      "learning_rate": 4.318481998053844e-05,
      "loss": 3.0315,
      "step": 211200
    },
    {
      "epoch": 68.49270664505673,
      "grad_norm": 0.9880039095878601,
      "learning_rate": 4.3181608822575415e-05,
      "loss": 3.0159,
      "step": 211300
    },
    {
      "epoch": 68.52512155591572,
      "grad_norm": 0.987166702747345,
      "learning_rate": 4.317839766461239e-05,
      "loss": 3.0253,
      "step": 211400
    },
    {
      "epoch": 68.55753646677472,
      "grad_norm": 1.2054524421691895,
      "learning_rate": 4.317515407071035e-05,
      "loss": 3.0296,
      "step": 211500
    },
    {
      "epoch": 68.58995137763371,
      "grad_norm": 0.869670033454895,
      "learning_rate": 4.317191047680831e-05,
      "loss": 3.0205,
      "step": 211600
    },
    {
      "epoch": 68.62236628849271,
      "grad_norm": 1.0144855976104736,
      "learning_rate": 4.316866688290626e-05,
      "loss": 3.0278,
      "step": 211700
    },
    {
      "epoch": 68.6547811993517,
      "grad_norm": 1.07700777053833,
      "learning_rate": 4.316542328900422e-05,
      "loss": 3.032,
      "step": 211800
    },
    {
      "epoch": 68.68719611021069,
      "grad_norm": 1.1793724298477173,
      "learning_rate": 4.316217969510217e-05,
      "loss": 3.0133,
      "step": 211900
    },
    {
      "epoch": 68.7196110210697,
      "grad_norm": 0.8309473395347595,
      "learning_rate": 4.315893610120013e-05,
      "loss": 3.0314,
      "step": 212000
    },
    {
      "epoch": 68.75202593192869,
      "grad_norm": 0.9605224132537842,
      "learning_rate": 4.315569250729809e-05,
      "loss": 3.049,
      "step": 212100
    },
    {
      "epoch": 68.78444084278769,
      "grad_norm": 0.8828433752059937,
      "learning_rate": 4.315244891339604e-05,
      "loss": 3.0383,
      "step": 212200
    },
    {
      "epoch": 68.81685575364668,
      "grad_norm": 1.1991353034973145,
      "learning_rate": 4.3149205319494e-05,
      "loss": 3.0398,
      "step": 212300
    },
    {
      "epoch": 68.84927066450567,
      "grad_norm": 0.9896833300590515,
      "learning_rate": 4.314596172559196e-05,
      "loss": 3.0302,
      "step": 212400
    },
    {
      "epoch": 68.88168557536467,
      "grad_norm": 0.8955299258232117,
      "learning_rate": 4.314271813168991e-05,
      "loss": 3.0367,
      "step": 212500
    },
    {
      "epoch": 68.91410048622366,
      "grad_norm": 1.232329249382019,
      "learning_rate": 4.313947453778787e-05,
      "loss": 3.0597,
      "step": 212600
    },
    {
      "epoch": 68.94651539708266,
      "grad_norm": 0.9713438153266907,
      "learning_rate": 4.313623094388583e-05,
      "loss": 3.0264,
      "step": 212700
    },
    {
      "epoch": 68.97893030794165,
      "grad_norm": 1.0412213802337646,
      "learning_rate": 4.313298734998378e-05,
      "loss": 3.0445,
      "step": 212800
    },
    {
      "epoch": 69.0,
      "eval_bleu": 1.0766369044935227,
      "eval_loss": 3.7870562076568604,
      "eval_runtime": 4.7714,
      "eval_samples_per_second": 103.115,
      "eval_steps_per_second": 1.677,
      "step": 212865
    },
    {
      "epoch": 69.01134521880064,
      "grad_norm": 1.010398268699646,
      "learning_rate": 4.312974375608174e-05,
      "loss": 3.0384,
      "step": 212900
    },
    {
      "epoch": 69.04376012965965,
      "grad_norm": 1.0025935173034668,
      "learning_rate": 4.312650016217969e-05,
      "loss": 3.0269,
      "step": 213000
    },
    {
      "epoch": 69.07617504051863,
      "grad_norm": 0.9946103096008301,
      "learning_rate": 4.312325656827765e-05,
      "loss": 3.025,
      "step": 213100
    },
    {
      "epoch": 69.10858995137764,
      "grad_norm": 0.9961799383163452,
      "learning_rate": 4.312001297437561e-05,
      "loss": 3.0198,
      "step": 213200
    },
    {
      "epoch": 69.14100486223663,
      "grad_norm": 1.0547457933425903,
      "learning_rate": 4.311676938047357e-05,
      "loss": 3.016,
      "step": 213300
    },
    {
      "epoch": 69.17341977309563,
      "grad_norm": 0.8803642392158508,
      "learning_rate": 4.311352578657153e-05,
      "loss": 3.0196,
      "step": 213400
    },
    {
      "epoch": 69.20583468395462,
      "grad_norm": 1.0730785131454468,
      "learning_rate": 4.311028219266948e-05,
      "loss": 3.0176,
      "step": 213500
    },
    {
      "epoch": 69.23824959481361,
      "grad_norm": 0.9427257776260376,
      "learning_rate": 4.310703859876744e-05,
      "loss": 3.0287,
      "step": 213600
    },
    {
      "epoch": 69.27066450567261,
      "grad_norm": 0.903620183467865,
      "learning_rate": 4.3103795004865396e-05,
      "loss": 3.0339,
      "step": 213700
    },
    {
      "epoch": 69.3030794165316,
      "grad_norm": 0.9232144951820374,
      "learning_rate": 4.310055141096335e-05,
      "loss": 3.0275,
      "step": 213800
    },
    {
      "epoch": 69.3354943273906,
      "grad_norm": 0.9259741902351379,
      "learning_rate": 4.309730781706131e-05,
      "loss": 3.0353,
      "step": 213900
    },
    {
      "epoch": 69.3679092382496,
      "grad_norm": 0.9461127519607544,
      "learning_rate": 4.3094064223159266e-05,
      "loss": 3.0304,
      "step": 214000
    },
    {
      "epoch": 69.40032414910858,
      "grad_norm": 1.1046639680862427,
      "learning_rate": 4.309082062925722e-05,
      "loss": 3.04,
      "step": 214100
    },
    {
      "epoch": 69.43273905996759,
      "grad_norm": 1.1786025762557983,
      "learning_rate": 4.3087577035355177e-05,
      "loss": 3.0181,
      "step": 214200
    },
    {
      "epoch": 69.46515397082658,
      "grad_norm": 1.036787986755371,
      "learning_rate": 4.3084333441453135e-05,
      "loss": 3.0378,
      "step": 214300
    },
    {
      "epoch": 69.49756888168558,
      "grad_norm": 0.953580915927887,
      "learning_rate": 4.308108984755109e-05,
      "loss": 3.0434,
      "step": 214400
    },
    {
      "epoch": 69.52998379254457,
      "grad_norm": 0.8663904666900635,
      "learning_rate": 4.3077846253649046e-05,
      "loss": 3.0428,
      "step": 214500
    },
    {
      "epoch": 69.56239870340356,
      "grad_norm": 0.9828327894210815,
      "learning_rate": 4.3074602659747005e-05,
      "loss": 3.0166,
      "step": 214600
    },
    {
      "epoch": 69.59481361426256,
      "grad_norm": 1.028395175933838,
      "learning_rate": 4.307135906584496e-05,
      "loss": 3.0139,
      "step": 214700
    },
    {
      "epoch": 69.62722852512155,
      "grad_norm": 1.0558754205703735,
      "learning_rate": 4.3068115471942915e-05,
      "loss": 3.0208,
      "step": 214800
    },
    {
      "epoch": 69.65964343598056,
      "grad_norm": 0.9291805028915405,
      "learning_rate": 4.306487187804087e-05,
      "loss": 3.0215,
      "step": 214900
    },
    {
      "epoch": 69.69205834683954,
      "grad_norm": 1.0174826383590698,
      "learning_rate": 4.3061628284138826e-05,
      "loss": 3.0317,
      "step": 215000
    },
    {
      "epoch": 69.72447325769855,
      "grad_norm": 0.9753902554512024,
      "learning_rate": 4.3058384690236785e-05,
      "loss": 3.043,
      "step": 215100
    },
    {
      "epoch": 69.75688816855754,
      "grad_norm": 0.9500665664672852,
      "learning_rate": 4.305514109633474e-05,
      "loss": 3.0297,
      "step": 215200
    },
    {
      "epoch": 69.78930307941653,
      "grad_norm": 1.0280424356460571,
      "learning_rate": 4.3051897502432696e-05,
      "loss": 3.0267,
      "step": 215300
    },
    {
      "epoch": 69.82171799027553,
      "grad_norm": 0.9077160954475403,
      "learning_rate": 4.3048653908530654e-05,
      "loss": 3.0216,
      "step": 215400
    },
    {
      "epoch": 69.85413290113452,
      "grad_norm": 0.9583052396774292,
      "learning_rate": 4.304544275056763e-05,
      "loss": 3.0267,
      "step": 215500
    },
    {
      "epoch": 69.88654781199352,
      "grad_norm": 0.992850124835968,
      "learning_rate": 4.3042199156665584e-05,
      "loss": 3.0065,
      "step": 215600
    },
    {
      "epoch": 69.91896272285251,
      "grad_norm": 0.9679660797119141,
      "learning_rate": 4.303895556276354e-05,
      "loss": 3.0337,
      "step": 215700
    },
    {
      "epoch": 69.9513776337115,
      "grad_norm": 0.9956992864608765,
      "learning_rate": 4.30357119688615e-05,
      "loss": 3.0369,
      "step": 215800
    },
    {
      "epoch": 69.9837925445705,
      "grad_norm": 1.2153536081314087,
      "learning_rate": 4.3032468374959454e-05,
      "loss": 3.0149,
      "step": 215900
    },
    {
      "epoch": 70.0,
      "eval_bleu": 1.1147630693110406,
      "eval_loss": 3.788935899734497,
      "eval_runtime": 4.4701,
      "eval_samples_per_second": 110.064,
      "eval_steps_per_second": 1.79,
      "step": 215950
    },
    {
      "epoch": 70.0162074554295,
      "grad_norm": 0.9911654591560364,
      "learning_rate": 4.302922478105741e-05,
      "loss": 3.0349,
      "step": 216000
    },
    {
      "epoch": 70.0486223662885,
      "grad_norm": 0.912945568561554,
      "learning_rate": 4.3025981187155364e-05,
      "loss": 3.0032,
      "step": 216100
    },
    {
      "epoch": 70.08103727714749,
      "grad_norm": 0.9901794791221619,
      "learning_rate": 4.302273759325332e-05,
      "loss": 3.0238,
      "step": 216200
    },
    {
      "epoch": 70.11345218800648,
      "grad_norm": 0.9724683165550232,
      "learning_rate": 4.301949399935128e-05,
      "loss": 3.0163,
      "step": 216300
    },
    {
      "epoch": 70.14586709886548,
      "grad_norm": 0.9655687212944031,
      "learning_rate": 4.301625040544924e-05,
      "loss": 3.0276,
      "step": 216400
    },
    {
      "epoch": 70.17828200972447,
      "grad_norm": 0.961936354637146,
      "learning_rate": 4.30130068115472e-05,
      "loss": 3.0104,
      "step": 216500
    },
    {
      "epoch": 70.21069692058347,
      "grad_norm": 1.0181795358657837,
      "learning_rate": 4.300976321764516e-05,
      "loss": 3.0264,
      "step": 216600
    },
    {
      "epoch": 70.24311183144246,
      "grad_norm": 1.0663292407989502,
      "learning_rate": 4.300651962374311e-05,
      "loss": 3.022,
      "step": 216700
    },
    {
      "epoch": 70.27552674230145,
      "grad_norm": 0.8903969526290894,
      "learning_rate": 4.300327602984107e-05,
      "loss": 3.0343,
      "step": 216800
    },
    {
      "epoch": 70.30794165316046,
      "grad_norm": 1.017232060432434,
      "learning_rate": 4.300003243593903e-05,
      "loss": 3.0228,
      "step": 216900
    },
    {
      "epoch": 70.34035656401944,
      "grad_norm": 1.1320340633392334,
      "learning_rate": 4.299678884203698e-05,
      "loss": 3.0265,
      "step": 217000
    },
    {
      "epoch": 70.37277147487845,
      "grad_norm": 1.040643334388733,
      "learning_rate": 4.299354524813494e-05,
      "loss": 3.0062,
      "step": 217100
    },
    {
      "epoch": 70.40518638573744,
      "grad_norm": 1.0996603965759277,
      "learning_rate": 4.299030165423289e-05,
      "loss": 3.0236,
      "step": 217200
    },
    {
      "epoch": 70.43760129659644,
      "grad_norm": 0.9473301768302917,
      "learning_rate": 4.298705806033085e-05,
      "loss": 3.0306,
      "step": 217300
    },
    {
      "epoch": 70.47001620745543,
      "grad_norm": 0.9167338013648987,
      "learning_rate": 4.298381446642881e-05,
      "loss": 3.0275,
      "step": 217400
    },
    {
      "epoch": 70.50243111831442,
      "grad_norm": 0.9238314032554626,
      "learning_rate": 4.2980603308465785e-05,
      "loss": 3.0135,
      "step": 217500
    },
    {
      "epoch": 70.53484602917342,
      "grad_norm": 0.9137956500053406,
      "learning_rate": 4.297735971456374e-05,
      "loss": 3.0358,
      "step": 217600
    },
    {
      "epoch": 70.56726094003241,
      "grad_norm": 0.9303743839263916,
      "learning_rate": 4.2974116120661696e-05,
      "loss": 3.0336,
      "step": 217700
    },
    {
      "epoch": 70.59967585089142,
      "grad_norm": 1.151563286781311,
      "learning_rate": 4.2970872526759655e-05,
      "loss": 3.0274,
      "step": 217800
    },
    {
      "epoch": 70.6320907617504,
      "grad_norm": 1.001854419708252,
      "learning_rate": 4.296762893285761e-05,
      "loss": 3.0186,
      "step": 217900
    },
    {
      "epoch": 70.6645056726094,
      "grad_norm": 0.9404789805412292,
      "learning_rate": 4.2964385338955566e-05,
      "loss": 3.0211,
      "step": 218000
    },
    {
      "epoch": 70.6969205834684,
      "grad_norm": 1.009298324584961,
      "learning_rate": 4.2961141745053524e-05,
      "loss": 3.0318,
      "step": 218100
    },
    {
      "epoch": 70.72933549432739,
      "grad_norm": 0.9352536201477051,
      "learning_rate": 4.2957898151151476e-05,
      "loss": 3.009,
      "step": 218200
    },
    {
      "epoch": 70.76175040518639,
      "grad_norm": 0.9798349142074585,
      "learning_rate": 4.2954654557249435e-05,
      "loss": 3.0395,
      "step": 218300
    },
    {
      "epoch": 70.79416531604538,
      "grad_norm": 1.139697551727295,
      "learning_rate": 4.295144339928641e-05,
      "loss": 3.0279,
      "step": 218400
    },
    {
      "epoch": 70.82658022690438,
      "grad_norm": 0.9239896535873413,
      "learning_rate": 4.294819980538437e-05,
      "loss": 3.0115,
      "step": 218500
    },
    {
      "epoch": 70.85899513776337,
      "grad_norm": 0.9393025040626526,
      "learning_rate": 4.2944956211482324e-05,
      "loss": 3.0273,
      "step": 218600
    },
    {
      "epoch": 70.89141004862236,
      "grad_norm": 0.9062139391899109,
      "learning_rate": 4.294171261758028e-05,
      "loss": 3.0279,
      "step": 218700
    },
    {
      "epoch": 70.92382495948137,
      "grad_norm": 0.967012345790863,
      "learning_rate": 4.2938469023678234e-05,
      "loss": 3.0294,
      "step": 218800
    },
    {
      "epoch": 70.95623987034035,
      "grad_norm": 0.969709038734436,
      "learning_rate": 4.293522542977619e-05,
      "loss": 3.0313,
      "step": 218900
    },
    {
      "epoch": 70.98865478119936,
      "grad_norm": 0.958719789981842,
      "learning_rate": 4.293201427181317e-05,
      "loss": 3.0284,
      "step": 219000
    },
    {
      "epoch": 71.0,
      "eval_bleu": 1.2419782083813002,
      "eval_loss": 3.797208547592163,
      "eval_runtime": 4.52,
      "eval_samples_per_second": 108.849,
      "eval_steps_per_second": 1.77,
      "step": 219035
    },
    {
      "epoch": 71.02106969205835,
      "grad_norm": 1.0118567943572998,
      "learning_rate": 4.292877067791113e-05,
      "loss": 3.0013,
      "step": 219100
    },
    {
      "epoch": 71.05348460291734,
      "grad_norm": 1.051900029182434,
      "learning_rate": 4.292552708400908e-05,
      "loss": 3.0158,
      "step": 219200
    },
    {
      "epoch": 71.08589951377634,
      "grad_norm": 0.9952330589294434,
      "learning_rate": 4.292228349010704e-05,
      "loss": 3.0118,
      "step": 219300
    },
    {
      "epoch": 71.11831442463533,
      "grad_norm": 1.017245888710022,
      "learning_rate": 4.2919039896205e-05,
      "loss": 3.009,
      "step": 219400
    },
    {
      "epoch": 71.15072933549433,
      "grad_norm": 1.037263035774231,
      "learning_rate": 4.291579630230295e-05,
      "loss": 3.023,
      "step": 219500
    },
    {
      "epoch": 71.18314424635332,
      "grad_norm": 0.8933784365653992,
      "learning_rate": 4.291255270840091e-05,
      "loss": 3.0069,
      "step": 219600
    },
    {
      "epoch": 71.21555915721231,
      "grad_norm": 1.033177137374878,
      "learning_rate": 4.290930911449887e-05,
      "loss": 3.0202,
      "step": 219700
    },
    {
      "epoch": 71.24797406807131,
      "grad_norm": 1.140195369720459,
      "learning_rate": 4.2906097956535846e-05,
      "loss": 3.0068,
      "step": 219800
    },
    {
      "epoch": 71.2803889789303,
      "grad_norm": 0.9221016764640808,
      "learning_rate": 4.29028543626338e-05,
      "loss": 3.0132,
      "step": 219900
    },
    {
      "epoch": 71.31280388978931,
      "grad_norm": 0.9290106892585754,
      "learning_rate": 4.289961076873176e-05,
      "loss": 3.014,
      "step": 220000
    },
    {
      "epoch": 71.3452188006483,
      "grad_norm": 0.9443814754486084,
      "learning_rate": 4.2896367174829716e-05,
      "loss": 3.0297,
      "step": 220100
    },
    {
      "epoch": 71.37763371150729,
      "grad_norm": 1.1420356035232544,
      "learning_rate": 4.289312358092767e-05,
      "loss": 3.021,
      "step": 220200
    },
    {
      "epoch": 71.41004862236629,
      "grad_norm": 1.023417592048645,
      "learning_rate": 4.2889879987025626e-05,
      "loss": 3.0161,
      "step": 220300
    },
    {
      "epoch": 71.44246353322528,
      "grad_norm": 0.9789552688598633,
      "learning_rate": 4.288663639312358e-05,
      "loss": 3.0495,
      "step": 220400
    },
    {
      "epoch": 71.47487844408428,
      "grad_norm": 0.8732361793518066,
      "learning_rate": 4.288339279922154e-05,
      "loss": 3.0238,
      "step": 220500
    },
    {
      "epoch": 71.50729335494327,
      "grad_norm": 1.0645778179168701,
      "learning_rate": 4.2880149205319496e-05,
      "loss": 3.0487,
      "step": 220600
    },
    {
      "epoch": 71.53970826580228,
      "grad_norm": 1.0098110437393188,
      "learning_rate": 4.287690561141745e-05,
      "loss": 3.008,
      "step": 220700
    },
    {
      "epoch": 71.57212317666126,
      "grad_norm": 1.0844104290008545,
      "learning_rate": 4.2873662017515407e-05,
      "loss": 3.0015,
      "step": 220800
    },
    {
      "epoch": 71.60453808752025,
      "grad_norm": 1.0342564582824707,
      "learning_rate": 4.2870418423613365e-05,
      "loss": 3.0425,
      "step": 220900
    },
    {
      "epoch": 71.63695299837926,
      "grad_norm": 0.8161481618881226,
      "learning_rate": 4.2867174829711324e-05,
      "loss": 3.0193,
      "step": 221000
    },
    {
      "epoch": 71.66936790923825,
      "grad_norm": 1.0681778192520142,
      "learning_rate": 4.2863931235809276e-05,
      "loss": 3.0091,
      "step": 221100
    },
    {
      "epoch": 71.70178282009725,
      "grad_norm": 1.0427581071853638,
      "learning_rate": 4.2860687641907235e-05,
      "loss": 3.0301,
      "step": 221200
    },
    {
      "epoch": 71.73419773095624,
      "grad_norm": 1.0320615768432617,
      "learning_rate": 4.2857444048005194e-05,
      "loss": 3.0169,
      "step": 221300
    },
    {
      "epoch": 71.76661264181523,
      "grad_norm": 0.9785946607589722,
      "learning_rate": 4.285420045410315e-05,
      "loss": 3.003,
      "step": 221400
    },
    {
      "epoch": 71.79902755267423,
      "grad_norm": 0.9665758609771729,
      "learning_rate": 4.2850956860201104e-05,
      "loss": 3.023,
      "step": 221500
    },
    {
      "epoch": 71.83144246353322,
      "grad_norm": 1.001916766166687,
      "learning_rate": 4.284771326629906e-05,
      "loss": 3.0214,
      "step": 221600
    },
    {
      "epoch": 71.86385737439223,
      "grad_norm": 1.1073857545852661,
      "learning_rate": 4.284446967239702e-05,
      "loss": 3.0323,
      "step": 221700
    },
    {
      "epoch": 71.89627228525121,
      "grad_norm": 0.968226969242096,
      "learning_rate": 4.2841226078494974e-05,
      "loss": 3.0123,
      "step": 221800
    },
    {
      "epoch": 71.9286871961102,
      "grad_norm": 1.0064232349395752,
      "learning_rate": 4.283798248459293e-05,
      "loss": 3.0194,
      "step": 221900
    },
    {
      "epoch": 71.96110210696921,
      "grad_norm": 0.9383906722068787,
      "learning_rate": 4.283473889069089e-05,
      "loss": 3.0157,
      "step": 222000
    },
    {
      "epoch": 71.9935170178282,
      "grad_norm": 0.9446894526481628,
      "learning_rate": 4.283149529678884e-05,
      "loss": 3.0499,
      "step": 222100
    },
    {
      "epoch": 72.0,
      "eval_bleu": 1.2161152276497207,
      "eval_loss": 3.7963578701019287,
      "eval_runtime": 4.9128,
      "eval_samples_per_second": 100.147,
      "eval_steps_per_second": 1.628,
      "step": 222120
    },
    {
      "epoch": 72.0259319286872,
      "grad_norm": 0.9312269687652588,
      "learning_rate": 4.28282517028868e-05,
      "loss": 3.0178,
      "step": 222200
    },
    {
      "epoch": 72.05834683954619,
      "grad_norm": 0.9115213751792908,
      "learning_rate": 4.282500810898476e-05,
      "loss": 3.0233,
      "step": 222300
    },
    {
      "epoch": 72.09076175040519,
      "grad_norm": 0.9317170977592468,
      "learning_rate": 4.282176451508271e-05,
      "loss": 2.9856,
      "step": 222400
    },
    {
      "epoch": 72.12317666126418,
      "grad_norm": 0.9798657298088074,
      "learning_rate": 4.281852092118067e-05,
      "loss": 3.006,
      "step": 222500
    },
    {
      "epoch": 72.15559157212317,
      "grad_norm": 1.1900506019592285,
      "learning_rate": 4.281527732727862e-05,
      "loss": 3.0283,
      "step": 222600
    },
    {
      "epoch": 72.18800648298217,
      "grad_norm": 0.9025070071220398,
      "learning_rate": 4.281203373337658e-05,
      "loss": 3.023,
      "step": 222700
    },
    {
      "epoch": 72.22042139384116,
      "grad_norm": 0.9814870357513428,
      "learning_rate": 4.280879013947454e-05,
      "loss": 3.0373,
      "step": 222800
    },
    {
      "epoch": 72.25283630470017,
      "grad_norm": 0.9578638672828674,
      "learning_rate": 4.280554654557249e-05,
      "loss": 2.9953,
      "step": 222900
    },
    {
      "epoch": 72.28525121555916,
      "grad_norm": 0.9381853342056274,
      "learning_rate": 4.280233538760947e-05,
      "loss": 3.0097,
      "step": 223000
    },
    {
      "epoch": 72.31766612641815,
      "grad_norm": 0.9146835803985596,
      "learning_rate": 4.279909179370743e-05,
      "loss": 3.0281,
      "step": 223100
    },
    {
      "epoch": 72.35008103727715,
      "grad_norm": 0.9878574013710022,
      "learning_rate": 4.279584819980539e-05,
      "loss": 3.0099,
      "step": 223200
    },
    {
      "epoch": 72.38249594813614,
      "grad_norm": 0.9801392555236816,
      "learning_rate": 4.279260460590334e-05,
      "loss": 2.9928,
      "step": 223300
    },
    {
      "epoch": 72.41491085899514,
      "grad_norm": 1.1284607648849487,
      "learning_rate": 4.27893610120013e-05,
      "loss": 3.0316,
      "step": 223400
    },
    {
      "epoch": 72.44732576985413,
      "grad_norm": 1.0011674165725708,
      "learning_rate": 4.278611741809926e-05,
      "loss": 3.0036,
      "step": 223500
    },
    {
      "epoch": 72.47974068071312,
      "grad_norm": 1.0524137020111084,
      "learning_rate": 4.278287382419721e-05,
      "loss": 3.0321,
      "step": 223600
    },
    {
      "epoch": 72.51215559157212,
      "grad_norm": 0.930594265460968,
      "learning_rate": 4.277963023029517e-05,
      "loss": 3.0026,
      "step": 223700
    },
    {
      "epoch": 72.54457050243111,
      "grad_norm": 0.9482957124710083,
      "learning_rate": 4.277638663639312e-05,
      "loss": 3.0222,
      "step": 223800
    },
    {
      "epoch": 72.57698541329012,
      "grad_norm": 0.8861395120620728,
      "learning_rate": 4.277314304249108e-05,
      "loss": 3.0167,
      "step": 223900
    },
    {
      "epoch": 72.6094003241491,
      "grad_norm": 0.9695923924446106,
      "learning_rate": 4.276989944858904e-05,
      "loss": 3.0121,
      "step": 224000
    },
    {
      "epoch": 72.64181523500811,
      "grad_norm": 1.1232049465179443,
      "learning_rate": 4.2766655854686996e-05,
      "loss": 3.0144,
      "step": 224100
    },
    {
      "epoch": 72.6742301458671,
      "grad_norm": 0.9545140862464905,
      "learning_rate": 4.2763412260784955e-05,
      "loss": 3.0117,
      "step": 224200
    },
    {
      "epoch": 72.70664505672609,
      "grad_norm": 1.1483705043792725,
      "learning_rate": 4.276016866688291e-05,
      "loss": 3.0277,
      "step": 224300
    },
    {
      "epoch": 72.73905996758509,
      "grad_norm": 0.93214350938797,
      "learning_rate": 4.2756925072980866e-05,
      "loss": 3.0245,
      "step": 224400
    },
    {
      "epoch": 72.77147487844408,
      "grad_norm": 0.864465594291687,
      "learning_rate": 4.2753681479078825e-05,
      "loss": 3.0329,
      "step": 224500
    },
    {
      "epoch": 72.80388978930308,
      "grad_norm": 0.9146140813827515,
      "learning_rate": 4.275043788517678e-05,
      "loss": 3.0218,
      "step": 224600
    },
    {
      "epoch": 72.83630470016207,
      "grad_norm": 0.9060612320899963,
      "learning_rate": 4.2747194291274735e-05,
      "loss": 3.0205,
      "step": 224700
    },
    {
      "epoch": 72.86871961102106,
      "grad_norm": 1.025734305381775,
      "learning_rate": 4.2743950697372694e-05,
      "loss": 3.0278,
      "step": 224800
    },
    {
      "epoch": 72.90113452188007,
      "grad_norm": 0.858806312084198,
      "learning_rate": 4.2740707103470646e-05,
      "loss": 3.0173,
      "step": 224900
    },
    {
      "epoch": 72.93354943273906,
      "grad_norm": 1.1431022882461548,
      "learning_rate": 4.2737463509568605e-05,
      "loss": 2.9943,
      "step": 225000
    },
    {
      "epoch": 72.96596434359806,
      "grad_norm": 0.9559085369110107,
      "learning_rate": 4.2734219915666564e-05,
      "loss": 3.0323,
      "step": 225100
    },
    {
      "epoch": 72.99837925445705,
      "grad_norm": 0.9115543365478516,
      "learning_rate": 4.2730976321764515e-05,
      "loss": 3.0185,
      "step": 225200
    },
    {
      "epoch": 73.0,
      "eval_bleu": 1.1877252859460952,
      "eval_loss": 3.7988929748535156,
      "eval_runtime": 4.3275,
      "eval_samples_per_second": 113.691,
      "eval_steps_per_second": 1.849,
      "step": 225205
    },
    {
      "epoch": 73.03079416531604,
      "grad_norm": 0.9660592675209045,
      "learning_rate": 4.2727732727862474e-05,
      "loss": 3.0197,
      "step": 225300
    },
    {
      "epoch": 73.06320907617504,
      "grad_norm": 0.9544588327407837,
      "learning_rate": 4.272452156989945e-05,
      "loss": 3.0037,
      "step": 225400
    },
    {
      "epoch": 73.09562398703403,
      "grad_norm": 0.9786755442619324,
      "learning_rate": 4.272127797599741e-05,
      "loss": 3.0124,
      "step": 225500
    },
    {
      "epoch": 73.12803889789303,
      "grad_norm": 0.9595096111297607,
      "learning_rate": 4.271803438209536e-05,
      "loss": 3.0072,
      "step": 225600
    },
    {
      "epoch": 73.16045380875202,
      "grad_norm": 0.9169298410415649,
      "learning_rate": 4.271479078819332e-05,
      "loss": 3.004,
      "step": 225700
    },
    {
      "epoch": 73.19286871961103,
      "grad_norm": 0.8988928198814392,
      "learning_rate": 4.271154719429128e-05,
      "loss": 3.0198,
      "step": 225800
    },
    {
      "epoch": 73.22528363047002,
      "grad_norm": 1.0933564901351929,
      "learning_rate": 4.270830360038923e-05,
      "loss": 3.008,
      "step": 225900
    },
    {
      "epoch": 73.257698541329,
      "grad_norm": 0.9606413841247559,
      "learning_rate": 4.270506000648719e-05,
      "loss": 2.9973,
      "step": 226000
    },
    {
      "epoch": 73.29011345218801,
      "grad_norm": 0.8578967452049255,
      "learning_rate": 4.270181641258514e-05,
      "loss": 3.0271,
      "step": 226100
    },
    {
      "epoch": 73.322528363047,
      "grad_norm": 0.9813776612281799,
      "learning_rate": 4.26985728186831e-05,
      "loss": 3.0047,
      "step": 226200
    },
    {
      "epoch": 73.354943273906,
      "grad_norm": 1.113995909690857,
      "learning_rate": 4.269532922478106e-05,
      "loss": 3.0166,
      "step": 226300
    },
    {
      "epoch": 73.38735818476499,
      "grad_norm": 0.9273882508277893,
      "learning_rate": 4.269208563087901e-05,
      "loss": 2.9938,
      "step": 226400
    },
    {
      "epoch": 73.41977309562398,
      "grad_norm": 1.006885290145874,
      "learning_rate": 4.268884203697697e-05,
      "loss": 3.008,
      "step": 226500
    },
    {
      "epoch": 73.45218800648298,
      "grad_norm": 0.9885825514793396,
      "learning_rate": 4.268559844307493e-05,
      "loss": 3.0201,
      "step": 226600
    },
    {
      "epoch": 73.48460291734197,
      "grad_norm": 1.0788192749023438,
      "learning_rate": 4.268235484917288e-05,
      "loss": 3.0077,
      "step": 226700
    },
    {
      "epoch": 73.51701782820098,
      "grad_norm": 1.1505763530731201,
      "learning_rate": 4.267911125527084e-05,
      "loss": 2.9988,
      "step": 226800
    },
    {
      "epoch": 73.54943273905997,
      "grad_norm": 1.137434482574463,
      "learning_rate": 4.26758676613688e-05,
      "loss": 3.0277,
      "step": 226900
    },
    {
      "epoch": 73.58184764991896,
      "grad_norm": 1.179266095161438,
      "learning_rate": 4.267262406746675e-05,
      "loss": 3.0177,
      "step": 227000
    },
    {
      "epoch": 73.61426256077796,
      "grad_norm": 1.612559199333191,
      "learning_rate": 4.266938047356471e-05,
      "loss": 3.0153,
      "step": 227100
    },
    {
      "epoch": 73.64667747163695,
      "grad_norm": 1.1719450950622559,
      "learning_rate": 4.266613687966267e-05,
      "loss": 2.9997,
      "step": 227200
    },
    {
      "epoch": 73.67909238249595,
      "grad_norm": 0.9841035008430481,
      "learning_rate": 4.266289328576063e-05,
      "loss": 3.0217,
      "step": 227300
    },
    {
      "epoch": 73.71150729335494,
      "grad_norm": 0.9195095300674438,
      "learning_rate": 4.2659649691858586e-05,
      "loss": 3.02,
      "step": 227400
    },
    {
      "epoch": 73.74392220421394,
      "grad_norm": 1.0675787925720215,
      "learning_rate": 4.265640609795654e-05,
      "loss": 3.0344,
      "step": 227500
    },
    {
      "epoch": 73.77633711507293,
      "grad_norm": 1.0421648025512695,
      "learning_rate": 4.26531625040545e-05,
      "loss": 3.0202,
      "step": 227600
    },
    {
      "epoch": 73.80875202593192,
      "grad_norm": 0.9331057667732239,
      "learning_rate": 4.264995134609147e-05,
      "loss": 3.011,
      "step": 227700
    },
    {
      "epoch": 73.84116693679093,
      "grad_norm": 1.2325910329818726,
      "learning_rate": 4.264670775218943e-05,
      "loss": 3.015,
      "step": 227800
    },
    {
      "epoch": 73.87358184764992,
      "grad_norm": 0.8995665907859802,
      "learning_rate": 4.2643464158287385e-05,
      "loss": 3.0121,
      "step": 227900
    },
    {
      "epoch": 73.90599675850892,
      "grad_norm": 0.97135990858078,
      "learning_rate": 4.2640220564385344e-05,
      "loss": 3.0049,
      "step": 228000
    },
    {
      "epoch": 73.93841166936791,
      "grad_norm": 1.2022418975830078,
      "learning_rate": 4.26369769704833e-05,
      "loss": 3.0233,
      "step": 228100
    },
    {
      "epoch": 73.9708265802269,
      "grad_norm": 1.0371406078338623,
      "learning_rate": 4.2633733376581255e-05,
      "loss": 3.0224,
      "step": 228200
    },
    {
      "epoch": 74.0,
      "eval_bleu": 1.1753084178980808,
      "eval_loss": 3.798126697540283,
      "eval_runtime": 4.2834,
      "eval_samples_per_second": 114.862,
      "eval_steps_per_second": 1.868,
      "step": 228290
    },
    {
      "epoch": 74.0032414910859,
      "grad_norm": 1.0116922855377197,
      "learning_rate": 4.2630489782679214e-05,
      "loss": 3.0252,
      "step": 228300
    },
    {
      "epoch": 74.03565640194489,
      "grad_norm": 0.9767429828643799,
      "learning_rate": 4.2627246188777166e-05,
      "loss": 3.0081,
      "step": 228400
    },
    {
      "epoch": 74.0680713128039,
      "grad_norm": 1.0876984596252441,
      "learning_rate": 4.2624002594875124e-05,
      "loss": 2.997,
      "step": 228500
    },
    {
      "epoch": 74.10048622366288,
      "grad_norm": 1.0080631971359253,
      "learning_rate": 4.262075900097308e-05,
      "loss": 3.0094,
      "step": 228600
    },
    {
      "epoch": 74.13290113452187,
      "grad_norm": 0.9725315570831299,
      "learning_rate": 4.2617515407071035e-05,
      "loss": 3.0165,
      "step": 228700
    },
    {
      "epoch": 74.16531604538088,
      "grad_norm": 1.0368995666503906,
      "learning_rate": 4.2614271813168994e-05,
      "loss": 3.0104,
      "step": 228800
    },
    {
      "epoch": 74.19773095623987,
      "grad_norm": 0.9370968341827393,
      "learning_rate": 4.261102821926695e-05,
      "loss": 2.9984,
      "step": 228900
    },
    {
      "epoch": 74.23014586709887,
      "grad_norm": 0.9337444305419922,
      "learning_rate": 4.2607784625364905e-05,
      "loss": 3.0095,
      "step": 229000
    },
    {
      "epoch": 74.26256077795786,
      "grad_norm": 0.9701345562934875,
      "learning_rate": 4.260454103146286e-05,
      "loss": 3.0046,
      "step": 229100
    },
    {
      "epoch": 74.29497568881686,
      "grad_norm": 1.1010020971298218,
      "learning_rate": 4.260129743756082e-05,
      "loss": 3.01,
      "step": 229200
    },
    {
      "epoch": 74.32739059967585,
      "grad_norm": 0.9829874038696289,
      "learning_rate": 4.2598053843658774e-05,
      "loss": 2.9903,
      "step": 229300
    },
    {
      "epoch": 74.35980551053484,
      "grad_norm": 1.0713329315185547,
      "learning_rate": 4.259481024975673e-05,
      "loss": 3.0223,
      "step": 229400
    },
    {
      "epoch": 74.39222042139384,
      "grad_norm": 0.9982863068580627,
      "learning_rate": 4.2591566655854685e-05,
      "loss": 3.0078,
      "step": 229500
    },
    {
      "epoch": 74.42463533225283,
      "grad_norm": 1.0079549551010132,
      "learning_rate": 4.2588323061952643e-05,
      "loss": 3.0068,
      "step": 229600
    },
    {
      "epoch": 74.45705024311184,
      "grad_norm": 1.029446005821228,
      "learning_rate": 4.25850794680506e-05,
      "loss": 2.9986,
      "step": 229700
    },
    {
      "epoch": 74.48946515397083,
      "grad_norm": 0.9587920308113098,
      "learning_rate": 4.2581835874148554e-05,
      "loss": 3.0174,
      "step": 229800
    },
    {
      "epoch": 74.52188006482982,
      "grad_norm": 0.9145508408546448,
      "learning_rate": 4.257859228024651e-05,
      "loss": 3.0167,
      "step": 229900
    },
    {
      "epoch": 74.55429497568882,
      "grad_norm": 1.0937868356704712,
      "learning_rate": 4.257534868634447e-05,
      "loss": 3.0204,
      "step": 230000
    },
    {
      "epoch": 74.58670988654781,
      "grad_norm": 1.0900925397872925,
      "learning_rate": 4.2572105092442424e-05,
      "loss": 3.0276,
      "step": 230100
    },
    {
      "epoch": 74.61912479740681,
      "grad_norm": 1.081492304801941,
      "learning_rate": 4.256886149854038e-05,
      "loss": 3.0287,
      "step": 230200
    },
    {
      "epoch": 74.6515397082658,
      "grad_norm": 0.9215598106384277,
      "learning_rate": 4.256561790463834e-05,
      "loss": 3.0071,
      "step": 230300
    },
    {
      "epoch": 74.68395461912479,
      "grad_norm": 1.163475513458252,
      "learning_rate": 4.25623743107363e-05,
      "loss": 2.9909,
      "step": 230400
    },
    {
      "epoch": 74.7163695299838,
      "grad_norm": 1.0637071132659912,
      "learning_rate": 4.255913071683426e-05,
      "loss": 2.9842,
      "step": 230500
    },
    {
      "epoch": 74.74878444084278,
      "grad_norm": 1.0246329307556152,
      "learning_rate": 4.255588712293221e-05,
      "loss": 3.0242,
      "step": 230600
    },
    {
      "epoch": 74.78119935170179,
      "grad_norm": 1.035521388053894,
      "learning_rate": 4.255264352903017e-05,
      "loss": 2.9949,
      "step": 230700
    },
    {
      "epoch": 74.81361426256078,
      "grad_norm": 1.066938042640686,
      "learning_rate": 4.254939993512813e-05,
      "loss": 3.0088,
      "step": 230800
    },
    {
      "epoch": 74.84602917341978,
      "grad_norm": 0.9991336464881897,
      "learning_rate": 4.254615634122608e-05,
      "loss": 3.002,
      "step": 230900
    },
    {
      "epoch": 74.87844408427877,
      "grad_norm": 0.9491111040115356,
      "learning_rate": 4.254291274732404e-05,
      "loss": 3.0198,
      "step": 231000
    },
    {
      "epoch": 74.91085899513776,
      "grad_norm": 1.0364127159118652,
      "learning_rate": 4.2539669153422e-05,
      "loss": 3.0054,
      "step": 231100
    },
    {
      "epoch": 74.94327390599676,
      "grad_norm": 1.060845136642456,
      "learning_rate": 4.253642555951995e-05,
      "loss": 2.9953,
      "step": 231200
    },
    {
      "epoch": 74.97568881685575,
      "grad_norm": 0.9655767679214478,
      "learning_rate": 4.253318196561791e-05,
      "loss": 3.0255,
      "step": 231300
    },
    {
      "epoch": 75.0,
      "eval_bleu": 1.1132553024299694,
      "eval_loss": 3.804202079772949,
      "eval_runtime": 4.6413,
      "eval_samples_per_second": 106.005,
      "eval_steps_per_second": 1.724,
      "step": 231375
    },
    {
      "epoch": 75.00810372771475,
      "grad_norm": 1.0661981105804443,
      "learning_rate": 4.252993837171586e-05,
      "loss": 3.0097,
      "step": 231400
    },
    {
      "epoch": 75.04051863857374,
      "grad_norm": 0.9939681887626648,
      "learning_rate": 4.252669477781382e-05,
      "loss": 3.013,
      "step": 231500
    },
    {
      "epoch": 75.07293354943273,
      "grad_norm": 0.9460613131523132,
      "learning_rate": 4.252345118391178e-05,
      "loss": 3.0009,
      "step": 231600
    },
    {
      "epoch": 75.10534846029174,
      "grad_norm": 0.980370044708252,
      "learning_rate": 4.252020759000973e-05,
      "loss": 2.9882,
      "step": 231700
    },
    {
      "epoch": 75.13776337115073,
      "grad_norm": 1.0232484340667725,
      "learning_rate": 4.251696399610769e-05,
      "loss": 3.0041,
      "step": 231800
    },
    {
      "epoch": 75.17017828200973,
      "grad_norm": 1.1084588766098022,
      "learning_rate": 4.251372040220565e-05,
      "loss": 3.0056,
      "step": 231900
    },
    {
      "epoch": 75.20259319286872,
      "grad_norm": 1.069765329360962,
      "learning_rate": 4.25104768083036e-05,
      "loss": 2.9922,
      "step": 232000
    },
    {
      "epoch": 75.23500810372771,
      "grad_norm": 1.0407143831253052,
      "learning_rate": 4.250723321440156e-05,
      "loss": 3.0172,
      "step": 232100
    },
    {
      "epoch": 75.26742301458671,
      "grad_norm": 1.0512853860855103,
      "learning_rate": 4.2503989620499517e-05,
      "loss": 3.0127,
      "step": 232200
    },
    {
      "epoch": 75.2998379254457,
      "grad_norm": 0.953782856464386,
      "learning_rate": 4.250074602659747e-05,
      "loss": 3.0056,
      "step": 232300
    },
    {
      "epoch": 75.3322528363047,
      "grad_norm": 0.9374799132347107,
      "learning_rate": 4.249750243269543e-05,
      "loss": 3.0029,
      "step": 232400
    },
    {
      "epoch": 75.3646677471637,
      "grad_norm": 0.9331226348876953,
      "learning_rate": 4.2494258838793386e-05,
      "loss": 2.9835,
      "step": 232500
    },
    {
      "epoch": 75.3970826580227,
      "grad_norm": 0.8374287486076355,
      "learning_rate": 4.249101524489134e-05,
      "loss": 2.9961,
      "step": 232600
    },
    {
      "epoch": 75.42949756888169,
      "grad_norm": 1.1221234798431396,
      "learning_rate": 4.24877716509893e-05,
      "loss": 3.0005,
      "step": 232700
    },
    {
      "epoch": 75.46191247974068,
      "grad_norm": 0.8951229453086853,
      "learning_rate": 4.2484528057087255e-05,
      "loss": 2.9991,
      "step": 232800
    },
    {
      "epoch": 75.49432739059968,
      "grad_norm": 0.8981218934059143,
      "learning_rate": 4.2481284463185214e-05,
      "loss": 3.0139,
      "step": 232900
    },
    {
      "epoch": 75.52674230145867,
      "grad_norm": 0.9372380971908569,
      "learning_rate": 4.247804086928317e-05,
      "loss": 3.0006,
      "step": 233000
    },
    {
      "epoch": 75.55915721231767,
      "grad_norm": 0.9201141595840454,
      "learning_rate": 4.2474797275381125e-05,
      "loss": 3.0146,
      "step": 233100
    },
    {
      "epoch": 75.59157212317666,
      "grad_norm": 0.9490286111831665,
      "learning_rate": 4.2471553681479084e-05,
      "loss": 2.9922,
      "step": 233200
    },
    {
      "epoch": 75.62398703403565,
      "grad_norm": 1.0318760871887207,
      "learning_rate": 4.246831008757704e-05,
      "loss": 3.0127,
      "step": 233300
    },
    {
      "epoch": 75.65640194489465,
      "grad_norm": 0.9602853059768677,
      "learning_rate": 4.2465066493674994e-05,
      "loss": 2.9996,
      "step": 233400
    },
    {
      "epoch": 75.68881685575364,
      "grad_norm": 0.9295884370803833,
      "learning_rate": 4.246182289977295e-05,
      "loss": 3.0075,
      "step": 233500
    },
    {
      "epoch": 75.72123176661265,
      "grad_norm": 1.282928705215454,
      "learning_rate": 4.2458579305870905e-05,
      "loss": 3.0059,
      "step": 233600
    },
    {
      "epoch": 75.75364667747164,
      "grad_norm": 0.9187174439430237,
      "learning_rate": 4.2455335711968864e-05,
      "loss": 3.0281,
      "step": 233700
    },
    {
      "epoch": 75.78606158833063,
      "grad_norm": 0.8854692578315735,
      "learning_rate": 4.245209211806682e-05,
      "loss": 3.0211,
      "step": 233800
    },
    {
      "epoch": 75.81847649918963,
      "grad_norm": 1.0703424215316772,
      "learning_rate": 4.2448848524164774e-05,
      "loss": 3.013,
      "step": 233900
    },
    {
      "epoch": 75.85089141004862,
      "grad_norm": 0.9728807210922241,
      "learning_rate": 4.244560493026273e-05,
      "loss": 2.9857,
      "step": 234000
    },
    {
      "epoch": 75.88330632090762,
      "grad_norm": 1.1877434253692627,
      "learning_rate": 4.244236133636069e-05,
      "loss": 3.0001,
      "step": 234100
    },
    {
      "epoch": 75.91572123176661,
      "grad_norm": 1.0294973850250244,
      "learning_rate": 4.2439117742458644e-05,
      "loss": 3.0312,
      "step": 234200
    },
    {
      "epoch": 75.94813614262561,
      "grad_norm": 1.11995530128479,
      "learning_rate": 4.24358741485566e-05,
      "loss": 3.025,
      "step": 234300
    },
    {
      "epoch": 75.9805510534846,
      "grad_norm": 1.129256248474121,
      "learning_rate": 4.2432630554654555e-05,
      "loss": 3.012,
      "step": 234400
    },
    {
      "epoch": 76.0,
      "eval_bleu": 1.216223642396467,
      "eval_loss": 3.804739475250244,
      "eval_runtime": 4.4389,
      "eval_samples_per_second": 110.839,
      "eval_steps_per_second": 1.802,
      "step": 234460
    },
    {
      "epoch": 76.01296596434359,
      "grad_norm": 1.0385600328445435,
      "learning_rate": 4.242938696075251e-05,
      "loss": 2.9962,
      "step": 234500
    },
    {
      "epoch": 76.0453808752026,
      "grad_norm": 0.9845083355903625,
      "learning_rate": 4.242614336685047e-05,
      "loss": 2.9909,
      "step": 234600
    },
    {
      "epoch": 76.07779578606159,
      "grad_norm": 1.0607292652130127,
      "learning_rate": 4.2422899772948424e-05,
      "loss": 2.9836,
      "step": 234700
    },
    {
      "epoch": 76.11021069692059,
      "grad_norm": 1.032041072845459,
      "learning_rate": 4.241965617904638e-05,
      "loss": 2.9955,
      "step": 234800
    },
    {
      "epoch": 76.14262560777958,
      "grad_norm": 1.1964490413665771,
      "learning_rate": 4.241641258514434e-05,
      "loss": 2.9966,
      "step": 234900
    },
    {
      "epoch": 76.17504051863857,
      "grad_norm": 1.012805700302124,
      "learning_rate": 4.2413168991242294e-05,
      "loss": 2.9967,
      "step": 235000
    },
    {
      "epoch": 76.20745542949757,
      "grad_norm": 0.9071995615959167,
      "learning_rate": 4.240992539734025e-05,
      "loss": 2.9913,
      "step": 235100
    },
    {
      "epoch": 76.23987034035656,
      "grad_norm": 0.8962684869766235,
      "learning_rate": 4.240668180343821e-05,
      "loss": 3.0239,
      "step": 235200
    },
    {
      "epoch": 76.27228525121556,
      "grad_norm": 1.0281341075897217,
      "learning_rate": 4.240343820953617e-05,
      "loss": 2.99,
      "step": 235300
    },
    {
      "epoch": 76.30470016207455,
      "grad_norm": 0.9536156058311462,
      "learning_rate": 4.240019461563413e-05,
      "loss": 2.9933,
      "step": 235400
    },
    {
      "epoch": 76.33711507293354,
      "grad_norm": 1.0238627195358276,
      "learning_rate": 4.239695102173208e-05,
      "loss": 3.0169,
      "step": 235500
    },
    {
      "epoch": 76.36952998379255,
      "grad_norm": 0.8831833600997925,
      "learning_rate": 4.239370742783004e-05,
      "loss": 2.9986,
      "step": 235600
    },
    {
      "epoch": 76.40194489465154,
      "grad_norm": 0.9366862177848816,
      "learning_rate": 4.2390463833928e-05,
      "loss": 2.9866,
      "step": 235700
    },
    {
      "epoch": 76.43435980551054,
      "grad_norm": 0.9637255072593689,
      "learning_rate": 4.238728511190399e-05,
      "loss": 3.0109,
      "step": 235800
    },
    {
      "epoch": 76.46677471636953,
      "grad_norm": 1.1511610746383667,
      "learning_rate": 4.238404151800195e-05,
      "loss": 2.9913,
      "step": 235900
    },
    {
      "epoch": 76.49918962722853,
      "grad_norm": 1.0542547702789307,
      "learning_rate": 4.23807979240999e-05,
      "loss": 3.0118,
      "step": 236000
    },
    {
      "epoch": 76.53160453808752,
      "grad_norm": 1.082669973373413,
      "learning_rate": 4.237755433019786e-05,
      "loss": 3.0156,
      "step": 236100
    },
    {
      "epoch": 76.56401944894651,
      "grad_norm": 1.1171941757202148,
      "learning_rate": 4.2374310736295816e-05,
      "loss": 3.0053,
      "step": 236200
    },
    {
      "epoch": 76.59643435980551,
      "grad_norm": 0.9839821457862854,
      "learning_rate": 4.2371067142393775e-05,
      "loss": 3.0035,
      "step": 236300
    },
    {
      "epoch": 76.6288492706645,
      "grad_norm": 0.8989562392234802,
      "learning_rate": 4.2367823548491734e-05,
      "loss": 3.0039,
      "step": 236400
    },
    {
      "epoch": 76.6612641815235,
      "grad_norm": 1.0268731117248535,
      "learning_rate": 4.236457995458969e-05,
      "loss": 3.0116,
      "step": 236500
    },
    {
      "epoch": 76.6936790923825,
      "grad_norm": 1.1760584115982056,
      "learning_rate": 4.2361336360687644e-05,
      "loss": 3.0083,
      "step": 236600
    },
    {
      "epoch": 76.72609400324149,
      "grad_norm": 0.9828698635101318,
      "learning_rate": 4.235812520272462e-05,
      "loss": 3.0137,
      "step": 236700
    },
    {
      "epoch": 76.75850891410049,
      "grad_norm": 1.0208523273468018,
      "learning_rate": 4.2354881608822574e-05,
      "loss": 3.0072,
      "step": 236800
    },
    {
      "epoch": 76.79092382495948,
      "grad_norm": 1.058044672012329,
      "learning_rate": 4.235163801492053e-05,
      "loss": 3.0019,
      "step": 236900
    },
    {
      "epoch": 76.82333873581848,
      "grad_norm": 0.9478278756141663,
      "learning_rate": 4.234839442101849e-05,
      "loss": 3.0141,
      "step": 237000
    },
    {
      "epoch": 76.85575364667747,
      "grad_norm": 1.2612597942352295,
      "learning_rate": 4.234515082711645e-05,
      "loss": 3.0054,
      "step": 237100
    },
    {
      "epoch": 76.88816855753646,
      "grad_norm": 0.9625019431114197,
      "learning_rate": 4.234190723321441e-05,
      "loss": 2.992,
      "step": 237200
    },
    {
      "epoch": 76.92058346839546,
      "grad_norm": 0.9551793932914734,
      "learning_rate": 4.233866363931236e-05,
      "loss": 2.9971,
      "step": 237300
    },
    {
      "epoch": 76.95299837925445,
      "grad_norm": 0.9317455887794495,
      "learning_rate": 4.233542004541032e-05,
      "loss": 3.0169,
      "step": 237400
    },
    {
      "epoch": 76.98541329011346,
      "grad_norm": 1.0252002477645874,
      "learning_rate": 4.233217645150827e-05,
      "loss": 3.0081,
      "step": 237500
    },
    {
      "epoch": 77.0,
      "eval_bleu": 1.1896778888382245,
      "eval_loss": 3.8075873851776123,
      "eval_runtime": 4.2821,
      "eval_samples_per_second": 114.898,
      "eval_steps_per_second": 1.868,
      "step": 237545
    },
    {
      "epoch": 77.01782820097245,
      "grad_norm": 1.0049386024475098,
      "learning_rate": 4.232893285760623e-05,
      "loss": 3.0032,
      "step": 237600
    },
    {
      "epoch": 77.05024311183145,
      "grad_norm": 0.9806503057479858,
      "learning_rate": 4.232568926370419e-05,
      "loss": 3.0039,
      "step": 237700
    },
    {
      "epoch": 77.08265802269044,
      "grad_norm": 0.9090269804000854,
      "learning_rate": 4.232244566980214e-05,
      "loss": 2.9927,
      "step": 237800
    },
    {
      "epoch": 77.11507293354943,
      "grad_norm": 1.1171976327896118,
      "learning_rate": 4.23192020759001e-05,
      "loss": 2.9949,
      "step": 237900
    },
    {
      "epoch": 77.14748784440843,
      "grad_norm": 0.9307818412780762,
      "learning_rate": 4.231595848199806e-05,
      "loss": 2.9891,
      "step": 238000
    },
    {
      "epoch": 77.17990275526742,
      "grad_norm": 1.127889633178711,
      "learning_rate": 4.231271488809601e-05,
      "loss": 2.9897,
      "step": 238100
    },
    {
      "epoch": 77.21231766612642,
      "grad_norm": 0.9421783685684204,
      "learning_rate": 4.230947129419397e-05,
      "loss": 3.0033,
      "step": 238200
    },
    {
      "epoch": 77.24473257698541,
      "grad_norm": 1.0958640575408936,
      "learning_rate": 4.230622770029192e-05,
      "loss": 2.9873,
      "step": 238300
    },
    {
      "epoch": 77.2771474878444,
      "grad_norm": 0.9370099306106567,
      "learning_rate": 4.230298410638988e-05,
      "loss": 2.9863,
      "step": 238400
    },
    {
      "epoch": 77.3095623987034,
      "grad_norm": 1.0054668188095093,
      "learning_rate": 4.229974051248784e-05,
      "loss": 2.9838,
      "step": 238500
    },
    {
      "epoch": 77.3419773095624,
      "grad_norm": 1.0436930656433105,
      "learning_rate": 4.229649691858579e-05,
      "loss": 2.999,
      "step": 238600
    },
    {
      "epoch": 77.3743922204214,
      "grad_norm": 0.9957905411720276,
      "learning_rate": 4.229325332468375e-05,
      "loss": 2.9995,
      "step": 238700
    },
    {
      "epoch": 77.40680713128039,
      "grad_norm": 0.9681530594825745,
      "learning_rate": 4.229000973078171e-05,
      "loss": 3.0048,
      "step": 238800
    },
    {
      "epoch": 77.43922204213938,
      "grad_norm": 0.8996002674102783,
      "learning_rate": 4.228676613687966e-05,
      "loss": 2.994,
      "step": 238900
    },
    {
      "epoch": 77.47163695299838,
      "grad_norm": 0.9322336316108704,
      "learning_rate": 4.228352254297762e-05,
      "loss": 2.9956,
      "step": 239000
    },
    {
      "epoch": 77.50405186385737,
      "grad_norm": 1.0864554643630981,
      "learning_rate": 4.228027894907558e-05,
      "loss": 2.9872,
      "step": 239100
    },
    {
      "epoch": 77.53646677471637,
      "grad_norm": 0.9105396270751953,
      "learning_rate": 4.227703535517353e-05,
      "loss": 3.0048,
      "step": 239200
    },
    {
      "epoch": 77.56888168557536,
      "grad_norm": 0.9718434810638428,
      "learning_rate": 4.227379176127149e-05,
      "loss": 3.0041,
      "step": 239300
    },
    {
      "epoch": 77.60129659643437,
      "grad_norm": 1.0722901821136475,
      "learning_rate": 4.227054816736945e-05,
      "loss": 3.0033,
      "step": 239400
    },
    {
      "epoch": 77.63371150729336,
      "grad_norm": 0.9034110903739929,
      "learning_rate": 4.2267304573467406e-05,
      "loss": 3.0146,
      "step": 239500
    },
    {
      "epoch": 77.66612641815234,
      "grad_norm": 0.9494526982307434,
      "learning_rate": 4.2264060979565365e-05,
      "loss": 3.0038,
      "step": 239600
    },
    {
      "epoch": 77.69854132901135,
      "grad_norm": 0.9205394983291626,
      "learning_rate": 4.226081738566332e-05,
      "loss": 3.0064,
      "step": 239700
    },
    {
      "epoch": 77.73095623987034,
      "grad_norm": 0.8740229606628418,
      "learning_rate": 4.2257573791761276e-05,
      "loss": 3.0011,
      "step": 239800
    },
    {
      "epoch": 77.76337115072934,
      "grad_norm": 1.0882816314697266,
      "learning_rate": 4.2254330197859234e-05,
      "loss": 2.9997,
      "step": 239900
    },
    {
      "epoch": 77.79578606158833,
      "grad_norm": 1.0719482898712158,
      "learning_rate": 4.2251086603957186e-05,
      "loss": 3.0021,
      "step": 240000
    },
    {
      "epoch": 77.82820097244732,
      "grad_norm": 0.9387555122375488,
      "learning_rate": 4.2247843010055145e-05,
      "loss": 3.0236,
      "step": 240100
    },
    {
      "epoch": 77.86061588330632,
      "grad_norm": 0.9850307703018188,
      "learning_rate": 4.2244599416153104e-05,
      "loss": 2.994,
      "step": 240200
    },
    {
      "epoch": 77.89303079416531,
      "grad_norm": 0.9032101035118103,
      "learning_rate": 4.2241355822251056e-05,
      "loss": 2.985,
      "step": 240300
    },
    {
      "epoch": 77.92544570502432,
      "grad_norm": 0.9962813258171082,
      "learning_rate": 4.2238112228349014e-05,
      "loss": 2.9927,
      "step": 240400
    },
    {
      "epoch": 77.9578606158833,
      "grad_norm": 0.9173154830932617,
      "learning_rate": 4.2234868634446966e-05,
      "loss": 2.9967,
      "step": 240500
    },
    {
      "epoch": 77.9902755267423,
      "grad_norm": 0.9339150190353394,
      "learning_rate": 4.2231625040544925e-05,
      "loss": 2.9965,
      "step": 240600
    },
    {
      "epoch": 78.0,
      "eval_bleu": 1.3074630469917226,
      "eval_loss": 3.805577278137207,
      "eval_runtime": 4.7128,
      "eval_samples_per_second": 104.396,
      "eval_steps_per_second": 1.697,
      "step": 240630
    },
    {
      "epoch": 78.0226904376013,
      "grad_norm": 1.0139206647872925,
      "learning_rate": 4.2228381446642884e-05,
      "loss": 2.9745,
      "step": 240700
    },
    {
      "epoch": 78.05510534846029,
      "grad_norm": 1.0710432529449463,
      "learning_rate": 4.2225137852740836e-05,
      "loss": 2.9717,
      "step": 240800
    },
    {
      "epoch": 78.08752025931929,
      "grad_norm": 1.127888798713684,
      "learning_rate": 4.2221894258838795e-05,
      "loss": 2.9855,
      "step": 240900
    },
    {
      "epoch": 78.11993517017828,
      "grad_norm": 1.0247498750686646,
      "learning_rate": 4.221865066493675e-05,
      "loss": 2.9827,
      "step": 241000
    },
    {
      "epoch": 78.15235008103728,
      "grad_norm": 1.3383569717407227,
      "learning_rate": 4.2215407071034705e-05,
      "loss": 2.9913,
      "step": 241100
    },
    {
      "epoch": 78.18476499189627,
      "grad_norm": 1.1304913759231567,
      "learning_rate": 4.2212163477132664e-05,
      "loss": 2.992,
      "step": 241200
    },
    {
      "epoch": 78.21717990275526,
      "grad_norm": 0.9650110602378845,
      "learning_rate": 4.220891988323062e-05,
      "loss": 3.0025,
      "step": 241300
    },
    {
      "epoch": 78.24959481361427,
      "grad_norm": 1.07081139087677,
      "learning_rate": 4.2205676289328575e-05,
      "loss": 2.973,
      "step": 241400
    },
    {
      "epoch": 78.28200972447326,
      "grad_norm": 1.0440435409545898,
      "learning_rate": 4.2202432695426533e-05,
      "loss": 2.9961,
      "step": 241500
    },
    {
      "epoch": 78.31442463533226,
      "grad_norm": 0.9601117372512817,
      "learning_rate": 4.219918910152449e-05,
      "loss": 2.9859,
      "step": 241600
    },
    {
      "epoch": 78.34683954619125,
      "grad_norm": 1.0170058012008667,
      "learning_rate": 4.2195945507622444e-05,
      "loss": 2.9838,
      "step": 241700
    },
    {
      "epoch": 78.37925445705024,
      "grad_norm": 1.0493425130844116,
      "learning_rate": 4.21927019137204e-05,
      "loss": 2.9952,
      "step": 241800
    },
    {
      "epoch": 78.41166936790924,
      "grad_norm": 0.9884775876998901,
      "learning_rate": 4.218945831981836e-05,
      "loss": 2.9919,
      "step": 241900
    },
    {
      "epoch": 78.44408427876823,
      "grad_norm": 0.9454813599586487,
      "learning_rate": 4.218621472591632e-05,
      "loss": 2.9915,
      "step": 242000
    },
    {
      "epoch": 78.47649918962723,
      "grad_norm": 1.1734586954116821,
      "learning_rate": 4.218297113201428e-05,
      "loss": 2.9939,
      "step": 242100
    },
    {
      "epoch": 78.50891410048622,
      "grad_norm": 1.0445871353149414,
      "learning_rate": 4.217972753811223e-05,
      "loss": 3.0104,
      "step": 242200
    },
    {
      "epoch": 78.54132901134521,
      "grad_norm": 0.8376871943473816,
      "learning_rate": 4.217648394421019e-05,
      "loss": 3.0138,
      "step": 242300
    },
    {
      "epoch": 78.57374392220422,
      "grad_norm": 0.9801080226898193,
      "learning_rate": 4.217324035030814e-05,
      "loss": 3.002,
      "step": 242400
    },
    {
      "epoch": 78.6061588330632,
      "grad_norm": 0.9383784532546997,
      "learning_rate": 4.217002919234512e-05,
      "loss": 3.0166,
      "step": 242500
    },
    {
      "epoch": 78.63857374392221,
      "grad_norm": 1.1309434175491333,
      "learning_rate": 4.216678559844308e-05,
      "loss": 2.9975,
      "step": 242600
    },
    {
      "epoch": 78.6709886547812,
      "grad_norm": 1.1967657804489136,
      "learning_rate": 4.216354200454104e-05,
      "loss": 3.0127,
      "step": 242700
    },
    {
      "epoch": 78.7034035656402,
      "grad_norm": 1.1365493535995483,
      "learning_rate": 4.216029841063899e-05,
      "loss": 2.9958,
      "step": 242800
    },
    {
      "epoch": 78.73581847649919,
      "grad_norm": 1.00609290599823,
      "learning_rate": 4.215705481673695e-05,
      "loss": 2.9853,
      "step": 242900
    },
    {
      "epoch": 78.76823338735818,
      "grad_norm": 1.07161545753479,
      "learning_rate": 4.2153811222834907e-05,
      "loss": 2.9889,
      "step": 243000
    },
    {
      "epoch": 78.80064829821718,
      "grad_norm": 1.014479160308838,
      "learning_rate": 4.215056762893286e-05,
      "loss": 2.9945,
      "step": 243100
    },
    {
      "epoch": 78.83306320907617,
      "grad_norm": 0.9551103711128235,
      "learning_rate": 4.214732403503082e-05,
      "loss": 3.0181,
      "step": 243200
    },
    {
      "epoch": 78.86547811993518,
      "grad_norm": 1.5022351741790771,
      "learning_rate": 4.2144080441128776e-05,
      "loss": 2.9904,
      "step": 243300
    },
    {
      "epoch": 78.89789303079417,
      "grad_norm": 1.035441517829895,
      "learning_rate": 4.214083684722673e-05,
      "loss": 3.0231,
      "step": 243400
    },
    {
      "epoch": 78.93030794165315,
      "grad_norm": 1.1410961151123047,
      "learning_rate": 4.213759325332469e-05,
      "loss": 3.022,
      "step": 243500
    },
    {
      "epoch": 78.96272285251216,
      "grad_norm": 0.9812159538269043,
      "learning_rate": 4.2134349659422645e-05,
      "loss": 2.9906,
      "step": 243600
    },
    {
      "epoch": 78.99513776337115,
      "grad_norm": 0.9964342713356018,
      "learning_rate": 4.21311060655206e-05,
      "loss": 2.9927,
      "step": 243700
    },
    {
      "epoch": 79.0,
      "eval_bleu": 1.188713651540546,
      "eval_loss": 3.802879810333252,
      "eval_runtime": 4.2695,
      "eval_samples_per_second": 115.235,
      "eval_steps_per_second": 1.874,
      "step": 243715
    },
    {
      "epoch": 79.02755267423015,
      "grad_norm": 1.0627212524414062,
      "learning_rate": 4.2127862471618556e-05,
      "loss": 2.9757,
      "step": 243800
    },
    {
      "epoch": 79.05996758508914,
      "grad_norm": 1.0484381914138794,
      "learning_rate": 4.212461887771651e-05,
      "loss": 2.9971,
      "step": 243900
    },
    {
      "epoch": 79.09238249594813,
      "grad_norm": 1.1147669553756714,
      "learning_rate": 4.212137528381447e-05,
      "loss": 2.9848,
      "step": 244000
    },
    {
      "epoch": 79.12479740680713,
      "grad_norm": 1.0276782512664795,
      "learning_rate": 4.2118131689912426e-05,
      "loss": 2.9751,
      "step": 244100
    },
    {
      "epoch": 79.15721231766612,
      "grad_norm": 0.8545264601707458,
      "learning_rate": 4.211488809601038e-05,
      "loss": 3.0048,
      "step": 244200
    },
    {
      "epoch": 79.18962722852513,
      "grad_norm": 1.2353538274765015,
      "learning_rate": 4.2111644502108336e-05,
      "loss": 2.9969,
      "step": 244300
    },
    {
      "epoch": 79.22204213938411,
      "grad_norm": 1.1516671180725098,
      "learning_rate": 4.2108400908206295e-05,
      "loss": 2.9663,
      "step": 244400
    },
    {
      "epoch": 79.25445705024312,
      "grad_norm": 1.2498854398727417,
      "learning_rate": 4.210515731430425e-05,
      "loss": 2.9883,
      "step": 244500
    },
    {
      "epoch": 79.28687196110211,
      "grad_norm": 0.953555166721344,
      "learning_rate": 4.2101913720402206e-05,
      "loss": 2.982,
      "step": 244600
    },
    {
      "epoch": 79.3192868719611,
      "grad_norm": 0.9437752366065979,
      "learning_rate": 4.2098702562439184e-05,
      "loss": 2.9664,
      "step": 244700
    },
    {
      "epoch": 79.3517017828201,
      "grad_norm": 0.9116535782814026,
      "learning_rate": 4.209545896853714e-05,
      "loss": 2.9898,
      "step": 244800
    },
    {
      "epoch": 79.38411669367909,
      "grad_norm": 0.969352126121521,
      "learning_rate": 4.2092215374635094e-05,
      "loss": 2.9872,
      "step": 244900
    },
    {
      "epoch": 79.4165316045381,
      "grad_norm": 0.9580119252204895,
      "learning_rate": 4.208897178073305e-05,
      "loss": 2.9828,
      "step": 245000
    },
    {
      "epoch": 79.44894651539708,
      "grad_norm": 0.9817381501197815,
      "learning_rate": 4.2085728186831005e-05,
      "loss": 3.0071,
      "step": 245100
    },
    {
      "epoch": 79.48136142625607,
      "grad_norm": 0.8978882431983948,
      "learning_rate": 4.2082484592928964e-05,
      "loss": 2.9936,
      "step": 245200
    },
    {
      "epoch": 79.51377633711508,
      "grad_norm": 1.344700574874878,
      "learning_rate": 4.207924099902692e-05,
      "loss": 2.9868,
      "step": 245300
    },
    {
      "epoch": 79.54619124797406,
      "grad_norm": 1.0571684837341309,
      "learning_rate": 4.207599740512488e-05,
      "loss": 3.0048,
      "step": 245400
    },
    {
      "epoch": 79.57860615883307,
      "grad_norm": 0.86741703748703,
      "learning_rate": 4.207275381122284e-05,
      "loss": 3.0016,
      "step": 245500
    },
    {
      "epoch": 79.61102106969206,
      "grad_norm": 0.9551133513450623,
      "learning_rate": 4.20695102173208e-05,
      "loss": 3.0007,
      "step": 245600
    },
    {
      "epoch": 79.64343598055105,
      "grad_norm": 1.0571993589401245,
      "learning_rate": 4.206626662341875e-05,
      "loss": 3.0018,
      "step": 245700
    },
    {
      "epoch": 79.67585089141005,
      "grad_norm": 0.9514489769935608,
      "learning_rate": 4.206302302951671e-05,
      "loss": 2.9947,
      "step": 245800
    },
    {
      "epoch": 79.70826580226904,
      "grad_norm": 0.9495440125465393,
      "learning_rate": 4.205977943561467e-05,
      "loss": 2.9918,
      "step": 245900
    },
    {
      "epoch": 79.74068071312804,
      "grad_norm": 1.0559308528900146,
      "learning_rate": 4.205653584171262e-05,
      "loss": 2.983,
      "step": 246000
    },
    {
      "epoch": 79.77309562398703,
      "grad_norm": 1.0481146574020386,
      "learning_rate": 4.205329224781058e-05,
      "loss": 2.9991,
      "step": 246100
    },
    {
      "epoch": 79.80551053484604,
      "grad_norm": 0.8659568428993225,
      "learning_rate": 4.205004865390853e-05,
      "loss": 3.0264,
      "step": 246200
    },
    {
      "epoch": 79.83792544570503,
      "grad_norm": 1.1229313611984253,
      "learning_rate": 4.204680506000649e-05,
      "loss": 3.0037,
      "step": 246300
    },
    {
      "epoch": 79.87034035656401,
      "grad_norm": 1.0346320867538452,
      "learning_rate": 4.204356146610445e-05,
      "loss": 3.0012,
      "step": 246400
    },
    {
      "epoch": 79.90275526742302,
      "grad_norm": 1.0432714223861694,
      "learning_rate": 4.20403178722024e-05,
      "loss": 2.975,
      "step": 246500
    },
    {
      "epoch": 79.93517017828201,
      "grad_norm": 1.397632122039795,
      "learning_rate": 4.203707427830036e-05,
      "loss": 2.9959,
      "step": 246600
    },
    {
      "epoch": 79.96758508914101,
      "grad_norm": 0.9996395111083984,
      "learning_rate": 4.203383068439832e-05,
      "loss": 2.9773,
      "step": 246700
    },
    {
      "epoch": 80.0,
      "grad_norm": 0.9635169506072998,
      "learning_rate": 4.203058709049627e-05,
      "loss": 2.9778,
      "step": 246800
    },
    {
      "epoch": 80.0,
      "eval_bleu": 1.4050730653141843,
      "eval_loss": 3.806168556213379,
      "eval_runtime": 4.382,
      "eval_samples_per_second": 112.277,
      "eval_steps_per_second": 1.826,
      "step": 246800
    },
    {
      "epoch": 80.03241491085899,
      "grad_norm": 0.8895384669303894,
      "learning_rate": 4.202737593253325e-05,
      "loss": 2.9649,
      "step": 246900
    },
    {
      "epoch": 80.06482982171799,
      "grad_norm": 0.9960419535636902,
      "learning_rate": 4.2024132338631206e-05,
      "loss": 2.9727,
      "step": 247000
    },
    {
      "epoch": 80.09724473257698,
      "grad_norm": 1.1086565256118774,
      "learning_rate": 4.2020888744729165e-05,
      "loss": 2.9861,
      "step": 247100
    },
    {
      "epoch": 80.12965964343599,
      "grad_norm": 0.9699922800064087,
      "learning_rate": 4.201764515082712e-05,
      "loss": 2.9996,
      "step": 247200
    },
    {
      "epoch": 80.16207455429497,
      "grad_norm": 0.9959181547164917,
      "learning_rate": 4.2014401556925076e-05,
      "loss": 2.9972,
      "step": 247300
    },
    {
      "epoch": 80.19448946515396,
      "grad_norm": 0.9947307705879211,
      "learning_rate": 4.201115796302303e-05,
      "loss": 2.9965,
      "step": 247400
    },
    {
      "epoch": 80.22690437601297,
      "grad_norm": 1.0834238529205322,
      "learning_rate": 4.2007914369120986e-05,
      "loss": 2.9829,
      "step": 247500
    },
    {
      "epoch": 80.25931928687196,
      "grad_norm": 1.1903133392333984,
      "learning_rate": 4.2004670775218945e-05,
      "loss": 2.992,
      "step": 247600
    },
    {
      "epoch": 80.29173419773096,
      "grad_norm": 1.109747052192688,
      "learning_rate": 4.20014271813169e-05,
      "loss": 2.9871,
      "step": 247700
    },
    {
      "epoch": 80.32414910858995,
      "grad_norm": 1.0299855470657349,
      "learning_rate": 4.1998183587414856e-05,
      "loss": 3.0016,
      "step": 247800
    },
    {
      "epoch": 80.35656401944895,
      "grad_norm": 0.9082565903663635,
      "learning_rate": 4.1994939993512815e-05,
      "loss": 2.9984,
      "step": 247900
    },
    {
      "epoch": 80.38897893030794,
      "grad_norm": 1.022657036781311,
      "learning_rate": 4.199169639961077e-05,
      "loss": 2.9876,
      "step": 248000
    },
    {
      "epoch": 80.42139384116693,
      "grad_norm": 0.9786322116851807,
      "learning_rate": 4.1988452805708725e-05,
      "loss": 2.9702,
      "step": 248100
    },
    {
      "epoch": 80.45380875202594,
      "grad_norm": 1.0659148693084717,
      "learning_rate": 4.1985209211806684e-05,
      "loss": 3.0124,
      "step": 248200
    },
    {
      "epoch": 80.48622366288492,
      "grad_norm": 1.103469967842102,
      "learning_rate": 4.1981965617904636e-05,
      "loss": 2.9894,
      "step": 248300
    },
    {
      "epoch": 80.51863857374393,
      "grad_norm": 1.0364081859588623,
      "learning_rate": 4.1978754459941614e-05,
      "loss": 2.9748,
      "step": 248400
    },
    {
      "epoch": 80.55105348460292,
      "grad_norm": 0.8697847723960876,
      "learning_rate": 4.197551086603957e-05,
      "loss": 2.9818,
      "step": 248500
    },
    {
      "epoch": 80.5834683954619,
      "grad_norm": 1.0838974714279175,
      "learning_rate": 4.1972267272137525e-05,
      "loss": 2.9735,
      "step": 248600
    },
    {
      "epoch": 80.61588330632091,
      "grad_norm": 0.9181150794029236,
      "learning_rate": 4.196902367823548e-05,
      "loss": 2.9819,
      "step": 248700
    },
    {
      "epoch": 80.6482982171799,
      "grad_norm": 0.9539294838905334,
      "learning_rate": 4.196578008433344e-05,
      "loss": 3.0018,
      "step": 248800
    },
    {
      "epoch": 80.6807131280389,
      "grad_norm": 0.8800589442253113,
      "learning_rate": 4.19625364904314e-05,
      "loss": 2.9959,
      "step": 248900
    },
    {
      "epoch": 80.71312803889789,
      "grad_norm": 0.9425434470176697,
      "learning_rate": 4.195929289652936e-05,
      "loss": 2.9891,
      "step": 249000
    },
    {
      "epoch": 80.74554294975688,
      "grad_norm": 1.076551914215088,
      "learning_rate": 4.195604930262731e-05,
      "loss": 2.9938,
      "step": 249100
    },
    {
      "epoch": 80.77795786061589,
      "grad_norm": 1.116684913635254,
      "learning_rate": 4.195280570872527e-05,
      "loss": 2.9626,
      "step": 249200
    },
    {
      "epoch": 80.81037277147487,
      "grad_norm": 1.01451575756073,
      "learning_rate": 4.194956211482323e-05,
      "loss": 2.9782,
      "step": 249300
    },
    {
      "epoch": 80.84278768233388,
      "grad_norm": 0.9300918579101562,
      "learning_rate": 4.194631852092119e-05,
      "loss": 2.9881,
      "step": 249400
    },
    {
      "epoch": 80.87520259319287,
      "grad_norm": 1.0729097127914429,
      "learning_rate": 4.194307492701914e-05,
      "loss": 3.0009,
      "step": 249500
    },
    {
      "epoch": 80.90761750405187,
      "grad_norm": 1.0604561567306519,
      "learning_rate": 4.19398313331171e-05,
      "loss": 3.0033,
      "step": 249600
    },
    {
      "epoch": 80.94003241491086,
      "grad_norm": 1.1381124258041382,
      "learning_rate": 4.193658773921505e-05,
      "loss": 2.9862,
      "step": 249700
    },
    {
      "epoch": 80.97244732576985,
      "grad_norm": 0.9868893623352051,
      "learning_rate": 4.193334414531301e-05,
      "loss": 2.9963,
      "step": 249800
    },
    {
      "epoch": 81.0,
      "eval_bleu": 1.2598138305632294,
      "eval_loss": 3.8109984397888184,
      "eval_runtime": 4.0166,
      "eval_samples_per_second": 122.492,
      "eval_steps_per_second": 1.992,
      "step": 249885
    },
    {
      "epoch": 81.00486223662885,
      "grad_norm": 1.0068159103393555,
      "learning_rate": 4.193010055141097e-05,
      "loss": 2.9977,
      "step": 249900
    },
    {
      "epoch": 81.03727714748784,
      "grad_norm": 0.9283863306045532,
      "learning_rate": 4.192685695750892e-05,
      "loss": 2.9976,
      "step": 250000
    },
    {
      "epoch": 81.06969205834685,
      "grad_norm": 1.0723085403442383,
      "learning_rate": 4.192361336360688e-05,
      "loss": 3.0065,
      "step": 250100
    },
    {
      "epoch": 81.10210696920583,
      "grad_norm": 1.2717705965042114,
      "learning_rate": 4.192036976970484e-05,
      "loss": 2.9849,
      "step": 250200
    },
    {
      "epoch": 81.13452188006482,
      "grad_norm": 1.0606482028961182,
      "learning_rate": 4.191712617580279e-05,
      "loss": 2.9693,
      "step": 250300
    },
    {
      "epoch": 81.16693679092383,
      "grad_norm": 1.1507025957107544,
      "learning_rate": 4.191388258190075e-05,
      "loss": 2.971,
      "step": 250400
    },
    {
      "epoch": 81.19935170178282,
      "grad_norm": 1.1306298971176147,
      "learning_rate": 4.191063898799871e-05,
      "loss": 2.9708,
      "step": 250500
    },
    {
      "epoch": 81.23176661264182,
      "grad_norm": 1.0318297147750854,
      "learning_rate": 4.190739539409666e-05,
      "loss": 2.9875,
      "step": 250600
    },
    {
      "epoch": 81.26418152350081,
      "grad_norm": 0.9593826532363892,
      "learning_rate": 4.190415180019462e-05,
      "loss": 2.9953,
      "step": 250700
    },
    {
      "epoch": 81.2965964343598,
      "grad_norm": 0.9433554410934448,
      "learning_rate": 4.190090820629257e-05,
      "loss": 2.9739,
      "step": 250800
    },
    {
      "epoch": 81.3290113452188,
      "grad_norm": 0.9198437929153442,
      "learning_rate": 4.189766461239053e-05,
      "loss": 2.9755,
      "step": 250900
    },
    {
      "epoch": 81.36142625607779,
      "grad_norm": 0.9698473215103149,
      "learning_rate": 4.189442101848849e-05,
      "loss": 2.9975,
      "step": 251000
    },
    {
      "epoch": 81.3938411669368,
      "grad_norm": 1.0549365282058716,
      "learning_rate": 4.189117742458644e-05,
      "loss": 2.9715,
      "step": 251100
    },
    {
      "epoch": 81.42625607779578,
      "grad_norm": 0.9934037923812866,
      "learning_rate": 4.18879338306844e-05,
      "loss": 2.9872,
      "step": 251200
    },
    {
      "epoch": 81.45867098865479,
      "grad_norm": 0.9460983872413635,
      "learning_rate": 4.1884690236782356e-05,
      "loss": 2.9903,
      "step": 251300
    },
    {
      "epoch": 81.49108589951378,
      "grad_norm": 1.0038453340530396,
      "learning_rate": 4.1881446642880315e-05,
      "loss": 2.9792,
      "step": 251400
    },
    {
      "epoch": 81.52350081037277,
      "grad_norm": 1.0807480812072754,
      "learning_rate": 4.1878203048978274e-05,
      "loss": 2.985,
      "step": 251500
    },
    {
      "epoch": 81.55591572123177,
      "grad_norm": 1.0664960145950317,
      "learning_rate": 4.1874959455076226e-05,
      "loss": 2.9986,
      "step": 251600
    },
    {
      "epoch": 81.58833063209076,
      "grad_norm": 0.916858971118927,
      "learning_rate": 4.1871715861174185e-05,
      "loss": 2.9772,
      "step": 251700
    },
    {
      "epoch": 81.62074554294976,
      "grad_norm": 0.9281333088874817,
      "learning_rate": 4.186847226727214e-05,
      "loss": 2.9791,
      "step": 251800
    },
    {
      "epoch": 81.65316045380875,
      "grad_norm": 0.96286541223526,
      "learning_rate": 4.1865228673370095e-05,
      "loss": 2.9883,
      "step": 251900
    },
    {
      "epoch": 81.68557536466774,
      "grad_norm": 0.9585593342781067,
      "learning_rate": 4.1861985079468054e-05,
      "loss": 2.9829,
      "step": 252000
    },
    {
      "epoch": 81.71799027552674,
      "grad_norm": 1.114574909210205,
      "learning_rate": 4.185874148556601e-05,
      "loss": 2.9743,
      "step": 252100
    },
    {
      "epoch": 81.75040518638573,
      "grad_norm": 0.848314642906189,
      "learning_rate": 4.1855497891663965e-05,
      "loss": 2.9895,
      "step": 252200
    },
    {
      "epoch": 81.78282009724474,
      "grad_norm": 1.044311285018921,
      "learning_rate": 4.1852254297761924e-05,
      "loss": 2.9785,
      "step": 252300
    },
    {
      "epoch": 81.81523500810373,
      "grad_norm": 0.9668982028961182,
      "learning_rate": 4.184901070385988e-05,
      "loss": 2.9882,
      "step": 252400
    },
    {
      "epoch": 81.84764991896272,
      "grad_norm": 0.9813168048858643,
      "learning_rate": 4.1845767109957834e-05,
      "loss": 2.9965,
      "step": 252500
    },
    {
      "epoch": 81.88006482982172,
      "grad_norm": 0.9928798675537109,
      "learning_rate": 4.184252351605579e-05,
      "loss": 2.9861,
      "step": 252600
    },
    {
      "epoch": 81.91247974068071,
      "grad_norm": 0.9177950620651245,
      "learning_rate": 4.1839279922153745e-05,
      "loss": 2.9917,
      "step": 252700
    },
    {
      "epoch": 81.94489465153971,
      "grad_norm": 0.9881356954574585,
      "learning_rate": 4.1836036328251704e-05,
      "loss": 2.9848,
      "step": 252800
    },
    {
      "epoch": 81.9773095623987,
      "grad_norm": 1.0295746326446533,
      "learning_rate": 4.183279273434966e-05,
      "loss": 2.9903,
      "step": 252900
    },
    {
      "epoch": 82.0,
      "eval_bleu": 1.1566804834556195,
      "eval_loss": 3.811169385910034,
      "eval_runtime": 4.3278,
      "eval_samples_per_second": 113.683,
      "eval_steps_per_second": 1.849,
      "step": 252970
    },
    {
      "epoch": 82.0097244732577,
      "grad_norm": 0.9879481792449951,
      "learning_rate": 4.1829549140447614e-05,
      "loss": 2.9876,
      "step": 253000
    },
    {
      "epoch": 82.0421393841167,
      "grad_norm": 1.064342737197876,
      "learning_rate": 4.182630554654557e-05,
      "loss": 2.9716,
      "step": 253100
    },
    {
      "epoch": 82.07455429497568,
      "grad_norm": 0.9271100759506226,
      "learning_rate": 4.182306195264353e-05,
      "loss": 2.9705,
      "step": 253200
    },
    {
      "epoch": 82.10696920583469,
      "grad_norm": 0.9503302574157715,
      "learning_rate": 4.1819818358741484e-05,
      "loss": 2.9726,
      "step": 253300
    },
    {
      "epoch": 82.13938411669368,
      "grad_norm": 0.9459041953086853,
      "learning_rate": 4.181657476483944e-05,
      "loss": 2.9774,
      "step": 253400
    },
    {
      "epoch": 82.17179902755268,
      "grad_norm": 0.9959860444068909,
      "learning_rate": 4.18133311709374e-05,
      "loss": 2.9857,
      "step": 253500
    },
    {
      "epoch": 82.20421393841167,
      "grad_norm": 1.0283501148223877,
      "learning_rate": 4.181008757703535e-05,
      "loss": 2.9877,
      "step": 253600
    },
    {
      "epoch": 82.23662884927066,
      "grad_norm": 1.0340814590454102,
      "learning_rate": 4.180684398313331e-05,
      "loss": 2.985,
      "step": 253700
    },
    {
      "epoch": 82.26904376012966,
      "grad_norm": 0.9975585341453552,
      "learning_rate": 4.180360038923127e-05,
      "loss": 2.9902,
      "step": 253800
    },
    {
      "epoch": 82.30145867098865,
      "grad_norm": 0.9813876748085022,
      "learning_rate": 4.180035679532923e-05,
      "loss": 2.9767,
      "step": 253900
    },
    {
      "epoch": 82.33387358184766,
      "grad_norm": 0.9811215996742249,
      "learning_rate": 4.179711320142718e-05,
      "loss": 2.985,
      "step": 254000
    },
    {
      "epoch": 82.36628849270664,
      "grad_norm": 1.6223260164260864,
      "learning_rate": 4.179386960752514e-05,
      "loss": 3.0015,
      "step": 254100
    },
    {
      "epoch": 82.39870340356563,
      "grad_norm": 1.1186455488204956,
      "learning_rate": 4.17906260136231e-05,
      "loss": 2.9812,
      "step": 254200
    },
    {
      "epoch": 82.43111831442464,
      "grad_norm": 0.9209352731704712,
      "learning_rate": 4.178738241972106e-05,
      "loss": 2.9758,
      "step": 254300
    },
    {
      "epoch": 82.46353322528363,
      "grad_norm": 1.1473133563995361,
      "learning_rate": 4.178417126175803e-05,
      "loss": 2.9906,
      "step": 254400
    },
    {
      "epoch": 82.49594813614263,
      "grad_norm": 0.8613999485969543,
      "learning_rate": 4.178092766785599e-05,
      "loss": 2.9666,
      "step": 254500
    },
    {
      "epoch": 82.52836304700162,
      "grad_norm": 0.9987491965293884,
      "learning_rate": 4.1777684073953946e-05,
      "loss": 2.9793,
      "step": 254600
    },
    {
      "epoch": 82.56077795786062,
      "grad_norm": 0.9178141951560974,
      "learning_rate": 4.1774440480051905e-05,
      "loss": 2.9833,
      "step": 254700
    },
    {
      "epoch": 82.59319286871961,
      "grad_norm": 1.0378092527389526,
      "learning_rate": 4.177119688614986e-05,
      "loss": 2.9806,
      "step": 254800
    },
    {
      "epoch": 82.6256077795786,
      "grad_norm": 0.9509238600730896,
      "learning_rate": 4.1767953292247816e-05,
      "loss": 2.9574,
      "step": 254900
    },
    {
      "epoch": 82.6580226904376,
      "grad_norm": 0.937838613986969,
      "learning_rate": 4.176470969834577e-05,
      "loss": 2.9979,
      "step": 255000
    },
    {
      "epoch": 82.6904376012966,
      "grad_norm": 0.953041136264801,
      "learning_rate": 4.1761466104443726e-05,
      "loss": 2.9791,
      "step": 255100
    },
    {
      "epoch": 82.7228525121556,
      "grad_norm": 1.0034432411193848,
      "learning_rate": 4.1758222510541685e-05,
      "loss": 2.9921,
      "step": 255200
    },
    {
      "epoch": 82.75526742301459,
      "grad_norm": 0.9033731818199158,
      "learning_rate": 4.175497891663964e-05,
      "loss": 2.9818,
      "step": 255300
    },
    {
      "epoch": 82.78768233387358,
      "grad_norm": 0.8384546637535095,
      "learning_rate": 4.1751735322737596e-05,
      "loss": 2.9644,
      "step": 255400
    },
    {
      "epoch": 82.82009724473258,
      "grad_norm": 0.9998888373374939,
      "learning_rate": 4.1748491728835555e-05,
      "loss": 2.9981,
      "step": 255500
    },
    {
      "epoch": 82.85251215559157,
      "grad_norm": 0.9485061168670654,
      "learning_rate": 4.1745248134933507e-05,
      "loss": 2.9672,
      "step": 255600
    },
    {
      "epoch": 82.88492706645057,
      "grad_norm": 1.1147005558013916,
      "learning_rate": 4.1742004541031465e-05,
      "loss": 3.004,
      "step": 255700
    },
    {
      "epoch": 82.91734197730956,
      "grad_norm": 0.9917923212051392,
      "learning_rate": 4.1738760947129424e-05,
      "loss": 2.9789,
      "step": 255800
    },
    {
      "epoch": 82.94975688816855,
      "grad_norm": 0.9929403066635132,
      "learning_rate": 4.1735517353227376e-05,
      "loss": 2.9754,
      "step": 255900
    },
    {
      "epoch": 82.98217179902755,
      "grad_norm": 1.1596941947937012,
      "learning_rate": 4.1732273759325335e-05,
      "loss": 3.016,
      "step": 256000
    },
    {
      "epoch": 83.0,
      "eval_bleu": 1.118267858639402,
      "eval_loss": 3.813721179962158,
      "eval_runtime": 4.2217,
      "eval_samples_per_second": 116.541,
      "eval_steps_per_second": 1.895,
      "step": 256055
    },
    {
      "epoch": 83.01458670988654,
      "grad_norm": 1.0258724689483643,
      "learning_rate": 4.172903016542329e-05,
      "loss": 2.9758,
      "step": 256100
    },
    {
      "epoch": 83.04700162074555,
      "grad_norm": 1.0399081707000732,
      "learning_rate": 4.1725786571521245e-05,
      "loss": 2.9723,
      "step": 256200
    },
    {
      "epoch": 83.07941653160454,
      "grad_norm": 1.0580991506576538,
      "learning_rate": 4.1722542977619204e-05,
      "loss": 2.9769,
      "step": 256300
    },
    {
      "epoch": 83.11183144246354,
      "grad_norm": 1.038944959640503,
      "learning_rate": 4.1719299383717156e-05,
      "loss": 2.9761,
      "step": 256400
    },
    {
      "epoch": 83.14424635332253,
      "grad_norm": 1.1554628610610962,
      "learning_rate": 4.1716088225754134e-05,
      "loss": 2.9765,
      "step": 256500
    },
    {
      "epoch": 83.17666126418152,
      "grad_norm": 1.1553833484649658,
      "learning_rate": 4.171284463185209e-05,
      "loss": 2.9657,
      "step": 256600
    },
    {
      "epoch": 83.20907617504052,
      "grad_norm": 0.9630934596061707,
      "learning_rate": 4.170960103795005e-05,
      "loss": 2.9825,
      "step": 256700
    },
    {
      "epoch": 83.24149108589951,
      "grad_norm": 1.0178276300430298,
      "learning_rate": 4.1706357444048003e-05,
      "loss": 2.977,
      "step": 256800
    },
    {
      "epoch": 83.27390599675851,
      "grad_norm": 1.042263388633728,
      "learning_rate": 4.170311385014596e-05,
      "loss": 2.9732,
      "step": 256900
    },
    {
      "epoch": 83.3063209076175,
      "grad_norm": 1.1022577285766602,
      "learning_rate": 4.169990269218294e-05,
      "loss": 2.9756,
      "step": 257000
    },
    {
      "epoch": 83.3387358184765,
      "grad_norm": 1.0448030233383179,
      "learning_rate": 4.16966590982809e-05,
      "loss": 2.9757,
      "step": 257100
    },
    {
      "epoch": 83.3711507293355,
      "grad_norm": 0.9644070863723755,
      "learning_rate": 4.169341550437885e-05,
      "loss": 2.9845,
      "step": 257200
    },
    {
      "epoch": 83.40356564019449,
      "grad_norm": 0.9848801493644714,
      "learning_rate": 4.169017191047681e-05,
      "loss": 2.9891,
      "step": 257300
    },
    {
      "epoch": 83.43598055105349,
      "grad_norm": 0.9551212787628174,
      "learning_rate": 4.168692831657477e-05,
      "loss": 2.9894,
      "step": 257400
    },
    {
      "epoch": 83.46839546191248,
      "grad_norm": 1.122887134552002,
      "learning_rate": 4.168368472267272e-05,
      "loss": 2.9938,
      "step": 257500
    },
    {
      "epoch": 83.50081037277147,
      "grad_norm": 1.0083990097045898,
      "learning_rate": 4.168044112877068e-05,
      "loss": 2.9774,
      "step": 257600
    },
    {
      "epoch": 83.53322528363047,
      "grad_norm": 0.8541858196258545,
      "learning_rate": 4.167719753486863e-05,
      "loss": 2.9714,
      "step": 257700
    },
    {
      "epoch": 83.56564019448946,
      "grad_norm": 0.8976190686225891,
      "learning_rate": 4.167395394096659e-05,
      "loss": 2.9759,
      "step": 257800
    },
    {
      "epoch": 83.59805510534846,
      "grad_norm": 1.0201904773712158,
      "learning_rate": 4.167071034706455e-05,
      "loss": 2.9698,
      "step": 257900
    },
    {
      "epoch": 83.63047001620745,
      "grad_norm": 0.9301727414131165,
      "learning_rate": 4.166746675316251e-05,
      "loss": 2.9799,
      "step": 258000
    },
    {
      "epoch": 83.66288492706646,
      "grad_norm": 0.9937247037887573,
      "learning_rate": 4.1664223159260466e-05,
      "loss": 2.9675,
      "step": 258100
    },
    {
      "epoch": 83.69529983792545,
      "grad_norm": 1.1191191673278809,
      "learning_rate": 4.166097956535842e-05,
      "loss": 2.9561,
      "step": 258200
    },
    {
      "epoch": 83.72771474878444,
      "grad_norm": 1.0817153453826904,
      "learning_rate": 4.1657735971456377e-05,
      "loss": 2.9836,
      "step": 258300
    },
    {
      "epoch": 83.76012965964344,
      "grad_norm": 0.9666690230369568,
      "learning_rate": 4.1654492377554335e-05,
      "loss": 2.9788,
      "step": 258400
    },
    {
      "epoch": 83.79254457050243,
      "grad_norm": 0.8759384155273438,
      "learning_rate": 4.1651248783652294e-05,
      "loss": 2.9917,
      "step": 258500
    },
    {
      "epoch": 83.82495948136143,
      "grad_norm": 1.008711576461792,
      "learning_rate": 4.1648005189750246e-05,
      "loss": 2.9627,
      "step": 258600
    },
    {
      "epoch": 83.85737439222042,
      "grad_norm": 1.084304928779602,
      "learning_rate": 4.1644761595848205e-05,
      "loss": 2.9879,
      "step": 258700
    },
    {
      "epoch": 83.88978930307941,
      "grad_norm": 0.9074088335037231,
      "learning_rate": 4.164151800194616e-05,
      "loss": 2.9834,
      "step": 258800
    },
    {
      "epoch": 83.92220421393841,
      "grad_norm": 0.9945586919784546,
      "learning_rate": 4.1638274408044115e-05,
      "loss": 2.981,
      "step": 258900
    },
    {
      "epoch": 83.9546191247974,
      "grad_norm": 0.9701778888702393,
      "learning_rate": 4.1635030814142074e-05,
      "loss": 2.9775,
      "step": 259000
    },
    {
      "epoch": 83.98703403565641,
      "grad_norm": 1.0204746723175049,
      "learning_rate": 4.1631787220240026e-05,
      "loss": 2.979,
      "step": 259100
    },
    {
      "epoch": 84.0,
      "eval_bleu": 1.250088570729222,
      "eval_loss": 3.8099660873413086,
      "eval_runtime": 4.0807,
      "eval_samples_per_second": 120.567,
      "eval_steps_per_second": 1.96,
      "step": 259140
    },
    {
      "epoch": 84.0194489465154,
      "grad_norm": 1.113958477973938,
      "learning_rate": 4.1628543626337985e-05,
      "loss": 2.9822,
      "step": 259200
    },
    {
      "epoch": 84.05186385737439,
      "grad_norm": 0.8997271656990051,
      "learning_rate": 4.1625300032435944e-05,
      "loss": 2.9699,
      "step": 259300
    },
    {
      "epoch": 84.08427876823339,
      "grad_norm": 0.9103083610534668,
      "learning_rate": 4.1622056438533896e-05,
      "loss": 2.9739,
      "step": 259400
    },
    {
      "epoch": 84.11669367909238,
      "grad_norm": 1.0313191413879395,
      "learning_rate": 4.1618812844631854e-05,
      "loss": 2.9569,
      "step": 259500
    },
    {
      "epoch": 84.14910858995138,
      "grad_norm": 0.9457970261573792,
      "learning_rate": 4.161556925072981e-05,
      "loss": 2.9775,
      "step": 259600
    },
    {
      "epoch": 84.18152350081037,
      "grad_norm": 1.2163773775100708,
      "learning_rate": 4.1612325656827765e-05,
      "loss": 2.9509,
      "step": 259700
    },
    {
      "epoch": 84.21393841166937,
      "grad_norm": 1.0647107362747192,
      "learning_rate": 4.1609082062925724e-05,
      "loss": 2.9615,
      "step": 259800
    },
    {
      "epoch": 84.24635332252836,
      "grad_norm": 0.9050819277763367,
      "learning_rate": 4.1605838469023676e-05,
      "loss": 2.9551,
      "step": 259900
    },
    {
      "epoch": 84.27876823338735,
      "grad_norm": 1.102478265762329,
      "learning_rate": 4.1602594875121635e-05,
      "loss": 2.9799,
      "step": 260000
    },
    {
      "epoch": 84.31118314424636,
      "grad_norm": 1.107885718345642,
      "learning_rate": 4.159935128121959e-05,
      "loss": 2.9793,
      "step": 260100
    },
    {
      "epoch": 84.34359805510535,
      "grad_norm": 1.251924991607666,
      "learning_rate": 4.1596107687317545e-05,
      "loss": 2.979,
      "step": 260200
    },
    {
      "epoch": 84.37601296596435,
      "grad_norm": 1.2014001607894897,
      "learning_rate": 4.1592864093415504e-05,
      "loss": 2.9733,
      "step": 260300
    },
    {
      "epoch": 84.40842787682334,
      "grad_norm": 0.9736379384994507,
      "learning_rate": 4.158962049951346e-05,
      "loss": 2.9803,
      "step": 260400
    },
    {
      "epoch": 84.44084278768233,
      "grad_norm": 0.8803133368492126,
      "learning_rate": 4.158637690561142e-05,
      "loss": 2.9757,
      "step": 260500
    },
    {
      "epoch": 84.47325769854133,
      "grad_norm": 1.0631885528564453,
      "learning_rate": 4.158316574764839e-05,
      "loss": 2.9783,
      "step": 260600
    },
    {
      "epoch": 84.50567260940032,
      "grad_norm": 1.049189567565918,
      "learning_rate": 4.157992215374635e-05,
      "loss": 2.9744,
      "step": 260700
    },
    {
      "epoch": 84.53808752025932,
      "grad_norm": 1.0331273078918457,
      "learning_rate": 4.157667855984431e-05,
      "loss": 2.9535,
      "step": 260800
    },
    {
      "epoch": 84.57050243111831,
      "grad_norm": 1.1988850831985474,
      "learning_rate": 4.157343496594226e-05,
      "loss": 2.9622,
      "step": 260900
    },
    {
      "epoch": 84.6029173419773,
      "grad_norm": 1.001986026763916,
      "learning_rate": 4.157019137204022e-05,
      "loss": 2.9687,
      "step": 261000
    },
    {
      "epoch": 84.6353322528363,
      "grad_norm": 0.92537921667099,
      "learning_rate": 4.156694777813818e-05,
      "loss": 2.9926,
      "step": 261100
    },
    {
      "epoch": 84.6677471636953,
      "grad_norm": 0.910291850566864,
      "learning_rate": 4.156370418423614e-05,
      "loss": 2.987,
      "step": 261200
    },
    {
      "epoch": 84.7001620745543,
      "grad_norm": 1.1042176485061646,
      "learning_rate": 4.15604605903341e-05,
      "loss": 2.962,
      "step": 261300
    },
    {
      "epoch": 84.73257698541329,
      "grad_norm": 1.036926031112671,
      "learning_rate": 4.155721699643205e-05,
      "loss": 2.9749,
      "step": 261400
    },
    {
      "epoch": 84.76499189627229,
      "grad_norm": 1.0760200023651123,
      "learning_rate": 4.155397340253001e-05,
      "loss": 2.988,
      "step": 261500
    },
    {
      "epoch": 84.79740680713128,
      "grad_norm": 1.280165195465088,
      "learning_rate": 4.1550729808627966e-05,
      "loss": 2.9848,
      "step": 261600
    },
    {
      "epoch": 84.82982171799027,
      "grad_norm": 0.9750184416770935,
      "learning_rate": 4.154748621472592e-05,
      "loss": 2.9836,
      "step": 261700
    },
    {
      "epoch": 84.86223662884927,
      "grad_norm": 1.1075598001480103,
      "learning_rate": 4.154424262082388e-05,
      "loss": 2.9725,
      "step": 261800
    },
    {
      "epoch": 84.89465153970826,
      "grad_norm": 1.0074275732040405,
      "learning_rate": 4.1540999026921836e-05,
      "loss": 2.9834,
      "step": 261900
    },
    {
      "epoch": 84.92706645056727,
      "grad_norm": 0.9979269504547119,
      "learning_rate": 4.153775543301979e-05,
      "loss": 2.9992,
      "step": 262000
    },
    {
      "epoch": 84.95948136142626,
      "grad_norm": 0.9891306757926941,
      "learning_rate": 4.1534544275056766e-05,
      "loss": 2.9937,
      "step": 262100
    },
    {
      "epoch": 84.99189627228525,
      "grad_norm": 0.963388204574585,
      "learning_rate": 4.1531300681154724e-05,
      "loss": 2.98,
      "step": 262200
    },
    {
      "epoch": 85.0,
      "eval_bleu": 1.1991943625590198,
      "eval_loss": 3.822542905807495,
      "eval_runtime": 4.3608,
      "eval_samples_per_second": 112.822,
      "eval_steps_per_second": 1.835,
      "step": 262225
    },
    {
      "epoch": 85.02431118314425,
      "grad_norm": 0.8772531747817993,
      "learning_rate": 4.1528057087252676e-05,
      "loss": 2.9675,
      "step": 262300
    },
    {
      "epoch": 85.05672609400324,
      "grad_norm": 1.0904505252838135,
      "learning_rate": 4.1524813493350635e-05,
      "loss": 2.976,
      "step": 262400
    },
    {
      "epoch": 85.08914100486224,
      "grad_norm": 0.9286066889762878,
      "learning_rate": 4.1521569899448594e-05,
      "loss": 2.9578,
      "step": 262500
    },
    {
      "epoch": 85.12155591572123,
      "grad_norm": 1.029628872871399,
      "learning_rate": 4.1518326305546546e-05,
      "loss": 2.967,
      "step": 262600
    },
    {
      "epoch": 85.15397082658022,
      "grad_norm": 1.1629196405410767,
      "learning_rate": 4.1515082711644504e-05,
      "loss": 2.9787,
      "step": 262700
    },
    {
      "epoch": 85.18638573743922,
      "grad_norm": 1.248045563697815,
      "learning_rate": 4.151183911774246e-05,
      "loss": 2.9763,
      "step": 262800
    },
    {
      "epoch": 85.21880064829821,
      "grad_norm": 1.0030999183654785,
      "learning_rate": 4.1508595523840415e-05,
      "loss": 2.9782,
      "step": 262900
    },
    {
      "epoch": 85.25121555915722,
      "grad_norm": 1.057161569595337,
      "learning_rate": 4.1505351929938374e-05,
      "loss": 2.9816,
      "step": 263000
    },
    {
      "epoch": 85.2836304700162,
      "grad_norm": 1.1797146797180176,
      "learning_rate": 4.150210833603633e-05,
      "loss": 2.963,
      "step": 263100
    },
    {
      "epoch": 85.31604538087521,
      "grad_norm": 1.1949493885040283,
      "learning_rate": 4.1498864742134285e-05,
      "loss": 2.9699,
      "step": 263200
    },
    {
      "epoch": 85.3484602917342,
      "grad_norm": 1.3361470699310303,
      "learning_rate": 4.149562114823224e-05,
      "loss": 2.9682,
      "step": 263300
    },
    {
      "epoch": 85.38087520259319,
      "grad_norm": 1.2205562591552734,
      "learning_rate": 4.1492377554330195e-05,
      "loss": 2.954,
      "step": 263400
    },
    {
      "epoch": 85.41329011345219,
      "grad_norm": 1.0413389205932617,
      "learning_rate": 4.1489133960428154e-05,
      "loss": 2.9782,
      "step": 263500
    },
    {
      "epoch": 85.44570502431118,
      "grad_norm": 0.9527267217636108,
      "learning_rate": 4.148589036652611e-05,
      "loss": 2.9741,
      "step": 263600
    },
    {
      "epoch": 85.47811993517018,
      "grad_norm": 1.0334386825561523,
      "learning_rate": 4.1482646772624065e-05,
      "loss": 2.9735,
      "step": 263700
    },
    {
      "epoch": 85.51053484602917,
      "grad_norm": 1.1220383644104004,
      "learning_rate": 4.1479403178722024e-05,
      "loss": 2.9723,
      "step": 263800
    },
    {
      "epoch": 85.54294975688816,
      "grad_norm": 0.9255664944648743,
      "learning_rate": 4.147615958481998e-05,
      "loss": 2.9567,
      "step": 263900
    },
    {
      "epoch": 85.57536466774717,
      "grad_norm": 1.2769850492477417,
      "learning_rate": 4.147291599091794e-05,
      "loss": 2.9623,
      "step": 264000
    },
    {
      "epoch": 85.60777957860616,
      "grad_norm": 0.9811448454856873,
      "learning_rate": 4.146970483295491e-05,
      "loss": 2.9678,
      "step": 264100
    },
    {
      "epoch": 85.64019448946516,
      "grad_norm": 1.0036693811416626,
      "learning_rate": 4.146646123905287e-05,
      "loss": 2.9566,
      "step": 264200
    },
    {
      "epoch": 85.67260940032415,
      "grad_norm": 1.0729105472564697,
      "learning_rate": 4.146321764515083e-05,
      "loss": 2.9685,
      "step": 264300
    },
    {
      "epoch": 85.70502431118314,
      "grad_norm": 1.0057111978530884,
      "learning_rate": 4.145997405124878e-05,
      "loss": 2.9875,
      "step": 264400
    },
    {
      "epoch": 85.73743922204214,
      "grad_norm": 1.1801292896270752,
      "learning_rate": 4.145673045734674e-05,
      "loss": 2.9565,
      "step": 264500
    },
    {
      "epoch": 85.76985413290113,
      "grad_norm": 1.1570850610733032,
      "learning_rate": 4.14534868634447e-05,
      "loss": 2.9715,
      "step": 264600
    },
    {
      "epoch": 85.80226904376013,
      "grad_norm": 1.0581344366073608,
      "learning_rate": 4.145024326954266e-05,
      "loss": 2.9914,
      "step": 264700
    },
    {
      "epoch": 85.83468395461912,
      "grad_norm": 0.906193196773529,
      "learning_rate": 4.1446999675640616e-05,
      "loss": 2.9665,
      "step": 264800
    },
    {
      "epoch": 85.86709886547813,
      "grad_norm": 0.9956126809120178,
      "learning_rate": 4.144375608173857e-05,
      "loss": 2.9855,
      "step": 264900
    },
    {
      "epoch": 85.89951377633712,
      "grad_norm": 1.0509072542190552,
      "learning_rate": 4.144051248783653e-05,
      "loss": 2.9924,
      "step": 265000
    },
    {
      "epoch": 85.9319286871961,
      "grad_norm": 0.9737553596496582,
      "learning_rate": 4.1437268893934486e-05,
      "loss": 2.9917,
      "step": 265100
    },
    {
      "epoch": 85.96434359805511,
      "grad_norm": 1.053156852722168,
      "learning_rate": 4.143402530003244e-05,
      "loss": 2.9719,
      "step": 265200
    },
    {
      "epoch": 85.9967585089141,
      "grad_norm": 0.9818183779716492,
      "learning_rate": 4.14307817061304e-05,
      "loss": 2.9625,
      "step": 265300
    },
    {
      "epoch": 86.0,
      "eval_bleu": 1.3221025592990179,
      "eval_loss": 3.8161160945892334,
      "eval_runtime": 4.4542,
      "eval_samples_per_second": 110.457,
      "eval_steps_per_second": 1.796,
      "step": 265310
    },
    {
      "epoch": 86.0291734197731,
      "grad_norm": 0.9102779626846313,
      "learning_rate": 4.1427538112228355e-05,
      "loss": 2.941,
      "step": 265400
    },
    {
      "epoch": 86.06158833063209,
      "grad_norm": 0.9668915867805481,
      "learning_rate": 4.142429451832631e-05,
      "loss": 2.9529,
      "step": 265500
    },
    {
      "epoch": 86.09400324149108,
      "grad_norm": 1.1367418766021729,
      "learning_rate": 4.1421050924424266e-05,
      "loss": 2.939,
      "step": 265600
    },
    {
      "epoch": 86.12641815235008,
      "grad_norm": 1.0770331621170044,
      "learning_rate": 4.141780733052222e-05,
      "loss": 2.969,
      "step": 265700
    },
    {
      "epoch": 86.15883306320907,
      "grad_norm": 1.0963410139083862,
      "learning_rate": 4.141456373662018e-05,
      "loss": 2.9731,
      "step": 265800
    },
    {
      "epoch": 86.19124797406808,
      "grad_norm": 0.960716187953949,
      "learning_rate": 4.1411320142718136e-05,
      "loss": 2.9656,
      "step": 265900
    },
    {
      "epoch": 86.22366288492707,
      "grad_norm": 1.0421409606933594,
      "learning_rate": 4.140807654881609e-05,
      "loss": 2.961,
      "step": 266000
    },
    {
      "epoch": 86.25607779578606,
      "grad_norm": 1.1171188354492188,
      "learning_rate": 4.1404832954914046e-05,
      "loss": 2.9655,
      "step": 266100
    },
    {
      "epoch": 86.28849270664506,
      "grad_norm": 1.0336898565292358,
      "learning_rate": 4.1401589361012005e-05,
      "loss": 2.9828,
      "step": 266200
    },
    {
      "epoch": 86.32090761750405,
      "grad_norm": 1.112467646598816,
      "learning_rate": 4.139834576710996e-05,
      "loss": 2.9552,
      "step": 266300
    },
    {
      "epoch": 86.35332252836305,
      "grad_norm": 0.9802281856536865,
      "learning_rate": 4.1395102173207916e-05,
      "loss": 2.9805,
      "step": 266400
    },
    {
      "epoch": 86.38573743922204,
      "grad_norm": 1.0852495431900024,
      "learning_rate": 4.1391858579305874e-05,
      "loss": 2.9668,
      "step": 266500
    },
    {
      "epoch": 86.41815235008104,
      "grad_norm": 0.9944254159927368,
      "learning_rate": 4.1388614985403826e-05,
      "loss": 2.9796,
      "step": 266600
    },
    {
      "epoch": 86.45056726094003,
      "grad_norm": 1.0318512916564941,
      "learning_rate": 4.1385371391501785e-05,
      "loss": 2.9606,
      "step": 266700
    },
    {
      "epoch": 86.48298217179902,
      "grad_norm": 0.9942865967750549,
      "learning_rate": 4.138212779759974e-05,
      "loss": 2.9658,
      "step": 266800
    },
    {
      "epoch": 86.51539708265803,
      "grad_norm": 1.009464144706726,
      "learning_rate": 4.1378884203697696e-05,
      "loss": 2.962,
      "step": 266900
    },
    {
      "epoch": 86.54781199351702,
      "grad_norm": 1.0823431015014648,
      "learning_rate": 4.1375640609795655e-05,
      "loss": 2.9677,
      "step": 267000
    },
    {
      "epoch": 86.58022690437602,
      "grad_norm": 1.0517157316207886,
      "learning_rate": 4.137239701589361e-05,
      "loss": 2.9873,
      "step": 267100
    },
    {
      "epoch": 86.61264181523501,
      "grad_norm": 1.1166430711746216,
      "learning_rate": 4.136915342199157e-05,
      "loss": 2.9752,
      "step": 267200
    },
    {
      "epoch": 86.645056726094,
      "grad_norm": 1.0488178730010986,
      "learning_rate": 4.1365909828089524e-05,
      "loss": 2.9758,
      "step": 267300
    },
    {
      "epoch": 86.677471636953,
      "grad_norm": 1.07783043384552,
      "learning_rate": 4.136266623418748e-05,
      "loss": 2.9698,
      "step": 267400
    },
    {
      "epoch": 86.70988654781199,
      "grad_norm": 0.8945768475532532,
      "learning_rate": 4.135942264028544e-05,
      "loss": 2.9762,
      "step": 267500
    },
    {
      "epoch": 86.742301458671,
      "grad_norm": 0.9056135416030884,
      "learning_rate": 4.1356179046383394e-05,
      "loss": 2.98,
      "step": 267600
    },
    {
      "epoch": 86.77471636952998,
      "grad_norm": 1.0313862562179565,
      "learning_rate": 4.135293545248135e-05,
      "loss": 2.9626,
      "step": 267700
    },
    {
      "epoch": 86.80713128038897,
      "grad_norm": 0.9401062726974487,
      "learning_rate": 4.134969185857931e-05,
      "loss": 2.9794,
      "step": 267800
    },
    {
      "epoch": 86.83954619124798,
      "grad_norm": 0.9708916544914246,
      "learning_rate": 4.134644826467726e-05,
      "loss": 2.9864,
      "step": 267900
    },
    {
      "epoch": 86.87196110210697,
      "grad_norm": 1.0092933177947998,
      "learning_rate": 4.134320467077522e-05,
      "loss": 2.9618,
      "step": 268000
    },
    {
      "epoch": 86.90437601296597,
      "grad_norm": 0.9462869167327881,
      "learning_rate": 4.133996107687318e-05,
      "loss": 2.9686,
      "step": 268100
    },
    {
      "epoch": 86.93679092382496,
      "grad_norm": 1.0506830215454102,
      "learning_rate": 4.133671748297113e-05,
      "loss": 2.9483,
      "step": 268200
    },
    {
      "epoch": 86.96920583468396,
      "grad_norm": 1.0219711065292358,
      "learning_rate": 4.133347388906909e-05,
      "loss": 2.9857,
      "step": 268300
    },
    {
      "epoch": 87.0,
      "eval_bleu": 1.2052060945908971,
      "eval_loss": 3.8162999153137207,
      "eval_runtime": 4.5011,
      "eval_samples_per_second": 109.306,
      "eval_steps_per_second": 1.777,
      "step": 268395
    },
    {
      "epoch": 87.00162074554295,
      "grad_norm": 1.019524097442627,
      "learning_rate": 4.133023029516705e-05,
      "loss": 2.9812,
      "step": 268400
    },
    {
      "epoch": 87.03403565640194,
      "grad_norm": 1.0971558094024658,
      "learning_rate": 4.1326986701265e-05,
      "loss": 2.9637,
      "step": 268500
    },
    {
      "epoch": 87.06645056726094,
      "grad_norm": 1.1387473344802856,
      "learning_rate": 4.132374310736296e-05,
      "loss": 2.9574,
      "step": 268600
    },
    {
      "epoch": 87.09886547811993,
      "grad_norm": 1.0016547441482544,
      "learning_rate": 4.132049951346091e-05,
      "loss": 2.9729,
      "step": 268700
    },
    {
      "epoch": 87.13128038897894,
      "grad_norm": 1.1723839044570923,
      "learning_rate": 4.131725591955887e-05,
      "loss": 2.9613,
      "step": 268800
    },
    {
      "epoch": 87.16369529983793,
      "grad_norm": 1.157535195350647,
      "learning_rate": 4.131401232565683e-05,
      "loss": 2.956,
      "step": 268900
    },
    {
      "epoch": 87.19611021069692,
      "grad_norm": 1.01220703125,
      "learning_rate": 4.131076873175478e-05,
      "loss": 2.9568,
      "step": 269000
    },
    {
      "epoch": 87.22852512155592,
      "grad_norm": 1.2602356672286987,
      "learning_rate": 4.130752513785274e-05,
      "loss": 2.9551,
      "step": 269100
    },
    {
      "epoch": 87.26094003241491,
      "grad_norm": 0.9408054351806641,
      "learning_rate": 4.13042815439507e-05,
      "loss": 2.9551,
      "step": 269200
    },
    {
      "epoch": 87.29335494327391,
      "grad_norm": 1.1715714931488037,
      "learning_rate": 4.130103795004865e-05,
      "loss": 2.9742,
      "step": 269300
    },
    {
      "epoch": 87.3257698541329,
      "grad_norm": 1.1458154916763306,
      "learning_rate": 4.129779435614661e-05,
      "loss": 2.9745,
      "step": 269400
    },
    {
      "epoch": 87.35818476499189,
      "grad_norm": 1.073732614517212,
      "learning_rate": 4.129455076224457e-05,
      "loss": 2.9674,
      "step": 269500
    },
    {
      "epoch": 87.3905996758509,
      "grad_norm": 1.037870168685913,
      "learning_rate": 4.129130716834253e-05,
      "loss": 2.9653,
      "step": 269600
    },
    {
      "epoch": 87.42301458670988,
      "grad_norm": 1.0509947538375854,
      "learning_rate": 4.1288063574440486e-05,
      "loss": 2.9564,
      "step": 269700
    },
    {
      "epoch": 87.45542949756889,
      "grad_norm": 1.0146429538726807,
      "learning_rate": 4.128481998053844e-05,
      "loss": 2.9701,
      "step": 269800
    },
    {
      "epoch": 87.48784440842788,
      "grad_norm": 1.1429647207260132,
      "learning_rate": 4.12815763866364e-05,
      "loss": 2.9567,
      "step": 269900
    },
    {
      "epoch": 87.52025931928688,
      "grad_norm": 0.9175451397895813,
      "learning_rate": 4.1278332792734356e-05,
      "loss": 2.952,
      "step": 270000
    },
    {
      "epoch": 87.55267423014587,
      "grad_norm": 1.1480032205581665,
      "learning_rate": 4.127508919883231e-05,
      "loss": 2.9599,
      "step": 270100
    },
    {
      "epoch": 87.58508914100486,
      "grad_norm": 0.9612313508987427,
      "learning_rate": 4.127184560493027e-05,
      "loss": 2.9907,
      "step": 270200
    },
    {
      "epoch": 87.61750405186386,
      "grad_norm": 1.1391457319259644,
      "learning_rate": 4.1268634446967244e-05,
      "loss": 2.9778,
      "step": 270300
    },
    {
      "epoch": 87.64991896272285,
      "grad_norm": 1.4487110376358032,
      "learning_rate": 4.12653908530652e-05,
      "loss": 2.9506,
      "step": 270400
    },
    {
      "epoch": 87.68233387358185,
      "grad_norm": 0.9504818320274353,
      "learning_rate": 4.1262147259163155e-05,
      "loss": 2.97,
      "step": 270500
    },
    {
      "epoch": 87.71474878444084,
      "grad_norm": 1.1307240724563599,
      "learning_rate": 4.1258903665261114e-05,
      "loss": 2.988,
      "step": 270600
    },
    {
      "epoch": 87.74716369529983,
      "grad_norm": 1.0312741994857788,
      "learning_rate": 4.125566007135907e-05,
      "loss": 2.9566,
      "step": 270700
    },
    {
      "epoch": 87.77957860615884,
      "grad_norm": 1.1037944555282593,
      "learning_rate": 4.1252416477457025e-05,
      "loss": 2.9793,
      "step": 270800
    },
    {
      "epoch": 87.81199351701783,
      "grad_norm": 1.1973844766616821,
      "learning_rate": 4.124917288355498e-05,
      "loss": 2.9389,
      "step": 270900
    },
    {
      "epoch": 87.84440842787683,
      "grad_norm": 1.0068806409835815,
      "learning_rate": 4.1245929289652935e-05,
      "loss": 2.9765,
      "step": 271000
    },
    {
      "epoch": 87.87682333873582,
      "grad_norm": 1.0428390502929688,
      "learning_rate": 4.1242685695750894e-05,
      "loss": 2.972,
      "step": 271100
    },
    {
      "epoch": 87.90923824959481,
      "grad_norm": 0.9455114603042603,
      "learning_rate": 4.123944210184885e-05,
      "loss": 2.9696,
      "step": 271200
    },
    {
      "epoch": 87.94165316045381,
      "grad_norm": 1.2062803506851196,
      "learning_rate": 4.1236198507946805e-05,
      "loss": 2.9662,
      "step": 271300
    },
    {
      "epoch": 87.9740680713128,
      "grad_norm": 1.0389347076416016,
      "learning_rate": 4.1232954914044763e-05,
      "loss": 2.9763,
      "step": 271400
    },
    {
      "epoch": 88.0,
      "eval_bleu": 1.1291037963513106,
      "eval_loss": 3.8279271125793457,
      "eval_runtime": 4.8807,
      "eval_samples_per_second": 100.806,
      "eval_steps_per_second": 1.639,
      "step": 271480
    },
    {
      "epoch": 88.0064829821718,
      "grad_norm": 0.9819968938827515,
      "learning_rate": 4.122971132014272e-05,
      "loss": 2.9644,
      "step": 271500
    },
    {
      "epoch": 88.03889789303079,
      "grad_norm": 1.0024350881576538,
      "learning_rate": 4.1226467726240674e-05,
      "loss": 2.9575,
      "step": 271600
    },
    {
      "epoch": 88.0713128038898,
      "grad_norm": 1.0191395282745361,
      "learning_rate": 4.122322413233863e-05,
      "loss": 2.9403,
      "step": 271700
    },
    {
      "epoch": 88.10372771474879,
      "grad_norm": 1.0534782409667969,
      "learning_rate": 4.121998053843659e-05,
      "loss": 2.9435,
      "step": 271800
    },
    {
      "epoch": 88.13614262560777,
      "grad_norm": 0.925753653049469,
      "learning_rate": 4.1216736944534544e-05,
      "loss": 2.9623,
      "step": 271900
    },
    {
      "epoch": 88.16855753646678,
      "grad_norm": 1.046806812286377,
      "learning_rate": 4.12134933506325e-05,
      "loss": 2.9481,
      "step": 272000
    },
    {
      "epoch": 88.20097244732577,
      "grad_norm": 1.0806273221969604,
      "learning_rate": 4.1210249756730454e-05,
      "loss": 2.9552,
      "step": 272100
    },
    {
      "epoch": 88.23338735818477,
      "grad_norm": 0.9276431798934937,
      "learning_rate": 4.120703859876744e-05,
      "loss": 2.9586,
      "step": 272200
    },
    {
      "epoch": 88.26580226904376,
      "grad_norm": 1.2291772365570068,
      "learning_rate": 4.120379500486539e-05,
      "loss": 2.9567,
      "step": 272300
    },
    {
      "epoch": 88.29821717990275,
      "grad_norm": 0.8604332208633423,
      "learning_rate": 4.120055141096335e-05,
      "loss": 2.932,
      "step": 272400
    },
    {
      "epoch": 88.33063209076175,
      "grad_norm": 1.009081482887268,
      "learning_rate": 4.11973078170613e-05,
      "loss": 2.9583,
      "step": 272500
    },
    {
      "epoch": 88.36304700162074,
      "grad_norm": 1.0374454259872437,
      "learning_rate": 4.119406422315926e-05,
      "loss": 2.95,
      "step": 272600
    },
    {
      "epoch": 88.39546191247975,
      "grad_norm": 1.1740885972976685,
      "learning_rate": 4.119082062925722e-05,
      "loss": 2.9577,
      "step": 272700
    },
    {
      "epoch": 88.42787682333874,
      "grad_norm": 1.0254507064819336,
      "learning_rate": 4.118757703535517e-05,
      "loss": 2.9669,
      "step": 272800
    },
    {
      "epoch": 88.46029173419772,
      "grad_norm": 0.9938289523124695,
      "learning_rate": 4.118433344145313e-05,
      "loss": 2.9496,
      "step": 272900
    },
    {
      "epoch": 88.49270664505673,
      "grad_norm": 0.9407452940940857,
      "learning_rate": 4.118108984755109e-05,
      "loss": 2.9602,
      "step": 273000
    },
    {
      "epoch": 88.52512155591572,
      "grad_norm": 1.0246375799179077,
      "learning_rate": 4.117784625364905e-05,
      "loss": 2.9696,
      "step": 273100
    },
    {
      "epoch": 88.55753646677472,
      "grad_norm": 1.1911910772323608,
      "learning_rate": 4.1174602659747e-05,
      "loss": 2.9577,
      "step": 273200
    },
    {
      "epoch": 88.58995137763371,
      "grad_norm": 1.2380850315093994,
      "learning_rate": 4.117135906584496e-05,
      "loss": 2.9671,
      "step": 273300
    },
    {
      "epoch": 88.62236628849271,
      "grad_norm": 1.1741033792495728,
      "learning_rate": 4.116811547194292e-05,
      "loss": 2.9645,
      "step": 273400
    },
    {
      "epoch": 88.6547811993517,
      "grad_norm": 0.9764150977134705,
      "learning_rate": 4.1164871878040875e-05,
      "loss": 2.9686,
      "step": 273500
    },
    {
      "epoch": 88.68719611021069,
      "grad_norm": 1.0993928909301758,
      "learning_rate": 4.116162828413883e-05,
      "loss": 2.9617,
      "step": 273600
    },
    {
      "epoch": 88.7196110210697,
      "grad_norm": 1.0592100620269775,
      "learning_rate": 4.1158384690236786e-05,
      "loss": 2.9614,
      "step": 273700
    },
    {
      "epoch": 88.75202593192869,
      "grad_norm": 1.0410962104797363,
      "learning_rate": 4.1155141096334745e-05,
      "loss": 2.9683,
      "step": 273800
    },
    {
      "epoch": 88.78444084278769,
      "grad_norm": 0.9688612222671509,
      "learning_rate": 4.11518975024327e-05,
      "loss": 2.9782,
      "step": 273900
    },
    {
      "epoch": 88.81685575364668,
      "grad_norm": 1.025395393371582,
      "learning_rate": 4.1148653908530656e-05,
      "loss": 3.0006,
      "step": 274000
    },
    {
      "epoch": 88.84927066450567,
      "grad_norm": 0.9837477803230286,
      "learning_rate": 4.1145410314628614e-05,
      "loss": 2.9679,
      "step": 274100
    },
    {
      "epoch": 88.88168557536467,
      "grad_norm": 0.9424073100090027,
      "learning_rate": 4.1142166720726566e-05,
      "loss": 2.9784,
      "step": 274200
    },
    {
      "epoch": 88.91410048622366,
      "grad_norm": 1.0394827127456665,
      "learning_rate": 4.1138923126824525e-05,
      "loss": 2.9784,
      "step": 274300
    },
    {
      "epoch": 88.94651539708266,
      "grad_norm": 1.135686993598938,
      "learning_rate": 4.113567953292248e-05,
      "loss": 2.9601,
      "step": 274400
    },
    {
      "epoch": 88.97893030794165,
      "grad_norm": 1.0081228017807007,
      "learning_rate": 4.1132435939020436e-05,
      "loss": 2.9728,
      "step": 274500
    },
    {
      "epoch": 89.0,
      "eval_bleu": 1.224920637216472,
      "eval_loss": 3.8243227005004883,
      "eval_runtime": 4.2445,
      "eval_samples_per_second": 115.914,
      "eval_steps_per_second": 1.885,
      "step": 274565
    },
    {
      "epoch": 89.01134521880064,
      "grad_norm": 1.0232317447662354,
      "learning_rate": 4.1129192345118395e-05,
      "loss": 2.9645,
      "step": 274600
    },
    {
      "epoch": 89.04376012965965,
      "grad_norm": 1.1598037481307983,
      "learning_rate": 4.1125948751216347e-05,
      "loss": 2.9545,
      "step": 274700
    },
    {
      "epoch": 89.07617504051863,
      "grad_norm": 1.0703486204147339,
      "learning_rate": 4.1122705157314305e-05,
      "loss": 2.9523,
      "step": 274800
    },
    {
      "epoch": 89.10858995137764,
      "grad_norm": 1.1438103914260864,
      "learning_rate": 4.1119461563412264e-05,
      "loss": 2.938,
      "step": 274900
    },
    {
      "epoch": 89.14100486223663,
      "grad_norm": 1.1469649076461792,
      "learning_rate": 4.1116217969510216e-05,
      "loss": 2.9432,
      "step": 275000
    },
    {
      "epoch": 89.17341977309563,
      "grad_norm": 0.8423970937728882,
      "learning_rate": 4.1112974375608175e-05,
      "loss": 2.9483,
      "step": 275100
    },
    {
      "epoch": 89.20583468395462,
      "grad_norm": 1.0271090269088745,
      "learning_rate": 4.1109730781706133e-05,
      "loss": 2.9533,
      "step": 275200
    },
    {
      "epoch": 89.23824959481361,
      "grad_norm": 1.0764082670211792,
      "learning_rate": 4.1106487187804085e-05,
      "loss": 2.9469,
      "step": 275300
    },
    {
      "epoch": 89.27066450567261,
      "grad_norm": 1.085614800453186,
      "learning_rate": 4.1103243593902044e-05,
      "loss": 2.9663,
      "step": 275400
    },
    {
      "epoch": 89.3030794165316,
      "grad_norm": 1.1172454357147217,
      "learning_rate": 4.11e-05,
      "loss": 2.9624,
      "step": 275500
    },
    {
      "epoch": 89.3354943273906,
      "grad_norm": 1.1143698692321777,
      "learning_rate": 4.1096756406097955e-05,
      "loss": 2.9648,
      "step": 275600
    },
    {
      "epoch": 89.3679092382496,
      "grad_norm": 1.033424735069275,
      "learning_rate": 4.1093512812195914e-05,
      "loss": 2.9592,
      "step": 275700
    },
    {
      "epoch": 89.40032414910858,
      "grad_norm": 1.089746117591858,
      "learning_rate": 4.109026921829387e-05,
      "loss": 2.9394,
      "step": 275800
    },
    {
      "epoch": 89.43273905996759,
      "grad_norm": 1.0406874418258667,
      "learning_rate": 4.108702562439183e-05,
      "loss": 2.9716,
      "step": 275900
    },
    {
      "epoch": 89.46515397082658,
      "grad_norm": 0.9947125315666199,
      "learning_rate": 4.108378203048979e-05,
      "loss": 2.9671,
      "step": 276000
    },
    {
      "epoch": 89.49756888168558,
      "grad_norm": 1.0725258588790894,
      "learning_rate": 4.108053843658774e-05,
      "loss": 2.947,
      "step": 276100
    },
    {
      "epoch": 89.52998379254457,
      "grad_norm": 1.289634108543396,
      "learning_rate": 4.10772948426857e-05,
      "loss": 2.969,
      "step": 276200
    },
    {
      "epoch": 89.56239870340356,
      "grad_norm": 1.049323320388794,
      "learning_rate": 4.107405124878365e-05,
      "loss": 2.9418,
      "step": 276300
    },
    {
      "epoch": 89.59481361426256,
      "grad_norm": 1.01338791847229,
      "learning_rate": 4.107080765488161e-05,
      "loss": 2.9631,
      "step": 276400
    },
    {
      "epoch": 89.62722852512155,
      "grad_norm": 0.9847949743270874,
      "learning_rate": 4.106756406097957e-05,
      "loss": 2.9565,
      "step": 276500
    },
    {
      "epoch": 89.65964343598056,
      "grad_norm": 1.1027944087982178,
      "learning_rate": 4.106432046707752e-05,
      "loss": 2.9609,
      "step": 276600
    },
    {
      "epoch": 89.69205834683954,
      "grad_norm": 0.9851645231246948,
      "learning_rate": 4.106107687317548e-05,
      "loss": 2.9707,
      "step": 276700
    },
    {
      "epoch": 89.72447325769855,
      "grad_norm": 0.9856714606285095,
      "learning_rate": 4.105783327927344e-05,
      "loss": 2.9653,
      "step": 276800
    },
    {
      "epoch": 89.75688816855754,
      "grad_norm": 0.9149547815322876,
      "learning_rate": 4.105458968537139e-05,
      "loss": 2.9543,
      "step": 276900
    },
    {
      "epoch": 89.78930307941653,
      "grad_norm": 0.968134880065918,
      "learning_rate": 4.105134609146935e-05,
      "loss": 2.9632,
      "step": 277000
    },
    {
      "epoch": 89.82171799027553,
      "grad_norm": 1.0891144275665283,
      "learning_rate": 4.104810249756731e-05,
      "loss": 2.9538,
      "step": 277100
    },
    {
      "epoch": 89.85413290113452,
      "grad_norm": 1.1209608316421509,
      "learning_rate": 4.104485890366526e-05,
      "loss": 2.9633,
      "step": 277200
    },
    {
      "epoch": 89.88654781199352,
      "grad_norm": 0.9632555246353149,
      "learning_rate": 4.104161530976322e-05,
      "loss": 2.9774,
      "step": 277300
    },
    {
      "epoch": 89.91896272285251,
      "grad_norm": 0.929623007774353,
      "learning_rate": 4.103837171586117e-05,
      "loss": 2.9731,
      "step": 277400
    },
    {
      "epoch": 89.9513776337115,
      "grad_norm": 0.8885801434516907,
      "learning_rate": 4.103512812195913e-05,
      "loss": 2.9759,
      "step": 277500
    },
    {
      "epoch": 89.9837925445705,
      "grad_norm": 0.9961168766021729,
      "learning_rate": 4.103188452805709e-05,
      "loss": 2.9711,
      "step": 277600
    },
    {
      "epoch": 90.0,
      "eval_bleu": 1.2764873939467465,
      "eval_loss": 3.8266701698303223,
      "eval_runtime": 4.4596,
      "eval_samples_per_second": 110.324,
      "eval_steps_per_second": 1.794,
      "step": 277650
    },
    {
      "epoch": 90.0162074554295,
      "grad_norm": 1.350592851638794,
      "learning_rate": 4.102864093415504e-05,
      "loss": 2.9545,
      "step": 277700
    },
    {
      "epoch": 90.0486223662885,
      "grad_norm": 1.1396048069000244,
      "learning_rate": 4.1025397340253e-05,
      "loss": 2.947,
      "step": 277800
    },
    {
      "epoch": 90.08103727714749,
      "grad_norm": 1.1019306182861328,
      "learning_rate": 4.102215374635096e-05,
      "loss": 2.9389,
      "step": 277900
    },
    {
      "epoch": 90.11345218800648,
      "grad_norm": 0.9657807350158691,
      "learning_rate": 4.101891015244892e-05,
      "loss": 2.9548,
      "step": 278000
    },
    {
      "epoch": 90.14586709886548,
      "grad_norm": 0.9174870848655701,
      "learning_rate": 4.101566655854687e-05,
      "loss": 2.9371,
      "step": 278100
    },
    {
      "epoch": 90.17828200972447,
      "grad_norm": 0.9501602649688721,
      "learning_rate": 4.101245540058385e-05,
      "loss": 2.9391,
      "step": 278200
    },
    {
      "epoch": 90.21069692058347,
      "grad_norm": 1.0777490139007568,
      "learning_rate": 4.1009211806681806e-05,
      "loss": 2.9429,
      "step": 278300
    },
    {
      "epoch": 90.24311183144246,
      "grad_norm": 1.0572458505630493,
      "learning_rate": 4.100596821277976e-05,
      "loss": 2.9814,
      "step": 278400
    },
    {
      "epoch": 90.27552674230145,
      "grad_norm": 1.095923900604248,
      "learning_rate": 4.1002724618877716e-05,
      "loss": 2.9419,
      "step": 278500
    },
    {
      "epoch": 90.30794165316046,
      "grad_norm": 1.1311620473861694,
      "learning_rate": 4.0999481024975675e-05,
      "loss": 2.9662,
      "step": 278600
    },
    {
      "epoch": 90.34035656401944,
      "grad_norm": 1.0802260637283325,
      "learning_rate": 4.0996237431073634e-05,
      "loss": 2.9496,
      "step": 278700
    },
    {
      "epoch": 90.37277147487845,
      "grad_norm": 0.9785715937614441,
      "learning_rate": 4.099299383717159e-05,
      "loss": 2.9688,
      "step": 278800
    },
    {
      "epoch": 90.40518638573744,
      "grad_norm": 0.911821186542511,
      "learning_rate": 4.0989750243269545e-05,
      "loss": 2.9605,
      "step": 278900
    },
    {
      "epoch": 90.43760129659644,
      "grad_norm": 1.0397056341171265,
      "learning_rate": 4.0986506649367503e-05,
      "loss": 2.9297,
      "step": 279000
    },
    {
      "epoch": 90.47001620745543,
      "grad_norm": 1.1260923147201538,
      "learning_rate": 4.098326305546546e-05,
      "loss": 2.9577,
      "step": 279100
    },
    {
      "epoch": 90.50243111831442,
      "grad_norm": 1.131548285484314,
      "learning_rate": 4.0980019461563414e-05,
      "loss": 2.9605,
      "step": 279200
    },
    {
      "epoch": 90.53484602917342,
      "grad_norm": 1.0160586833953857,
      "learning_rate": 4.097677586766137e-05,
      "loss": 2.9624,
      "step": 279300
    },
    {
      "epoch": 90.56726094003241,
      "grad_norm": 1.126898169517517,
      "learning_rate": 4.097353227375933e-05,
      "loss": 2.9502,
      "step": 279400
    },
    {
      "epoch": 90.59967585089142,
      "grad_norm": 1.0956223011016846,
      "learning_rate": 4.0970288679857284e-05,
      "loss": 2.9742,
      "step": 279500
    },
    {
      "epoch": 90.6320907617504,
      "grad_norm": 1.0581485033035278,
      "learning_rate": 4.096704508595524e-05,
      "loss": 2.9396,
      "step": 279600
    },
    {
      "epoch": 90.6645056726094,
      "grad_norm": 1.0170212984085083,
      "learning_rate": 4.0963801492053194e-05,
      "loss": 2.9643,
      "step": 279700
    },
    {
      "epoch": 90.6969205834684,
      "grad_norm": 1.0489184856414795,
      "learning_rate": 4.096055789815115e-05,
      "loss": 2.9617,
      "step": 279800
    },
    {
      "epoch": 90.72933549432739,
      "grad_norm": 0.979564368724823,
      "learning_rate": 4.095731430424911e-05,
      "loss": 2.9643,
      "step": 279900
    },
    {
      "epoch": 90.76175040518639,
      "grad_norm": 0.9882828593254089,
      "learning_rate": 4.0954070710347064e-05,
      "loss": 2.9579,
      "step": 280000
    },
    {
      "epoch": 90.79416531604538,
      "grad_norm": 1.113797664642334,
      "learning_rate": 4.095082711644502e-05,
      "loss": 2.962,
      "step": 280100
    },
    {
      "epoch": 90.82658022690438,
      "grad_norm": 1.1816035509109497,
      "learning_rate": 4.0947615958482e-05,
      "loss": 2.962,
      "step": 280200
    },
    {
      "epoch": 90.85899513776337,
      "grad_norm": 1.0045803785324097,
      "learning_rate": 4.094437236457996e-05,
      "loss": 2.9605,
      "step": 280300
    },
    {
      "epoch": 90.89141004862236,
      "grad_norm": 1.1601394414901733,
      "learning_rate": 4.094112877067791e-05,
      "loss": 2.968,
      "step": 280400
    },
    {
      "epoch": 90.92382495948137,
      "grad_norm": 0.9553777575492859,
      "learning_rate": 4.093788517677587e-05,
      "loss": 2.9604,
      "step": 280500
    },
    {
      "epoch": 90.95623987034035,
      "grad_norm": 1.0233732461929321,
      "learning_rate": 4.093464158287383e-05,
      "loss": 2.9609,
      "step": 280600
    },
    {
      "epoch": 90.98865478119936,
      "grad_norm": 1.1071884632110596,
      "learning_rate": 4.093139798897178e-05,
      "loss": 2.9606,
      "step": 280700
    },
    {
      "epoch": 91.0,
      "eval_bleu": 1.3241250500173507,
      "eval_loss": 3.83201265335083,
      "eval_runtime": 4.2866,
      "eval_samples_per_second": 114.776,
      "eval_steps_per_second": 1.866,
      "step": 280735
    },
    {
      "epoch": 91.02106969205835,
      "grad_norm": 1.0905187129974365,
      "learning_rate": 4.092815439506974e-05,
      "loss": 2.9421,
      "step": 280800
    },
    {
      "epoch": 91.05348460291734,
      "grad_norm": 1.0966497659683228,
      "learning_rate": 4.09249108011677e-05,
      "loss": 2.9397,
      "step": 280900
    },
    {
      "epoch": 91.08589951377634,
      "grad_norm": 0.9299384951591492,
      "learning_rate": 4.092166720726565e-05,
      "loss": 2.9541,
      "step": 281000
    },
    {
      "epoch": 91.11831442463533,
      "grad_norm": 1.0604623556137085,
      "learning_rate": 4.091842361336361e-05,
      "loss": 2.9646,
      "step": 281100
    },
    {
      "epoch": 91.15072933549433,
      "grad_norm": 1.1991214752197266,
      "learning_rate": 4.091518001946156e-05,
      "loss": 2.9441,
      "step": 281200
    },
    {
      "epoch": 91.18314424635332,
      "grad_norm": 0.9878652095794678,
      "learning_rate": 4.091193642555952e-05,
      "loss": 2.929,
      "step": 281300
    },
    {
      "epoch": 91.21555915721231,
      "grad_norm": 0.9884023666381836,
      "learning_rate": 4.090869283165748e-05,
      "loss": 2.9361,
      "step": 281400
    },
    {
      "epoch": 91.24797406807131,
      "grad_norm": 1.0810905694961548,
      "learning_rate": 4.090544923775543e-05,
      "loss": 2.9432,
      "step": 281500
    },
    {
      "epoch": 91.2803889789303,
      "grad_norm": 1.0635476112365723,
      "learning_rate": 4.090220564385339e-05,
      "loss": 2.9577,
      "step": 281600
    },
    {
      "epoch": 91.31280388978931,
      "grad_norm": 1.0310050249099731,
      "learning_rate": 4.089896204995135e-05,
      "loss": 2.9604,
      "step": 281700
    },
    {
      "epoch": 91.3452188006483,
      "grad_norm": 1.042275309562683,
      "learning_rate": 4.0895718456049306e-05,
      "loss": 2.9494,
      "step": 281800
    },
    {
      "epoch": 91.37763371150729,
      "grad_norm": 0.9905509948730469,
      "learning_rate": 4.0892474862147265e-05,
      "loss": 2.9303,
      "step": 281900
    },
    {
      "epoch": 91.41004862236629,
      "grad_norm": 0.9784840941429138,
      "learning_rate": 4.088923126824522e-05,
      "loss": 2.9564,
      "step": 282000
    },
    {
      "epoch": 91.44246353322528,
      "grad_norm": 1.0848393440246582,
      "learning_rate": 4.0885987674343176e-05,
      "loss": 2.9421,
      "step": 282100
    },
    {
      "epoch": 91.47487844408428,
      "grad_norm": 1.0457247495651245,
      "learning_rate": 4.0882776516380154e-05,
      "loss": 2.9228,
      "step": 282200
    },
    {
      "epoch": 91.50729335494327,
      "grad_norm": 1.1236815452575684,
      "learning_rate": 4.0879532922478106e-05,
      "loss": 2.945,
      "step": 282300
    },
    {
      "epoch": 91.53970826580228,
      "grad_norm": 0.9900059103965759,
      "learning_rate": 4.0876289328576064e-05,
      "loss": 2.9667,
      "step": 282400
    },
    {
      "epoch": 91.57212317666126,
      "grad_norm": 1.0795074701309204,
      "learning_rate": 4.087304573467402e-05,
      "loss": 2.9756,
      "step": 282500
    },
    {
      "epoch": 91.60453808752025,
      "grad_norm": 1.0365146398544312,
      "learning_rate": 4.086980214077198e-05,
      "loss": 2.9679,
      "step": 282600
    },
    {
      "epoch": 91.63695299837926,
      "grad_norm": 1.0276758670806885,
      "learning_rate": 4.0866558546869934e-05,
      "loss": 2.9634,
      "step": 282700
    },
    {
      "epoch": 91.66936790923825,
      "grad_norm": 1.09829843044281,
      "learning_rate": 4.086331495296789e-05,
      "loss": 2.9624,
      "step": 282800
    },
    {
      "epoch": 91.70178282009725,
      "grad_norm": 1.1428327560424805,
      "learning_rate": 4.086007135906585e-05,
      "loss": 2.9638,
      "step": 282900
    },
    {
      "epoch": 91.73419773095624,
      "grad_norm": 1.0616919994354248,
      "learning_rate": 4.08568277651638e-05,
      "loss": 2.958,
      "step": 283000
    },
    {
      "epoch": 91.76661264181523,
      "grad_norm": 1.1211705207824707,
      "learning_rate": 4.085358417126176e-05,
      "loss": 2.9307,
      "step": 283100
    },
    {
      "epoch": 91.79902755267423,
      "grad_norm": 1.0358951091766357,
      "learning_rate": 4.085034057735972e-05,
      "loss": 2.9581,
      "step": 283200
    },
    {
      "epoch": 91.83144246353322,
      "grad_norm": 0.9271038174629211,
      "learning_rate": 4.084709698345767e-05,
      "loss": 2.95,
      "step": 283300
    },
    {
      "epoch": 91.86385737439223,
      "grad_norm": 1.0521299839019775,
      "learning_rate": 4.084385338955563e-05,
      "loss": 2.9466,
      "step": 283400
    },
    {
      "epoch": 91.89627228525121,
      "grad_norm": 0.9778268933296204,
      "learning_rate": 4.084060979565358e-05,
      "loss": 2.9669,
      "step": 283500
    },
    {
      "epoch": 91.9286871961102,
      "grad_norm": 1.0181889533996582,
      "learning_rate": 4.083736620175154e-05,
      "loss": 2.9517,
      "step": 283600
    },
    {
      "epoch": 91.96110210696921,
      "grad_norm": 0.9303720593452454,
      "learning_rate": 4.08341226078495e-05,
      "loss": 2.9677,
      "step": 283700
    },
    {
      "epoch": 91.9935170178282,
      "grad_norm": 1.1462910175323486,
      "learning_rate": 4.083087901394745e-05,
      "loss": 2.9658,
      "step": 283800
    },
    {
      "epoch": 92.0,
      "eval_bleu": 1.346734848507193,
      "eval_loss": 3.8338656425476074,
      "eval_runtime": 4.3006,
      "eval_samples_per_second": 114.403,
      "eval_steps_per_second": 1.86,
      "step": 283820
    },
    {
      "epoch": 92.0259319286872,
      "grad_norm": 1.1603690385818481,
      "learning_rate": 4.082763542004541e-05,
      "loss": 2.9486,
      "step": 283900
    },
    {
      "epoch": 92.05834683954619,
      "grad_norm": 0.9862362146377563,
      "learning_rate": 4.082439182614337e-05,
      "loss": 2.9437,
      "step": 284000
    },
    {
      "epoch": 92.09076175040519,
      "grad_norm": 1.013717770576477,
      "learning_rate": 4.082114823224132e-05,
      "loss": 2.9516,
      "step": 284100
    },
    {
      "epoch": 92.12317666126418,
      "grad_norm": 1.041197419166565,
      "learning_rate": 4.08179370742783e-05,
      "loss": 2.9608,
      "step": 284200
    },
    {
      "epoch": 92.15559157212317,
      "grad_norm": 1.1559503078460693,
      "learning_rate": 4.081469348037626e-05,
      "loss": 2.9361,
      "step": 284300
    },
    {
      "epoch": 92.18800648298217,
      "grad_norm": 1.1111079454421997,
      "learning_rate": 4.081144988647422e-05,
      "loss": 2.9443,
      "step": 284400
    },
    {
      "epoch": 92.22042139384116,
      "grad_norm": 0.9093911051750183,
      "learning_rate": 4.080820629257217e-05,
      "loss": 2.9562,
      "step": 284500
    },
    {
      "epoch": 92.25283630470017,
      "grad_norm": 1.003798484802246,
      "learning_rate": 4.080496269867013e-05,
      "loss": 2.9613,
      "step": 284600
    },
    {
      "epoch": 92.28525121555916,
      "grad_norm": 1.0045779943466187,
      "learning_rate": 4.080171910476808e-05,
      "loss": 2.9569,
      "step": 284700
    },
    {
      "epoch": 92.31766612641815,
      "grad_norm": 0.9835828542709351,
      "learning_rate": 4.079847551086604e-05,
      "loss": 2.9489,
      "step": 284800
    },
    {
      "epoch": 92.35008103727715,
      "grad_norm": 0.9449289441108704,
      "learning_rate": 4.0795231916964e-05,
      "loss": 2.9494,
      "step": 284900
    },
    {
      "epoch": 92.38249594813614,
      "grad_norm": 1.0628210306167603,
      "learning_rate": 4.079198832306195e-05,
      "loss": 2.9516,
      "step": 285000
    },
    {
      "epoch": 92.41491085899514,
      "grad_norm": 1.0895107984542847,
      "learning_rate": 4.078874472915991e-05,
      "loss": 2.9557,
      "step": 285100
    },
    {
      "epoch": 92.44732576985413,
      "grad_norm": 0.9986967444419861,
      "learning_rate": 4.078550113525787e-05,
      "loss": 2.9534,
      "step": 285200
    },
    {
      "epoch": 92.47974068071312,
      "grad_norm": 1.0072041749954224,
      "learning_rate": 4.0782257541355826e-05,
      "loss": 2.952,
      "step": 285300
    },
    {
      "epoch": 92.51215559157212,
      "grad_norm": 1.074217438697815,
      "learning_rate": 4.0779013947453785e-05,
      "loss": 2.9447,
      "step": 285400
    },
    {
      "epoch": 92.54457050243111,
      "grad_norm": 1.0397640466690063,
      "learning_rate": 4.0775770353551737e-05,
      "loss": 2.9405,
      "step": 285500
    },
    {
      "epoch": 92.57698541329012,
      "grad_norm": 1.1465293169021606,
      "learning_rate": 4.0772526759649695e-05,
      "loss": 2.9158,
      "step": 285600
    },
    {
      "epoch": 92.6094003241491,
      "grad_norm": 0.9805529713630676,
      "learning_rate": 4.0769283165747654e-05,
      "loss": 2.9622,
      "step": 285700
    },
    {
      "epoch": 92.64181523500811,
      "grad_norm": 1.0395528078079224,
      "learning_rate": 4.0766039571845606e-05,
      "loss": 2.9559,
      "step": 285800
    },
    {
      "epoch": 92.6742301458671,
      "grad_norm": 1.0241765975952148,
      "learning_rate": 4.0762795977943565e-05,
      "loss": 2.9194,
      "step": 285900
    },
    {
      "epoch": 92.70664505672609,
      "grad_norm": 0.9970563650131226,
      "learning_rate": 4.0759552384041524e-05,
      "loss": 2.945,
      "step": 286000
    },
    {
      "epoch": 92.73905996758509,
      "grad_norm": 0.9687361717224121,
      "learning_rate": 4.07563412260785e-05,
      "loss": 2.9343,
      "step": 286100
    },
    {
      "epoch": 92.77147487844408,
      "grad_norm": 1.0329620838165283,
      "learning_rate": 4.075309763217645e-05,
      "loss": 2.944,
      "step": 286200
    },
    {
      "epoch": 92.80388978930308,
      "grad_norm": 0.9170767664909363,
      "learning_rate": 4.074985403827441e-05,
      "loss": 2.9705,
      "step": 286300
    },
    {
      "epoch": 92.83630470016207,
      "grad_norm": 1.0509047508239746,
      "learning_rate": 4.074661044437237e-05,
      "loss": 2.9439,
      "step": 286400
    },
    {
      "epoch": 92.86871961102106,
      "grad_norm": 1.0394980907440186,
      "learning_rate": 4.074336685047032e-05,
      "loss": 2.9618,
      "step": 286500
    },
    {
      "epoch": 92.90113452188007,
      "grad_norm": 0.919701337814331,
      "learning_rate": 4.074012325656828e-05,
      "loss": 2.9748,
      "step": 286600
    },
    {
      "epoch": 92.93354943273906,
      "grad_norm": 1.008992075920105,
      "learning_rate": 4.073687966266624e-05,
      "loss": 2.9712,
      "step": 286700
    },
    {
      "epoch": 92.96596434359806,
      "grad_norm": 1.121376872062683,
      "learning_rate": 4.073363606876419e-05,
      "loss": 2.9479,
      "step": 286800
    },
    {
      "epoch": 92.99837925445705,
      "grad_norm": 0.9434400200843811,
      "learning_rate": 4.073039247486215e-05,
      "loss": 2.9651,
      "step": 286900
    },
    {
      "epoch": 93.0,
      "eval_bleu": 1.212544820252274,
      "eval_loss": 3.830958127975464,
      "eval_runtime": 4.3999,
      "eval_samples_per_second": 111.821,
      "eval_steps_per_second": 1.818,
      "step": 286905
    },
    {
      "epoch": 93.03079416531604,
      "grad_norm": 1.1099494695663452,
      "learning_rate": 4.07271488809601e-05,
      "loss": 2.9478,
      "step": 287000
    },
    {
      "epoch": 93.06320907617504,
      "grad_norm": 1.001609444618225,
      "learning_rate": 4.072390528705806e-05,
      "loss": 2.9362,
      "step": 287100
    },
    {
      "epoch": 93.09562398703403,
      "grad_norm": 1.01832115650177,
      "learning_rate": 4.072066169315602e-05,
      "loss": 2.9309,
      "step": 287200
    },
    {
      "epoch": 93.12803889789303,
      "grad_norm": 1.0114173889160156,
      "learning_rate": 4.071741809925397e-05,
      "loss": 2.942,
      "step": 287300
    },
    {
      "epoch": 93.16045380875202,
      "grad_norm": 1.0725280046463013,
      "learning_rate": 4.071417450535193e-05,
      "loss": 2.9293,
      "step": 287400
    },
    {
      "epoch": 93.19286871961103,
      "grad_norm": 0.9874682426452637,
      "learning_rate": 4.071096334738891e-05,
      "loss": 2.9453,
      "step": 287500
    },
    {
      "epoch": 93.22528363047002,
      "grad_norm": 1.1428241729736328,
      "learning_rate": 4.070771975348687e-05,
      "loss": 2.9384,
      "step": 287600
    },
    {
      "epoch": 93.257698541329,
      "grad_norm": 1.0375845432281494,
      "learning_rate": 4.070447615958482e-05,
      "loss": 2.9361,
      "step": 287700
    },
    {
      "epoch": 93.29011345218801,
      "grad_norm": 1.223799228668213,
      "learning_rate": 4.070123256568278e-05,
      "loss": 2.9522,
      "step": 287800
    },
    {
      "epoch": 93.322528363047,
      "grad_norm": 1.2322872877120972,
      "learning_rate": 4.069798897178074e-05,
      "loss": 2.9343,
      "step": 287900
    },
    {
      "epoch": 93.354943273906,
      "grad_norm": 1.1194791793823242,
      "learning_rate": 4.069474537787869e-05,
      "loss": 2.9428,
      "step": 288000
    },
    {
      "epoch": 93.38735818476499,
      "grad_norm": 0.9245004057884216,
      "learning_rate": 4.069150178397665e-05,
      "loss": 2.9428,
      "step": 288100
    },
    {
      "epoch": 93.41977309562398,
      "grad_norm": 0.8859745860099792,
      "learning_rate": 4.0688258190074607e-05,
      "loss": 2.959,
      "step": 288200
    },
    {
      "epoch": 93.45218800648298,
      "grad_norm": 1.1586780548095703,
      "learning_rate": 4.068501459617256e-05,
      "loss": 2.949,
      "step": 288300
    },
    {
      "epoch": 93.48460291734197,
      "grad_norm": 1.1726930141448975,
      "learning_rate": 4.068177100227052e-05,
      "loss": 2.9484,
      "step": 288400
    },
    {
      "epoch": 93.51701782820098,
      "grad_norm": 1.1352014541625977,
      "learning_rate": 4.067852740836847e-05,
      "loss": 2.9373,
      "step": 288500
    },
    {
      "epoch": 93.54943273905997,
      "grad_norm": 0.9407503008842468,
      "learning_rate": 4.067528381446643e-05,
      "loss": 2.9565,
      "step": 288600
    },
    {
      "epoch": 93.58184764991896,
      "grad_norm": 1.0930113792419434,
      "learning_rate": 4.067204022056439e-05,
      "loss": 2.9529,
      "step": 288700
    },
    {
      "epoch": 93.61426256077796,
      "grad_norm": 1.1125820875167847,
      "learning_rate": 4.0668796626662345e-05,
      "loss": 2.9415,
      "step": 288800
    },
    {
      "epoch": 93.64667747163695,
      "grad_norm": 0.9065437316894531,
      "learning_rate": 4.06655530327603e-05,
      "loss": 2.949,
      "step": 288900
    },
    {
      "epoch": 93.67909238249595,
      "grad_norm": 1.2278646230697632,
      "learning_rate": 4.0662309438858256e-05,
      "loss": 2.9568,
      "step": 289000
    },
    {
      "epoch": 93.71150729335494,
      "grad_norm": 1.024966835975647,
      "learning_rate": 4.0659065844956215e-05,
      "loss": 2.9548,
      "step": 289100
    },
    {
      "epoch": 93.74392220421394,
      "grad_norm": 1.1324834823608398,
      "learning_rate": 4.0655822251054174e-05,
      "loss": 2.9493,
      "step": 289200
    },
    {
      "epoch": 93.77633711507293,
      "grad_norm": 1.2385579347610474,
      "learning_rate": 4.0652578657152126e-05,
      "loss": 2.9433,
      "step": 289300
    },
    {
      "epoch": 93.80875202593192,
      "grad_norm": 0.9918434619903564,
      "learning_rate": 4.0649335063250084e-05,
      "loss": 2.953,
      "step": 289400
    },
    {
      "epoch": 93.84116693679093,
      "grad_norm": 0.9521793127059937,
      "learning_rate": 4.064609146934804e-05,
      "loss": 2.9475,
      "step": 289500
    },
    {
      "epoch": 93.87358184764992,
      "grad_norm": 0.9096787571907043,
      "learning_rate": 4.0642847875445995e-05,
      "loss": 2.9734,
      "step": 289600
    },
    {
      "epoch": 93.90599675850892,
      "grad_norm": 1.0559691190719604,
      "learning_rate": 4.0639604281543954e-05,
      "loss": 2.9643,
      "step": 289700
    },
    {
      "epoch": 93.93841166936791,
      "grad_norm": 0.9588376879692078,
      "learning_rate": 4.063636068764191e-05,
      "loss": 2.9615,
      "step": 289800
    },
    {
      "epoch": 93.9708265802269,
      "grad_norm": 1.0389585494995117,
      "learning_rate": 4.0633117093739865e-05,
      "loss": 2.9485,
      "step": 289900
    },
    {
      "epoch": 94.0,
      "eval_bleu": 1.3282020527494993,
      "eval_loss": 3.8350906372070312,
      "eval_runtime": 4.2826,
      "eval_samples_per_second": 114.882,
      "eval_steps_per_second": 1.868,
      "step": 289990
    },
    {
      "epoch": 94.0032414910859,
      "grad_norm": 0.9793599247932434,
      "learning_rate": 4.062987349983782e-05,
      "loss": 2.9427,
      "step": 290000
    },
    {
      "epoch": 94.03565640194489,
      "grad_norm": 0.9648587703704834,
      "learning_rate": 4.062662990593578e-05,
      "loss": 2.934,
      "step": 290100
    },
    {
      "epoch": 94.0680713128039,
      "grad_norm": 1.2805107831954956,
      "learning_rate": 4.0623386312033734e-05,
      "loss": 2.9255,
      "step": 290200
    },
    {
      "epoch": 94.10048622366288,
      "grad_norm": 1.2307331562042236,
      "learning_rate": 4.062014271813169e-05,
      "loss": 2.9567,
      "step": 290300
    },
    {
      "epoch": 94.13290113452187,
      "grad_norm": 1.1230131387710571,
      "learning_rate": 4.0616899124229645e-05,
      "loss": 2.939,
      "step": 290400
    },
    {
      "epoch": 94.16531604538088,
      "grad_norm": 1.0517158508300781,
      "learning_rate": 4.0613655530327603e-05,
      "loss": 2.951,
      "step": 290500
    },
    {
      "epoch": 94.19773095623987,
      "grad_norm": 1.121591329574585,
      "learning_rate": 4.061041193642556e-05,
      "loss": 2.9505,
      "step": 290600
    },
    {
      "epoch": 94.23014586709887,
      "grad_norm": 1.111070990562439,
      "learning_rate": 4.0607168342523514e-05,
      "loss": 2.943,
      "step": 290700
    },
    {
      "epoch": 94.26256077795786,
      "grad_norm": 0.9916964769363403,
      "learning_rate": 4.060392474862147e-05,
      "loss": 2.9271,
      "step": 290800
    },
    {
      "epoch": 94.29497568881686,
      "grad_norm": 0.9116641283035278,
      "learning_rate": 4.060068115471943e-05,
      "loss": 2.9485,
      "step": 290900
    },
    {
      "epoch": 94.32739059967585,
      "grad_norm": 1.0084902048110962,
      "learning_rate": 4.0597437560817384e-05,
      "loss": 2.9227,
      "step": 291000
    },
    {
      "epoch": 94.35980551053484,
      "grad_norm": 1.1186692714691162,
      "learning_rate": 4.059419396691534e-05,
      "loss": 2.9463,
      "step": 291100
    },
    {
      "epoch": 94.39222042139384,
      "grad_norm": 0.9361566305160522,
      "learning_rate": 4.05909503730133e-05,
      "loss": 2.9347,
      "step": 291200
    },
    {
      "epoch": 94.42463533225283,
      "grad_norm": 1.2263157367706299,
      "learning_rate": 4.058770677911125e-05,
      "loss": 2.9142,
      "step": 291300
    },
    {
      "epoch": 94.45705024311184,
      "grad_norm": 1.0262751579284668,
      "learning_rate": 4.058446318520921e-05,
      "loss": 2.9267,
      "step": 291400
    },
    {
      "epoch": 94.48946515397083,
      "grad_norm": 0.9772717952728271,
      "learning_rate": 4.058121959130717e-05,
      "loss": 2.9308,
      "step": 291500
    },
    {
      "epoch": 94.52188006482982,
      "grad_norm": 0.9960376620292664,
      "learning_rate": 4.057797599740513e-05,
      "loss": 2.9408,
      "step": 291600
    },
    {
      "epoch": 94.55429497568882,
      "grad_norm": 1.3759229183197021,
      "learning_rate": 4.057473240350309e-05,
      "loss": 2.9777,
      "step": 291700
    },
    {
      "epoch": 94.58670988654781,
      "grad_norm": 1.0097442865371704,
      "learning_rate": 4.057148880960104e-05,
      "loss": 2.9507,
      "step": 291800
    },
    {
      "epoch": 94.61912479740681,
      "grad_norm": 1.1162325143814087,
      "learning_rate": 4.0568245215699e-05,
      "loss": 2.9583,
      "step": 291900
    },
    {
      "epoch": 94.6515397082658,
      "grad_norm": 1.0299760103225708,
      "learning_rate": 4.056500162179696e-05,
      "loss": 2.9634,
      "step": 292000
    },
    {
      "epoch": 94.68395461912479,
      "grad_norm": 1.0263724327087402,
      "learning_rate": 4.056175802789491e-05,
      "loss": 2.9529,
      "step": 292100
    },
    {
      "epoch": 94.7163695299838,
      "grad_norm": 1.0277677774429321,
      "learning_rate": 4.055851443399287e-05,
      "loss": 2.9383,
      "step": 292200
    },
    {
      "epoch": 94.74878444084278,
      "grad_norm": 1.096450924873352,
      "learning_rate": 4.055527084009082e-05,
      "loss": 2.9394,
      "step": 292300
    },
    {
      "epoch": 94.78119935170179,
      "grad_norm": 0.8949727416038513,
      "learning_rate": 4.055202724618878e-05,
      "loss": 2.9466,
      "step": 292400
    },
    {
      "epoch": 94.81361426256078,
      "grad_norm": 1.013791561126709,
      "learning_rate": 4.054878365228674e-05,
      "loss": 2.9573,
      "step": 292500
    },
    {
      "epoch": 94.84602917341978,
      "grad_norm": 1.0511598587036133,
      "learning_rate": 4.054554005838469e-05,
      "loss": 2.9523,
      "step": 292600
    },
    {
      "epoch": 94.87844408427877,
      "grad_norm": 0.9658355116844177,
      "learning_rate": 4.054229646448265e-05,
      "loss": 2.9523,
      "step": 292700
    },
    {
      "epoch": 94.91085899513776,
      "grad_norm": 1.0536569356918335,
      "learning_rate": 4.053905287058061e-05,
      "loss": 2.9436,
      "step": 292800
    },
    {
      "epoch": 94.94327390599676,
      "grad_norm": 1.0156253576278687,
      "learning_rate": 4.053580927667856e-05,
      "loss": 2.9613,
      "step": 292900
    },
    {
      "epoch": 94.97568881685575,
      "grad_norm": 0.8956160545349121,
      "learning_rate": 4.053256568277652e-05,
      "loss": 2.9306,
      "step": 293000
    },
    {
      "epoch": 95.0,
      "eval_bleu": 1.2804757982927166,
      "eval_loss": 3.836164712905884,
      "eval_runtime": 4.0683,
      "eval_samples_per_second": 120.936,
      "eval_steps_per_second": 1.966,
      "step": 293075
    },
    {
      "epoch": 95.00810372771475,
      "grad_norm": 0.9857432842254639,
      "learning_rate": 4.0529322088874477e-05,
      "loss": 2.9427,
      "step": 293100
    },
    {
      "epoch": 95.04051863857374,
      "grad_norm": 0.9195988774299622,
      "learning_rate": 4.052607849497243e-05,
      "loss": 2.9322,
      "step": 293200
    },
    {
      "epoch": 95.07293354943273,
      "grad_norm": 1.0675972700119019,
      "learning_rate": 4.052283490107039e-05,
      "loss": 2.9266,
      "step": 293300
    },
    {
      "epoch": 95.10534846029174,
      "grad_norm": 0.9451661705970764,
      "learning_rate": 4.051959130716834e-05,
      "loss": 2.9338,
      "step": 293400
    },
    {
      "epoch": 95.13776337115073,
      "grad_norm": 1.067091941833496,
      "learning_rate": 4.0516380149205324e-05,
      "loss": 2.935,
      "step": 293500
    },
    {
      "epoch": 95.17017828200973,
      "grad_norm": 1.1179852485656738,
      "learning_rate": 4.0513136555303276e-05,
      "loss": 2.9293,
      "step": 293600
    },
    {
      "epoch": 95.20259319286872,
      "grad_norm": 1.0281739234924316,
      "learning_rate": 4.0509892961401234e-05,
      "loss": 2.9457,
      "step": 293700
    },
    {
      "epoch": 95.23500810372771,
      "grad_norm": 1.050864338874817,
      "learning_rate": 4.0506649367499186e-05,
      "loss": 2.9391,
      "step": 293800
    },
    {
      "epoch": 95.26742301458671,
      "grad_norm": 0.9959465861320496,
      "learning_rate": 4.0503405773597145e-05,
      "loss": 2.9411,
      "step": 293900
    },
    {
      "epoch": 95.2998379254457,
      "grad_norm": 1.0715965032577515,
      "learning_rate": 4.0500162179695104e-05,
      "loss": 2.9588,
      "step": 294000
    },
    {
      "epoch": 95.3322528363047,
      "grad_norm": 1.0086238384246826,
      "learning_rate": 4.0496918585793056e-05,
      "loss": 2.9413,
      "step": 294100
    },
    {
      "epoch": 95.3646677471637,
      "grad_norm": 1.1096876859664917,
      "learning_rate": 4.0493674991891015e-05,
      "loss": 2.9454,
      "step": 294200
    },
    {
      "epoch": 95.3970826580227,
      "grad_norm": 0.9582750797271729,
      "learning_rate": 4.0490431397988973e-05,
      "loss": 2.9283,
      "step": 294300
    },
    {
      "epoch": 95.42949756888169,
      "grad_norm": 1.219041347503662,
      "learning_rate": 4.048718780408693e-05,
      "loss": 2.9284,
      "step": 294400
    },
    {
      "epoch": 95.46191247974068,
      "grad_norm": 1.2203902006149292,
      "learning_rate": 4.048394421018489e-05,
      "loss": 2.9412,
      "step": 294500
    },
    {
      "epoch": 95.49432739059968,
      "grad_norm": 1.183817744255066,
      "learning_rate": 4.048070061628284e-05,
      "loss": 2.9452,
      "step": 294600
    },
    {
      "epoch": 95.52674230145867,
      "grad_norm": 1.0500788688659668,
      "learning_rate": 4.04774570223808e-05,
      "loss": 2.9485,
      "step": 294700
    },
    {
      "epoch": 95.55915721231767,
      "grad_norm": 0.9601581692695618,
      "learning_rate": 4.047421342847876e-05,
      "loss": 2.9379,
      "step": 294800
    },
    {
      "epoch": 95.59157212317666,
      "grad_norm": 1.204259991645813,
      "learning_rate": 4.047096983457671e-05,
      "loss": 2.9464,
      "step": 294900
    },
    {
      "epoch": 95.62398703403565,
      "grad_norm": 0.9648251533508301,
      "learning_rate": 4.046772624067467e-05,
      "loss": 2.9501,
      "step": 295000
    },
    {
      "epoch": 95.65640194489465,
      "grad_norm": 0.9893198013305664,
      "learning_rate": 4.046448264677263e-05,
      "loss": 2.9535,
      "step": 295100
    },
    {
      "epoch": 95.68881685575364,
      "grad_norm": 1.1693480014801025,
      "learning_rate": 4.046123905287058e-05,
      "loss": 2.9314,
      "step": 295200
    },
    {
      "epoch": 95.72123176661265,
      "grad_norm": 0.9001691937446594,
      "learning_rate": 4.045799545896854e-05,
      "loss": 2.9437,
      "step": 295300
    },
    {
      "epoch": 95.75364667747164,
      "grad_norm": 1.0959866046905518,
      "learning_rate": 4.04547518650665e-05,
      "loss": 2.9342,
      "step": 295400
    },
    {
      "epoch": 95.78606158833063,
      "grad_norm": 1.0477640628814697,
      "learning_rate": 4.045154070710348e-05,
      "loss": 2.9544,
      "step": 295500
    },
    {
      "epoch": 95.81847649918963,
      "grad_norm": 1.0092259645462036,
      "learning_rate": 4.044829711320143e-05,
      "loss": 2.9468,
      "step": 295600
    },
    {
      "epoch": 95.85089141004862,
      "grad_norm": 1.2433555126190186,
      "learning_rate": 4.044505351929939e-05,
      "loss": 2.9464,
      "step": 295700
    },
    {
      "epoch": 95.88330632090762,
      "grad_norm": 1.0613176822662354,
      "learning_rate": 4.0441809925397346e-05,
      "loss": 2.9414,
      "step": 295800
    },
    {
      "epoch": 95.91572123176661,
      "grad_norm": 1.1535489559173584,
      "learning_rate": 4.04385663314953e-05,
      "loss": 2.9595,
      "step": 295900
    },
    {
      "epoch": 95.94813614262561,
      "grad_norm": 1.0214024782180786,
      "learning_rate": 4.043532273759326e-05,
      "loss": 2.9269,
      "step": 296000
    },
    {
      "epoch": 95.9805510534846,
      "grad_norm": 1.400458574295044,
      "learning_rate": 4.043207914369121e-05,
      "loss": 2.9229,
      "step": 296100
    },
    {
      "epoch": 96.0,
      "eval_bleu": 1.3120137344911977,
      "eval_loss": 3.8398239612579346,
      "eval_runtime": 4.1497,
      "eval_samples_per_second": 118.564,
      "eval_steps_per_second": 1.928,
      "step": 296160
    },
    {
      "epoch": 96.01296596434359,
      "grad_norm": 0.9517666101455688,
      "learning_rate": 4.042883554978917e-05,
      "loss": 2.9492,
      "step": 296200
    },
    {
      "epoch": 96.0453808752026,
      "grad_norm": 1.1648797988891602,
      "learning_rate": 4.042559195588713e-05,
      "loss": 2.9342,
      "step": 296300
    },
    {
      "epoch": 96.07779578606159,
      "grad_norm": 1.0956488847732544,
      "learning_rate": 4.042234836198508e-05,
      "loss": 2.9274,
      "step": 296400
    },
    {
      "epoch": 96.11021069692059,
      "grad_norm": 1.0280206203460693,
      "learning_rate": 4.041910476808304e-05,
      "loss": 2.9362,
      "step": 296500
    },
    {
      "epoch": 96.14262560777958,
      "grad_norm": 1.2010706663131714,
      "learning_rate": 4.0415861174180996e-05,
      "loss": 2.9365,
      "step": 296600
    },
    {
      "epoch": 96.17504051863857,
      "grad_norm": 1.140337347984314,
      "learning_rate": 4.041261758027895e-05,
      "loss": 2.9272,
      "step": 296700
    },
    {
      "epoch": 96.20745542949757,
      "grad_norm": 1.163631796836853,
      "learning_rate": 4.040937398637691e-05,
      "loss": 2.9323,
      "step": 296800
    },
    {
      "epoch": 96.23987034035656,
      "grad_norm": 1.0496927499771118,
      "learning_rate": 4.040613039247486e-05,
      "loss": 2.9331,
      "step": 296900
    },
    {
      "epoch": 96.27228525121556,
      "grad_norm": 1.214524745941162,
      "learning_rate": 4.040288679857282e-05,
      "loss": 2.9428,
      "step": 297000
    },
    {
      "epoch": 96.30470016207455,
      "grad_norm": 0.903415858745575,
      "learning_rate": 4.0399643204670776e-05,
      "loss": 2.9453,
      "step": 297100
    },
    {
      "epoch": 96.33711507293354,
      "grad_norm": 0.9493932127952576,
      "learning_rate": 4.039639961076873e-05,
      "loss": 2.9313,
      "step": 297200
    },
    {
      "epoch": 96.36952998379255,
      "grad_norm": 1.4141846895217896,
      "learning_rate": 4.039315601686669e-05,
      "loss": 2.929,
      "step": 297300
    },
    {
      "epoch": 96.40194489465154,
      "grad_norm": 1.1164684295654297,
      "learning_rate": 4.0389912422964646e-05,
      "loss": 2.9454,
      "step": 297400
    },
    {
      "epoch": 96.43435980551054,
      "grad_norm": 1.0988117456436157,
      "learning_rate": 4.0386701265001624e-05,
      "loss": 2.9289,
      "step": 297500
    },
    {
      "epoch": 96.46677471636953,
      "grad_norm": 1.2932963371276855,
      "learning_rate": 4.0383457671099575e-05,
      "loss": 2.927,
      "step": 297600
    },
    {
      "epoch": 96.49918962722853,
      "grad_norm": 1.3369953632354736,
      "learning_rate": 4.0380214077197534e-05,
      "loss": 2.9632,
      "step": 297700
    },
    {
      "epoch": 96.53160453808752,
      "grad_norm": 1.032144546508789,
      "learning_rate": 4.037697048329549e-05,
      "loss": 2.9367,
      "step": 297800
    },
    {
      "epoch": 96.56401944894651,
      "grad_norm": 1.1777693033218384,
      "learning_rate": 4.037372688939345e-05,
      "loss": 2.9247,
      "step": 297900
    },
    {
      "epoch": 96.59643435980551,
      "grad_norm": 0.9840558171272278,
      "learning_rate": 4.0370483295491404e-05,
      "loss": 2.9346,
      "step": 298000
    },
    {
      "epoch": 96.6288492706645,
      "grad_norm": 1.0923974514007568,
      "learning_rate": 4.036723970158936e-05,
      "loss": 2.952,
      "step": 298100
    },
    {
      "epoch": 96.6612641815235,
      "grad_norm": 1.0879123210906982,
      "learning_rate": 4.036399610768732e-05,
      "loss": 2.9277,
      "step": 298200
    },
    {
      "epoch": 96.6936790923825,
      "grad_norm": 1.075640320777893,
      "learning_rate": 4.036075251378528e-05,
      "loss": 2.9506,
      "step": 298300
    },
    {
      "epoch": 96.72609400324149,
      "grad_norm": 0.9836851954460144,
      "learning_rate": 4.035750891988323e-05,
      "loss": 2.9275,
      "step": 298400
    },
    {
      "epoch": 96.75850891410049,
      "grad_norm": 1.3483370542526245,
      "learning_rate": 4.035426532598119e-05,
      "loss": 2.947,
      "step": 298500
    },
    {
      "epoch": 96.79092382495948,
      "grad_norm": 1.0836701393127441,
      "learning_rate": 4.035102173207915e-05,
      "loss": 2.9384,
      "step": 298600
    },
    {
      "epoch": 96.82333873581848,
      "grad_norm": 1.0606448650360107,
      "learning_rate": 4.03477781381771e-05,
      "loss": 2.9566,
      "step": 298700
    },
    {
      "epoch": 96.85575364667747,
      "grad_norm": 1.118963599205017,
      "learning_rate": 4.034453454427506e-05,
      "loss": 2.9202,
      "step": 298800
    },
    {
      "epoch": 96.88816855753646,
      "grad_norm": 1.1785871982574463,
      "learning_rate": 4.034129095037302e-05,
      "loss": 2.9456,
      "step": 298900
    },
    {
      "epoch": 96.92058346839546,
      "grad_norm": 1.0628036260604858,
      "learning_rate": 4.033804735647097e-05,
      "loss": 2.9506,
      "step": 299000
    },
    {
      "epoch": 96.95299837925445,
      "grad_norm": 1.1566730737686157,
      "learning_rate": 4.033480376256893e-05,
      "loss": 2.9355,
      "step": 299100
    },
    {
      "epoch": 96.98541329011346,
      "grad_norm": 1.166972041130066,
      "learning_rate": 4.033156016866689e-05,
      "loss": 2.9562,
      "step": 299200
    },
    {
      "epoch": 97.0,
      "eval_bleu": 1.1709239224374668,
      "eval_loss": 3.836646795272827,
      "eval_runtime": 4.3687,
      "eval_samples_per_second": 112.619,
      "eval_steps_per_second": 1.831,
      "step": 299245
    },
    {
      "epoch": 97.01782820097245,
      "grad_norm": 1.0814698934555054,
      "learning_rate": 4.032831657476484e-05,
      "loss": 2.9404,
      "step": 299300
    },
    {
      "epoch": 97.05024311183145,
      "grad_norm": 1.2201520204544067,
      "learning_rate": 4.03250729808628e-05,
      "loss": 2.9285,
      "step": 299400
    },
    {
      "epoch": 97.08265802269044,
      "grad_norm": 1.0114262104034424,
      "learning_rate": 4.032186182289978e-05,
      "loss": 2.9233,
      "step": 299500
    },
    {
      "epoch": 97.11507293354943,
      "grad_norm": 1.1021568775177002,
      "learning_rate": 4.031861822899773e-05,
      "loss": 2.9146,
      "step": 299600
    },
    {
      "epoch": 97.14748784440843,
      "grad_norm": 1.112625241279602,
      "learning_rate": 4.031537463509569e-05,
      "loss": 2.9389,
      "step": 299700
    },
    {
      "epoch": 97.17990275526742,
      "grad_norm": 0.9212826490402222,
      "learning_rate": 4.0312131041193646e-05,
      "loss": 2.9366,
      "step": 299800
    },
    {
      "epoch": 97.21231766612642,
      "grad_norm": 1.3043339252471924,
      "learning_rate": 4.0308919883230624e-05,
      "loss": 2.9343,
      "step": 299900
    },
    {
      "epoch": 97.24473257698541,
      "grad_norm": 0.936924397945404,
      "learning_rate": 4.0305676289328576e-05,
      "loss": 2.9478,
      "step": 300000
    },
    {
      "epoch": 97.2771474878444,
      "grad_norm": 0.9755570292472839,
      "learning_rate": 4.0302432695426535e-05,
      "loss": 2.9455,
      "step": 300100
    },
    {
      "epoch": 97.3095623987034,
      "grad_norm": 1.0194110870361328,
      "learning_rate": 4.0299189101524493e-05,
      "loss": 2.9301,
      "step": 300200
    },
    {
      "epoch": 97.3419773095624,
      "grad_norm": 0.9482161402702332,
      "learning_rate": 4.0295945507622445e-05,
      "loss": 2.9411,
      "step": 300300
    },
    {
      "epoch": 97.3743922204214,
      "grad_norm": 1.1226143836975098,
      "learning_rate": 4.0292701913720404e-05,
      "loss": 2.9418,
      "step": 300400
    },
    {
      "epoch": 97.40680713128039,
      "grad_norm": 1.0410324335098267,
      "learning_rate": 4.028945831981836e-05,
      "loss": 2.927,
      "step": 300500
    },
    {
      "epoch": 97.43922204213938,
      "grad_norm": 1.0394134521484375,
      "learning_rate": 4.0286214725916315e-05,
      "loss": 2.9365,
      "step": 300600
    },
    {
      "epoch": 97.47163695299838,
      "grad_norm": 0.9570193290710449,
      "learning_rate": 4.0282971132014274e-05,
      "loss": 2.938,
      "step": 300700
    },
    {
      "epoch": 97.50405186385737,
      "grad_norm": 1.1856269836425781,
      "learning_rate": 4.027972753811223e-05,
      "loss": 2.9341,
      "step": 300800
    },
    {
      "epoch": 97.53646677471637,
      "grad_norm": 1.1361629962921143,
      "learning_rate": 4.0276483944210184e-05,
      "loss": 2.9195,
      "step": 300900
    },
    {
      "epoch": 97.56888168557536,
      "grad_norm": 1.0504790544509888,
      "learning_rate": 4.027324035030814e-05,
      "loss": 2.9264,
      "step": 301000
    },
    {
      "epoch": 97.60129659643437,
      "grad_norm": 1.1364669799804688,
      "learning_rate": 4.0269996756406095e-05,
      "loss": 2.9331,
      "step": 301100
    },
    {
      "epoch": 97.63371150729336,
      "grad_norm": 0.9088594317436218,
      "learning_rate": 4.0266753162504054e-05,
      "loss": 2.9298,
      "step": 301200
    },
    {
      "epoch": 97.66612641815234,
      "grad_norm": 1.0897424221038818,
      "learning_rate": 4.026350956860201e-05,
      "loss": 2.926,
      "step": 301300
    },
    {
      "epoch": 97.69854132901135,
      "grad_norm": 1.159854531288147,
      "learning_rate": 4.0260265974699965e-05,
      "loss": 2.9366,
      "step": 301400
    },
    {
      "epoch": 97.73095623987034,
      "grad_norm": 1.0083609819412231,
      "learning_rate": 4.025702238079792e-05,
      "loss": 2.9317,
      "step": 301500
    },
    {
      "epoch": 97.76337115072934,
      "grad_norm": 1.0319925546646118,
      "learning_rate": 4.025377878689588e-05,
      "loss": 2.933,
      "step": 301600
    },
    {
      "epoch": 97.79578606158833,
      "grad_norm": 1.110092282295227,
      "learning_rate": 4.025053519299384e-05,
      "loss": 2.9307,
      "step": 301700
    },
    {
      "epoch": 97.82820097244732,
      "grad_norm": 1.1707698106765747,
      "learning_rate": 4.02472915990918e-05,
      "loss": 2.9541,
      "step": 301800
    },
    {
      "epoch": 97.86061588330632,
      "grad_norm": 1.0288618803024292,
      "learning_rate": 4.024404800518975e-05,
      "loss": 2.9195,
      "step": 301900
    },
    {
      "epoch": 97.89303079416531,
      "grad_norm": 1.2386555671691895,
      "learning_rate": 4.024080441128771e-05,
      "loss": 2.9375,
      "step": 302000
    },
    {
      "epoch": 97.92544570502432,
      "grad_norm": 1.1013768911361694,
      "learning_rate": 4.023756081738567e-05,
      "loss": 2.9304,
      "step": 302100
    },
    {
      "epoch": 97.9578606158833,
      "grad_norm": 0.9856208562850952,
      "learning_rate": 4.023431722348362e-05,
      "loss": 2.9506,
      "step": 302200
    },
    {
      "epoch": 97.9902755267423,
      "grad_norm": 1.0352463722229004,
      "learning_rate": 4.023107362958158e-05,
      "loss": 2.9506,
      "step": 302300
    },
    {
      "epoch": 98.0,
      "eval_bleu": 1.103286160890495,
      "eval_loss": 3.849550485610962,
      "eval_runtime": 4.3624,
      "eval_samples_per_second": 112.781,
      "eval_steps_per_second": 1.834,
      "step": 302330
    },
    {
      "epoch": 98.0226904376013,
      "grad_norm": 1.1924312114715576,
      "learning_rate": 4.022783003567954e-05,
      "loss": 2.9098,
      "step": 302400
    },
    {
      "epoch": 98.05510534846029,
      "grad_norm": 1.0555709600448608,
      "learning_rate": 4.022458644177749e-05,
      "loss": 2.9261,
      "step": 302500
    },
    {
      "epoch": 98.08752025931929,
      "grad_norm": 1.186994194984436,
      "learning_rate": 4.022134284787545e-05,
      "loss": 2.9349,
      "step": 302600
    },
    {
      "epoch": 98.11993517017828,
      "grad_norm": 1.0485049486160278,
      "learning_rate": 4.021809925397341e-05,
      "loss": 2.9272,
      "step": 302700
    },
    {
      "epoch": 98.15235008103728,
      "grad_norm": 1.0652546882629395,
      "learning_rate": 4.021485566007136e-05,
      "loss": 2.9323,
      "step": 302800
    },
    {
      "epoch": 98.18476499189627,
      "grad_norm": 0.9795584082603455,
      "learning_rate": 4.021161206616932e-05,
      "loss": 2.932,
      "step": 302900
    },
    {
      "epoch": 98.21717990275526,
      "grad_norm": 1.2741296291351318,
      "learning_rate": 4.020836847226727e-05,
      "loss": 2.9373,
      "step": 303000
    },
    {
      "epoch": 98.24959481361427,
      "grad_norm": 0.9476050138473511,
      "learning_rate": 4.020512487836523e-05,
      "loss": 2.938,
      "step": 303100
    },
    {
      "epoch": 98.28200972447326,
      "grad_norm": 1.1026802062988281,
      "learning_rate": 4.020188128446319e-05,
      "loss": 2.9255,
      "step": 303200
    },
    {
      "epoch": 98.31442463533226,
      "grad_norm": 0.962456226348877,
      "learning_rate": 4.019863769056114e-05,
      "loss": 2.9061,
      "step": 303300
    },
    {
      "epoch": 98.34683954619125,
      "grad_norm": 1.2157553434371948,
      "learning_rate": 4.01953940966591e-05,
      "loss": 2.9289,
      "step": 303400
    },
    {
      "epoch": 98.37925445705024,
      "grad_norm": 1.1527317762374878,
      "learning_rate": 4.019215050275706e-05,
      "loss": 2.9494,
      "step": 303500
    },
    {
      "epoch": 98.41166936790924,
      "grad_norm": 1.1273319721221924,
      "learning_rate": 4.018890690885501e-05,
      "loss": 2.9201,
      "step": 303600
    },
    {
      "epoch": 98.44408427876823,
      "grad_norm": 1.0753910541534424,
      "learning_rate": 4.018566331495297e-05,
      "loss": 2.9351,
      "step": 303700
    },
    {
      "epoch": 98.47649918962723,
      "grad_norm": 1.2523040771484375,
      "learning_rate": 4.018241972105093e-05,
      "loss": 2.9222,
      "step": 303800
    },
    {
      "epoch": 98.50891410048622,
      "grad_norm": 0.9421334266662598,
      "learning_rate": 4.0179208563087905e-05,
      "loss": 2.9276,
      "step": 303900
    },
    {
      "epoch": 98.54132901134521,
      "grad_norm": 0.9256894588470459,
      "learning_rate": 4.017596496918586e-05,
      "loss": 2.9232,
      "step": 304000
    },
    {
      "epoch": 98.57374392220422,
      "grad_norm": 1.1173397302627563,
      "learning_rate": 4.0172721375283815e-05,
      "loss": 2.9304,
      "step": 304100
    },
    {
      "epoch": 98.6061588330632,
      "grad_norm": 1.2023630142211914,
      "learning_rate": 4.016947778138177e-05,
      "loss": 2.9451,
      "step": 304200
    },
    {
      "epoch": 98.63857374392221,
      "grad_norm": 1.1589511632919312,
      "learning_rate": 4.0166234187479726e-05,
      "loss": 2.9313,
      "step": 304300
    },
    {
      "epoch": 98.6709886547812,
      "grad_norm": 0.9867286682128906,
      "learning_rate": 4.0162990593577685e-05,
      "loss": 2.9319,
      "step": 304400
    },
    {
      "epoch": 98.7034035656402,
      "grad_norm": 1.317189335823059,
      "learning_rate": 4.0159746999675644e-05,
      "loss": 2.9216,
      "step": 304500
    },
    {
      "epoch": 98.73581847649919,
      "grad_norm": 1.0588290691375732,
      "learning_rate": 4.0156503405773596e-05,
      "loss": 2.9473,
      "step": 304600
    },
    {
      "epoch": 98.76823338735818,
      "grad_norm": 1.0196443796157837,
      "learning_rate": 4.0153259811871554e-05,
      "loss": 2.9345,
      "step": 304700
    },
    {
      "epoch": 98.80064829821718,
      "grad_norm": 0.955334484577179,
      "learning_rate": 4.015001621796951e-05,
      "loss": 2.9353,
      "step": 304800
    },
    {
      "epoch": 98.83306320907617,
      "grad_norm": 1.1874539852142334,
      "learning_rate": 4.014677262406747e-05,
      "loss": 2.9461,
      "step": 304900
    },
    {
      "epoch": 98.86547811993518,
      "grad_norm": 0.9954909682273865,
      "learning_rate": 4.014352903016543e-05,
      "loss": 2.9503,
      "step": 305000
    },
    {
      "epoch": 98.89789303079417,
      "grad_norm": 1.0177171230316162,
      "learning_rate": 4.014028543626338e-05,
      "loss": 2.9334,
      "step": 305100
    },
    {
      "epoch": 98.93030794165315,
      "grad_norm": 1.0513473749160767,
      "learning_rate": 4.013704184236134e-05,
      "loss": 2.9389,
      "step": 305200
    },
    {
      "epoch": 98.96272285251216,
      "grad_norm": 1.0251274108886719,
      "learning_rate": 4.013379824845929e-05,
      "loss": 2.9235,
      "step": 305300
    },
    {
      "epoch": 98.99513776337115,
      "grad_norm": 1.152625560760498,
      "learning_rate": 4.013055465455725e-05,
      "loss": 2.9288,
      "step": 305400
    },
    {
      "epoch": 99.0,
      "eval_bleu": 1.3428315480573811,
      "eval_loss": 3.8494057655334473,
      "eval_runtime": 4.0165,
      "eval_samples_per_second": 122.495,
      "eval_steps_per_second": 1.992,
      "step": 305415
    },
    {
      "epoch": 99.02755267423015,
      "grad_norm": 1.1063729524612427,
      "learning_rate": 4.012731106065521e-05,
      "loss": 2.9095,
      "step": 305500
    },
    {
      "epoch": 99.05996758508914,
      "grad_norm": 1.1885218620300293,
      "learning_rate": 4.012406746675316e-05,
      "loss": 2.9172,
      "step": 305600
    },
    {
      "epoch": 99.09238249594813,
      "grad_norm": 1.1621761322021484,
      "learning_rate": 4.012082387285112e-05,
      "loss": 2.9495,
      "step": 305700
    },
    {
      "epoch": 99.12479740680713,
      "grad_norm": 0.9865806102752686,
      "learning_rate": 4.011758027894908e-05,
      "loss": 2.9421,
      "step": 305800
    },
    {
      "epoch": 99.15721231766612,
      "grad_norm": 1.0292257070541382,
      "learning_rate": 4.011436912098606e-05,
      "loss": 2.9406,
      "step": 305900
    },
    {
      "epoch": 99.18962722852513,
      "grad_norm": 1.077118992805481,
      "learning_rate": 4.011112552708401e-05,
      "loss": 2.9243,
      "step": 306000
    },
    {
      "epoch": 99.22204213938411,
      "grad_norm": 1.040490746498108,
      "learning_rate": 4.010788193318197e-05,
      "loss": 2.9278,
      "step": 306100
    },
    {
      "epoch": 99.25445705024312,
      "grad_norm": 1.01779043674469,
      "learning_rate": 4.010463833927993e-05,
      "loss": 2.9152,
      "step": 306200
    },
    {
      "epoch": 99.28687196110211,
      "grad_norm": 0.9876404404640198,
      "learning_rate": 4.010139474537788e-05,
      "loss": 2.9023,
      "step": 306300
    },
    {
      "epoch": 99.3192868719611,
      "grad_norm": 1.0299004316329956,
      "learning_rate": 4.009815115147584e-05,
      "loss": 2.915,
      "step": 306400
    },
    {
      "epoch": 99.3517017828201,
      "grad_norm": 1.2005184888839722,
      "learning_rate": 4.009490755757379e-05,
      "loss": 2.9213,
      "step": 306500
    },
    {
      "epoch": 99.38411669367909,
      "grad_norm": 1.1364960670471191,
      "learning_rate": 4.009166396367175e-05,
      "loss": 2.9079,
      "step": 306600
    },
    {
      "epoch": 99.4165316045381,
      "grad_norm": 0.9639818668365479,
      "learning_rate": 4.008842036976971e-05,
      "loss": 2.9412,
      "step": 306700
    },
    {
      "epoch": 99.44894651539708,
      "grad_norm": 1.0703006982803345,
      "learning_rate": 4.008517677586766e-05,
      "loss": 2.9403,
      "step": 306800
    },
    {
      "epoch": 99.48136142625607,
      "grad_norm": 1.2056081295013428,
      "learning_rate": 4.008193318196562e-05,
      "loss": 2.924,
      "step": 306900
    },
    {
      "epoch": 99.51377633711508,
      "grad_norm": 1.0695735216140747,
      "learning_rate": 4.007868958806358e-05,
      "loss": 2.9141,
      "step": 307000
    },
    {
      "epoch": 99.54619124797406,
      "grad_norm": 1.0551183223724365,
      "learning_rate": 4.007544599416153e-05,
      "loss": 2.9227,
      "step": 307100
    },
    {
      "epoch": 99.57860615883307,
      "grad_norm": 1.101233720779419,
      "learning_rate": 4.007220240025949e-05,
      "loss": 2.937,
      "step": 307200
    },
    {
      "epoch": 99.61102106969206,
      "grad_norm": 1.033032774925232,
      "learning_rate": 4.0068958806357446e-05,
      "loss": 2.9416,
      "step": 307300
    },
    {
      "epoch": 99.64343598055105,
      "grad_norm": 1.1743354797363281,
      "learning_rate": 4.00657152124554e-05,
      "loss": 2.934,
      "step": 307400
    },
    {
      "epoch": 99.67585089141005,
      "grad_norm": 0.987459123134613,
      "learning_rate": 4.006247161855336e-05,
      "loss": 2.9199,
      "step": 307500
    },
    {
      "epoch": 99.70826580226904,
      "grad_norm": 0.9947842359542847,
      "learning_rate": 4.0059228024651316e-05,
      "loss": 2.9412,
      "step": 307600
    },
    {
      "epoch": 99.74068071312804,
      "grad_norm": 1.0415045022964478,
      "learning_rate": 4.0055984430749275e-05,
      "loss": 2.9387,
      "step": 307700
    },
    {
      "epoch": 99.77309562398703,
      "grad_norm": 1.0606664419174194,
      "learning_rate": 4.0052740836847233e-05,
      "loss": 2.9431,
      "step": 307800
    },
    {
      "epoch": 99.80551053484604,
      "grad_norm": 1.0383630990982056,
      "learning_rate": 4.0049529678884204e-05,
      "loss": 2.929,
      "step": 307900
    },
    {
      "epoch": 99.83792544570503,
      "grad_norm": 1.2232705354690552,
      "learning_rate": 4.0046286084982156e-05,
      "loss": 2.9315,
      "step": 308000
    },
    {
      "epoch": 99.87034035656401,
      "grad_norm": 1.2472087144851685,
      "learning_rate": 4.0043042491080115e-05,
      "loss": 2.9091,
      "step": 308100
    },
    {
      "epoch": 99.90275526742302,
      "grad_norm": 1.2380298376083374,
      "learning_rate": 4.0039798897178074e-05,
      "loss": 2.9524,
      "step": 308200
    },
    {
      "epoch": 99.93517017828201,
      "grad_norm": 1.1203590631484985,
      "learning_rate": 4.003655530327603e-05,
      "loss": 2.9306,
      "step": 308300
    },
    {
      "epoch": 99.96758508914101,
      "grad_norm": 0.8951849341392517,
      "learning_rate": 4.003331170937399e-05,
      "loss": 2.9314,
      "step": 308400
    },
    {
      "epoch": 100.0,
      "grad_norm": 1.0179522037506104,
      "learning_rate": 4.003006811547195e-05,
      "loss": 2.9423,
      "step": 308500
    },
    {
      "epoch": 100.0,
      "eval_bleu": 1.0965955645594374,
      "eval_loss": 3.849236011505127,
      "eval_runtime": 4.3089,
      "eval_samples_per_second": 114.183,
      "eval_steps_per_second": 1.857,
      "step": 308500
    },
    {
      "epoch": 100.03241491085899,
      "grad_norm": 1.017062783241272,
      "learning_rate": 4.00268245215699e-05,
      "loss": 2.9203,
      "step": 308600
    },
    {
      "epoch": 100.06482982171799,
      "grad_norm": 1.0898712873458862,
      "learning_rate": 4.002358092766786e-05,
      "loss": 2.9294,
      "step": 308700
    },
    {
      "epoch": 100.09724473257698,
      "grad_norm": 0.9627083539962769,
      "learning_rate": 4.002033733376582e-05,
      "loss": 2.9272,
      "step": 308800
    },
    {
      "epoch": 100.12965964343599,
      "grad_norm": 1.0886273384094238,
      "learning_rate": 4.001709373986377e-05,
      "loss": 2.9249,
      "step": 308900
    },
    {
      "epoch": 100.16207455429497,
      "grad_norm": 0.974884033203125,
      "learning_rate": 4.001385014596173e-05,
      "loss": 2.9233,
      "step": 309000
    },
    {
      "epoch": 100.19448946515396,
      "grad_norm": 1.0948549509048462,
      "learning_rate": 4.001060655205968e-05,
      "loss": 2.9174,
      "step": 309100
    },
    {
      "epoch": 100.22690437601297,
      "grad_norm": 1.1076455116271973,
      "learning_rate": 4.000736295815764e-05,
      "loss": 2.9253,
      "step": 309200
    },
    {
      "epoch": 100.25931928687196,
      "grad_norm": 0.9672174453735352,
      "learning_rate": 4.00041193642556e-05,
      "loss": 2.9047,
      "step": 309300
    },
    {
      "epoch": 100.29173419773096,
      "grad_norm": 1.0563331842422485,
      "learning_rate": 4.000087577035355e-05,
      "loss": 2.9358,
      "step": 309400
    },
    {
      "epoch": 100.32414910858995,
      "grad_norm": 1.0249346494674683,
      "learning_rate": 3.999763217645151e-05,
      "loss": 2.9096,
      "step": 309500
    },
    {
      "epoch": 100.35656401944895,
      "grad_norm": 1.2471007108688354,
      "learning_rate": 3.999442101848849e-05,
      "loss": 2.9182,
      "step": 309600
    },
    {
      "epoch": 100.38897893030794,
      "grad_norm": 0.9823135733604431,
      "learning_rate": 3.999117742458645e-05,
      "loss": 2.9271,
      "step": 309700
    },
    {
      "epoch": 100.42139384116693,
      "grad_norm": 1.0930284261703491,
      "learning_rate": 3.99879338306844e-05,
      "loss": 2.9364,
      "step": 309800
    },
    {
      "epoch": 100.45380875202594,
      "grad_norm": 1.0045888423919678,
      "learning_rate": 3.998469023678236e-05,
      "loss": 2.9265,
      "step": 309900
    },
    {
      "epoch": 100.48622366288492,
      "grad_norm": 1.1624127626419067,
      "learning_rate": 3.9981446642880316e-05,
      "loss": 2.9192,
      "step": 310000
    },
    {
      "epoch": 100.51863857374393,
      "grad_norm": 1.031137228012085,
      "learning_rate": 3.997820304897827e-05,
      "loss": 2.9117,
      "step": 310100
    },
    {
      "epoch": 100.55105348460292,
      "grad_norm": 1.0984221696853638,
      "learning_rate": 3.997495945507623e-05,
      "loss": 2.9272,
      "step": 310200
    },
    {
      "epoch": 100.5834683954619,
      "grad_norm": 0.9634167551994324,
      "learning_rate": 3.997171586117418e-05,
      "loss": 2.9173,
      "step": 310300
    },
    {
      "epoch": 100.61588330632091,
      "grad_norm": 1.0618972778320312,
      "learning_rate": 3.996847226727214e-05,
      "loss": 2.9257,
      "step": 310400
    },
    {
      "epoch": 100.6482982171799,
      "grad_norm": 1.0499591827392578,
      "learning_rate": 3.9965228673370097e-05,
      "loss": 2.9102,
      "step": 310500
    },
    {
      "epoch": 100.6807131280389,
      "grad_norm": 0.981842041015625,
      "learning_rate": 3.996198507946805e-05,
      "loss": 2.9366,
      "step": 310600
    },
    {
      "epoch": 100.71312803889789,
      "grad_norm": 0.9220919013023376,
      "learning_rate": 3.995874148556601e-05,
      "loss": 2.9325,
      "step": 310700
    },
    {
      "epoch": 100.74554294975688,
      "grad_norm": 1.34172785282135,
      "learning_rate": 3.9955497891663966e-05,
      "loss": 2.9465,
      "step": 310800
    },
    {
      "epoch": 100.77795786061589,
      "grad_norm": 0.9941956996917725,
      "learning_rate": 3.995225429776192e-05,
      "loss": 2.9349,
      "step": 310900
    },
    {
      "epoch": 100.81037277147487,
      "grad_norm": 1.0899080038070679,
      "learning_rate": 3.994901070385988e-05,
      "loss": 2.9311,
      "step": 311000
    },
    {
      "epoch": 100.84278768233388,
      "grad_norm": 1.1235672235488892,
      "learning_rate": 3.9945767109957836e-05,
      "loss": 2.9238,
      "step": 311100
    },
    {
      "epoch": 100.87520259319287,
      "grad_norm": 1.0453957319259644,
      "learning_rate": 3.9942523516055794e-05,
      "loss": 2.9379,
      "step": 311200
    },
    {
      "epoch": 100.90761750405187,
      "grad_norm": 1.0372982025146484,
      "learning_rate": 3.9939279922153746e-05,
      "loss": 2.929,
      "step": 311300
    },
    {
      "epoch": 100.94003241491086,
      "grad_norm": 0.982216477394104,
      "learning_rate": 3.9936036328251705e-05,
      "loss": 2.9142,
      "step": 311400
    },
    {
      "epoch": 100.97244732576985,
      "grad_norm": 0.8871184587478638,
      "learning_rate": 3.9932792734349664e-05,
      "loss": 2.922,
      "step": 311500
    },
    {
      "epoch": 101.0,
      "eval_bleu": 1.1748536939446943,
      "eval_loss": 3.848965883255005,
      "eval_runtime": 3.7305,
      "eval_samples_per_second": 131.885,
      "eval_steps_per_second": 2.144,
      "step": 311585
    },
    {
      "epoch": 101.00486223662885,
      "grad_norm": 0.939047634601593,
      "learning_rate": 3.992954914044762e-05,
      "loss": 2.9317,
      "step": 311600
    },
    {
      "epoch": 101.03727714748784,
      "grad_norm": 0.8923578262329102,
      "learning_rate": 3.9926305546545574e-05,
      "loss": 2.9169,
      "step": 311700
    },
    {
      "epoch": 101.06969205834685,
      "grad_norm": 1.1523507833480835,
      "learning_rate": 3.992306195264353e-05,
      "loss": 2.9258,
      "step": 311800
    },
    {
      "epoch": 101.10210696920583,
      "grad_norm": 1.0501646995544434,
      "learning_rate": 3.991981835874149e-05,
      "loss": 2.9342,
      "step": 311900
    },
    {
      "epoch": 101.13452188006482,
      "grad_norm": 1.0039023160934448,
      "learning_rate": 3.9916574764839444e-05,
      "loss": 2.9211,
      "step": 312000
    },
    {
      "epoch": 101.16693679092383,
      "grad_norm": 1.1430931091308594,
      "learning_rate": 3.99133311709374e-05,
      "loss": 2.9237,
      "step": 312100
    },
    {
      "epoch": 101.19935170178282,
      "grad_norm": 1.131946325302124,
      "learning_rate": 3.9910087577035355e-05,
      "loss": 2.9059,
      "step": 312200
    },
    {
      "epoch": 101.23176661264182,
      "grad_norm": 0.996748149394989,
      "learning_rate": 3.990684398313331e-05,
      "loss": 2.9099,
      "step": 312300
    },
    {
      "epoch": 101.26418152350081,
      "grad_norm": 1.2638863325119019,
      "learning_rate": 3.990360038923127e-05,
      "loss": 2.9142,
      "step": 312400
    },
    {
      "epoch": 101.2965964343598,
      "grad_norm": 1.1564759016036987,
      "learning_rate": 3.9900356795329224e-05,
      "loss": 2.9123,
      "step": 312500
    },
    {
      "epoch": 101.3290113452188,
      "grad_norm": 1.2322096824645996,
      "learning_rate": 3.989711320142718e-05,
      "loss": 2.9233,
      "step": 312600
    },
    {
      "epoch": 101.36142625607779,
      "grad_norm": 0.9583835601806641,
      "learning_rate": 3.989386960752514e-05,
      "loss": 2.9225,
      "step": 312700
    },
    {
      "epoch": 101.3938411669368,
      "grad_norm": 0.9118027091026306,
      "learning_rate": 3.9890626013623093e-05,
      "loss": 2.9252,
      "step": 312800
    },
    {
      "epoch": 101.42625607779578,
      "grad_norm": 1.0375049114227295,
      "learning_rate": 3.988738241972105e-05,
      "loss": 2.9074,
      "step": 312900
    },
    {
      "epoch": 101.45867098865479,
      "grad_norm": 1.2513573169708252,
      "learning_rate": 3.988413882581901e-05,
      "loss": 2.9201,
      "step": 313000
    },
    {
      "epoch": 101.49108589951378,
      "grad_norm": 1.0072742700576782,
      "learning_rate": 3.988089523191696e-05,
      "loss": 2.9287,
      "step": 313100
    },
    {
      "epoch": 101.52350081037277,
      "grad_norm": 1.0457688570022583,
      "learning_rate": 3.987765163801492e-05,
      "loss": 2.9219,
      "step": 313200
    },
    {
      "epoch": 101.55591572123177,
      "grad_norm": 1.1135945320129395,
      "learning_rate": 3.9874408044112874e-05,
      "loss": 2.9296,
      "step": 313300
    },
    {
      "epoch": 101.58833063209076,
      "grad_norm": 1.1063761711120605,
      "learning_rate": 3.987116445021083e-05,
      "loss": 2.9381,
      "step": 313400
    },
    {
      "epoch": 101.62074554294976,
      "grad_norm": 1.0311226844787598,
      "learning_rate": 3.986792085630879e-05,
      "loss": 2.8957,
      "step": 313500
    },
    {
      "epoch": 101.65316045380875,
      "grad_norm": 1.17058265209198,
      "learning_rate": 3.986470969834577e-05,
      "loss": 2.9342,
      "step": 313600
    },
    {
      "epoch": 101.68557536466774,
      "grad_norm": 1.143430471420288,
      "learning_rate": 3.986146610444372e-05,
      "loss": 2.9261,
      "step": 313700
    },
    {
      "epoch": 101.71799027552674,
      "grad_norm": 1.1997164487838745,
      "learning_rate": 3.985822251054168e-05,
      "loss": 2.8955,
      "step": 313800
    },
    {
      "epoch": 101.75040518638573,
      "grad_norm": 1.1111485958099365,
      "learning_rate": 3.985497891663964e-05,
      "loss": 2.916,
      "step": 313900
    },
    {
      "epoch": 101.78282009724474,
      "grad_norm": 1.018471598625183,
      "learning_rate": 3.9851767758676616e-05,
      "loss": 2.9266,
      "step": 314000
    },
    {
      "epoch": 101.81523500810373,
      "grad_norm": 1.0235934257507324,
      "learning_rate": 3.9848556600713594e-05,
      "loss": 2.9316,
      "step": 314100
    },
    {
      "epoch": 101.84764991896272,
      "grad_norm": 1.0536143779754639,
      "learning_rate": 3.9845313006811546e-05,
      "loss": 2.9247,
      "step": 314200
    },
    {
      "epoch": 101.88006482982172,
      "grad_norm": 0.9736931324005127,
      "learning_rate": 3.9842069412909505e-05,
      "loss": 2.9501,
      "step": 314300
    },
    {
      "epoch": 101.91247974068071,
      "grad_norm": 1.0505447387695312,
      "learning_rate": 3.983885825494648e-05,
      "loss": 2.9256,
      "step": 314400
    },
    {
      "epoch": 101.94489465153971,
      "grad_norm": 0.9448298811912537,
      "learning_rate": 3.983564709698346e-05,
      "loss": 2.9291,
      "step": 314500
    },
    {
      "epoch": 101.9773095623987,
      "grad_norm": 0.939163327217102,
      "learning_rate": 3.983240350308142e-05,
      "loss": 2.9374,
      "step": 314600
    },
    {
      "epoch": 102.0,
      "eval_bleu": 1.2117302727221955,
      "eval_loss": 3.852598190307617,
      "eval_runtime": 4.4244,
      "eval_samples_per_second": 111.202,
      "eval_steps_per_second": 1.808,
      "step": 314670
    },
    {
      "epoch": 102.0097244732577,
      "grad_norm": 0.962603747844696,
      "learning_rate": 3.982915990917937e-05,
      "loss": 2.9325,
      "step": 314700
    },
    {
      "epoch": 102.0421393841167,
      "grad_norm": 1.0206074714660645,
      "learning_rate": 3.982591631527733e-05,
      "loss": 2.9368,
      "step": 314800
    },
    {
      "epoch": 102.07455429497568,
      "grad_norm": 1.2612760066986084,
      "learning_rate": 3.982267272137529e-05,
      "loss": 2.9279,
      "step": 314900
    },
    {
      "epoch": 102.10696920583469,
      "grad_norm": 1.1329694986343384,
      "learning_rate": 3.981942912747324e-05,
      "loss": 2.9085,
      "step": 315000
    },
    {
      "epoch": 102.13938411669368,
      "grad_norm": 0.9474468231201172,
      "learning_rate": 3.98161855335712e-05,
      "loss": 2.917,
      "step": 315100
    },
    {
      "epoch": 102.17179902755268,
      "grad_norm": 0.939175546169281,
      "learning_rate": 3.981294193966916e-05,
      "loss": 2.9173,
      "step": 315200
    },
    {
      "epoch": 102.20421393841167,
      "grad_norm": 1.2022812366485596,
      "learning_rate": 3.980969834576711e-05,
      "loss": 2.9121,
      "step": 315300
    },
    {
      "epoch": 102.23662884927066,
      "grad_norm": 1.0734997987747192,
      "learning_rate": 3.980645475186507e-05,
      "loss": 2.9261,
      "step": 315400
    },
    {
      "epoch": 102.26904376012966,
      "grad_norm": 1.113374948501587,
      "learning_rate": 3.980321115796303e-05,
      "loss": 2.9282,
      "step": 315500
    },
    {
      "epoch": 102.30145867098865,
      "grad_norm": 1.1296076774597168,
      "learning_rate": 3.979996756406098e-05,
      "loss": 2.9266,
      "step": 315600
    },
    {
      "epoch": 102.33387358184766,
      "grad_norm": 1.0401183366775513,
      "learning_rate": 3.979672397015894e-05,
      "loss": 2.9208,
      "step": 315700
    },
    {
      "epoch": 102.36628849270664,
      "grad_norm": 0.8977195024490356,
      "learning_rate": 3.979348037625689e-05,
      "loss": 2.909,
      "step": 315800
    },
    {
      "epoch": 102.39870340356563,
      "grad_norm": 1.0688116550445557,
      "learning_rate": 3.979023678235485e-05,
      "loss": 2.9143,
      "step": 315900
    },
    {
      "epoch": 102.43111831442464,
      "grad_norm": 1.2266148328781128,
      "learning_rate": 3.978699318845281e-05,
      "loss": 2.9044,
      "step": 316000
    },
    {
      "epoch": 102.46353322528363,
      "grad_norm": 0.9279797077178955,
      "learning_rate": 3.978374959455076e-05,
      "loss": 2.9208,
      "step": 316100
    },
    {
      "epoch": 102.49594813614263,
      "grad_norm": 1.0240797996520996,
      "learning_rate": 3.978050600064872e-05,
      "loss": 2.9238,
      "step": 316200
    },
    {
      "epoch": 102.52836304700162,
      "grad_norm": 0.9805321097373962,
      "learning_rate": 3.977726240674668e-05,
      "loss": 2.9032,
      "step": 316300
    },
    {
      "epoch": 102.56077795786062,
      "grad_norm": 1.1009987592697144,
      "learning_rate": 3.977401881284463e-05,
      "loss": 2.9248,
      "step": 316400
    },
    {
      "epoch": 102.59319286871961,
      "grad_norm": 0.9310450553894043,
      "learning_rate": 3.977077521894259e-05,
      "loss": 2.9325,
      "step": 316500
    },
    {
      "epoch": 102.6256077795786,
      "grad_norm": 1.123111605644226,
      "learning_rate": 3.9767531625040546e-05,
      "loss": 2.9409,
      "step": 316600
    },
    {
      "epoch": 102.6580226904376,
      "grad_norm": 1.1561620235443115,
      "learning_rate": 3.9764288031138505e-05,
      "loss": 2.9209,
      "step": 316700
    },
    {
      "epoch": 102.6904376012966,
      "grad_norm": 0.9897751212120056,
      "learning_rate": 3.9761044437236464e-05,
      "loss": 2.9112,
      "step": 316800
    },
    {
      "epoch": 102.7228525121556,
      "grad_norm": 1.089489221572876,
      "learning_rate": 3.9757800843334416e-05,
      "loss": 2.9224,
      "step": 316900
    },
    {
      "epoch": 102.75526742301459,
      "grad_norm": 1.1501529216766357,
      "learning_rate": 3.9754557249432375e-05,
      "loss": 2.9244,
      "step": 317000
    },
    {
      "epoch": 102.78768233387358,
      "grad_norm": 1.021471619606018,
      "learning_rate": 3.9751313655530333e-05,
      "loss": 2.9232,
      "step": 317100
    },
    {
      "epoch": 102.82009724473258,
      "grad_norm": 1.1445776224136353,
      "learning_rate": 3.9748070061628285e-05,
      "loss": 2.9205,
      "step": 317200
    },
    {
      "epoch": 102.85251215559157,
      "grad_norm": 0.9767832159996033,
      "learning_rate": 3.9744826467726244e-05,
      "loss": 2.9335,
      "step": 317300
    },
    {
      "epoch": 102.88492706645057,
      "grad_norm": 1.0410999059677124,
      "learning_rate": 3.97415828738242e-05,
      "loss": 2.9027,
      "step": 317400
    },
    {
      "epoch": 102.91734197730956,
      "grad_norm": 1.0193450450897217,
      "learning_rate": 3.9738339279922155e-05,
      "loss": 2.9207,
      "step": 317500
    },
    {
      "epoch": 102.94975688816855,
      "grad_norm": 1.1331828832626343,
      "learning_rate": 3.9735095686020114e-05,
      "loss": 2.9328,
      "step": 317600
    },
    {
      "epoch": 102.98217179902755,
      "grad_norm": 1.1618406772613525,
      "learning_rate": 3.973185209211807e-05,
      "loss": 2.9145,
      "step": 317700
    },
    {
      "epoch": 103.0,
      "eval_bleu": 1.2885384339374004,
      "eval_loss": 3.8556618690490723,
      "eval_runtime": 4.1359,
      "eval_samples_per_second": 118.959,
      "eval_steps_per_second": 1.934,
      "step": 317755
    },
    {
      "epoch": 103.01458670988654,
      "grad_norm": 1.0179592370986938,
      "learning_rate": 3.9728608498216024e-05,
      "loss": 2.9173,
      "step": 317800
    },
    {
      "epoch": 103.04700162074555,
      "grad_norm": 0.9645649194717407,
      "learning_rate": 3.972536490431398e-05,
      "loss": 2.9186,
      "step": 317900
    },
    {
      "epoch": 103.07941653160454,
      "grad_norm": 1.0272555351257324,
      "learning_rate": 3.9722121310411935e-05,
      "loss": 2.9204,
      "step": 318000
    },
    {
      "epoch": 103.11183144246354,
      "grad_norm": 1.033915400505066,
      "learning_rate": 3.9718877716509894e-05,
      "loss": 2.9056,
      "step": 318100
    },
    {
      "epoch": 103.14424635332253,
      "grad_norm": 1.0413094758987427,
      "learning_rate": 3.971563412260785e-05,
      "loss": 2.9295,
      "step": 318200
    },
    {
      "epoch": 103.17666126418152,
      "grad_norm": 0.9712717533111572,
      "learning_rate": 3.9712390528705804e-05,
      "loss": 2.9164,
      "step": 318300
    },
    {
      "epoch": 103.20907617504052,
      "grad_norm": 1.067243218421936,
      "learning_rate": 3.970914693480376e-05,
      "loss": 2.898,
      "step": 318400
    },
    {
      "epoch": 103.24149108589951,
      "grad_norm": 1.10434889793396,
      "learning_rate": 3.970590334090172e-05,
      "loss": 2.9136,
      "step": 318500
    },
    {
      "epoch": 103.27390599675851,
      "grad_norm": 0.9942077398300171,
      "learning_rate": 3.9702659746999674e-05,
      "loss": 2.9228,
      "step": 318600
    },
    {
      "epoch": 103.3063209076175,
      "grad_norm": 1.0049806833267212,
      "learning_rate": 3.969941615309763e-05,
      "loss": 2.9168,
      "step": 318700
    },
    {
      "epoch": 103.3387358184765,
      "grad_norm": 1.2892216444015503,
      "learning_rate": 3.9696172559195585e-05,
      "loss": 2.931,
      "step": 318800
    },
    {
      "epoch": 103.3711507293355,
      "grad_norm": 1.099031925201416,
      "learning_rate": 3.969292896529354e-05,
      "loss": 2.9291,
      "step": 318900
    },
    {
      "epoch": 103.40356564019449,
      "grad_norm": 1.2324012517929077,
      "learning_rate": 3.968971780733052e-05,
      "loss": 2.9184,
      "step": 319000
    },
    {
      "epoch": 103.43598055105349,
      "grad_norm": 1.033300518989563,
      "learning_rate": 3.968647421342848e-05,
      "loss": 2.9191,
      "step": 319100
    },
    {
      "epoch": 103.46839546191248,
      "grad_norm": 1.2241222858428955,
      "learning_rate": 3.968323061952643e-05,
      "loss": 2.925,
      "step": 319200
    },
    {
      "epoch": 103.50081037277147,
      "grad_norm": 1.202937126159668,
      "learning_rate": 3.967998702562439e-05,
      "loss": 2.9372,
      "step": 319300
    },
    {
      "epoch": 103.53322528363047,
      "grad_norm": 1.1271543502807617,
      "learning_rate": 3.967674343172235e-05,
      "loss": 2.9174,
      "step": 319400
    },
    {
      "epoch": 103.56564019448946,
      "grad_norm": 1.0037438869476318,
      "learning_rate": 3.967349983782031e-05,
      "loss": 2.8996,
      "step": 319500
    },
    {
      "epoch": 103.59805510534846,
      "grad_norm": 1.210095763206482,
      "learning_rate": 3.967025624391827e-05,
      "loss": 2.9132,
      "step": 319600
    },
    {
      "epoch": 103.63047001620745,
      "grad_norm": 1.0908042192459106,
      "learning_rate": 3.966701265001622e-05,
      "loss": 2.9065,
      "step": 319700
    },
    {
      "epoch": 103.66288492706646,
      "grad_norm": 1.0020333528518677,
      "learning_rate": 3.966376905611418e-05,
      "loss": 2.9208,
      "step": 319800
    },
    {
      "epoch": 103.69529983792545,
      "grad_norm": 1.022714376449585,
      "learning_rate": 3.9660525462212136e-05,
      "loss": 2.9315,
      "step": 319900
    },
    {
      "epoch": 103.72771474878444,
      "grad_norm": 1.0213955640792847,
      "learning_rate": 3.9657281868310095e-05,
      "loss": 2.9178,
      "step": 320000
    },
    {
      "epoch": 103.76012965964344,
      "grad_norm": 1.152984619140625,
      "learning_rate": 3.965403827440805e-05,
      "loss": 2.9222,
      "step": 320100
    },
    {
      "epoch": 103.79254457050243,
      "grad_norm": 1.0024663209915161,
      "learning_rate": 3.9650794680506006e-05,
      "loss": 2.9031,
      "step": 320200
    },
    {
      "epoch": 103.82495948136143,
      "grad_norm": 1.035940408706665,
      "learning_rate": 3.964755108660396e-05,
      "loss": 2.9136,
      "step": 320300
    },
    {
      "epoch": 103.85737439222042,
      "grad_norm": 1.088823914527893,
      "learning_rate": 3.9644307492701916e-05,
      "loss": 2.9279,
      "step": 320400
    },
    {
      "epoch": 103.88978930307941,
      "grad_norm": 0.9388331174850464,
      "learning_rate": 3.9641063898799875e-05,
      "loss": 2.9253,
      "step": 320500
    },
    {
      "epoch": 103.92220421393841,
      "grad_norm": 1.0712335109710693,
      "learning_rate": 3.963782030489783e-05,
      "loss": 2.9142,
      "step": 320600
    },
    {
      "epoch": 103.9546191247974,
      "grad_norm": 1.1505153179168701,
      "learning_rate": 3.9634576710995786e-05,
      "loss": 2.9179,
      "step": 320700
    },
    {
      "epoch": 103.98703403565641,
      "grad_norm": 0.9290869235992432,
      "learning_rate": 3.9631333117093745e-05,
      "loss": 2.9069,
      "step": 320800
    },
    {
      "epoch": 104.0,
      "eval_bleu": 1.1479520666883256,
      "eval_loss": 3.8519723415374756,
      "eval_runtime": 4.1398,
      "eval_samples_per_second": 118.845,
      "eval_steps_per_second": 1.932,
      "step": 320840
    },
    {
      "epoch": 104.0194489465154,
      "grad_norm": 0.9985394477844238,
      "learning_rate": 3.9628089523191697e-05,
      "loss": 2.8965,
      "step": 320900
    },
    {
      "epoch": 104.05186385737439,
      "grad_norm": 1.0649011135101318,
      "learning_rate": 3.9624845929289655e-05,
      "loss": 2.8851,
      "step": 321000
    },
    {
      "epoch": 104.08427876823339,
      "grad_norm": 1.1228400468826294,
      "learning_rate": 3.962160233538761e-05,
      "loss": 2.9308,
      "step": 321100
    },
    {
      "epoch": 104.11669367909238,
      "grad_norm": 1.238375186920166,
      "learning_rate": 3.9618358741485566e-05,
      "loss": 2.8926,
      "step": 321200
    },
    {
      "epoch": 104.14910858995138,
      "grad_norm": 1.1031169891357422,
      "learning_rate": 3.9615115147583525e-05,
      "loss": 2.9131,
      "step": 321300
    },
    {
      "epoch": 104.18152350081037,
      "grad_norm": 1.1548655033111572,
      "learning_rate": 3.961187155368148e-05,
      "loss": 2.9086,
      "step": 321400
    },
    {
      "epoch": 104.21393841166937,
      "grad_norm": 1.1947413682937622,
      "learning_rate": 3.9608627959779435e-05,
      "loss": 2.8965,
      "step": 321500
    },
    {
      "epoch": 104.24635332252836,
      "grad_norm": 0.8508046865463257,
      "learning_rate": 3.9605384365877394e-05,
      "loss": 2.8995,
      "step": 321600
    },
    {
      "epoch": 104.27876823338735,
      "grad_norm": 1.1821166276931763,
      "learning_rate": 3.9602140771975346e-05,
      "loss": 2.8955,
      "step": 321700
    },
    {
      "epoch": 104.31118314424636,
      "grad_norm": 1.1189215183258057,
      "learning_rate": 3.9598897178073305e-05,
      "loss": 2.9026,
      "step": 321800
    },
    {
      "epoch": 104.34359805510535,
      "grad_norm": 1.024836778640747,
      "learning_rate": 3.9595653584171264e-05,
      "loss": 2.9438,
      "step": 321900
    },
    {
      "epoch": 104.37601296596435,
      "grad_norm": 1.0652916431427002,
      "learning_rate": 3.959240999026922e-05,
      "loss": 2.9345,
      "step": 322000
    },
    {
      "epoch": 104.40842787682334,
      "grad_norm": 1.220931053161621,
      "learning_rate": 3.9589166396367174e-05,
      "loss": 2.9177,
      "step": 322100
    },
    {
      "epoch": 104.44084278768233,
      "grad_norm": 1.04473876953125,
      "learning_rate": 3.958592280246513e-05,
      "loss": 2.9102,
      "step": 322200
    },
    {
      "epoch": 104.47325769854133,
      "grad_norm": 1.0599802732467651,
      "learning_rate": 3.958267920856309e-05,
      "loss": 2.9259,
      "step": 322300
    },
    {
      "epoch": 104.50567260940032,
      "grad_norm": 1.0104732513427734,
      "learning_rate": 3.957943561466105e-05,
      "loss": 2.9364,
      "step": 322400
    },
    {
      "epoch": 104.53808752025932,
      "grad_norm": 0.9654894471168518,
      "learning_rate": 3.9576192020759e-05,
      "loss": 2.8959,
      "step": 322500
    },
    {
      "epoch": 104.57050243111831,
      "grad_norm": 1.0537298917770386,
      "learning_rate": 3.957294842685696e-05,
      "loss": 2.9162,
      "step": 322600
    },
    {
      "epoch": 104.6029173419773,
      "grad_norm": 1.2133184671401978,
      "learning_rate": 3.956970483295492e-05,
      "loss": 2.9296,
      "step": 322700
    },
    {
      "epoch": 104.6353322528363,
      "grad_norm": 1.0568320751190186,
      "learning_rate": 3.956646123905287e-05,
      "loss": 2.9135,
      "step": 322800
    },
    {
      "epoch": 104.6677471636953,
      "grad_norm": 1.107210397720337,
      "learning_rate": 3.956321764515083e-05,
      "loss": 2.9088,
      "step": 322900
    },
    {
      "epoch": 104.7001620745543,
      "grad_norm": 1.0173108577728271,
      "learning_rate": 3.955997405124879e-05,
      "loss": 2.9231,
      "step": 323000
    },
    {
      "epoch": 104.73257698541329,
      "grad_norm": 0.9720263481140137,
      "learning_rate": 3.955673045734674e-05,
      "loss": 2.9154,
      "step": 323100
    },
    {
      "epoch": 104.76499189627229,
      "grad_norm": 1.0393404960632324,
      "learning_rate": 3.95534868634447e-05,
      "loss": 2.9057,
      "step": 323200
    },
    {
      "epoch": 104.79740680713128,
      "grad_norm": 1.0948694944381714,
      "learning_rate": 3.955024326954265e-05,
      "loss": 2.9157,
      "step": 323300
    },
    {
      "epoch": 104.82982171799027,
      "grad_norm": 0.9262309670448303,
      "learning_rate": 3.954699967564061e-05,
      "loss": 2.9355,
      "step": 323400
    },
    {
      "epoch": 104.86223662884927,
      "grad_norm": 1.0822831392288208,
      "learning_rate": 3.954375608173857e-05,
      "loss": 2.9304,
      "step": 323500
    },
    {
      "epoch": 104.89465153970826,
      "grad_norm": 1.111067533493042,
      "learning_rate": 3.954051248783652e-05,
      "loss": 2.9239,
      "step": 323600
    },
    {
      "epoch": 104.92706645056727,
      "grad_norm": 1.1861400604248047,
      "learning_rate": 3.953726889393448e-05,
      "loss": 2.9178,
      "step": 323700
    },
    {
      "epoch": 104.95948136142626,
      "grad_norm": 1.0175005197525024,
      "learning_rate": 3.953402530003244e-05,
      "loss": 2.9173,
      "step": 323800
    },
    {
      "epoch": 104.99189627228525,
      "grad_norm": 0.9643306732177734,
      "learning_rate": 3.953078170613039e-05,
      "loss": 2.9244,
      "step": 323900
    },
    {
      "epoch": 105.0,
      "eval_bleu": 1.047873793715931,
      "eval_loss": 3.853773355484009,
      "eval_runtime": 4.247,
      "eval_samples_per_second": 115.847,
      "eval_steps_per_second": 1.884,
      "step": 323925
    },
    {
      "epoch": 105.02431118314425,
      "grad_norm": 1.0291725397109985,
      "learning_rate": 3.952753811222835e-05,
      "loss": 2.9198,
      "step": 324000
    },
    {
      "epoch": 105.05672609400324,
      "grad_norm": 1.0885218381881714,
      "learning_rate": 3.95242945183263e-05,
      "loss": 2.9016,
      "step": 324100
    },
    {
      "epoch": 105.08914100486224,
      "grad_norm": 0.9615256190299988,
      "learning_rate": 3.952105092442426e-05,
      "loss": 2.9057,
      "step": 324200
    },
    {
      "epoch": 105.12155591572123,
      "grad_norm": 1.1105282306671143,
      "learning_rate": 3.951780733052222e-05,
      "loss": 2.897,
      "step": 324300
    },
    {
      "epoch": 105.15397082658022,
      "grad_norm": 1.034716248512268,
      "learning_rate": 3.951456373662018e-05,
      "loss": 2.8984,
      "step": 324400
    },
    {
      "epoch": 105.18638573743922,
      "grad_norm": 1.1518782377243042,
      "learning_rate": 3.951132014271814e-05,
      "loss": 2.915,
      "step": 324500
    },
    {
      "epoch": 105.21880064829821,
      "grad_norm": 1.087796926498413,
      "learning_rate": 3.950807654881609e-05,
      "loss": 2.9137,
      "step": 324600
    },
    {
      "epoch": 105.25121555915722,
      "grad_norm": 0.9980595707893372,
      "learning_rate": 3.950483295491405e-05,
      "loss": 2.9144,
      "step": 324700
    },
    {
      "epoch": 105.2836304700162,
      "grad_norm": 1.011918544769287,
      "learning_rate": 3.9501589361012006e-05,
      "loss": 2.9155,
      "step": 324800
    },
    {
      "epoch": 105.31604538087521,
      "grad_norm": 1.1799598932266235,
      "learning_rate": 3.9498345767109965e-05,
      "loss": 2.914,
      "step": 324900
    },
    {
      "epoch": 105.3484602917342,
      "grad_norm": 1.0309349298477173,
      "learning_rate": 3.949510217320792e-05,
      "loss": 2.9203,
      "step": 325000
    },
    {
      "epoch": 105.38087520259319,
      "grad_norm": 0.9945153594017029,
      "learning_rate": 3.9491858579305876e-05,
      "loss": 2.9235,
      "step": 325100
    },
    {
      "epoch": 105.41329011345219,
      "grad_norm": 1.2071099281311035,
      "learning_rate": 3.948861498540383e-05,
      "loss": 2.9143,
      "step": 325200
    },
    {
      "epoch": 105.44570502431118,
      "grad_norm": 0.9064913392066956,
      "learning_rate": 3.9485371391501786e-05,
      "loss": 2.8906,
      "step": 325300
    },
    {
      "epoch": 105.47811993517018,
      "grad_norm": 1.1451201438903809,
      "learning_rate": 3.9482127797599745e-05,
      "loss": 2.9086,
      "step": 325400
    },
    {
      "epoch": 105.51053484602917,
      "grad_norm": 1.3702888488769531,
      "learning_rate": 3.94788842036977e-05,
      "loss": 2.9187,
      "step": 325500
    },
    {
      "epoch": 105.54294975688816,
      "grad_norm": 1.1977524757385254,
      "learning_rate": 3.9475640609795656e-05,
      "loss": 2.8942,
      "step": 325600
    },
    {
      "epoch": 105.57536466774717,
      "grad_norm": 0.9953140020370483,
      "learning_rate": 3.9472397015893615e-05,
      "loss": 2.9126,
      "step": 325700
    },
    {
      "epoch": 105.60777957860616,
      "grad_norm": 1.1045552492141724,
      "learning_rate": 3.9469153421991567e-05,
      "loss": 2.922,
      "step": 325800
    },
    {
      "epoch": 105.64019448946516,
      "grad_norm": 0.9364017844200134,
      "learning_rate": 3.9465909828089525e-05,
      "loss": 2.911,
      "step": 325900
    },
    {
      "epoch": 105.67260940032415,
      "grad_norm": 1.0464255809783936,
      "learning_rate": 3.9462666234187484e-05,
      "loss": 2.904,
      "step": 326000
    },
    {
      "epoch": 105.70502431118314,
      "grad_norm": 1.0498296022415161,
      "learning_rate": 3.9459422640285436e-05,
      "loss": 2.9032,
      "step": 326100
    },
    {
      "epoch": 105.73743922204214,
      "grad_norm": 0.9894095659255981,
      "learning_rate": 3.9456179046383395e-05,
      "loss": 2.9181,
      "step": 326200
    },
    {
      "epoch": 105.76985413290113,
      "grad_norm": 1.0773359537124634,
      "learning_rate": 3.945293545248135e-05,
      "loss": 2.922,
      "step": 326300
    },
    {
      "epoch": 105.80226904376013,
      "grad_norm": 1.0886754989624023,
      "learning_rate": 3.9449691858579305e-05,
      "loss": 2.9148,
      "step": 326400
    },
    {
      "epoch": 105.83468395461912,
      "grad_norm": 1.026044249534607,
      "learning_rate": 3.9446448264677264e-05,
      "loss": 2.9015,
      "step": 326500
    },
    {
      "epoch": 105.86709886547813,
      "grad_norm": 1.1499409675598145,
      "learning_rate": 3.9443204670775216e-05,
      "loss": 2.9164,
      "step": 326600
    },
    {
      "epoch": 105.89951377633712,
      "grad_norm": 1.265318512916565,
      "learning_rate": 3.9439961076873175e-05,
      "loss": 2.923,
      "step": 326700
    },
    {
      "epoch": 105.9319286871961,
      "grad_norm": 1.0114262104034424,
      "learning_rate": 3.9436717482971134e-05,
      "loss": 2.9331,
      "step": 326800
    },
    {
      "epoch": 105.96434359805511,
      "grad_norm": 1.0260895490646362,
      "learning_rate": 3.943347388906909e-05,
      "loss": 2.8995,
      "step": 326900
    },
    {
      "epoch": 105.9967585089141,
      "grad_norm": 1.3506721258163452,
      "learning_rate": 3.9430262731106063e-05,
      "loss": 2.9206,
      "step": 327000
    },
    {
      "epoch": 106.0,
      "eval_bleu": 1.0643992774529216,
      "eval_loss": 3.85634708404541,
      "eval_runtime": 4.047,
      "eval_samples_per_second": 121.573,
      "eval_steps_per_second": 1.977,
      "step": 327010
    },
    {
      "epoch": 106.0291734197731,
      "grad_norm": 1.0920989513397217,
      "learning_rate": 3.942701913720402e-05,
      "loss": 2.9204,
      "step": 327100
    },
    {
      "epoch": 106.06158833063209,
      "grad_norm": 1.1285470724105835,
      "learning_rate": 3.942377554330198e-05,
      "loss": 2.9069,
      "step": 327200
    },
    {
      "epoch": 106.09400324149108,
      "grad_norm": 1.1248031854629517,
      "learning_rate": 3.942053194939993e-05,
      "loss": 2.9147,
      "step": 327300
    },
    {
      "epoch": 106.12641815235008,
      "grad_norm": 1.091620922088623,
      "learning_rate": 3.941728835549789e-05,
      "loss": 2.8884,
      "step": 327400
    },
    {
      "epoch": 106.15883306320907,
      "grad_norm": 1.2152080535888672,
      "learning_rate": 3.941404476159585e-05,
      "loss": 2.9046,
      "step": 327500
    },
    {
      "epoch": 106.19124797406808,
      "grad_norm": 0.9852417707443237,
      "learning_rate": 3.941080116769381e-05,
      "loss": 2.8809,
      "step": 327600
    },
    {
      "epoch": 106.22366288492707,
      "grad_norm": 1.172900915145874,
      "learning_rate": 3.940755757379177e-05,
      "loss": 2.896,
      "step": 327700
    },
    {
      "epoch": 106.25607779578606,
      "grad_norm": 1.092315435409546,
      "learning_rate": 3.940431397988972e-05,
      "loss": 2.9002,
      "step": 327800
    },
    {
      "epoch": 106.28849270664506,
      "grad_norm": 1.0884008407592773,
      "learning_rate": 3.940107038598768e-05,
      "loss": 2.9062,
      "step": 327900
    },
    {
      "epoch": 106.32090761750405,
      "grad_norm": 1.026104211807251,
      "learning_rate": 3.939782679208564e-05,
      "loss": 2.9103,
      "step": 328000
    },
    {
      "epoch": 106.35332252836305,
      "grad_norm": 0.9836724996566772,
      "learning_rate": 3.939458319818359e-05,
      "loss": 2.9135,
      "step": 328100
    },
    {
      "epoch": 106.38573743922204,
      "grad_norm": 1.2671231031417847,
      "learning_rate": 3.939133960428155e-05,
      "loss": 2.9155,
      "step": 328200
    },
    {
      "epoch": 106.41815235008104,
      "grad_norm": 1.1359257698059082,
      "learning_rate": 3.938809601037951e-05,
      "loss": 2.9086,
      "step": 328300
    },
    {
      "epoch": 106.45056726094003,
      "grad_norm": 0.9769952893257141,
      "learning_rate": 3.938485241647746e-05,
      "loss": 2.9178,
      "step": 328400
    },
    {
      "epoch": 106.48298217179902,
      "grad_norm": 1.139354944229126,
      "learning_rate": 3.938160882257542e-05,
      "loss": 2.9083,
      "step": 328500
    },
    {
      "epoch": 106.51539708265803,
      "grad_norm": 1.0402342081069946,
      "learning_rate": 3.9378397664612395e-05,
      "loss": 2.9243,
      "step": 328600
    },
    {
      "epoch": 106.54781199351702,
      "grad_norm": 0.9845670461654663,
      "learning_rate": 3.9375154070710354e-05,
      "loss": 2.9288,
      "step": 328700
    },
    {
      "epoch": 106.58022690437602,
      "grad_norm": 1.2986074686050415,
      "learning_rate": 3.9371910476808306e-05,
      "loss": 2.9023,
      "step": 328800
    },
    {
      "epoch": 106.61264181523501,
      "grad_norm": 0.9397072196006775,
      "learning_rate": 3.9368666882906265e-05,
      "loss": 2.9089,
      "step": 328900
    },
    {
      "epoch": 106.645056726094,
      "grad_norm": 1.0380208492279053,
      "learning_rate": 3.936542328900422e-05,
      "loss": 2.9288,
      "step": 329000
    },
    {
      "epoch": 106.677471636953,
      "grad_norm": 1.0777090787887573,
      "learning_rate": 3.9362179695102175e-05,
      "loss": 2.9014,
      "step": 329100
    },
    {
      "epoch": 106.70988654781199,
      "grad_norm": 1.073078989982605,
      "learning_rate": 3.9358936101200134e-05,
      "loss": 2.9113,
      "step": 329200
    },
    {
      "epoch": 106.742301458671,
      "grad_norm": 1.0122684240341187,
      "learning_rate": 3.9355692507298086e-05,
      "loss": 2.8926,
      "step": 329300
    },
    {
      "epoch": 106.77471636952998,
      "grad_norm": 0.9379307627677917,
      "learning_rate": 3.9352448913396045e-05,
      "loss": 2.9011,
      "step": 329400
    },
    {
      "epoch": 106.80713128038897,
      "grad_norm": 0.9608479738235474,
      "learning_rate": 3.9349205319494004e-05,
      "loss": 2.8984,
      "step": 329500
    },
    {
      "epoch": 106.83954619124798,
      "grad_norm": 1.1748613119125366,
      "learning_rate": 3.9345961725591956e-05,
      "loss": 2.9102,
      "step": 329600
    },
    {
      "epoch": 106.87196110210697,
      "grad_norm": 1.0755478143692017,
      "learning_rate": 3.9342718131689914e-05,
      "loss": 2.901,
      "step": 329700
    },
    {
      "epoch": 106.90437601296597,
      "grad_norm": 0.9950270056724548,
      "learning_rate": 3.9339474537787866e-05,
      "loss": 2.9179,
      "step": 329800
    },
    {
      "epoch": 106.93679092382496,
      "grad_norm": 0.9924715161323547,
      "learning_rate": 3.9336230943885825e-05,
      "loss": 2.9326,
      "step": 329900
    },
    {
      "epoch": 106.96920583468396,
      "grad_norm": 1.1154319047927856,
      "learning_rate": 3.9332987349983784e-05,
      "loss": 2.8999,
      "step": 330000
    },
    {
      "epoch": 107.0,
      "eval_bleu": 1.248585152919167,
      "eval_loss": 3.8606350421905518,
      "eval_runtime": 3.9805,
      "eval_samples_per_second": 123.603,
      "eval_steps_per_second": 2.01,
      "step": 330095
    },
    {
      "epoch": 107.00162074554295,
      "grad_norm": 0.9173325300216675,
      "learning_rate": 3.9329743756081736e-05,
      "loss": 2.9136,
      "step": 330100
    },
    {
      "epoch": 107.03403565640194,
      "grad_norm": 1.0336443185806274,
      "learning_rate": 3.9326500162179694e-05,
      "loss": 2.8909,
      "step": 330200
    },
    {
      "epoch": 107.06645056726094,
      "grad_norm": 1.1934492588043213,
      "learning_rate": 3.932325656827765e-05,
      "loss": 2.8977,
      "step": 330300
    },
    {
      "epoch": 107.09886547811993,
      "grad_norm": 1.0699210166931152,
      "learning_rate": 3.932007784625365e-05,
      "loss": 2.8879,
      "step": 330400
    },
    {
      "epoch": 107.13128038897894,
      "grad_norm": 1.0663901567459106,
      "learning_rate": 3.931683425235161e-05,
      "loss": 2.888,
      "step": 330500
    },
    {
      "epoch": 107.16369529983793,
      "grad_norm": 1.2063461542129517,
      "learning_rate": 3.931359065844956e-05,
      "loss": 2.9144,
      "step": 330600
    },
    {
      "epoch": 107.19611021069692,
      "grad_norm": 1.2479051351547241,
      "learning_rate": 3.931034706454752e-05,
      "loss": 2.886,
      "step": 330700
    },
    {
      "epoch": 107.22852512155592,
      "grad_norm": 0.9736348390579224,
      "learning_rate": 3.930710347064548e-05,
      "loss": 2.9181,
      "step": 330800
    },
    {
      "epoch": 107.26094003241491,
      "grad_norm": 0.9968436360359192,
      "learning_rate": 3.930385987674343e-05,
      "loss": 2.897,
      "step": 330900
    },
    {
      "epoch": 107.29335494327391,
      "grad_norm": 1.0692261457443237,
      "learning_rate": 3.930061628284139e-05,
      "loss": 2.8911,
      "step": 331000
    },
    {
      "epoch": 107.3257698541329,
      "grad_norm": 1.150137186050415,
      "learning_rate": 3.929737268893935e-05,
      "loss": 2.8931,
      "step": 331100
    },
    {
      "epoch": 107.35818476499189,
      "grad_norm": 1.2487739324569702,
      "learning_rate": 3.92941290950373e-05,
      "loss": 2.9115,
      "step": 331200
    },
    {
      "epoch": 107.3905996758509,
      "grad_norm": 1.0542635917663574,
      "learning_rate": 3.929088550113526e-05,
      "loss": 2.8992,
      "step": 331300
    },
    {
      "epoch": 107.42301458670988,
      "grad_norm": 1.1565425395965576,
      "learning_rate": 3.928764190723321e-05,
      "loss": 2.906,
      "step": 331400
    },
    {
      "epoch": 107.45542949756889,
      "grad_norm": 1.1018339395523071,
      "learning_rate": 3.928439831333117e-05,
      "loss": 2.9035,
      "step": 331500
    },
    {
      "epoch": 107.48784440842788,
      "grad_norm": 0.9781034588813782,
      "learning_rate": 3.928115471942913e-05,
      "loss": 2.8966,
      "step": 331600
    },
    {
      "epoch": 107.52025931928688,
      "grad_norm": 0.9699769616127014,
      "learning_rate": 3.927791112552709e-05,
      "loss": 2.8975,
      "step": 331700
    },
    {
      "epoch": 107.55267423014587,
      "grad_norm": 0.9897674322128296,
      "learning_rate": 3.9274667531625045e-05,
      "loss": 2.9163,
      "step": 331800
    },
    {
      "epoch": 107.58508914100486,
      "grad_norm": 1.0219775438308716,
      "learning_rate": 3.9271423937723004e-05,
      "loss": 2.9,
      "step": 331900
    },
    {
      "epoch": 107.61750405186386,
      "grad_norm": 1.036410927772522,
      "learning_rate": 3.9268180343820956e-05,
      "loss": 2.9107,
      "step": 332000
    },
    {
      "epoch": 107.64991896272285,
      "grad_norm": 1.0008876323699951,
      "learning_rate": 3.9264936749918915e-05,
      "loss": 2.9227,
      "step": 332100
    },
    {
      "epoch": 107.68233387358185,
      "grad_norm": 1.2520403861999512,
      "learning_rate": 3.9261693156016874e-05,
      "loss": 2.9334,
      "step": 332200
    },
    {
      "epoch": 107.71474878444084,
      "grad_norm": 1.0216894149780273,
      "learning_rate": 3.9258449562114826e-05,
      "loss": 2.894,
      "step": 332300
    },
    {
      "epoch": 107.74716369529983,
      "grad_norm": 0.9305036067962646,
      "learning_rate": 3.9255205968212784e-05,
      "loss": 2.9132,
      "step": 332400
    },
    {
      "epoch": 107.77957860615884,
      "grad_norm": 1.1921958923339844,
      "learning_rate": 3.9251962374310736e-05,
      "loss": 2.9148,
      "step": 332500
    },
    {
      "epoch": 107.81199351701783,
      "grad_norm": 1.0616475343704224,
      "learning_rate": 3.9248718780408695e-05,
      "loss": 2.9269,
      "step": 332600
    },
    {
      "epoch": 107.84440842787683,
      "grad_norm": 1.0612998008728027,
      "learning_rate": 3.9245475186506654e-05,
      "loss": 2.9214,
      "step": 332700
    },
    {
      "epoch": 107.87682333873582,
      "grad_norm": 1.080132246017456,
      "learning_rate": 3.9242231592604606e-05,
      "loss": 2.9084,
      "step": 332800
    },
    {
      "epoch": 107.90923824959481,
      "grad_norm": 1.054539680480957,
      "learning_rate": 3.9238987998702564e-05,
      "loss": 2.9181,
      "step": 332900
    },
    {
      "epoch": 107.94165316045381,
      "grad_norm": 1.1092617511749268,
      "learning_rate": 3.923574440480052e-05,
      "loss": 2.9008,
      "step": 333000
    },
    {
      "epoch": 107.9740680713128,
      "grad_norm": 1.1136406660079956,
      "learning_rate": 3.9232500810898475e-05,
      "loss": 2.9261,
      "step": 333100
    },
    {
      "epoch": 108.0,
      "eval_bleu": 1.085721551470281,
      "eval_loss": 3.8612279891967773,
      "eval_runtime": 3.9941,
      "eval_samples_per_second": 123.182,
      "eval_steps_per_second": 2.003,
      "step": 333180
    },
    {
      "epoch": 108.0064829821718,
      "grad_norm": 1.2519512176513672,
      "learning_rate": 3.9229257216996434e-05,
      "loss": 2.8974,
      "step": 333200
    },
    {
      "epoch": 108.03889789303079,
      "grad_norm": 1.1081796884536743,
      "learning_rate": 3.922601362309439e-05,
      "loss": 2.8908,
      "step": 333300
    },
    {
      "epoch": 108.0713128038898,
      "grad_norm": 1.0646660327911377,
      "learning_rate": 3.9222770029192345e-05,
      "loss": 2.8943,
      "step": 333400
    },
    {
      "epoch": 108.10372771474879,
      "grad_norm": 1.1155277490615845,
      "learning_rate": 3.92195264352903e-05,
      "loss": 2.909,
      "step": 333500
    },
    {
      "epoch": 108.13614262560777,
      "grad_norm": 1.0096156597137451,
      "learning_rate": 3.9216282841388255e-05,
      "loss": 2.9093,
      "step": 333600
    },
    {
      "epoch": 108.16855753646678,
      "grad_norm": 1.1317365169525146,
      "learning_rate": 3.9213039247486214e-05,
      "loss": 2.9156,
      "step": 333700
    },
    {
      "epoch": 108.20097244732577,
      "grad_norm": 1.116964340209961,
      "learning_rate": 3.920979565358417e-05,
      "loss": 2.8793,
      "step": 333800
    },
    {
      "epoch": 108.23338735818477,
      "grad_norm": 1.0563817024230957,
      "learning_rate": 3.9206552059682125e-05,
      "loss": 2.9025,
      "step": 333900
    },
    {
      "epoch": 108.26580226904376,
      "grad_norm": 1.0533229112625122,
      "learning_rate": 3.9203308465780084e-05,
      "loss": 2.8863,
      "step": 334000
    },
    {
      "epoch": 108.29821717990275,
      "grad_norm": 1.2829991579055786,
      "learning_rate": 3.920006487187804e-05,
      "loss": 2.896,
      "step": 334100
    },
    {
      "epoch": 108.33063209076175,
      "grad_norm": 1.197808861732483,
      "learning_rate": 3.919685371391502e-05,
      "loss": 2.9199,
      "step": 334200
    },
    {
      "epoch": 108.36304700162074,
      "grad_norm": 1.043216586112976,
      "learning_rate": 3.919361012001297e-05,
      "loss": 2.8727,
      "step": 334300
    },
    {
      "epoch": 108.39546191247975,
      "grad_norm": 0.9803025126457214,
      "learning_rate": 3.919036652611093e-05,
      "loss": 2.9218,
      "step": 334400
    },
    {
      "epoch": 108.42787682333874,
      "grad_norm": 1.034028172492981,
      "learning_rate": 3.918712293220889e-05,
      "loss": 2.8952,
      "step": 334500
    },
    {
      "epoch": 108.46029173419772,
      "grad_norm": 1.0934098958969116,
      "learning_rate": 3.918387933830684e-05,
      "loss": 2.9007,
      "step": 334600
    },
    {
      "epoch": 108.49270664505673,
      "grad_norm": 1.0104138851165771,
      "learning_rate": 3.91806357444048e-05,
      "loss": 2.896,
      "step": 334700
    },
    {
      "epoch": 108.52512155591572,
      "grad_norm": 1.1836702823638916,
      "learning_rate": 3.917739215050276e-05,
      "loss": 2.9094,
      "step": 334800
    },
    {
      "epoch": 108.55753646677472,
      "grad_norm": 1.2084659337997437,
      "learning_rate": 3.917414855660072e-05,
      "loss": 2.9071,
      "step": 334900
    },
    {
      "epoch": 108.58995137763371,
      "grad_norm": 1.1483030319213867,
      "learning_rate": 3.9170904962698676e-05,
      "loss": 2.8841,
      "step": 335000
    },
    {
      "epoch": 108.62236628849271,
      "grad_norm": 1.0724865198135376,
      "learning_rate": 3.916766136879663e-05,
      "loss": 2.9098,
      "step": 335100
    },
    {
      "epoch": 108.6547811993517,
      "grad_norm": 1.0512924194335938,
      "learning_rate": 3.916441777489459e-05,
      "loss": 2.9016,
      "step": 335200
    },
    {
      "epoch": 108.68719611021069,
      "grad_norm": 1.1663000583648682,
      "learning_rate": 3.9161174180992546e-05,
      "loss": 2.9065,
      "step": 335300
    },
    {
      "epoch": 108.7196110210697,
      "grad_norm": 1.1418273448944092,
      "learning_rate": 3.91579305870905e-05,
      "loss": 2.908,
      "step": 335400
    },
    {
      "epoch": 108.75202593192869,
      "grad_norm": 1.0745625495910645,
      "learning_rate": 3.915468699318846e-05,
      "loss": 2.9089,
      "step": 335500
    },
    {
      "epoch": 108.78444084278769,
      "grad_norm": 1.1839271783828735,
      "learning_rate": 3.9151443399286415e-05,
      "loss": 2.9195,
      "step": 335600
    },
    {
      "epoch": 108.81685575364668,
      "grad_norm": 1.146390676498413,
      "learning_rate": 3.914819980538437e-05,
      "loss": 2.9127,
      "step": 335700
    },
    {
      "epoch": 108.84927066450567,
      "grad_norm": 1.0199763774871826,
      "learning_rate": 3.9144956211482326e-05,
      "loss": 2.9166,
      "step": 335800
    },
    {
      "epoch": 108.88168557536467,
      "grad_norm": 1.1133061647415161,
      "learning_rate": 3.914171261758028e-05,
      "loss": 2.9025,
      "step": 335900
    },
    {
      "epoch": 108.91410048622366,
      "grad_norm": 1.173295021057129,
      "learning_rate": 3.913846902367824e-05,
      "loss": 2.9204,
      "step": 336000
    },
    {
      "epoch": 108.94651539708266,
      "grad_norm": 1.0131289958953857,
      "learning_rate": 3.9135225429776196e-05,
      "loss": 2.8991,
      "step": 336100
    },
    {
      "epoch": 108.97893030794165,
      "grad_norm": 1.1297892332077026,
      "learning_rate": 3.913198183587415e-05,
      "loss": 2.9103,
      "step": 336200
    },
    {
      "epoch": 109.0,
      "eval_bleu": 1.198628474777108,
      "eval_loss": 3.866330623626709,
      "eval_runtime": 4.0896,
      "eval_samples_per_second": 120.305,
      "eval_steps_per_second": 1.956,
      "step": 336265
    },
    {
      "epoch": 109.01134521880064,
      "grad_norm": 1.1432037353515625,
      "learning_rate": 3.9128738241972106e-05,
      "loss": 2.8874,
      "step": 336300
    },
    {
      "epoch": 109.04376012965965,
      "grad_norm": 1.0864289999008179,
      "learning_rate": 3.9125494648070065e-05,
      "loss": 2.904,
      "step": 336400
    },
    {
      "epoch": 109.07617504051863,
      "grad_norm": 1.1671980619430542,
      "learning_rate": 3.912225105416802e-05,
      "loss": 2.8889,
      "step": 336500
    },
    {
      "epoch": 109.10858995137764,
      "grad_norm": 0.9739606380462646,
      "learning_rate": 3.9119007460265976e-05,
      "loss": 2.897,
      "step": 336600
    },
    {
      "epoch": 109.14100486223663,
      "grad_norm": 1.0772868394851685,
      "learning_rate": 3.9115763866363934e-05,
      "loss": 2.9039,
      "step": 336700
    },
    {
      "epoch": 109.17341977309563,
      "grad_norm": 1.1292545795440674,
      "learning_rate": 3.9112520272461886e-05,
      "loss": 2.8835,
      "step": 336800
    },
    {
      "epoch": 109.20583468395462,
      "grad_norm": 1.1131607294082642,
      "learning_rate": 3.9109276678559845e-05,
      "loss": 2.9014,
      "step": 336900
    },
    {
      "epoch": 109.23824959481361,
      "grad_norm": 0.9110896587371826,
      "learning_rate": 3.91060330846578e-05,
      "loss": 2.9178,
      "step": 337000
    },
    {
      "epoch": 109.27066450567261,
      "grad_norm": 1.1967320442199707,
      "learning_rate": 3.9102789490755756e-05,
      "loss": 2.8903,
      "step": 337100
    },
    {
      "epoch": 109.3030794165316,
      "grad_norm": 0.9884259700775146,
      "learning_rate": 3.9099545896853715e-05,
      "loss": 2.9005,
      "step": 337200
    },
    {
      "epoch": 109.3354943273906,
      "grad_norm": 1.0175247192382812,
      "learning_rate": 3.909630230295167e-05,
      "loss": 2.8888,
      "step": 337300
    },
    {
      "epoch": 109.3679092382496,
      "grad_norm": 1.026874303817749,
      "learning_rate": 3.909305870904963e-05,
      "loss": 2.8826,
      "step": 337400
    },
    {
      "epoch": 109.40032414910858,
      "grad_norm": 0.9722518920898438,
      "learning_rate": 3.908981511514759e-05,
      "loss": 2.8955,
      "step": 337500
    },
    {
      "epoch": 109.43273905996759,
      "grad_norm": 1.1174284219741821,
      "learning_rate": 3.908657152124554e-05,
      "loss": 2.889,
      "step": 337600
    },
    {
      "epoch": 109.46515397082658,
      "grad_norm": 0.9975358247756958,
      "learning_rate": 3.90833279273435e-05,
      "loss": 2.9017,
      "step": 337700
    },
    {
      "epoch": 109.49756888168558,
      "grad_norm": 1.0796842575073242,
      "learning_rate": 3.9080084333441453e-05,
      "loss": 2.8958,
      "step": 337800
    },
    {
      "epoch": 109.52998379254457,
      "grad_norm": 1.1672874689102173,
      "learning_rate": 3.907684073953941e-05,
      "loss": 2.8877,
      "step": 337900
    },
    {
      "epoch": 109.56239870340356,
      "grad_norm": 1.1032311916351318,
      "learning_rate": 3.907359714563737e-05,
      "loss": 2.905,
      "step": 338000
    },
    {
      "epoch": 109.59481361426256,
      "grad_norm": 1.071160078048706,
      "learning_rate": 3.907035355173532e-05,
      "loss": 2.9013,
      "step": 338100
    },
    {
      "epoch": 109.62722852512155,
      "grad_norm": 0.9202460050582886,
      "learning_rate": 3.906710995783328e-05,
      "loss": 2.8983,
      "step": 338200
    },
    {
      "epoch": 109.65964343598056,
      "grad_norm": 1.0712664127349854,
      "learning_rate": 3.906386636393124e-05,
      "loss": 2.9005,
      "step": 338300
    },
    {
      "epoch": 109.69205834683954,
      "grad_norm": 0.9984357953071594,
      "learning_rate": 3.906062277002919e-05,
      "loss": 2.8995,
      "step": 338400
    },
    {
      "epoch": 109.72447325769855,
      "grad_norm": 0.9751498699188232,
      "learning_rate": 3.905737917612715e-05,
      "loss": 2.9058,
      "step": 338500
    },
    {
      "epoch": 109.75688816855754,
      "grad_norm": 1.0863593816757202,
      "learning_rate": 3.905413558222511e-05,
      "loss": 2.9117,
      "step": 338600
    },
    {
      "epoch": 109.78930307941653,
      "grad_norm": 1.0016250610351562,
      "learning_rate": 3.905089198832306e-05,
      "loss": 2.8827,
      "step": 338700
    },
    {
      "epoch": 109.82171799027553,
      "grad_norm": 0.9621082544326782,
      "learning_rate": 3.904764839442102e-05,
      "loss": 2.9055,
      "step": 338800
    },
    {
      "epoch": 109.85413290113452,
      "grad_norm": 1.173016905784607,
      "learning_rate": 3.904440480051897e-05,
      "loss": 2.9061,
      "step": 338900
    },
    {
      "epoch": 109.88654781199352,
      "grad_norm": 1.0745792388916016,
      "learning_rate": 3.904116120661693e-05,
      "loss": 2.9037,
      "step": 339000
    },
    {
      "epoch": 109.91896272285251,
      "grad_norm": 1.2553642988204956,
      "learning_rate": 3.903791761271489e-05,
      "loss": 2.9341,
      "step": 339100
    },
    {
      "epoch": 109.9513776337115,
      "grad_norm": 1.062501311302185,
      "learning_rate": 3.903467401881284e-05,
      "loss": 2.9162,
      "step": 339200
    },
    {
      "epoch": 109.9837925445705,
      "grad_norm": 0.9771682620048523,
      "learning_rate": 3.90314304249108e-05,
      "loss": 2.9126,
      "step": 339300
    },
    {
      "epoch": 110.0,
      "eval_bleu": 1.4015406655439022,
      "eval_loss": 3.8653438091278076,
      "eval_runtime": 4.1752,
      "eval_samples_per_second": 117.838,
      "eval_steps_per_second": 1.916,
      "step": 339350
    },
    {
      "epoch": 110.0162074554295,
      "grad_norm": 1.1981176137924194,
      "learning_rate": 3.902818683100876e-05,
      "loss": 2.886,
      "step": 339400
    },
    {
      "epoch": 110.0486223662885,
      "grad_norm": 1.2866569757461548,
      "learning_rate": 3.902494323710671e-05,
      "loss": 2.8652,
      "step": 339500
    },
    {
      "epoch": 110.08103727714749,
      "grad_norm": 0.966680645942688,
      "learning_rate": 3.902169964320467e-05,
      "loss": 2.9016,
      "step": 339600
    },
    {
      "epoch": 110.11345218800648,
      "grad_norm": 1.080859661102295,
      "learning_rate": 3.901845604930263e-05,
      "loss": 2.88,
      "step": 339700
    },
    {
      "epoch": 110.14586709886548,
      "grad_norm": 1.0016987323760986,
      "learning_rate": 3.901521245540059e-05,
      "loss": 2.894,
      "step": 339800
    },
    {
      "epoch": 110.17828200972447,
      "grad_norm": 1.0254384279251099,
      "learning_rate": 3.9011968861498546e-05,
      "loss": 2.8939,
      "step": 339900
    },
    {
      "epoch": 110.21069692058347,
      "grad_norm": 1.2826917171478271,
      "learning_rate": 3.90087252675965e-05,
      "loss": 2.893,
      "step": 340000
    },
    {
      "epoch": 110.24311183144246,
      "grad_norm": 1.1120725870132446,
      "learning_rate": 3.900548167369446e-05,
      "loss": 2.8874,
      "step": 340100
    },
    {
      "epoch": 110.27552674230145,
      "grad_norm": 1.1935303211212158,
      "learning_rate": 3.9002238079792416e-05,
      "loss": 2.8971,
      "step": 340200
    },
    {
      "epoch": 110.30794165316046,
      "grad_norm": 1.1044574975967407,
      "learning_rate": 3.899902692182939e-05,
      "loss": 2.8856,
      "step": 340300
    },
    {
      "epoch": 110.34035656401944,
      "grad_norm": 1.0841416120529175,
      "learning_rate": 3.8995783327927346e-05,
      "loss": 2.9015,
      "step": 340400
    },
    {
      "epoch": 110.37277147487845,
      "grad_norm": 1.2178646326065063,
      "learning_rate": 3.8992539734025304e-05,
      "loss": 2.9081,
      "step": 340500
    },
    {
      "epoch": 110.40518638573744,
      "grad_norm": 0.9230843782424927,
      "learning_rate": 3.898929614012326e-05,
      "loss": 2.9122,
      "step": 340600
    },
    {
      "epoch": 110.43760129659644,
      "grad_norm": 1.1665133237838745,
      "learning_rate": 3.8986052546221215e-05,
      "loss": 2.8987,
      "step": 340700
    },
    {
      "epoch": 110.47001620745543,
      "grad_norm": 1.1478203535079956,
      "learning_rate": 3.8982808952319174e-05,
      "loss": 2.8918,
      "step": 340800
    },
    {
      "epoch": 110.50243111831442,
      "grad_norm": 0.9468987584114075,
      "learning_rate": 3.897956535841713e-05,
      "loss": 2.903,
      "step": 340900
    },
    {
      "epoch": 110.53484602917342,
      "grad_norm": 1.0238537788391113,
      "learning_rate": 3.8976321764515085e-05,
      "loss": 2.8989,
      "step": 341000
    },
    {
      "epoch": 110.56726094003241,
      "grad_norm": 1.019299030303955,
      "learning_rate": 3.897307817061304e-05,
      "loss": 2.9188,
      "step": 341100
    },
    {
      "epoch": 110.59967585089142,
      "grad_norm": 1.093170404434204,
      "learning_rate": 3.8969834576710995e-05,
      "loss": 2.9216,
      "step": 341200
    },
    {
      "epoch": 110.6320907617504,
      "grad_norm": 1.0755345821380615,
      "learning_rate": 3.8966590982808954e-05,
      "loss": 2.9195,
      "step": 341300
    },
    {
      "epoch": 110.6645056726094,
      "grad_norm": 1.1204427480697632,
      "learning_rate": 3.896334738890691e-05,
      "loss": 2.896,
      "step": 341400
    },
    {
      "epoch": 110.6969205834684,
      "grad_norm": 1.1815770864486694,
      "learning_rate": 3.8960103795004865e-05,
      "loss": 2.9134,
      "step": 341500
    },
    {
      "epoch": 110.72933549432739,
      "grad_norm": 1.0266629457473755,
      "learning_rate": 3.8956860201102823e-05,
      "loss": 2.8934,
      "step": 341600
    },
    {
      "epoch": 110.76175040518639,
      "grad_norm": 1.0525213479995728,
      "learning_rate": 3.895361660720078e-05,
      "loss": 2.8992,
      "step": 341700
    },
    {
      "epoch": 110.79416531604538,
      "grad_norm": 1.0553064346313477,
      "learning_rate": 3.8950373013298734e-05,
      "loss": 2.879,
      "step": 341800
    },
    {
      "epoch": 110.82658022690438,
      "grad_norm": 1.0167582035064697,
      "learning_rate": 3.894712941939669e-05,
      "loss": 2.9103,
      "step": 341900
    },
    {
      "epoch": 110.85899513776337,
      "grad_norm": 1.0903979539871216,
      "learning_rate": 3.894388582549465e-05,
      "loss": 2.8955,
      "step": 342000
    },
    {
      "epoch": 110.89141004862236,
      "grad_norm": 1.0032631158828735,
      "learning_rate": 3.8940642231592604e-05,
      "loss": 2.9057,
      "step": 342100
    },
    {
      "epoch": 110.92382495948137,
      "grad_norm": 1.1307859420776367,
      "learning_rate": 3.893739863769056e-05,
      "loss": 2.8941,
      "step": 342200
    },
    {
      "epoch": 110.95623987034035,
      "grad_norm": 1.0440804958343506,
      "learning_rate": 3.8934155043788514e-05,
      "loss": 2.902,
      "step": 342300
    },
    {
      "epoch": 110.98865478119936,
      "grad_norm": 1.1893051862716675,
      "learning_rate": 3.893091144988647e-05,
      "loss": 2.9016,
      "step": 342400
    },
    {
      "epoch": 111.0,
      "eval_bleu": 0.997628300185729,
      "eval_loss": 3.862598419189453,
      "eval_runtime": 3.9361,
      "eval_samples_per_second": 124.998,
      "eval_steps_per_second": 2.032,
      "step": 342435
    },
    {
      "epoch": 111.02106969205835,
      "grad_norm": 1.0446231365203857,
      "learning_rate": 3.892766785598443e-05,
      "loss": 2.8863,
      "step": 342500
    },
    {
      "epoch": 111.05348460291734,
      "grad_norm": 1.168972134590149,
      "learning_rate": 3.892442426208239e-05,
      "loss": 2.8918,
      "step": 342600
    },
    {
      "epoch": 111.08589951377634,
      "grad_norm": 0.9712740778923035,
      "learning_rate": 3.892118066818034e-05,
      "loss": 2.8956,
      "step": 342700
    },
    {
      "epoch": 111.11831442463533,
      "grad_norm": 1.1018251180648804,
      "learning_rate": 3.89179370742783e-05,
      "loss": 2.91,
      "step": 342800
    },
    {
      "epoch": 111.15072933549433,
      "grad_norm": 0.975943922996521,
      "learning_rate": 3.891469348037626e-05,
      "loss": 2.8968,
      "step": 342900
    },
    {
      "epoch": 111.18314424635332,
      "grad_norm": 1.0219591856002808,
      "learning_rate": 3.891144988647422e-05,
      "loss": 2.8979,
      "step": 343000
    },
    {
      "epoch": 111.21555915721231,
      "grad_norm": 1.1087698936462402,
      "learning_rate": 3.890820629257217e-05,
      "loss": 2.8901,
      "step": 343100
    },
    {
      "epoch": 111.24797406807131,
      "grad_norm": 1.1784696578979492,
      "learning_rate": 3.890496269867013e-05,
      "loss": 2.88,
      "step": 343200
    },
    {
      "epoch": 111.2803889789303,
      "grad_norm": 1.370374321937561,
      "learning_rate": 3.890171910476809e-05,
      "loss": 2.8813,
      "step": 343300
    },
    {
      "epoch": 111.31280388978931,
      "grad_norm": 1.0077400207519531,
      "learning_rate": 3.889847551086604e-05,
      "loss": 2.9028,
      "step": 343400
    },
    {
      "epoch": 111.3452188006483,
      "grad_norm": 1.1744959354400635,
      "learning_rate": 3.8895231916964e-05,
      "loss": 2.9147,
      "step": 343500
    },
    {
      "epoch": 111.37763371150729,
      "grad_norm": 1.0652296543121338,
      "learning_rate": 3.889198832306196e-05,
      "loss": 2.9023,
      "step": 343600
    },
    {
      "epoch": 111.41004862236629,
      "grad_norm": 1.1199744939804077,
      "learning_rate": 3.888874472915991e-05,
      "loss": 2.8854,
      "step": 343700
    },
    {
      "epoch": 111.44246353322528,
      "grad_norm": 1.0838795900344849,
      "learning_rate": 3.888550113525787e-05,
      "loss": 2.8842,
      "step": 343800
    },
    {
      "epoch": 111.47487844408428,
      "grad_norm": 1.0250319242477417,
      "learning_rate": 3.888225754135583e-05,
      "loss": 2.8852,
      "step": 343900
    },
    {
      "epoch": 111.50729335494327,
      "grad_norm": 1.1553459167480469,
      "learning_rate": 3.887901394745378e-05,
      "loss": 2.8877,
      "step": 344000
    },
    {
      "epoch": 111.53970826580228,
      "grad_norm": 1.0431182384490967,
      "learning_rate": 3.887577035355174e-05,
      "loss": 2.8896,
      "step": 344100
    },
    {
      "epoch": 111.57212317666126,
      "grad_norm": 1.0565319061279297,
      "learning_rate": 3.887252675964969e-05,
      "loss": 2.8753,
      "step": 344200
    },
    {
      "epoch": 111.60453808752025,
      "grad_norm": 0.9911031126976013,
      "learning_rate": 3.8869315601686674e-05,
      "loss": 2.9037,
      "step": 344300
    },
    {
      "epoch": 111.63695299837926,
      "grad_norm": 1.1793040037155151,
      "learning_rate": 3.8866072007784626e-05,
      "loss": 2.9189,
      "step": 344400
    },
    {
      "epoch": 111.66936790923825,
      "grad_norm": 1.094494104385376,
      "learning_rate": 3.8862828413882585e-05,
      "loss": 2.9002,
      "step": 344500
    },
    {
      "epoch": 111.70178282009725,
      "grad_norm": 1.0748759508132935,
      "learning_rate": 3.885958481998054e-05,
      "loss": 2.909,
      "step": 344600
    },
    {
      "epoch": 111.73419773095624,
      "grad_norm": 0.9466596245765686,
      "learning_rate": 3.8856341226078496e-05,
      "loss": 2.899,
      "step": 344700
    },
    {
      "epoch": 111.76661264181523,
      "grad_norm": 1.040631651878357,
      "learning_rate": 3.8853097632176455e-05,
      "loss": 2.8793,
      "step": 344800
    },
    {
      "epoch": 111.79902755267423,
      "grad_norm": 1.0692064762115479,
      "learning_rate": 3.8849854038274407e-05,
      "loss": 2.9086,
      "step": 344900
    },
    {
      "epoch": 111.83144246353322,
      "grad_norm": 1.0064492225646973,
      "learning_rate": 3.8846610444372365e-05,
      "loss": 2.9011,
      "step": 345000
    },
    {
      "epoch": 111.86385737439223,
      "grad_norm": 1.014865756034851,
      "learning_rate": 3.8843366850470324e-05,
      "loss": 2.8819,
      "step": 345100
    },
    {
      "epoch": 111.89627228525121,
      "grad_norm": 1.1424047946929932,
      "learning_rate": 3.8840123256568276e-05,
      "loss": 2.9039,
      "step": 345200
    },
    {
      "epoch": 111.9286871961102,
      "grad_norm": 1.1314003467559814,
      "learning_rate": 3.8836879662666235e-05,
      "loss": 2.8884,
      "step": 345300
    },
    {
      "epoch": 111.96110210696921,
      "grad_norm": 1.0592845678329468,
      "learning_rate": 3.8833636068764193e-05,
      "loss": 2.906,
      "step": 345400
    },
    {
      "epoch": 111.9935170178282,
      "grad_norm": 1.1323903799057007,
      "learning_rate": 3.8830392474862145e-05,
      "loss": 2.8975,
      "step": 345500
    },
    {
      "epoch": 112.0,
      "eval_bleu": 1.2939530809745121,
      "eval_loss": 3.8710975646972656,
      "eval_runtime": 4.4064,
      "eval_samples_per_second": 111.656,
      "eval_steps_per_second": 1.816,
      "step": 345520
    },
    {
      "epoch": 112.0259319286872,
      "grad_norm": 1.0626951456069946,
      "learning_rate": 3.8827148880960104e-05,
      "loss": 2.8872,
      "step": 345600
    },
    {
      "epoch": 112.05834683954619,
      "grad_norm": 1.1637097597122192,
      "learning_rate": 3.882390528705806e-05,
      "loss": 2.882,
      "step": 345700
    },
    {
      "epoch": 112.09076175040519,
      "grad_norm": 1.0297378301620483,
      "learning_rate": 3.882066169315602e-05,
      "loss": 2.8858,
      "step": 345800
    },
    {
      "epoch": 112.12317666126418,
      "grad_norm": 1.3797341585159302,
      "learning_rate": 3.881741809925398e-05,
      "loss": 2.9075,
      "step": 345900
    },
    {
      "epoch": 112.15559157212317,
      "grad_norm": 0.9701129794120789,
      "learning_rate": 3.881417450535193e-05,
      "loss": 2.9002,
      "step": 346000
    },
    {
      "epoch": 112.18800648298217,
      "grad_norm": 1.031058669090271,
      "learning_rate": 3.881093091144989e-05,
      "loss": 2.8876,
      "step": 346100
    },
    {
      "epoch": 112.22042139384116,
      "grad_norm": 1.203894853591919,
      "learning_rate": 3.880768731754785e-05,
      "loss": 2.8895,
      "step": 346200
    },
    {
      "epoch": 112.25283630470017,
      "grad_norm": 1.0211331844329834,
      "learning_rate": 3.880447615958482e-05,
      "loss": 2.8797,
      "step": 346300
    },
    {
      "epoch": 112.28525121555916,
      "grad_norm": 1.193428874015808,
      "learning_rate": 3.880123256568278e-05,
      "loss": 2.8933,
      "step": 346400
    },
    {
      "epoch": 112.31766612641815,
      "grad_norm": 1.1320027112960815,
      "learning_rate": 3.879798897178074e-05,
      "loss": 2.8705,
      "step": 346500
    },
    {
      "epoch": 112.35008103727715,
      "grad_norm": 0.9880766868591309,
      "learning_rate": 3.87947453778787e-05,
      "loss": 2.8831,
      "step": 346600
    },
    {
      "epoch": 112.38249594813614,
      "grad_norm": 1.0918532609939575,
      "learning_rate": 3.879150178397665e-05,
      "loss": 2.8916,
      "step": 346700
    },
    {
      "epoch": 112.41491085899514,
      "grad_norm": 1.2185465097427368,
      "learning_rate": 3.878825819007461e-05,
      "loss": 2.8864,
      "step": 346800
    },
    {
      "epoch": 112.44732576985413,
      "grad_norm": 1.0806238651275635,
      "learning_rate": 3.878501459617256e-05,
      "loss": 2.8789,
      "step": 346900
    },
    {
      "epoch": 112.47974068071312,
      "grad_norm": 0.9458813071250916,
      "learning_rate": 3.878177100227052e-05,
      "loss": 2.8948,
      "step": 347000
    },
    {
      "epoch": 112.51215559157212,
      "grad_norm": 0.9862857460975647,
      "learning_rate": 3.877852740836848e-05,
      "loss": 2.886,
      "step": 347100
    },
    {
      "epoch": 112.54457050243111,
      "grad_norm": 1.0656015872955322,
      "learning_rate": 3.877528381446643e-05,
      "loss": 2.8663,
      "step": 347200
    },
    {
      "epoch": 112.57698541329012,
      "grad_norm": 1.081067681312561,
      "learning_rate": 3.877204022056439e-05,
      "loss": 2.917,
      "step": 347300
    },
    {
      "epoch": 112.6094003241491,
      "grad_norm": 1.1467446088790894,
      "learning_rate": 3.876879662666235e-05,
      "loss": 2.9012,
      "step": 347400
    },
    {
      "epoch": 112.64181523500811,
      "grad_norm": 1.0153377056121826,
      "learning_rate": 3.87655530327603e-05,
      "loss": 2.8976,
      "step": 347500
    },
    {
      "epoch": 112.6742301458671,
      "grad_norm": 1.1380029916763306,
      "learning_rate": 3.876230943885826e-05,
      "loss": 2.8926,
      "step": 347600
    },
    {
      "epoch": 112.70664505672609,
      "grad_norm": 1.0633902549743652,
      "learning_rate": 3.8759065844956216e-05,
      "loss": 2.8909,
      "step": 347700
    },
    {
      "epoch": 112.73905996758509,
      "grad_norm": 0.9647023677825928,
      "learning_rate": 3.875582225105417e-05,
      "loss": 2.869,
      "step": 347800
    },
    {
      "epoch": 112.77147487844408,
      "grad_norm": 1.0157570838928223,
      "learning_rate": 3.875257865715213e-05,
      "loss": 2.8928,
      "step": 347900
    },
    {
      "epoch": 112.80388978930308,
      "grad_norm": 1.1606531143188477,
      "learning_rate": 3.874933506325008e-05,
      "loss": 2.9178,
      "step": 348000
    },
    {
      "epoch": 112.83630470016207,
      "grad_norm": 0.9501543045043945,
      "learning_rate": 3.874609146934804e-05,
      "loss": 2.891,
      "step": 348100
    },
    {
      "epoch": 112.86871961102106,
      "grad_norm": 1.036684274673462,
      "learning_rate": 3.8742847875445996e-05,
      "loss": 2.8784,
      "step": 348200
    },
    {
      "epoch": 112.90113452188007,
      "grad_norm": 1.3510714769363403,
      "learning_rate": 3.8739636717482974e-05,
      "loss": 2.9085,
      "step": 348300
    },
    {
      "epoch": 112.93354943273906,
      "grad_norm": 1.0938878059387207,
      "learning_rate": 3.8736393123580926e-05,
      "loss": 2.9029,
      "step": 348400
    },
    {
      "epoch": 112.96596434359806,
      "grad_norm": 1.0693333148956299,
      "learning_rate": 3.8733181965617904e-05,
      "loss": 2.9057,
      "step": 348500
    },
    {
      "epoch": 112.99837925445705,
      "grad_norm": 1.1047972440719604,
      "learning_rate": 3.872993837171586e-05,
      "loss": 2.9101,
      "step": 348600
    },
    {
      "epoch": 113.0,
      "eval_bleu": 1.1644153851151442,
      "eval_loss": 3.866483688354492,
      "eval_runtime": 4.1691,
      "eval_samples_per_second": 118.012,
      "eval_steps_per_second": 1.919,
      "step": 348605
    },
    {
      "epoch": 113.03079416531604,
      "grad_norm": 1.3321634531021118,
      "learning_rate": 3.872669477781382e-05,
      "loss": 2.8726,
      "step": 348700
    },
    {
      "epoch": 113.06320907617504,
      "grad_norm": 1.1086817979812622,
      "learning_rate": 3.872345118391177e-05,
      "loss": 2.8744,
      "step": 348800
    },
    {
      "epoch": 113.09562398703403,
      "grad_norm": 1.0444819927215576,
      "learning_rate": 3.872020759000973e-05,
      "loss": 2.9035,
      "step": 348900
    },
    {
      "epoch": 113.12803889789303,
      "grad_norm": 0.9905822277069092,
      "learning_rate": 3.871696399610769e-05,
      "loss": 2.8815,
      "step": 349000
    },
    {
      "epoch": 113.16045380875202,
      "grad_norm": 1.0383613109588623,
      "learning_rate": 3.871372040220564e-05,
      "loss": 2.9028,
      "step": 349100
    },
    {
      "epoch": 113.19286871961103,
      "grad_norm": 1.108712077140808,
      "learning_rate": 3.87104768083036e-05,
      "loss": 2.8895,
      "step": 349200
    },
    {
      "epoch": 113.22528363047002,
      "grad_norm": 1.2700796127319336,
      "learning_rate": 3.870723321440156e-05,
      "loss": 2.8729,
      "step": 349300
    },
    {
      "epoch": 113.257698541329,
      "grad_norm": 0.9344261288642883,
      "learning_rate": 3.870398962049951e-05,
      "loss": 2.9177,
      "step": 349400
    },
    {
      "epoch": 113.29011345218801,
      "grad_norm": 1.185096025466919,
      "learning_rate": 3.870074602659747e-05,
      "loss": 2.8756,
      "step": 349500
    },
    {
      "epoch": 113.322528363047,
      "grad_norm": 1.1129703521728516,
      "learning_rate": 3.869750243269542e-05,
      "loss": 2.8817,
      "step": 349600
    },
    {
      "epoch": 113.354943273906,
      "grad_norm": 1.0925018787384033,
      "learning_rate": 3.869425883879338e-05,
      "loss": 2.8994,
      "step": 349700
    },
    {
      "epoch": 113.38735818476499,
      "grad_norm": 1.0987244844436646,
      "learning_rate": 3.869101524489134e-05,
      "loss": 2.89,
      "step": 349800
    },
    {
      "epoch": 113.41977309562398,
      "grad_norm": 1.1661028861999512,
      "learning_rate": 3.86877716509893e-05,
      "loss": 2.8934,
      "step": 349900
    },
    {
      "epoch": 113.45218800648298,
      "grad_norm": 1.0239418745040894,
      "learning_rate": 3.868452805708726e-05,
      "loss": 2.8892,
      "step": 350000
    },
    {
      "epoch": 113.48460291734197,
      "grad_norm": 0.9649352431297302,
      "learning_rate": 3.868128446318522e-05,
      "loss": 2.8907,
      "step": 350100
    },
    {
      "epoch": 113.51701782820098,
      "grad_norm": 1.05093252658844,
      "learning_rate": 3.867804086928317e-05,
      "loss": 2.8947,
      "step": 350200
    },
    {
      "epoch": 113.54943273905997,
      "grad_norm": 1.285279393196106,
      "learning_rate": 3.867479727538113e-05,
      "loss": 2.8919,
      "step": 350300
    },
    {
      "epoch": 113.58184764991896,
      "grad_norm": 1.2642030715942383,
      "learning_rate": 3.867155368147908e-05,
      "loss": 2.8745,
      "step": 350400
    },
    {
      "epoch": 113.61426256077796,
      "grad_norm": 1.3518341779708862,
      "learning_rate": 3.866831008757704e-05,
      "loss": 2.8917,
      "step": 350500
    },
    {
      "epoch": 113.64667747163695,
      "grad_norm": 1.07964026927948,
      "learning_rate": 3.8665066493675e-05,
      "loss": 2.8824,
      "step": 350600
    },
    {
      "epoch": 113.67909238249595,
      "grad_norm": 1.033176302909851,
      "learning_rate": 3.866182289977295e-05,
      "loss": 2.89,
      "step": 350700
    },
    {
      "epoch": 113.71150729335494,
      "grad_norm": 1.0910464525222778,
      "learning_rate": 3.865857930587091e-05,
      "loss": 2.8847,
      "step": 350800
    },
    {
      "epoch": 113.74392220421394,
      "grad_norm": 0.9823815226554871,
      "learning_rate": 3.8655335711968866e-05,
      "loss": 2.8972,
      "step": 350900
    },
    {
      "epoch": 113.77633711507293,
      "grad_norm": 1.066447377204895,
      "learning_rate": 3.865209211806682e-05,
      "loss": 2.8926,
      "step": 351000
    },
    {
      "epoch": 113.80875202593192,
      "grad_norm": 1.0823431015014648,
      "learning_rate": 3.864884852416478e-05,
      "loss": 2.9011,
      "step": 351100
    },
    {
      "epoch": 113.84116693679093,
      "grad_norm": 1.0818849802017212,
      "learning_rate": 3.8645604930262736e-05,
      "loss": 2.9107,
      "step": 351200
    },
    {
      "epoch": 113.87358184764992,
      "grad_norm": 1.1944459676742554,
      "learning_rate": 3.864236133636069e-05,
      "loss": 2.8809,
      "step": 351300
    },
    {
      "epoch": 113.90599675850892,
      "grad_norm": 0.9237092733383179,
      "learning_rate": 3.8639117742458646e-05,
      "loss": 2.8821,
      "step": 351400
    },
    {
      "epoch": 113.93841166936791,
      "grad_norm": 1.0006684064865112,
      "learning_rate": 3.86358741485566e-05,
      "loss": 2.8847,
      "step": 351500
    },
    {
      "epoch": 113.9708265802269,
      "grad_norm": 1.269822597503662,
      "learning_rate": 3.863263055465456e-05,
      "loss": 2.8889,
      "step": 351600
    },
    {
      "epoch": 114.0,
      "eval_bleu": 1.3160951344769423,
      "eval_loss": 3.86838436126709,
      "eval_runtime": 4.1283,
      "eval_samples_per_second": 119.178,
      "eval_steps_per_second": 1.938,
      "step": 351690
    },
    {
      "epoch": 114.0032414910859,
      "grad_norm": 1.1903473138809204,
      "learning_rate": 3.8629386960752516e-05,
      "loss": 2.8942,
      "step": 351700
    },
    {
      "epoch": 114.03565640194489,
      "grad_norm": 1.032131314277649,
      "learning_rate": 3.862614336685047e-05,
      "loss": 2.8783,
      "step": 351800
    },
    {
      "epoch": 114.0680713128039,
      "grad_norm": 1.0478050708770752,
      "learning_rate": 3.8622899772948427e-05,
      "loss": 2.8644,
      "step": 351900
    },
    {
      "epoch": 114.10048622366288,
      "grad_norm": 1.1397897005081177,
      "learning_rate": 3.8619656179046385e-05,
      "loss": 2.8734,
      "step": 352000
    },
    {
      "epoch": 114.13290113452187,
      "grad_norm": 1.3001631498336792,
      "learning_rate": 3.861641258514434e-05,
      "loss": 2.8734,
      "step": 352100
    },
    {
      "epoch": 114.16531604538088,
      "grad_norm": 1.2059046030044556,
      "learning_rate": 3.8613168991242296e-05,
      "loss": 2.8998,
      "step": 352200
    },
    {
      "epoch": 114.19773095623987,
      "grad_norm": 1.2544927597045898,
      "learning_rate": 3.8609925397340255e-05,
      "loss": 2.8676,
      "step": 352300
    },
    {
      "epoch": 114.23014586709887,
      "grad_norm": 1.1185925006866455,
      "learning_rate": 3.8606681803438214e-05,
      "loss": 2.9053,
      "step": 352400
    },
    {
      "epoch": 114.26256077795786,
      "grad_norm": 1.0586915016174316,
      "learning_rate": 3.860343820953617e-05,
      "loss": 2.8804,
      "step": 352500
    },
    {
      "epoch": 114.29497568881686,
      "grad_norm": 1.1322017908096313,
      "learning_rate": 3.860022705157314e-05,
      "loss": 2.8782,
      "step": 352600
    },
    {
      "epoch": 114.32739059967585,
      "grad_norm": 1.1937593221664429,
      "learning_rate": 3.85969834576711e-05,
      "loss": 2.8783,
      "step": 352700
    },
    {
      "epoch": 114.35980551053484,
      "grad_norm": 1.0744637250900269,
      "learning_rate": 3.8593739863769054e-05,
      "loss": 2.8825,
      "step": 352800
    },
    {
      "epoch": 114.39222042139384,
      "grad_norm": 0.991550624370575,
      "learning_rate": 3.859049626986701e-05,
      "loss": 2.8943,
      "step": 352900
    },
    {
      "epoch": 114.42463533225283,
      "grad_norm": 0.9986716508865356,
      "learning_rate": 3.858725267596497e-05,
      "loss": 2.89,
      "step": 353000
    },
    {
      "epoch": 114.45705024311184,
      "grad_norm": 1.1086493730545044,
      "learning_rate": 3.858400908206293e-05,
      "loss": 2.8766,
      "step": 353100
    },
    {
      "epoch": 114.48946515397083,
      "grad_norm": 1.2084503173828125,
      "learning_rate": 3.858076548816089e-05,
      "loss": 2.8891,
      "step": 353200
    },
    {
      "epoch": 114.52188006482982,
      "grad_norm": 1.060011863708496,
      "learning_rate": 3.857752189425884e-05,
      "loss": 2.8949,
      "step": 353300
    },
    {
      "epoch": 114.55429497568882,
      "grad_norm": 1.0176570415496826,
      "learning_rate": 3.85742783003568e-05,
      "loss": 2.8863,
      "step": 353400
    },
    {
      "epoch": 114.58670988654781,
      "grad_norm": 1.066023588180542,
      "learning_rate": 3.857103470645476e-05,
      "loss": 2.9079,
      "step": 353500
    },
    {
      "epoch": 114.61912479740681,
      "grad_norm": 1.0056828260421753,
      "learning_rate": 3.856779111255271e-05,
      "loss": 2.8974,
      "step": 353600
    },
    {
      "epoch": 114.6515397082658,
      "grad_norm": 1.0331510305404663,
      "learning_rate": 3.856454751865067e-05,
      "loss": 2.8815,
      "step": 353700
    },
    {
      "epoch": 114.68395461912479,
      "grad_norm": 1.1131764650344849,
      "learning_rate": 3.856130392474862e-05,
      "loss": 2.8896,
      "step": 353800
    },
    {
      "epoch": 114.7163695299838,
      "grad_norm": 1.0063058137893677,
      "learning_rate": 3.855806033084658e-05,
      "loss": 2.9116,
      "step": 353900
    },
    {
      "epoch": 114.74878444084278,
      "grad_norm": 1.090762972831726,
      "learning_rate": 3.855481673694454e-05,
      "loss": 2.8951,
      "step": 354000
    },
    {
      "epoch": 114.78119935170179,
      "grad_norm": 1.106831431388855,
      "learning_rate": 3.855157314304249e-05,
      "loss": 2.8811,
      "step": 354100
    },
    {
      "epoch": 114.81361426256078,
      "grad_norm": 1.0879188776016235,
      "learning_rate": 3.854832954914045e-05,
      "loss": 2.8917,
      "step": 354200
    },
    {
      "epoch": 114.84602917341978,
      "grad_norm": 1.171790599822998,
      "learning_rate": 3.854508595523841e-05,
      "loss": 2.8781,
      "step": 354300
    },
    {
      "epoch": 114.87844408427877,
      "grad_norm": 1.2207114696502686,
      "learning_rate": 3.854184236133636e-05,
      "loss": 2.8958,
      "step": 354400
    },
    {
      "epoch": 114.91085899513776,
      "grad_norm": 1.2853472232818604,
      "learning_rate": 3.853859876743432e-05,
      "loss": 2.8975,
      "step": 354500
    },
    {
      "epoch": 114.94327390599676,
      "grad_norm": 1.071534514427185,
      "learning_rate": 3.8535387609471297e-05,
      "loss": 2.8801,
      "step": 354600
    },
    {
      "epoch": 114.97568881685575,
      "grad_norm": 0.9964802861213684,
      "learning_rate": 3.8532144015569255e-05,
      "loss": 2.8844,
      "step": 354700
    },
    {
      "epoch": 115.0,
      "eval_bleu": 1.2616202453985639,
      "eval_loss": 3.8756120204925537,
      "eval_runtime": 3.8515,
      "eval_samples_per_second": 127.742,
      "eval_steps_per_second": 2.077,
      "step": 354775
    },
    {
      "epoch": 115.00810372771475,
      "grad_norm": 0.9362094402313232,
      "learning_rate": 3.852890042166721e-05,
      "loss": 2.889,
      "step": 354800
    },
    {
      "epoch": 115.04051863857374,
      "grad_norm": 1.0239909887313843,
      "learning_rate": 3.8525689263704185e-05,
      "loss": 2.8792,
      "step": 354900
    },
    {
      "epoch": 115.07293354943273,
      "grad_norm": 1.2439565658569336,
      "learning_rate": 3.8522445669802144e-05,
      "loss": 2.8885,
      "step": 355000
    },
    {
      "epoch": 115.10534846029174,
      "grad_norm": 1.0158573389053345,
      "learning_rate": 3.85192020759001e-05,
      "loss": 2.8983,
      "step": 355100
    },
    {
      "epoch": 115.13776337115073,
      "grad_norm": 1.1689702272415161,
      "learning_rate": 3.8515958481998055e-05,
      "loss": 2.9061,
      "step": 355200
    },
    {
      "epoch": 115.17017828200973,
      "grad_norm": 1.039175033569336,
      "learning_rate": 3.851271488809601e-05,
      "loss": 2.8675,
      "step": 355300
    },
    {
      "epoch": 115.20259319286872,
      "grad_norm": 1.0358606576919556,
      "learning_rate": 3.8509471294193965e-05,
      "loss": 2.8451,
      "step": 355400
    },
    {
      "epoch": 115.23500810372771,
      "grad_norm": 1.391056776046753,
      "learning_rate": 3.8506227700291924e-05,
      "loss": 2.8536,
      "step": 355500
    },
    {
      "epoch": 115.26742301458671,
      "grad_norm": 0.956813395023346,
      "learning_rate": 3.850298410638988e-05,
      "loss": 2.8957,
      "step": 355600
    },
    {
      "epoch": 115.2998379254457,
      "grad_norm": 1.1248726844787598,
      "learning_rate": 3.8499740512487835e-05,
      "loss": 2.8738,
      "step": 355700
    },
    {
      "epoch": 115.3322528363047,
      "grad_norm": 1.1190624237060547,
      "learning_rate": 3.8496496918585793e-05,
      "loss": 2.8834,
      "step": 355800
    },
    {
      "epoch": 115.3646677471637,
      "grad_norm": 1.0491669178009033,
      "learning_rate": 3.849325332468375e-05,
      "loss": 2.8669,
      "step": 355900
    },
    {
      "epoch": 115.3970826580227,
      "grad_norm": 1.0353922843933105,
      "learning_rate": 3.8490009730781704e-05,
      "loss": 2.8859,
      "step": 356000
    },
    {
      "epoch": 115.42949756888169,
      "grad_norm": 1.153017520904541,
      "learning_rate": 3.848676613687966e-05,
      "loss": 2.899,
      "step": 356100
    },
    {
      "epoch": 115.46191247974068,
      "grad_norm": 0.9153327941894531,
      "learning_rate": 3.848352254297762e-05,
      "loss": 2.8838,
      "step": 356200
    },
    {
      "epoch": 115.49432739059968,
      "grad_norm": 1.1642118692398071,
      "learning_rate": 3.8480278949075574e-05,
      "loss": 2.8729,
      "step": 356300
    },
    {
      "epoch": 115.52674230145867,
      "grad_norm": 1.1257942914962769,
      "learning_rate": 3.847703535517353e-05,
      "loss": 2.878,
      "step": 356400
    },
    {
      "epoch": 115.55915721231767,
      "grad_norm": 1.1880741119384766,
      "learning_rate": 3.847379176127149e-05,
      "loss": 2.8736,
      "step": 356500
    },
    {
      "epoch": 115.59157212317666,
      "grad_norm": 1.1156291961669922,
      "learning_rate": 3.847054816736945e-05,
      "loss": 2.8657,
      "step": 356600
    },
    {
      "epoch": 115.62398703403565,
      "grad_norm": 0.9463405013084412,
      "learning_rate": 3.846730457346741e-05,
      "loss": 2.8947,
      "step": 356700
    },
    {
      "epoch": 115.65640194489465,
      "grad_norm": 1.1189641952514648,
      "learning_rate": 3.846406097956536e-05,
      "loss": 2.8841,
      "step": 356800
    },
    {
      "epoch": 115.68881685575364,
      "grad_norm": 1.0867600440979004,
      "learning_rate": 3.846081738566332e-05,
      "loss": 2.9091,
      "step": 356900
    },
    {
      "epoch": 115.72123176661265,
      "grad_norm": 1.2126880884170532,
      "learning_rate": 3.845760622770029e-05,
      "loss": 2.8965,
      "step": 357000
    },
    {
      "epoch": 115.75364667747164,
      "grad_norm": 1.0070552825927734,
      "learning_rate": 3.845436263379825e-05,
      "loss": 2.8872,
      "step": 357100
    },
    {
      "epoch": 115.78606158833063,
      "grad_norm": 1.0420476198196411,
      "learning_rate": 3.845111903989621e-05,
      "loss": 2.8914,
      "step": 357200
    },
    {
      "epoch": 115.81847649918963,
      "grad_norm": 0.9699397087097168,
      "learning_rate": 3.8447875445994167e-05,
      "loss": 2.8869,
      "step": 357300
    },
    {
      "epoch": 115.85089141004862,
      "grad_norm": 1.0051345825195312,
      "learning_rate": 3.8444631852092125e-05,
      "loss": 2.8911,
      "step": 357400
    },
    {
      "epoch": 115.88330632090762,
      "grad_norm": 1.0798373222351074,
      "learning_rate": 3.844138825819008e-05,
      "loss": 2.9026,
      "step": 357500
    },
    {
      "epoch": 115.91572123176661,
      "grad_norm": 1.157008171081543,
      "learning_rate": 3.8438144664288036e-05,
      "loss": 2.8687,
      "step": 357600
    },
    {
      "epoch": 115.94813614262561,
      "grad_norm": 1.0331358909606934,
      "learning_rate": 3.843490107038599e-05,
      "loss": 2.8765,
      "step": 357700
    },
    {
      "epoch": 115.9805510534846,
      "grad_norm": 1.291920781135559,
      "learning_rate": 3.843165747648395e-05,
      "loss": 2.8851,
      "step": 357800
    },
    {
      "epoch": 116.0,
      "eval_bleu": 1.1705664800993356,
      "eval_loss": 3.8703362941741943,
      "eval_runtime": 3.9302,
      "eval_samples_per_second": 125.186,
      "eval_steps_per_second": 2.036,
      "step": 357860
    },
    {
      "epoch": 116.01296596434359,
      "grad_norm": 1.0359851121902466,
      "learning_rate": 3.8428413882581905e-05,
      "loss": 2.8923,
      "step": 357900
    },
    {
      "epoch": 116.0453808752026,
      "grad_norm": 0.9743092060089111,
      "learning_rate": 3.842517028867986e-05,
      "loss": 2.8842,
      "step": 358000
    },
    {
      "epoch": 116.07779578606159,
      "grad_norm": 1.2535812854766846,
      "learning_rate": 3.8421926694777816e-05,
      "loss": 2.87,
      "step": 358100
    },
    {
      "epoch": 116.11021069692059,
      "grad_norm": 1.0072866678237915,
      "learning_rate": 3.8418683100875775e-05,
      "loss": 2.8584,
      "step": 358200
    },
    {
      "epoch": 116.14262560777958,
      "grad_norm": 1.0063625574111938,
      "learning_rate": 3.841543950697373e-05,
      "loss": 2.8793,
      "step": 358300
    },
    {
      "epoch": 116.17504051863857,
      "grad_norm": 1.170190691947937,
      "learning_rate": 3.8412195913071686e-05,
      "loss": 2.8751,
      "step": 358400
    },
    {
      "epoch": 116.20745542949757,
      "grad_norm": 0.9990513324737549,
      "learning_rate": 3.8408952319169644e-05,
      "loss": 2.8768,
      "step": 358500
    },
    {
      "epoch": 116.23987034035656,
      "grad_norm": 1.3530844449996948,
      "learning_rate": 3.8405708725267596e-05,
      "loss": 2.8824,
      "step": 358600
    },
    {
      "epoch": 116.27228525121556,
      "grad_norm": 0.9963895678520203,
      "learning_rate": 3.8402465131365555e-05,
      "loss": 2.8759,
      "step": 358700
    },
    {
      "epoch": 116.30470016207455,
      "grad_norm": 1.2924160957336426,
      "learning_rate": 3.839922153746351e-05,
      "loss": 2.8711,
      "step": 358800
    },
    {
      "epoch": 116.33711507293354,
      "grad_norm": 1.0100212097167969,
      "learning_rate": 3.8395977943561466e-05,
      "loss": 2.8788,
      "step": 358900
    },
    {
      "epoch": 116.36952998379255,
      "grad_norm": 1.1026893854141235,
      "learning_rate": 3.8392734349659424e-05,
      "loss": 2.8854,
      "step": 359000
    },
    {
      "epoch": 116.40194489465154,
      "grad_norm": 1.0231398344039917,
      "learning_rate": 3.8389490755757376e-05,
      "loss": 2.8725,
      "step": 359100
    },
    {
      "epoch": 116.43435980551054,
      "grad_norm": 1.2831218242645264,
      "learning_rate": 3.8386247161855335e-05,
      "loss": 2.875,
      "step": 359200
    },
    {
      "epoch": 116.46677471636953,
      "grad_norm": 1.0216652154922485,
      "learning_rate": 3.8383003567953294e-05,
      "loss": 2.8725,
      "step": 359300
    },
    {
      "epoch": 116.49918962722853,
      "grad_norm": 1.1821396350860596,
      "learning_rate": 3.8379759974051246e-05,
      "loss": 2.8869,
      "step": 359400
    },
    {
      "epoch": 116.53160453808752,
      "grad_norm": 0.9246839880943298,
      "learning_rate": 3.8376516380149205e-05,
      "loss": 2.879,
      "step": 359500
    },
    {
      "epoch": 116.56401944894651,
      "grad_norm": 1.1778953075408936,
      "learning_rate": 3.8373272786247163e-05,
      "loss": 2.8705,
      "step": 359600
    },
    {
      "epoch": 116.59643435980551,
      "grad_norm": 0.9327309131622314,
      "learning_rate": 3.837002919234512e-05,
      "loss": 2.9114,
      "step": 359700
    },
    {
      "epoch": 116.6288492706645,
      "grad_norm": 1.1082686185836792,
      "learning_rate": 3.836678559844308e-05,
      "loss": 2.9038,
      "step": 359800
    },
    {
      "epoch": 116.6612641815235,
      "grad_norm": 1.1326910257339478,
      "learning_rate": 3.836354200454103e-05,
      "loss": 2.873,
      "step": 359900
    },
    {
      "epoch": 116.6936790923825,
      "grad_norm": 1.0480356216430664,
      "learning_rate": 3.836029841063899e-05,
      "loss": 2.9011,
      "step": 360000
    },
    {
      "epoch": 116.72609400324149,
      "grad_norm": 0.9651527404785156,
      "learning_rate": 3.835705481673695e-05,
      "loss": 2.8798,
      "step": 360100
    },
    {
      "epoch": 116.75850891410049,
      "grad_norm": 1.2297507524490356,
      "learning_rate": 3.83538112228349e-05,
      "loss": 2.8929,
      "step": 360200
    },
    {
      "epoch": 116.79092382495948,
      "grad_norm": 1.0710445642471313,
      "learning_rate": 3.835056762893286e-05,
      "loss": 2.8809,
      "step": 360300
    },
    {
      "epoch": 116.82333873581848,
      "grad_norm": 1.141191005706787,
      "learning_rate": 3.834732403503082e-05,
      "loss": 2.8815,
      "step": 360400
    },
    {
      "epoch": 116.85575364667747,
      "grad_norm": 1.0522189140319824,
      "learning_rate": 3.834408044112877e-05,
      "loss": 2.9145,
      "step": 360500
    },
    {
      "epoch": 116.88816855753646,
      "grad_norm": 1.1360270977020264,
      "learning_rate": 3.834083684722673e-05,
      "loss": 2.8798,
      "step": 360600
    },
    {
      "epoch": 116.92058346839546,
      "grad_norm": 1.0789375305175781,
      "learning_rate": 3.833759325332468e-05,
      "loss": 2.8891,
      "step": 360700
    },
    {
      "epoch": 116.95299837925445,
      "grad_norm": 1.0018486976623535,
      "learning_rate": 3.833434965942264e-05,
      "loss": 2.8612,
      "step": 360800
    },
    {
      "epoch": 116.98541329011346,
      "grad_norm": 1.1599704027175903,
      "learning_rate": 3.83311060655206e-05,
      "loss": 2.8901,
      "step": 360900
    },
    {
      "epoch": 117.0,
      "eval_bleu": 1.190767553116617,
      "eval_loss": 3.8794009685516357,
      "eval_runtime": 4.3613,
      "eval_samples_per_second": 112.81,
      "eval_steps_per_second": 1.834,
      "step": 360945
    },
    {
      "epoch": 117.01782820097245,
      "grad_norm": 1.093714952468872,
      "learning_rate": 3.832786247161855e-05,
      "loss": 2.8637,
      "step": 361000
    },
    {
      "epoch": 117.05024311183145,
      "grad_norm": 1.0662685632705688,
      "learning_rate": 3.832465131365553e-05,
      "loss": 2.8705,
      "step": 361100
    },
    {
      "epoch": 117.08265802269044,
      "grad_norm": 1.069220781326294,
      "learning_rate": 3.832140771975349e-05,
      "loss": 2.8685,
      "step": 361200
    },
    {
      "epoch": 117.11507293354943,
      "grad_norm": 1.0044240951538086,
      "learning_rate": 3.831816412585145e-05,
      "loss": 2.8728,
      "step": 361300
    },
    {
      "epoch": 117.14748784440843,
      "grad_norm": 0.9477189779281616,
      "learning_rate": 3.83149205319494e-05,
      "loss": 2.8685,
      "step": 361400
    },
    {
      "epoch": 117.17990275526742,
      "grad_norm": 1.0709561109542847,
      "learning_rate": 3.831167693804736e-05,
      "loss": 2.8712,
      "step": 361500
    },
    {
      "epoch": 117.21231766612642,
      "grad_norm": 1.0758205652236938,
      "learning_rate": 3.830843334414532e-05,
      "loss": 2.867,
      "step": 361600
    },
    {
      "epoch": 117.24473257698541,
      "grad_norm": 1.0713249444961548,
      "learning_rate": 3.830518975024327e-05,
      "loss": 2.8739,
      "step": 361700
    },
    {
      "epoch": 117.2771474878444,
      "grad_norm": 1.3499338626861572,
      "learning_rate": 3.830194615634123e-05,
      "loss": 2.891,
      "step": 361800
    },
    {
      "epoch": 117.3095623987034,
      "grad_norm": 0.9622127413749695,
      "learning_rate": 3.8298702562439186e-05,
      "loss": 2.8813,
      "step": 361900
    },
    {
      "epoch": 117.3419773095624,
      "grad_norm": 1.0684782266616821,
      "learning_rate": 3.829545896853714e-05,
      "loss": 2.8819,
      "step": 362000
    },
    {
      "epoch": 117.3743922204214,
      "grad_norm": 1.0237255096435547,
      "learning_rate": 3.82922153746351e-05,
      "loss": 2.8746,
      "step": 362100
    },
    {
      "epoch": 117.40680713128039,
      "grad_norm": 1.1825330257415771,
      "learning_rate": 3.828897178073305e-05,
      "loss": 2.8817,
      "step": 362200
    },
    {
      "epoch": 117.43922204213938,
      "grad_norm": 0.9979435801506042,
      "learning_rate": 3.828572818683101e-05,
      "loss": 2.8862,
      "step": 362300
    },
    {
      "epoch": 117.47163695299838,
      "grad_norm": 0.9864240884780884,
      "learning_rate": 3.8282484592928966e-05,
      "loss": 2.8764,
      "step": 362400
    },
    {
      "epoch": 117.50405186385737,
      "grad_norm": 1.0176987648010254,
      "learning_rate": 3.8279273434965944e-05,
      "loss": 2.874,
      "step": 362500
    },
    {
      "epoch": 117.53646677471637,
      "grad_norm": 1.2774163484573364,
      "learning_rate": 3.8276029841063896e-05,
      "loss": 2.8955,
      "step": 362600
    },
    {
      "epoch": 117.56888168557536,
      "grad_norm": 1.200102686882019,
      "learning_rate": 3.8272786247161855e-05,
      "loss": 2.8668,
      "step": 362700
    },
    {
      "epoch": 117.60129659643437,
      "grad_norm": 1.1014039516448975,
      "learning_rate": 3.8269542653259814e-05,
      "loss": 2.8872,
      "step": 362800
    },
    {
      "epoch": 117.63371150729336,
      "grad_norm": 0.977958083152771,
      "learning_rate": 3.8266299059357765e-05,
      "loss": 2.893,
      "step": 362900
    },
    {
      "epoch": 117.66612641815234,
      "grad_norm": 0.9438585638999939,
      "learning_rate": 3.8263055465455724e-05,
      "loss": 2.8887,
      "step": 363000
    },
    {
      "epoch": 117.69854132901135,
      "grad_norm": 1.0328236818313599,
      "learning_rate": 3.825981187155368e-05,
      "loss": 2.8871,
      "step": 363100
    },
    {
      "epoch": 117.73095623987034,
      "grad_norm": 1.1844233274459839,
      "learning_rate": 3.825656827765164e-05,
      "loss": 2.8935,
      "step": 363200
    },
    {
      "epoch": 117.76337115072934,
      "grad_norm": 1.2998815774917603,
      "learning_rate": 3.82533246837496e-05,
      "loss": 2.864,
      "step": 363300
    },
    {
      "epoch": 117.79578606158833,
      "grad_norm": 1.1027312278747559,
      "learning_rate": 3.825008108984755e-05,
      "loss": 2.8829,
      "step": 363400
    },
    {
      "epoch": 117.82820097244732,
      "grad_norm": 1.0484802722930908,
      "learning_rate": 3.824683749594551e-05,
      "loss": 2.8645,
      "step": 363500
    },
    {
      "epoch": 117.86061588330632,
      "grad_norm": 1.0175189971923828,
      "learning_rate": 3.824359390204347e-05,
      "loss": 2.8618,
      "step": 363600
    },
    {
      "epoch": 117.89303079416531,
      "grad_norm": 1.0062164068222046,
      "learning_rate": 3.824035030814142e-05,
      "loss": 2.8627,
      "step": 363700
    },
    {
      "epoch": 117.92544570502432,
      "grad_norm": 1.0530263185501099,
      "learning_rate": 3.823710671423938e-05,
      "loss": 2.9042,
      "step": 363800
    },
    {
      "epoch": 117.9578606158833,
      "grad_norm": 1.128846526145935,
      "learning_rate": 3.823386312033734e-05,
      "loss": 2.8808,
      "step": 363900
    },
    {
      "epoch": 117.9902755267423,
      "grad_norm": 1.0437712669372559,
      "learning_rate": 3.823061952643529e-05,
      "loss": 2.9152,
      "step": 364000
    },
    {
      "epoch": 118.0,
      "eval_bleu": 1.2112978632778943,
      "eval_loss": 3.8796184062957764,
      "eval_runtime": 4.0993,
      "eval_samples_per_second": 120.021,
      "eval_steps_per_second": 1.952,
      "step": 364030
    },
    {
      "epoch": 118.0226904376013,
      "grad_norm": 1.0182101726531982,
      "learning_rate": 3.822737593253325e-05,
      "loss": 2.864,
      "step": 364100
    },
    {
      "epoch": 118.05510534846029,
      "grad_norm": 1.0311015844345093,
      "learning_rate": 3.822413233863121e-05,
      "loss": 2.8618,
      "step": 364200
    },
    {
      "epoch": 118.08752025931929,
      "grad_norm": 1.0918833017349243,
      "learning_rate": 3.822088874472916e-05,
      "loss": 2.8706,
      "step": 364300
    },
    {
      "epoch": 118.11993517017828,
      "grad_norm": 1.0097483396530151,
      "learning_rate": 3.821764515082712e-05,
      "loss": 2.874,
      "step": 364400
    },
    {
      "epoch": 118.15235008103728,
      "grad_norm": 1.1399626731872559,
      "learning_rate": 3.821440155692507e-05,
      "loss": 2.8816,
      "step": 364500
    },
    {
      "epoch": 118.18476499189627,
      "grad_norm": 1.1120473146438599,
      "learning_rate": 3.821115796302303e-05,
      "loss": 2.8815,
      "step": 364600
    },
    {
      "epoch": 118.21717990275526,
      "grad_norm": 1.0676891803741455,
      "learning_rate": 3.820791436912099e-05,
      "loss": 2.8837,
      "step": 364700
    },
    {
      "epoch": 118.24959481361427,
      "grad_norm": 1.1871006488800049,
      "learning_rate": 3.820467077521894e-05,
      "loss": 2.8643,
      "step": 364800
    },
    {
      "epoch": 118.28200972447326,
      "grad_norm": 0.9657085537910461,
      "learning_rate": 3.82014271813169e-05,
      "loss": 2.8703,
      "step": 364900
    },
    {
      "epoch": 118.31442463533226,
      "grad_norm": 1.1683847904205322,
      "learning_rate": 3.819818358741486e-05,
      "loss": 2.8569,
      "step": 365000
    },
    {
      "epoch": 118.34683954619125,
      "grad_norm": 0.9821855425834656,
      "learning_rate": 3.819493999351281e-05,
      "loss": 2.8936,
      "step": 365100
    },
    {
      "epoch": 118.37925445705024,
      "grad_norm": 0.9914355874061584,
      "learning_rate": 3.819169639961077e-05,
      "loss": 2.8678,
      "step": 365200
    },
    {
      "epoch": 118.41166936790924,
      "grad_norm": 1.2092257738113403,
      "learning_rate": 3.818845280570873e-05,
      "loss": 2.8796,
      "step": 365300
    },
    {
      "epoch": 118.44408427876823,
      "grad_norm": 1.2672054767608643,
      "learning_rate": 3.818520921180668e-05,
      "loss": 2.8603,
      "step": 365400
    },
    {
      "epoch": 118.47649918962723,
      "grad_norm": 0.9937816262245178,
      "learning_rate": 3.818196561790464e-05,
      "loss": 2.8525,
      "step": 365500
    },
    {
      "epoch": 118.50891410048622,
      "grad_norm": 1.1505796909332275,
      "learning_rate": 3.81787220240026e-05,
      "loss": 2.8719,
      "step": 365600
    },
    {
      "epoch": 118.54132901134521,
      "grad_norm": 1.0542652606964111,
      "learning_rate": 3.8175478430100556e-05,
      "loss": 2.8783,
      "step": 365700
    },
    {
      "epoch": 118.57374392220422,
      "grad_norm": 1.0347206592559814,
      "learning_rate": 3.8172234836198515e-05,
      "loss": 2.8634,
      "step": 365800
    },
    {
      "epoch": 118.6061588330632,
      "grad_norm": 1.042816400527954,
      "learning_rate": 3.816899124229647e-05,
      "loss": 2.8755,
      "step": 365900
    },
    {
      "epoch": 118.63857374392221,
      "grad_norm": 1.085893988609314,
      "learning_rate": 3.8165747648394426e-05,
      "loss": 2.8883,
      "step": 366000
    },
    {
      "epoch": 118.6709886547812,
      "grad_norm": 1.0132241249084473,
      "learning_rate": 3.8162504054492384e-05,
      "loss": 2.8814,
      "step": 366100
    },
    {
      "epoch": 118.7034035656402,
      "grad_norm": 1.1535812616348267,
      "learning_rate": 3.8159260460590336e-05,
      "loss": 2.8864,
      "step": 366200
    },
    {
      "epoch": 118.73581847649919,
      "grad_norm": 1.0255002975463867,
      "learning_rate": 3.8156016866688295e-05,
      "loss": 2.8833,
      "step": 366300
    },
    {
      "epoch": 118.76823338735818,
      "grad_norm": 1.0770596265792847,
      "learning_rate": 3.815277327278625e-05,
      "loss": 2.8975,
      "step": 366400
    },
    {
      "epoch": 118.80064829821718,
      "grad_norm": 1.155747652053833,
      "learning_rate": 3.8149529678884206e-05,
      "loss": 2.8701,
      "step": 366500
    },
    {
      "epoch": 118.83306320907617,
      "grad_norm": 1.0815722942352295,
      "learning_rate": 3.8146318520921183e-05,
      "loss": 2.8943,
      "step": 366600
    },
    {
      "epoch": 118.86547811993518,
      "grad_norm": 1.0412124395370483,
      "learning_rate": 3.814307492701914e-05,
      "loss": 2.9107,
      "step": 366700
    },
    {
      "epoch": 118.89789303079417,
      "grad_norm": 1.0798659324645996,
      "learning_rate": 3.8139831333117094e-05,
      "loss": 2.886,
      "step": 366800
    },
    {
      "epoch": 118.93030794165315,
      "grad_norm": 1.2302770614624023,
      "learning_rate": 3.813658773921505e-05,
      "loss": 2.8528,
      "step": 366900
    },
    {
      "epoch": 118.96272285251216,
      "grad_norm": 1.0571582317352295,
      "learning_rate": 3.813334414531301e-05,
      "loss": 2.8847,
      "step": 367000
    },
    {
      "epoch": 118.99513776337115,
      "grad_norm": 1.069810152053833,
      "learning_rate": 3.8130100551410964e-05,
      "loss": 2.8708,
      "step": 367100
    },
    {
      "epoch": 119.0,
      "eval_bleu": 1.0889855101895103,
      "eval_loss": 3.890202045440674,
      "eval_runtime": 4.088,
      "eval_samples_per_second": 120.351,
      "eval_steps_per_second": 1.957,
      "step": 367115
    },
    {
      "epoch": 119.02755267423015,
      "grad_norm": 1.0164536237716675,
      "learning_rate": 3.812685695750892e-05,
      "loss": 2.8837,
      "step": 367200
    },
    {
      "epoch": 119.05996758508914,
      "grad_norm": 1.0639417171478271,
      "learning_rate": 3.812361336360688e-05,
      "loss": 2.8674,
      "step": 367300
    },
    {
      "epoch": 119.09238249594813,
      "grad_norm": 1.0945061445236206,
      "learning_rate": 3.812036976970483e-05,
      "loss": 2.8694,
      "step": 367400
    },
    {
      "epoch": 119.12479740680713,
      "grad_norm": 1.04438316822052,
      "learning_rate": 3.811712617580279e-05,
      "loss": 2.8762,
      "step": 367500
    },
    {
      "epoch": 119.15721231766612,
      "grad_norm": 1.09003746509552,
      "learning_rate": 3.811388258190075e-05,
      "loss": 2.8671,
      "step": 367600
    },
    {
      "epoch": 119.18962722852513,
      "grad_norm": 0.9110232591629028,
      "learning_rate": 3.81106389879987e-05,
      "loss": 2.8526,
      "step": 367700
    },
    {
      "epoch": 119.22204213938411,
      "grad_norm": 1.1299045085906982,
      "learning_rate": 3.810739539409666e-05,
      "loss": 2.8653,
      "step": 367800
    },
    {
      "epoch": 119.25445705024312,
      "grad_norm": 1.0203254222869873,
      "learning_rate": 3.810415180019461e-05,
      "loss": 2.8737,
      "step": 367900
    },
    {
      "epoch": 119.28687196110211,
      "grad_norm": 1.1675910949707031,
      "learning_rate": 3.810090820629257e-05,
      "loss": 2.869,
      "step": 368000
    },
    {
      "epoch": 119.3192868719611,
      "grad_norm": 1.0414273738861084,
      "learning_rate": 3.809766461239053e-05,
      "loss": 2.8829,
      "step": 368100
    },
    {
      "epoch": 119.3517017828201,
      "grad_norm": 1.0771005153656006,
      "learning_rate": 3.809442101848848e-05,
      "loss": 2.8811,
      "step": 368200
    },
    {
      "epoch": 119.38411669367909,
      "grad_norm": 1.0704678297042847,
      "learning_rate": 3.809117742458644e-05,
      "loss": 2.8821,
      "step": 368300
    },
    {
      "epoch": 119.4165316045381,
      "grad_norm": 1.0641347169876099,
      "learning_rate": 3.80879338306844e-05,
      "loss": 2.8939,
      "step": 368400
    },
    {
      "epoch": 119.44894651539708,
      "grad_norm": 0.9840791821479797,
      "learning_rate": 3.808469023678235e-05,
      "loss": 2.879,
      "step": 368500
    },
    {
      "epoch": 119.48136142625607,
      "grad_norm": 1.1418877840042114,
      "learning_rate": 3.808147907881933e-05,
      "loss": 2.8636,
      "step": 368600
    },
    {
      "epoch": 119.51377633711508,
      "grad_norm": 1.0315128564834595,
      "learning_rate": 3.807823548491729e-05,
      "loss": 2.8693,
      "step": 368700
    },
    {
      "epoch": 119.54619124797406,
      "grad_norm": 0.9880619049072266,
      "learning_rate": 3.807499189101525e-05,
      "loss": 2.8844,
      "step": 368800
    },
    {
      "epoch": 119.57860615883307,
      "grad_norm": 1.1729800701141357,
      "learning_rate": 3.80717482971132e-05,
      "loss": 2.8692,
      "step": 368900
    },
    {
      "epoch": 119.61102106969206,
      "grad_norm": 1.380050539970398,
      "learning_rate": 3.806850470321116e-05,
      "loss": 2.8864,
      "step": 369000
    },
    {
      "epoch": 119.64343598055105,
      "grad_norm": 1.0335195064544678,
      "learning_rate": 3.806526110930912e-05,
      "loss": 2.8775,
      "step": 369100
    },
    {
      "epoch": 119.67585089141005,
      "grad_norm": 1.3000693321228027,
      "learning_rate": 3.8062049951346095e-05,
      "loss": 2.8759,
      "step": 369200
    },
    {
      "epoch": 119.70826580226904,
      "grad_norm": 1.1796247959136963,
      "learning_rate": 3.805880635744405e-05,
      "loss": 2.8572,
      "step": 369300
    },
    {
      "epoch": 119.74068071312804,
      "grad_norm": 1.011786937713623,
      "learning_rate": 3.8055562763542005e-05,
      "loss": 2.8386,
      "step": 369400
    },
    {
      "epoch": 119.77309562398703,
      "grad_norm": 1.1238653659820557,
      "learning_rate": 3.805231916963996e-05,
      "loss": 2.8691,
      "step": 369500
    },
    {
      "epoch": 119.80551053484604,
      "grad_norm": 1.1829969882965088,
      "learning_rate": 3.8049075575737916e-05,
      "loss": 2.879,
      "step": 369600
    },
    {
      "epoch": 119.83792544570503,
      "grad_norm": 1.0399571657180786,
      "learning_rate": 3.8045831981835875e-05,
      "loss": 2.878,
      "step": 369700
    },
    {
      "epoch": 119.87034035656401,
      "grad_norm": 1.1718615293502808,
      "learning_rate": 3.8042588387933834e-05,
      "loss": 2.8971,
      "step": 369800
    },
    {
      "epoch": 119.90275526742302,
      "grad_norm": 1.283326268196106,
      "learning_rate": 3.803934479403179e-05,
      "loss": 2.8833,
      "step": 369900
    },
    {
      "epoch": 119.93517017828201,
      "grad_norm": 1.1759089231491089,
      "learning_rate": 3.803610120012975e-05,
      "loss": 2.8758,
      "step": 370000
    },
    {
      "epoch": 119.96758508914101,
      "grad_norm": 1.1071475744247437,
      "learning_rate": 3.80328576062277e-05,
      "loss": 2.8809,
      "step": 370100
    },
    {
      "epoch": 120.0,
      "grad_norm": 1.2112230062484741,
      "learning_rate": 3.802964644826468e-05,
      "loss": 2.8662,
      "step": 370200
    },
    {
      "epoch": 120.0,
      "eval_bleu": 1.2677399239375648,
      "eval_loss": 3.8854053020477295,
      "eval_runtime": 4.3521,
      "eval_samples_per_second": 113.049,
      "eval_steps_per_second": 1.838,
      "step": 370200
    },
    {
      "epoch": 120.03241491085899,
      "grad_norm": 1.054325819015503,
      "learning_rate": 3.802640285436263e-05,
      "loss": 2.866,
      "step": 370300
    },
    {
      "epoch": 120.06482982171799,
      "grad_norm": 1.0804848670959473,
      "learning_rate": 3.802315926046059e-05,
      "loss": 2.8685,
      "step": 370400
    },
    {
      "epoch": 120.09724473257698,
      "grad_norm": 1.1303757429122925,
      "learning_rate": 3.801991566655855e-05,
      "loss": 2.8766,
      "step": 370500
    },
    {
      "epoch": 120.12965964343599,
      "grad_norm": 1.0922369956970215,
      "learning_rate": 3.801667207265651e-05,
      "loss": 2.864,
      "step": 370600
    },
    {
      "epoch": 120.16207455429497,
      "grad_norm": 1.1036715507507324,
      "learning_rate": 3.801342847875446e-05,
      "loss": 2.8702,
      "step": 370700
    },
    {
      "epoch": 120.19448946515396,
      "grad_norm": 1.3575727939605713,
      "learning_rate": 3.801018488485242e-05,
      "loss": 2.852,
      "step": 370800
    },
    {
      "epoch": 120.22690437601297,
      "grad_norm": 1.0539335012435913,
      "learning_rate": 3.800694129095038e-05,
      "loss": 2.8741,
      "step": 370900
    },
    {
      "epoch": 120.25931928687196,
      "grad_norm": 1.2000577449798584,
      "learning_rate": 3.800369769704833e-05,
      "loss": 2.8611,
      "step": 371000
    },
    {
      "epoch": 120.29173419773096,
      "grad_norm": 0.9131059050559998,
      "learning_rate": 3.800045410314629e-05,
      "loss": 2.8558,
      "step": 371100
    },
    {
      "epoch": 120.32414910858995,
      "grad_norm": 1.1956140995025635,
      "learning_rate": 3.799721050924425e-05,
      "loss": 2.8866,
      "step": 371200
    },
    {
      "epoch": 120.35656401944895,
      "grad_norm": 1.056080937385559,
      "learning_rate": 3.79939669153422e-05,
      "loss": 2.8659,
      "step": 371300
    },
    {
      "epoch": 120.38897893030794,
      "grad_norm": 1.0894914865493774,
      "learning_rate": 3.799072332144016e-05,
      "loss": 2.8618,
      "step": 371400
    },
    {
      "epoch": 120.42139384116693,
      "grad_norm": 1.317068099975586,
      "learning_rate": 3.798747972753812e-05,
      "loss": 2.8767,
      "step": 371500
    },
    {
      "epoch": 120.45380875202594,
      "grad_norm": 1.0623208284378052,
      "learning_rate": 3.798423613363607e-05,
      "loss": 2.869,
      "step": 371600
    },
    {
      "epoch": 120.48622366288492,
      "grad_norm": 1.0656800270080566,
      "learning_rate": 3.798099253973403e-05,
      "loss": 2.8644,
      "step": 371700
    },
    {
      "epoch": 120.51863857374393,
      "grad_norm": 1.2301703691482544,
      "learning_rate": 3.797774894583198e-05,
      "loss": 2.8748,
      "step": 371800
    },
    {
      "epoch": 120.55105348460292,
      "grad_norm": 1.1355527639389038,
      "learning_rate": 3.797450535192994e-05,
      "loss": 2.8741,
      "step": 371900
    },
    {
      "epoch": 120.5834683954619,
      "grad_norm": 1.2011016607284546,
      "learning_rate": 3.79712617580279e-05,
      "loss": 2.8646,
      "step": 372000
    },
    {
      "epoch": 120.61588330632091,
      "grad_norm": 1.0906848907470703,
      "learning_rate": 3.796801816412585e-05,
      "loss": 2.871,
      "step": 372100
    },
    {
      "epoch": 120.6482982171799,
      "grad_norm": 1.1051459312438965,
      "learning_rate": 3.796477457022381e-05,
      "loss": 2.8775,
      "step": 372200
    },
    {
      "epoch": 120.6807131280389,
      "grad_norm": 1.1010186672210693,
      "learning_rate": 3.7961563412260786e-05,
      "loss": 2.8704,
      "step": 372300
    },
    {
      "epoch": 120.71312803889789,
      "grad_norm": 1.019873023033142,
      "learning_rate": 3.7958319818358745e-05,
      "loss": 2.9051,
      "step": 372400
    },
    {
      "epoch": 120.74554294975688,
      "grad_norm": 1.009253740310669,
      "learning_rate": 3.79550762244567e-05,
      "loss": 2.8798,
      "step": 372500
    },
    {
      "epoch": 120.77795786061589,
      "grad_norm": 1.0360790491104126,
      "learning_rate": 3.7951832630554656e-05,
      "loss": 2.8629,
      "step": 372600
    },
    {
      "epoch": 120.81037277147487,
      "grad_norm": 0.9669515490531921,
      "learning_rate": 3.7948589036652614e-05,
      "loss": 2.8651,
      "step": 372700
    },
    {
      "epoch": 120.84278768233388,
      "grad_norm": 1.0644421577453613,
      "learning_rate": 3.7945345442750566e-05,
      "loss": 2.883,
      "step": 372800
    },
    {
      "epoch": 120.87520259319287,
      "grad_norm": 1.056479573249817,
      "learning_rate": 3.7942101848848525e-05,
      "loss": 2.864,
      "step": 372900
    },
    {
      "epoch": 120.90761750405187,
      "grad_norm": 1.0618971586227417,
      "learning_rate": 3.793885825494648e-05,
      "loss": 2.8718,
      "step": 373000
    },
    {
      "epoch": 120.94003241491086,
      "grad_norm": 0.9503608345985413,
      "learning_rate": 3.7935614661044436e-05,
      "loss": 2.8777,
      "step": 373100
    },
    {
      "epoch": 120.97244732576985,
      "grad_norm": 1.0659816265106201,
      "learning_rate": 3.7932371067142394e-05,
      "loss": 2.8693,
      "step": 373200
    },
    {
      "epoch": 121.0,
      "eval_bleu": 1.0560153520397844,
      "eval_loss": 3.885655641555786,
      "eval_runtime": 3.7413,
      "eval_samples_per_second": 131.504,
      "eval_steps_per_second": 2.138,
      "step": 373285
    },
    {
      "epoch": 121.00486223662885,
      "grad_norm": 1.0888742208480835,
      "learning_rate": 3.792912747324035e-05,
      "loss": 2.8747,
      "step": 373300
    },
    {
      "epoch": 121.03727714748784,
      "grad_norm": 1.5844053030014038,
      "learning_rate": 3.792588387933831e-05,
      "loss": 2.8641,
      "step": 373400
    },
    {
      "epoch": 121.06969205834685,
      "grad_norm": 1.1341021060943604,
      "learning_rate": 3.7922640285436264e-05,
      "loss": 2.8447,
      "step": 373500
    },
    {
      "epoch": 121.10210696920583,
      "grad_norm": 1.2469720840454102,
      "learning_rate": 3.791939669153422e-05,
      "loss": 2.874,
      "step": 373600
    },
    {
      "epoch": 121.13452188006482,
      "grad_norm": 1.293130874633789,
      "learning_rate": 3.791615309763218e-05,
      "loss": 2.8605,
      "step": 373700
    },
    {
      "epoch": 121.16693679092383,
      "grad_norm": 0.9811099767684937,
      "learning_rate": 3.791290950373014e-05,
      "loss": 2.8642,
      "step": 373800
    },
    {
      "epoch": 121.19935170178282,
      "grad_norm": 1.2118762731552124,
      "learning_rate": 3.790966590982809e-05,
      "loss": 2.8732,
      "step": 373900
    },
    {
      "epoch": 121.23176661264182,
      "grad_norm": 1.0462536811828613,
      "learning_rate": 3.790642231592605e-05,
      "loss": 2.875,
      "step": 374000
    },
    {
      "epoch": 121.26418152350081,
      "grad_norm": 1.1089524030685425,
      "learning_rate": 3.7903178722024e-05,
      "loss": 2.8504,
      "step": 374100
    },
    {
      "epoch": 121.2965964343598,
      "grad_norm": 1.2150514125823975,
      "learning_rate": 3.789993512812196e-05,
      "loss": 2.8772,
      "step": 374200
    },
    {
      "epoch": 121.3290113452188,
      "grad_norm": 1.458174228668213,
      "learning_rate": 3.789669153421992e-05,
      "loss": 2.8483,
      "step": 374300
    },
    {
      "epoch": 121.36142625607779,
      "grad_norm": 1.160740852355957,
      "learning_rate": 3.789344794031787e-05,
      "loss": 2.8684,
      "step": 374400
    },
    {
      "epoch": 121.3938411669368,
      "grad_norm": 1.1083283424377441,
      "learning_rate": 3.789020434641583e-05,
      "loss": 2.8492,
      "step": 374500
    },
    {
      "epoch": 121.42625607779578,
      "grad_norm": 1.0764951705932617,
      "learning_rate": 3.788696075251379e-05,
      "loss": 2.8571,
      "step": 374600
    },
    {
      "epoch": 121.45867098865479,
      "grad_norm": 1.075091004371643,
      "learning_rate": 3.788371715861174e-05,
      "loss": 2.8828,
      "step": 374700
    },
    {
      "epoch": 121.49108589951378,
      "grad_norm": 0.9865667819976807,
      "learning_rate": 3.78804735647097e-05,
      "loss": 2.883,
      "step": 374800
    },
    {
      "epoch": 121.52350081037277,
      "grad_norm": 1.3794459104537964,
      "learning_rate": 3.787722997080766e-05,
      "loss": 2.8608,
      "step": 374900
    },
    {
      "epoch": 121.55591572123177,
      "grad_norm": 1.0278172492980957,
      "learning_rate": 3.787398637690561e-05,
      "loss": 2.8602,
      "step": 375000
    },
    {
      "epoch": 121.58833063209076,
      "grad_norm": 0.9985268712043762,
      "learning_rate": 3.787074278300357e-05,
      "loss": 2.8648,
      "step": 375100
    },
    {
      "epoch": 121.62074554294976,
      "grad_norm": 1.0372898578643799,
      "learning_rate": 3.786753162504055e-05,
      "loss": 2.8716,
      "step": 375200
    },
    {
      "epoch": 121.65316045380875,
      "grad_norm": 1.0758558511734009,
      "learning_rate": 3.78642880311385e-05,
      "loss": 2.868,
      "step": 375300
    },
    {
      "epoch": 121.68557536466774,
      "grad_norm": 0.9814138412475586,
      "learning_rate": 3.786104443723646e-05,
      "loss": 2.8661,
      "step": 375400
    },
    {
      "epoch": 121.71799027552674,
      "grad_norm": 1.236245036125183,
      "learning_rate": 3.785780084333442e-05,
      "loss": 2.8635,
      "step": 375500
    },
    {
      "epoch": 121.75040518638573,
      "grad_norm": 1.1172053813934326,
      "learning_rate": 3.785455724943237e-05,
      "loss": 2.8824,
      "step": 375600
    },
    {
      "epoch": 121.78282009724474,
      "grad_norm": 1.0652575492858887,
      "learning_rate": 3.785131365553033e-05,
      "loss": 2.877,
      "step": 375700
    },
    {
      "epoch": 121.81523500810373,
      "grad_norm": 1.1513088941574097,
      "learning_rate": 3.7848070061628287e-05,
      "loss": 2.8788,
      "step": 375800
    },
    {
      "epoch": 121.84764991896272,
      "grad_norm": 1.1312243938446045,
      "learning_rate": 3.7844858903665264e-05,
      "loss": 2.8606,
      "step": 375900
    },
    {
      "epoch": 121.88006482982172,
      "grad_norm": 1.1122410297393799,
      "learning_rate": 3.7841615309763216e-05,
      "loss": 2.8953,
      "step": 376000
    },
    {
      "epoch": 121.91247974068071,
      "grad_norm": 1.0228554010391235,
      "learning_rate": 3.7838371715861175e-05,
      "loss": 2.8844,
      "step": 376100
    },
    {
      "epoch": 121.94489465153971,
      "grad_norm": 1.198503851890564,
      "learning_rate": 3.7835128121959134e-05,
      "loss": 2.8773,
      "step": 376200
    },
    {
      "epoch": 121.9773095623987,
      "grad_norm": 1.187930941581726,
      "learning_rate": 3.7831884528057086e-05,
      "loss": 2.8756,
      "step": 376300
    },
    {
      "epoch": 122.0,
      "eval_bleu": 1.0809274658016836,
      "eval_loss": 3.8768467903137207,
      "eval_runtime": 4.2424,
      "eval_samples_per_second": 115.973,
      "eval_steps_per_second": 1.886,
      "step": 376370
    },
    {
      "epoch": 122.0097244732577,
      "grad_norm": 0.9618393778800964,
      "learning_rate": 3.7828640934155045e-05,
      "loss": 2.8846,
      "step": 376400
    },
    {
      "epoch": 122.0421393841167,
      "grad_norm": 0.983508288860321,
      "learning_rate": 3.7825397340253e-05,
      "loss": 2.85,
      "step": 376500
    },
    {
      "epoch": 122.07455429497568,
      "grad_norm": 1.070337176322937,
      "learning_rate": 3.7822153746350955e-05,
      "loss": 2.881,
      "step": 376600
    },
    {
      "epoch": 122.10696920583469,
      "grad_norm": 1.0235130786895752,
      "learning_rate": 3.7818910152448914e-05,
      "loss": 2.8635,
      "step": 376700
    },
    {
      "epoch": 122.13938411669368,
      "grad_norm": 1.1368398666381836,
      "learning_rate": 3.781566655854687e-05,
      "loss": 2.8539,
      "step": 376800
    },
    {
      "epoch": 122.17179902755268,
      "grad_norm": 1.1604764461517334,
      "learning_rate": 3.7812422964644825e-05,
      "loss": 2.8796,
      "step": 376900
    },
    {
      "epoch": 122.20421393841167,
      "grad_norm": 1.0906771421432495,
      "learning_rate": 3.7809179370742783e-05,
      "loss": 2.8699,
      "step": 377000
    },
    {
      "epoch": 122.23662884927066,
      "grad_norm": 1.1214275360107422,
      "learning_rate": 3.780593577684074e-05,
      "loss": 2.8514,
      "step": 377100
    },
    {
      "epoch": 122.26904376012966,
      "grad_norm": 1.0666766166687012,
      "learning_rate": 3.78026921829387e-05,
      "loss": 2.8693,
      "step": 377200
    },
    {
      "epoch": 122.30145867098865,
      "grad_norm": 1.1033152341842651,
      "learning_rate": 3.779944858903666e-05,
      "loss": 2.8656,
      "step": 377300
    },
    {
      "epoch": 122.33387358184766,
      "grad_norm": 1.2251067161560059,
      "learning_rate": 3.779620499513461e-05,
      "loss": 2.8685,
      "step": 377400
    },
    {
      "epoch": 122.36628849270664,
      "grad_norm": 1.096043348312378,
      "learning_rate": 3.779296140123257e-05,
      "loss": 2.8971,
      "step": 377500
    },
    {
      "epoch": 122.39870340356563,
      "grad_norm": 1.035037636756897,
      "learning_rate": 3.778971780733052e-05,
      "loss": 2.8757,
      "step": 377600
    },
    {
      "epoch": 122.43111831442464,
      "grad_norm": 1.1628270149230957,
      "learning_rate": 3.778647421342848e-05,
      "loss": 2.8492,
      "step": 377700
    },
    {
      "epoch": 122.46353322528363,
      "grad_norm": 1.2265359163284302,
      "learning_rate": 3.778323061952644e-05,
      "loss": 2.8347,
      "step": 377800
    },
    {
      "epoch": 122.49594813614263,
      "grad_norm": 1.1361290216445923,
      "learning_rate": 3.777998702562439e-05,
      "loss": 2.8872,
      "step": 377900
    },
    {
      "epoch": 122.52836304700162,
      "grad_norm": 1.0095633268356323,
      "learning_rate": 3.777674343172235e-05,
      "loss": 2.8796,
      "step": 378000
    },
    {
      "epoch": 122.56077795786062,
      "grad_norm": 1.0360918045043945,
      "learning_rate": 3.777349983782031e-05,
      "loss": 2.8674,
      "step": 378100
    },
    {
      "epoch": 122.59319286871961,
      "grad_norm": 1.1048697233200073,
      "learning_rate": 3.777025624391826e-05,
      "loss": 2.8586,
      "step": 378200
    },
    {
      "epoch": 122.6256077795786,
      "grad_norm": 1.006088376045227,
      "learning_rate": 3.776701265001622e-05,
      "loss": 2.8591,
      "step": 378300
    },
    {
      "epoch": 122.6580226904376,
      "grad_norm": 1.085175633430481,
      "learning_rate": 3.776376905611418e-05,
      "loss": 2.8624,
      "step": 378400
    },
    {
      "epoch": 122.6904376012966,
      "grad_norm": 1.0172280073165894,
      "learning_rate": 3.776052546221213e-05,
      "loss": 2.8839,
      "step": 378500
    },
    {
      "epoch": 122.7228525121556,
      "grad_norm": 1.0919506549835205,
      "learning_rate": 3.775728186831009e-05,
      "loss": 2.8772,
      "step": 378600
    },
    {
      "epoch": 122.75526742301459,
      "grad_norm": 1.032584309577942,
      "learning_rate": 3.775403827440804e-05,
      "loss": 2.8659,
      "step": 378700
    },
    {
      "epoch": 122.78768233387358,
      "grad_norm": 1.0921059846878052,
      "learning_rate": 3.7750794680506e-05,
      "loss": 2.8662,
      "step": 378800
    },
    {
      "epoch": 122.82009724473258,
      "grad_norm": 1.1425610780715942,
      "learning_rate": 3.774755108660396e-05,
      "loss": 2.8762,
      "step": 378900
    },
    {
      "epoch": 122.85251215559157,
      "grad_norm": 1.148240566253662,
      "learning_rate": 3.774430749270191e-05,
      "loss": 2.8741,
      "step": 379000
    },
    {
      "epoch": 122.88492706645057,
      "grad_norm": 1.0987201929092407,
      "learning_rate": 3.774106389879987e-05,
      "loss": 2.8757,
      "step": 379100
    },
    {
      "epoch": 122.91734197730956,
      "grad_norm": 1.0690345764160156,
      "learning_rate": 3.773782030489783e-05,
      "loss": 2.8706,
      "step": 379200
    },
    {
      "epoch": 122.94975688816855,
      "grad_norm": 0.9786434173583984,
      "learning_rate": 3.773457671099578e-05,
      "loss": 2.8663,
      "step": 379300
    },
    {
      "epoch": 122.98217179902755,
      "grad_norm": 1.0482916831970215,
      "learning_rate": 3.773133311709374e-05,
      "loss": 2.8523,
      "step": 379400
    },
    {
      "epoch": 123.0,
      "eval_bleu": 1.1760065310384533,
      "eval_loss": 3.8908684253692627,
      "eval_runtime": 4.1009,
      "eval_samples_per_second": 119.973,
      "eval_steps_per_second": 1.951,
      "step": 379455
    },
    {
      "epoch": 123.01458670988654,
      "grad_norm": 1.09323251247406,
      "learning_rate": 3.77280895231917e-05,
      "loss": 2.8548,
      "step": 379500
    },
    {
      "epoch": 123.04700162074555,
      "grad_norm": 1.086203694343567,
      "learning_rate": 3.7724845929289657e-05,
      "loss": 2.8549,
      "step": 379600
    },
    {
      "epoch": 123.07941653160454,
      "grad_norm": 1.380500078201294,
      "learning_rate": 3.7721602335387615e-05,
      "loss": 2.8427,
      "step": 379700
    },
    {
      "epoch": 123.11183144246354,
      "grad_norm": 1.0291554927825928,
      "learning_rate": 3.771835874148557e-05,
      "loss": 2.8672,
      "step": 379800
    },
    {
      "epoch": 123.14424635332253,
      "grad_norm": 1.0271908044815063,
      "learning_rate": 3.7715115147583526e-05,
      "loss": 2.8467,
      "step": 379900
    },
    {
      "epoch": 123.17666126418152,
      "grad_norm": 1.2029366493225098,
      "learning_rate": 3.7711871553681485e-05,
      "loss": 2.8641,
      "step": 380000
    },
    {
      "epoch": 123.20907617504052,
      "grad_norm": 1.1014121770858765,
      "learning_rate": 3.770862795977944e-05,
      "loss": 2.8553,
      "step": 380100
    },
    {
      "epoch": 123.24149108589951,
      "grad_norm": 1.0337868928909302,
      "learning_rate": 3.7705384365877395e-05,
      "loss": 2.8661,
      "step": 380200
    },
    {
      "epoch": 123.27390599675851,
      "grad_norm": 1.2963253259658813,
      "learning_rate": 3.7702140771975354e-05,
      "loss": 2.8852,
      "step": 380300
    },
    {
      "epoch": 123.3063209076175,
      "grad_norm": 1.0947422981262207,
      "learning_rate": 3.7698897178073306e-05,
      "loss": 2.8674,
      "step": 380400
    },
    {
      "epoch": 123.3387358184765,
      "grad_norm": 1.0820246934890747,
      "learning_rate": 3.7695653584171265e-05,
      "loss": 2.8631,
      "step": 380500
    },
    {
      "epoch": 123.3711507293355,
      "grad_norm": 1.0747134685516357,
      "learning_rate": 3.769240999026922e-05,
      "loss": 2.8703,
      "step": 380600
    },
    {
      "epoch": 123.40356564019449,
      "grad_norm": 1.1405621767044067,
      "learning_rate": 3.7689166396367176e-05,
      "loss": 2.8525,
      "step": 380700
    },
    {
      "epoch": 123.43598055105349,
      "grad_norm": 0.938515305519104,
      "learning_rate": 3.7685922802465134e-05,
      "loss": 2.8436,
      "step": 380800
    },
    {
      "epoch": 123.46839546191248,
      "grad_norm": 0.9689856767654419,
      "learning_rate": 3.7682679208563086e-05,
      "loss": 2.8695,
      "step": 380900
    },
    {
      "epoch": 123.50081037277147,
      "grad_norm": 1.1322444677352905,
      "learning_rate": 3.7679435614661045e-05,
      "loss": 2.8569,
      "step": 381000
    },
    {
      "epoch": 123.53322528363047,
      "grad_norm": 0.9449103474617004,
      "learning_rate": 3.7676192020759004e-05,
      "loss": 2.861,
      "step": 381100
    },
    {
      "epoch": 123.56564019448946,
      "grad_norm": 1.1887078285217285,
      "learning_rate": 3.7672948426856956e-05,
      "loss": 2.8669,
      "step": 381200
    },
    {
      "epoch": 123.59805510534846,
      "grad_norm": 1.3741639852523804,
      "learning_rate": 3.7669704832954915e-05,
      "loss": 2.8797,
      "step": 381300
    },
    {
      "epoch": 123.63047001620745,
      "grad_norm": 1.1859430074691772,
      "learning_rate": 3.766646123905287e-05,
      "loss": 2.8502,
      "step": 381400
    },
    {
      "epoch": 123.66288492706646,
      "grad_norm": 1.0371652841567993,
      "learning_rate": 3.7663217645150825e-05,
      "loss": 2.873,
      "step": 381500
    },
    {
      "epoch": 123.69529983792545,
      "grad_norm": 1.0776631832122803,
      "learning_rate": 3.7659974051248784e-05,
      "loss": 2.871,
      "step": 381600
    },
    {
      "epoch": 123.72771474878444,
      "grad_norm": 1.0211595296859741,
      "learning_rate": 3.765673045734674e-05,
      "loss": 2.866,
      "step": 381700
    },
    {
      "epoch": 123.76012965964344,
      "grad_norm": 1.249451994895935,
      "learning_rate": 3.7653486863444695e-05,
      "loss": 2.8515,
      "step": 381800
    },
    {
      "epoch": 123.79254457050243,
      "grad_norm": 1.1695153713226318,
      "learning_rate": 3.7650243269542653e-05,
      "loss": 2.8721,
      "step": 381900
    },
    {
      "epoch": 123.82495948136143,
      "grad_norm": 1.058497667312622,
      "learning_rate": 3.764703211157963e-05,
      "loss": 2.852,
      "step": 382000
    },
    {
      "epoch": 123.85737439222042,
      "grad_norm": 1.251700520515442,
      "learning_rate": 3.764378851767758e-05,
      "loss": 2.843,
      "step": 382100
    },
    {
      "epoch": 123.88978930307941,
      "grad_norm": 1.0012662410736084,
      "learning_rate": 3.764054492377554e-05,
      "loss": 2.8723,
      "step": 382200
    },
    {
      "epoch": 123.92220421393841,
      "grad_norm": 1.0995031595230103,
      "learning_rate": 3.76373013298735e-05,
      "loss": 2.8735,
      "step": 382300
    },
    {
      "epoch": 123.9546191247974,
      "grad_norm": 1.0645586252212524,
      "learning_rate": 3.763405773597146e-05,
      "loss": 2.8955,
      "step": 382400
    },
    {
      "epoch": 123.98703403565641,
      "grad_norm": 1.104636788368225,
      "learning_rate": 3.763081414206942e-05,
      "loss": 2.8746,
      "step": 382500
    },
    {
      "epoch": 124.0,
      "eval_bleu": 1.0606447393764946,
      "eval_loss": 3.8874988555908203,
      "eval_runtime": 4.5766,
      "eval_samples_per_second": 107.502,
      "eval_steps_per_second": 1.748,
      "step": 382540
    },
    {
      "epoch": 124.0194489465154,
      "grad_norm": 1.0447089672088623,
      "learning_rate": 3.762757054816737e-05,
      "loss": 2.8524,
      "step": 382600
    },
    {
      "epoch": 124.05186385737439,
      "grad_norm": 1.0394940376281738,
      "learning_rate": 3.762432695426533e-05,
      "loss": 2.8352,
      "step": 382700
    },
    {
      "epoch": 124.08427876823339,
      "grad_norm": 1.3409993648529053,
      "learning_rate": 3.762108336036329e-05,
      "loss": 2.8419,
      "step": 382800
    },
    {
      "epoch": 124.11669367909238,
      "grad_norm": 1.1158385276794434,
      "learning_rate": 3.761783976646124e-05,
      "loss": 2.8379,
      "step": 382900
    },
    {
      "epoch": 124.14910858995138,
      "grad_norm": 1.1562618017196655,
      "learning_rate": 3.76145961725592e-05,
      "loss": 2.8735,
      "step": 383000
    },
    {
      "epoch": 124.18152350081037,
      "grad_norm": 1.4195985794067383,
      "learning_rate": 3.761135257865716e-05,
      "loss": 2.8774,
      "step": 383100
    },
    {
      "epoch": 124.21393841166937,
      "grad_norm": 1.1061370372772217,
      "learning_rate": 3.760810898475511e-05,
      "loss": 2.8581,
      "step": 383200
    },
    {
      "epoch": 124.24635332252836,
      "grad_norm": 1.1310787200927734,
      "learning_rate": 3.760486539085307e-05,
      "loss": 2.8548,
      "step": 383300
    },
    {
      "epoch": 124.27876823338735,
      "grad_norm": 0.9688039422035217,
      "learning_rate": 3.7601621796951027e-05,
      "loss": 2.8636,
      "step": 383400
    },
    {
      "epoch": 124.31118314424636,
      "grad_norm": 1.1036380529403687,
      "learning_rate": 3.759837820304898e-05,
      "loss": 2.8853,
      "step": 383500
    },
    {
      "epoch": 124.34359805510535,
      "grad_norm": 1.140707015991211,
      "learning_rate": 3.759513460914694e-05,
      "loss": 2.8528,
      "step": 383600
    },
    {
      "epoch": 124.37601296596435,
      "grad_norm": 1.0676910877227783,
      "learning_rate": 3.7591891015244896e-05,
      "loss": 2.8599,
      "step": 383700
    },
    {
      "epoch": 124.40842787682334,
      "grad_norm": 1.0874260663986206,
      "learning_rate": 3.758864742134285e-05,
      "loss": 2.8293,
      "step": 383800
    },
    {
      "epoch": 124.44084278768233,
      "grad_norm": 1.1941694021224976,
      "learning_rate": 3.758540382744081e-05,
      "loss": 2.8804,
      "step": 383900
    },
    {
      "epoch": 124.47325769854133,
      "grad_norm": 1.1195985078811646,
      "learning_rate": 3.758216023353876e-05,
      "loss": 2.8621,
      "step": 384000
    },
    {
      "epoch": 124.50567260940032,
      "grad_norm": 1.1779850721359253,
      "learning_rate": 3.757891663963672e-05,
      "loss": 2.8659,
      "step": 384100
    },
    {
      "epoch": 124.53808752025932,
      "grad_norm": 1.2602665424346924,
      "learning_rate": 3.7575673045734676e-05,
      "loss": 2.8717,
      "step": 384200
    },
    {
      "epoch": 124.57050243111831,
      "grad_norm": 1.2177280187606812,
      "learning_rate": 3.757242945183263e-05,
      "loss": 2.8658,
      "step": 384300
    },
    {
      "epoch": 124.6029173419773,
      "grad_norm": 1.118045449256897,
      "learning_rate": 3.756918585793059e-05,
      "loss": 2.8516,
      "step": 384400
    },
    {
      "epoch": 124.6353322528363,
      "grad_norm": 1.3792284727096558,
      "learning_rate": 3.7565942264028546e-05,
      "loss": 2.8721,
      "step": 384500
    },
    {
      "epoch": 124.6677471636953,
      "grad_norm": 1.1131699085235596,
      "learning_rate": 3.7562731106065523e-05,
      "loss": 2.8731,
      "step": 384600
    },
    {
      "epoch": 124.7001620745543,
      "grad_norm": 1.2654131650924683,
      "learning_rate": 3.755955238404152e-05,
      "loss": 2.8523,
      "step": 384700
    },
    {
      "epoch": 124.73257698541329,
      "grad_norm": 1.0334880352020264,
      "learning_rate": 3.755630879013948e-05,
      "loss": 2.8598,
      "step": 384800
    },
    {
      "epoch": 124.76499189627229,
      "grad_norm": 0.9743354916572571,
      "learning_rate": 3.755306519623743e-05,
      "loss": 2.877,
      "step": 384900
    },
    {
      "epoch": 124.79740680713128,
      "grad_norm": 0.9266600608825684,
      "learning_rate": 3.754982160233539e-05,
      "loss": 2.8653,
      "step": 385000
    },
    {
      "epoch": 124.82982171799027,
      "grad_norm": 1.135232925415039,
      "learning_rate": 3.754657800843335e-05,
      "loss": 2.8764,
      "step": 385100
    },
    {
      "epoch": 124.86223662884927,
      "grad_norm": 1.0811090469360352,
      "learning_rate": 3.75433344145313e-05,
      "loss": 2.8513,
      "step": 385200
    },
    {
      "epoch": 124.89465153970826,
      "grad_norm": 0.9901009798049927,
      "learning_rate": 3.754009082062926e-05,
      "loss": 2.8605,
      "step": 385300
    },
    {
      "epoch": 124.92706645056727,
      "grad_norm": 1.0978516340255737,
      "learning_rate": 3.753684722672722e-05,
      "loss": 2.8635,
      "step": 385400
    },
    {
      "epoch": 124.95948136142626,
      "grad_norm": 1.0968453884124756,
      "learning_rate": 3.753360363282517e-05,
      "loss": 2.8722,
      "step": 385500
    },
    {
      "epoch": 124.99189627228525,
      "grad_norm": 1.128767728805542,
      "learning_rate": 3.753036003892313e-05,
      "loss": 2.8385,
      "step": 385600
    },
    {
      "epoch": 125.0,
      "eval_bleu": 1.1656868478925322,
      "eval_loss": 3.896331548690796,
      "eval_runtime": 4.408,
      "eval_samples_per_second": 111.615,
      "eval_steps_per_second": 1.815,
      "step": 385625
    },
    {
      "epoch": 125.02431118314425,
      "grad_norm": 1.1371761560440063,
      "learning_rate": 3.752711644502109e-05,
      "loss": 2.8693,
      "step": 385700
    },
    {
      "epoch": 125.05672609400324,
      "grad_norm": 1.1439073085784912,
      "learning_rate": 3.752387285111904e-05,
      "loss": 2.8447,
      "step": 385800
    },
    {
      "epoch": 125.08914100486224,
      "grad_norm": 1.1472595930099487,
      "learning_rate": 3.7520629257217e-05,
      "loss": 2.8582,
      "step": 385900
    },
    {
      "epoch": 125.12155591572123,
      "grad_norm": 1.067583441734314,
      "learning_rate": 3.751738566331495e-05,
      "loss": 2.8496,
      "step": 386000
    },
    {
      "epoch": 125.15397082658022,
      "grad_norm": 1.0580886602401733,
      "learning_rate": 3.751414206941291e-05,
      "loss": 2.8378,
      "step": 386100
    },
    {
      "epoch": 125.18638573743922,
      "grad_norm": 1.1075576543807983,
      "learning_rate": 3.751089847551087e-05,
      "loss": 2.8494,
      "step": 386200
    },
    {
      "epoch": 125.21880064829821,
      "grad_norm": 0.971312940120697,
      "learning_rate": 3.750765488160882e-05,
      "loss": 2.8506,
      "step": 386300
    },
    {
      "epoch": 125.25121555915722,
      "grad_norm": 1.0623711347579956,
      "learning_rate": 3.750441128770678e-05,
      "loss": 2.8694,
      "step": 386400
    },
    {
      "epoch": 125.2836304700162,
      "grad_norm": 0.9691382050514221,
      "learning_rate": 3.750116769380474e-05,
      "loss": 2.8567,
      "step": 386500
    },
    {
      "epoch": 125.31604538087521,
      "grad_norm": 1.0578608512878418,
      "learning_rate": 3.7497924099902696e-05,
      "loss": 2.8621,
      "step": 386600
    },
    {
      "epoch": 125.3484602917342,
      "grad_norm": 1.0131343603134155,
      "learning_rate": 3.7494680506000654e-05,
      "loss": 2.8719,
      "step": 386700
    },
    {
      "epoch": 125.38087520259319,
      "grad_norm": 1.0765149593353271,
      "learning_rate": 3.7491436912098606e-05,
      "loss": 2.8544,
      "step": 386800
    },
    {
      "epoch": 125.41329011345219,
      "grad_norm": 0.9354522228240967,
      "learning_rate": 3.7488193318196565e-05,
      "loss": 2.8324,
      "step": 386900
    },
    {
      "epoch": 125.44570502431118,
      "grad_norm": 0.9622545838356018,
      "learning_rate": 3.7484949724294524e-05,
      "loss": 2.8762,
      "step": 387000
    },
    {
      "epoch": 125.47811993517018,
      "grad_norm": 1.0669739246368408,
      "learning_rate": 3.7481706130392476e-05,
      "loss": 2.8521,
      "step": 387100
    },
    {
      "epoch": 125.51053484602917,
      "grad_norm": 1.065537691116333,
      "learning_rate": 3.7478462536490435e-05,
      "loss": 2.8399,
      "step": 387200
    },
    {
      "epoch": 125.54294975688816,
      "grad_norm": 1.1433157920837402,
      "learning_rate": 3.747521894258839e-05,
      "loss": 2.873,
      "step": 387300
    },
    {
      "epoch": 125.57536466774717,
      "grad_norm": 1.227613091468811,
      "learning_rate": 3.7471975348686345e-05,
      "loss": 2.86,
      "step": 387400
    },
    {
      "epoch": 125.60777957860616,
      "grad_norm": 1.0369677543640137,
      "learning_rate": 3.7468731754784304e-05,
      "loss": 2.8569,
      "step": 387500
    },
    {
      "epoch": 125.64019448946516,
      "grad_norm": 1.092903733253479,
      "learning_rate": 3.746548816088226e-05,
      "loss": 2.8646,
      "step": 387600
    },
    {
      "epoch": 125.67260940032415,
      "grad_norm": 1.0366599559783936,
      "learning_rate": 3.7462244566980215e-05,
      "loss": 2.8629,
      "step": 387700
    },
    {
      "epoch": 125.70502431118314,
      "grad_norm": 0.9700728058815002,
      "learning_rate": 3.7459000973078174e-05,
      "loss": 2.8473,
      "step": 387800
    },
    {
      "epoch": 125.73743922204214,
      "grad_norm": 1.269705057144165,
      "learning_rate": 3.7455757379176126e-05,
      "loss": 2.8641,
      "step": 387900
    },
    {
      "epoch": 125.76985413290113,
      "grad_norm": 1.1231908798217773,
      "learning_rate": 3.7452513785274084e-05,
      "loss": 2.8513,
      "step": 388000
    },
    {
      "epoch": 125.80226904376013,
      "grad_norm": 1.0549312829971313,
      "learning_rate": 3.744927019137204e-05,
      "loss": 2.8818,
      "step": 388100
    },
    {
      "epoch": 125.83468395461912,
      "grad_norm": 1.1515785455703735,
      "learning_rate": 3.7446026597469995e-05,
      "loss": 2.8561,
      "step": 388200
    },
    {
      "epoch": 125.86709886547813,
      "grad_norm": 1.2804243564605713,
      "learning_rate": 3.7442783003567954e-05,
      "loss": 2.8525,
      "step": 388300
    },
    {
      "epoch": 125.89951377633712,
      "grad_norm": 1.007112979888916,
      "learning_rate": 3.743953940966591e-05,
      "loss": 2.8721,
      "step": 388400
    },
    {
      "epoch": 125.9319286871961,
      "grad_norm": 1.1457313299179077,
      "learning_rate": 3.7436295815763864e-05,
      "loss": 2.8654,
      "step": 388500
    },
    {
      "epoch": 125.96434359805511,
      "grad_norm": 1.2318167686462402,
      "learning_rate": 3.743305222186182e-05,
      "loss": 2.8726,
      "step": 388600
    },
    {
      "epoch": 125.9967585089141,
      "grad_norm": 1.1270641088485718,
      "learning_rate": 3.742980862795978e-05,
      "loss": 2.8589,
      "step": 388700
    },
    {
      "epoch": 126.0,
      "eval_bleu": 1.144234760434017,
      "eval_loss": 3.890739917755127,
      "eval_runtime": 3.9613,
      "eval_samples_per_second": 124.203,
      "eval_steps_per_second": 2.02,
      "step": 388710
    },
    {
      "epoch": 126.0291734197731,
      "grad_norm": 1.1095497608184814,
      "learning_rate": 3.7426565034057734e-05,
      "loss": 2.8339,
      "step": 388800
    },
    {
      "epoch": 126.06158833063209,
      "grad_norm": 1.3348326683044434,
      "learning_rate": 3.742332144015569e-05,
      "loss": 2.8503,
      "step": 388900
    },
    {
      "epoch": 126.09400324149108,
      "grad_norm": 1.3527048826217651,
      "learning_rate": 3.742007784625365e-05,
      "loss": 2.8417,
      "step": 389000
    },
    {
      "epoch": 126.12641815235008,
      "grad_norm": 1.0278602838516235,
      "learning_rate": 3.741683425235161e-05,
      "loss": 2.8391,
      "step": 389100
    },
    {
      "epoch": 126.15883306320907,
      "grad_norm": 1.3834482431411743,
      "learning_rate": 3.741359065844957e-05,
      "loss": 2.8379,
      "step": 389200
    },
    {
      "epoch": 126.19124797406808,
      "grad_norm": 1.1826516389846802,
      "learning_rate": 3.741034706454752e-05,
      "loss": 2.8366,
      "step": 389300
    },
    {
      "epoch": 126.22366288492707,
      "grad_norm": 1.1976529359817505,
      "learning_rate": 3.740710347064548e-05,
      "loss": 2.8653,
      "step": 389400
    },
    {
      "epoch": 126.25607779578606,
      "grad_norm": 1.1144225597381592,
      "learning_rate": 3.740385987674344e-05,
      "loss": 2.8508,
      "step": 389500
    },
    {
      "epoch": 126.28849270664506,
      "grad_norm": 1.154296875,
      "learning_rate": 3.740061628284139e-05,
      "loss": 2.8707,
      "step": 389600
    },
    {
      "epoch": 126.32090761750405,
      "grad_norm": 1.1390736103057861,
      "learning_rate": 3.739737268893935e-05,
      "loss": 2.8473,
      "step": 389700
    },
    {
      "epoch": 126.35332252836305,
      "grad_norm": 1.007563829421997,
      "learning_rate": 3.739412909503731e-05,
      "loss": 2.8626,
      "step": 389800
    },
    {
      "epoch": 126.38573743922204,
      "grad_norm": 1.2222906351089478,
      "learning_rate": 3.739088550113526e-05,
      "loss": 2.8567,
      "step": 389900
    },
    {
      "epoch": 126.41815235008104,
      "grad_norm": 1.0925341844558716,
      "learning_rate": 3.738764190723322e-05,
      "loss": 2.858,
      "step": 390000
    },
    {
      "epoch": 126.45056726094003,
      "grad_norm": 1.1391985416412354,
      "learning_rate": 3.738439831333117e-05,
      "loss": 2.8488,
      "step": 390100
    },
    {
      "epoch": 126.48298217179902,
      "grad_norm": 1.4034161567687988,
      "learning_rate": 3.738115471942913e-05,
      "loss": 2.8655,
      "step": 390200
    },
    {
      "epoch": 126.51539708265803,
      "grad_norm": 1.0443358421325684,
      "learning_rate": 3.737791112552709e-05,
      "loss": 2.8632,
      "step": 390300
    },
    {
      "epoch": 126.54781199351702,
      "grad_norm": 1.056084394454956,
      "learning_rate": 3.737466753162504e-05,
      "loss": 2.8467,
      "step": 390400
    },
    {
      "epoch": 126.58022690437602,
      "grad_norm": 1.039080023765564,
      "learning_rate": 3.7371423937723e-05,
      "loss": 2.8613,
      "step": 390500
    },
    {
      "epoch": 126.61264181523501,
      "grad_norm": 1.0943008661270142,
      "learning_rate": 3.736818034382096e-05,
      "loss": 2.8697,
      "step": 390600
    },
    {
      "epoch": 126.645056726094,
      "grad_norm": 1.0455683469772339,
      "learning_rate": 3.736493674991891e-05,
      "loss": 2.8411,
      "step": 390700
    },
    {
      "epoch": 126.677471636953,
      "grad_norm": 1.111341953277588,
      "learning_rate": 3.736169315601687e-05,
      "loss": 2.8683,
      "step": 390800
    },
    {
      "epoch": 126.70988654781199,
      "grad_norm": 1.1289030313491821,
      "learning_rate": 3.735844956211482e-05,
      "loss": 2.843,
      "step": 390900
    },
    {
      "epoch": 126.742301458671,
      "grad_norm": 1.0827826261520386,
      "learning_rate": 3.735520596821278e-05,
      "loss": 2.868,
      "step": 391000
    },
    {
      "epoch": 126.77471636952998,
      "grad_norm": 1.1757768392562866,
      "learning_rate": 3.735196237431074e-05,
      "loss": 2.8687,
      "step": 391100
    },
    {
      "epoch": 126.80713128038897,
      "grad_norm": 1.1204373836517334,
      "learning_rate": 3.734871878040869e-05,
      "loss": 2.8622,
      "step": 391200
    },
    {
      "epoch": 126.83954619124798,
      "grad_norm": 1.0272464752197266,
      "learning_rate": 3.734547518650665e-05,
      "loss": 2.8571,
      "step": 391300
    },
    {
      "epoch": 126.87196110210697,
      "grad_norm": 1.0119333267211914,
      "learning_rate": 3.734223159260461e-05,
      "loss": 2.8704,
      "step": 391400
    },
    {
      "epoch": 126.90437601296597,
      "grad_norm": 1.1352518796920776,
      "learning_rate": 3.7338987998702566e-05,
      "loss": 2.8537,
      "step": 391500
    },
    {
      "epoch": 126.93679092382496,
      "grad_norm": 1.3656423091888428,
      "learning_rate": 3.7335744404800524e-05,
      "loss": 2.8678,
      "step": 391600
    },
    {
      "epoch": 126.96920583468396,
      "grad_norm": 1.1923563480377197,
      "learning_rate": 3.7332500810898476e-05,
      "loss": 2.867,
      "step": 391700
    },
    {
      "epoch": 127.0,
      "eval_bleu": 1.0332046646518678,
      "eval_loss": 3.8995251655578613,
      "eval_runtime": 4.2412,
      "eval_samples_per_second": 116.006,
      "eval_steps_per_second": 1.886,
      "step": 391795
    },
    {
      "epoch": 127.00162074554295,
      "grad_norm": 1.0265494585037231,
      "learning_rate": 3.7329257216996435e-05,
      "loss": 2.8568,
      "step": 391800
    },
    {
      "epoch": 127.03403565640194,
      "grad_norm": 1.2449259757995605,
      "learning_rate": 3.7326013623094394e-05,
      "loss": 2.8303,
      "step": 391900
    },
    {
      "epoch": 127.06645056726094,
      "grad_norm": 1.1556648015975952,
      "learning_rate": 3.7322770029192346e-05,
      "loss": 2.8409,
      "step": 392000
    },
    {
      "epoch": 127.09886547811993,
      "grad_norm": 1.1245088577270508,
      "learning_rate": 3.7319526435290305e-05,
      "loss": 2.8483,
      "step": 392100
    },
    {
      "epoch": 127.13128038897894,
      "grad_norm": 1.0291237831115723,
      "learning_rate": 3.731628284138826e-05,
      "loss": 2.8445,
      "step": 392200
    },
    {
      "epoch": 127.16369529983793,
      "grad_norm": 1.1118323802947998,
      "learning_rate": 3.7313039247486215e-05,
      "loss": 2.8432,
      "step": 392300
    },
    {
      "epoch": 127.19611021069692,
      "grad_norm": 0.9680117964744568,
      "learning_rate": 3.7309795653584174e-05,
      "loss": 2.8555,
      "step": 392400
    },
    {
      "epoch": 127.22852512155592,
      "grad_norm": 1.1464568376541138,
      "learning_rate": 3.730655205968213e-05,
      "loss": 2.8648,
      "step": 392500
    },
    {
      "epoch": 127.26094003241491,
      "grad_norm": 1.183963418006897,
      "learning_rate": 3.7303308465780085e-05,
      "loss": 2.8577,
      "step": 392600
    },
    {
      "epoch": 127.29335494327391,
      "grad_norm": 1.0766096115112305,
      "learning_rate": 3.730012974375608e-05,
      "loss": 2.8377,
      "step": 392700
    },
    {
      "epoch": 127.3257698541329,
      "grad_norm": 1.0374741554260254,
      "learning_rate": 3.729688614985404e-05,
      "loss": 2.8395,
      "step": 392800
    },
    {
      "epoch": 127.35818476499189,
      "grad_norm": 1.0509469509124756,
      "learning_rate": 3.7293642555952e-05,
      "loss": 2.8557,
      "step": 392900
    },
    {
      "epoch": 127.3905996758509,
      "grad_norm": 1.0802826881408691,
      "learning_rate": 3.729039896204996e-05,
      "loss": 2.851,
      "step": 393000
    },
    {
      "epoch": 127.42301458670988,
      "grad_norm": 0.9590715765953064,
      "learning_rate": 3.728715536814791e-05,
      "loss": 2.8377,
      "step": 393100
    },
    {
      "epoch": 127.45542949756889,
      "grad_norm": 1.083048701286316,
      "learning_rate": 3.728391177424587e-05,
      "loss": 2.8525,
      "step": 393200
    },
    {
      "epoch": 127.48784440842788,
      "grad_norm": 1.012449860572815,
      "learning_rate": 3.728066818034383e-05,
      "loss": 2.8602,
      "step": 393300
    },
    {
      "epoch": 127.52025931928688,
      "grad_norm": 1.1224114894866943,
      "learning_rate": 3.727742458644178e-05,
      "loss": 2.8627,
      "step": 393400
    },
    {
      "epoch": 127.55267423014587,
      "grad_norm": 0.9750493764877319,
      "learning_rate": 3.727418099253974e-05,
      "loss": 2.8432,
      "step": 393500
    },
    {
      "epoch": 127.58508914100486,
      "grad_norm": 1.015386700630188,
      "learning_rate": 3.727093739863769e-05,
      "loss": 2.8568,
      "step": 393600
    },
    {
      "epoch": 127.61750405186386,
      "grad_norm": 0.964790940284729,
      "learning_rate": 3.726769380473565e-05,
      "loss": 2.8522,
      "step": 393700
    },
    {
      "epoch": 127.64991896272285,
      "grad_norm": 1.103628396987915,
      "learning_rate": 3.726445021083361e-05,
      "loss": 2.8769,
      "step": 393800
    },
    {
      "epoch": 127.68233387358185,
      "grad_norm": 1.21985924243927,
      "learning_rate": 3.726120661693156e-05,
      "loss": 2.8574,
      "step": 393900
    },
    {
      "epoch": 127.71474878444084,
      "grad_norm": 1.189236044883728,
      "learning_rate": 3.725796302302952e-05,
      "loss": 2.8589,
      "step": 394000
    },
    {
      "epoch": 127.74716369529983,
      "grad_norm": 1.1294915676116943,
      "learning_rate": 3.725471942912748e-05,
      "loss": 2.8561,
      "step": 394100
    },
    {
      "epoch": 127.77957860615884,
      "grad_norm": 1.182120680809021,
      "learning_rate": 3.725147583522543e-05,
      "loss": 2.8693,
      "step": 394200
    },
    {
      "epoch": 127.81199351701783,
      "grad_norm": 0.9523868560791016,
      "learning_rate": 3.724823224132339e-05,
      "loss": 2.8677,
      "step": 394300
    },
    {
      "epoch": 127.84440842787683,
      "grad_norm": 1.1323860883712769,
      "learning_rate": 3.7244988647421346e-05,
      "loss": 2.8681,
      "step": 394400
    },
    {
      "epoch": 127.87682333873582,
      "grad_norm": 1.1248334646224976,
      "learning_rate": 3.72417450535193e-05,
      "loss": 2.8585,
      "step": 394500
    },
    {
      "epoch": 127.90923824959481,
      "grad_norm": 0.9878029227256775,
      "learning_rate": 3.723850145961726e-05,
      "loss": 2.8526,
      "step": 394600
    },
    {
      "epoch": 127.94165316045381,
      "grad_norm": 0.9826904535293579,
      "learning_rate": 3.723525786571521e-05,
      "loss": 2.846,
      "step": 394700
    },
    {
      "epoch": 127.9740680713128,
      "grad_norm": 1.1938576698303223,
      "learning_rate": 3.723201427181317e-05,
      "loss": 2.8542,
      "step": 394800
    },
    {
      "epoch": 128.0,
      "eval_bleu": 1.0004628763089498,
      "eval_loss": 3.8971853256225586,
      "eval_runtime": 5.8449,
      "eval_samples_per_second": 84.176,
      "eval_steps_per_second": 1.369,
      "step": 394880
    },
    {
      "epoch": 128.0064829821718,
      "grad_norm": 1.0941208600997925,
      "learning_rate": 3.7228770677911127e-05,
      "loss": 2.8591,
      "step": 394900
    },
    {
      "epoch": 128.0388978930308,
      "grad_norm": 1.0463836193084717,
      "learning_rate": 3.7225527084009085e-05,
      "loss": 2.8516,
      "step": 395000
    },
    {
      "epoch": 128.07131280388978,
      "grad_norm": 1.431215763092041,
      "learning_rate": 3.722228349010704e-05,
      "loss": 2.847,
      "step": 395100
    },
    {
      "epoch": 128.10372771474877,
      "grad_norm": 1.131486415863037,
      "learning_rate": 3.7219039896204996e-05,
      "loss": 2.8471,
      "step": 395200
    },
    {
      "epoch": 128.1361426256078,
      "grad_norm": 1.1070035696029663,
      "learning_rate": 3.7215796302302955e-05,
      "loss": 2.8548,
      "step": 395300
    },
    {
      "epoch": 128.16855753646678,
      "grad_norm": 1.1266489028930664,
      "learning_rate": 3.7212552708400913e-05,
      "loss": 2.8346,
      "step": 395400
    },
    {
      "epoch": 128.20097244732577,
      "grad_norm": 1.2018897533416748,
      "learning_rate": 3.720930911449887e-05,
      "loss": 2.8283,
      "step": 395500
    },
    {
      "epoch": 128.23338735818476,
      "grad_norm": 1.0151081085205078,
      "learning_rate": 3.7206065520596824e-05,
      "loss": 2.8527,
      "step": 395600
    },
    {
      "epoch": 128.26580226904375,
      "grad_norm": 1.1337919235229492,
      "learning_rate": 3.720282192669478e-05,
      "loss": 2.8318,
      "step": 395700
    },
    {
      "epoch": 128.29821717990276,
      "grad_norm": 1.1875773668289185,
      "learning_rate": 3.7199578332792735e-05,
      "loss": 2.8726,
      "step": 395800
    },
    {
      "epoch": 128.33063209076175,
      "grad_norm": 1.1151907444000244,
      "learning_rate": 3.7196334738890694e-05,
      "loss": 2.845,
      "step": 395900
    },
    {
      "epoch": 128.36304700162074,
      "grad_norm": 1.3311759233474731,
      "learning_rate": 3.719309114498865e-05,
      "loss": 2.8412,
      "step": 396000
    },
    {
      "epoch": 128.39546191247973,
      "grad_norm": 1.0417510271072388,
      "learning_rate": 3.7189847551086604e-05,
      "loss": 2.855,
      "step": 396100
    },
    {
      "epoch": 128.42787682333875,
      "grad_norm": 1.2229914665222168,
      "learning_rate": 3.718660395718456e-05,
      "loss": 2.8483,
      "step": 396200
    },
    {
      "epoch": 128.46029173419774,
      "grad_norm": 1.0727155208587646,
      "learning_rate": 3.718336036328252e-05,
      "loss": 2.8525,
      "step": 396300
    },
    {
      "epoch": 128.49270664505673,
      "grad_norm": 1.1953445672988892,
      "learning_rate": 3.7180116769380474e-05,
      "loss": 2.8372,
      "step": 396400
    },
    {
      "epoch": 128.52512155591572,
      "grad_norm": 1.072111964225769,
      "learning_rate": 3.717687317547843e-05,
      "loss": 2.8558,
      "step": 396500
    },
    {
      "epoch": 128.5575364667747,
      "grad_norm": 1.167895793914795,
      "learning_rate": 3.7173629581576385e-05,
      "loss": 2.8534,
      "step": 396600
    },
    {
      "epoch": 128.58995137763372,
      "grad_norm": 1.0823969841003418,
      "learning_rate": 3.717041842361337e-05,
      "loss": 2.8541,
      "step": 396700
    },
    {
      "epoch": 128.6223662884927,
      "grad_norm": 1.1457768678665161,
      "learning_rate": 3.716717482971132e-05,
      "loss": 2.8667,
      "step": 396800
    },
    {
      "epoch": 128.6547811993517,
      "grad_norm": 1.1500948667526245,
      "learning_rate": 3.716393123580928e-05,
      "loss": 2.8588,
      "step": 396900
    },
    {
      "epoch": 128.6871961102107,
      "grad_norm": 0.9372639656066895,
      "learning_rate": 3.716068764190723e-05,
      "loss": 2.8564,
      "step": 397000
    },
    {
      "epoch": 128.71961102106968,
      "grad_norm": 1.2349752187728882,
      "learning_rate": 3.715744404800519e-05,
      "loss": 2.8709,
      "step": 397100
    },
    {
      "epoch": 128.7520259319287,
      "grad_norm": 1.0839190483093262,
      "learning_rate": 3.715420045410315e-05,
      "loss": 2.867,
      "step": 397200
    },
    {
      "epoch": 128.7844408427877,
      "grad_norm": 1.2744922637939453,
      "learning_rate": 3.71509568602011e-05,
      "loss": 2.8485,
      "step": 397300
    },
    {
      "epoch": 128.81685575364668,
      "grad_norm": 1.0958517789840698,
      "learning_rate": 3.714771326629906e-05,
      "loss": 2.8276,
      "step": 397400
    },
    {
      "epoch": 128.84927066450567,
      "grad_norm": 1.0940908193588257,
      "learning_rate": 3.714446967239702e-05,
      "loss": 2.8379,
      "step": 397500
    },
    {
      "epoch": 128.88168557536466,
      "grad_norm": 1.2194173336029053,
      "learning_rate": 3.714122607849497e-05,
      "loss": 2.8593,
      "step": 397600
    },
    {
      "epoch": 128.91410048622367,
      "grad_norm": 1.0907814502716064,
      "learning_rate": 3.713798248459293e-05,
      "loss": 2.8641,
      "step": 397700
    },
    {
      "epoch": 128.94651539708266,
      "grad_norm": 1.116102695465088,
      "learning_rate": 3.713473889069089e-05,
      "loss": 2.8511,
      "step": 397800
    },
    {
      "epoch": 128.97893030794165,
      "grad_norm": 1.296525478363037,
      "learning_rate": 3.713149529678884e-05,
      "loss": 2.8608,
      "step": 397900
    },
    {
      "epoch": 129.0,
      "eval_bleu": 1.183185802310993,
      "eval_loss": 3.8930981159210205,
      "eval_runtime": 4.6684,
      "eval_samples_per_second": 105.39,
      "eval_steps_per_second": 1.714,
      "step": 397965
    },
    {
      "epoch": 129.01134521880064,
      "grad_norm": 1.0913752317428589,
      "learning_rate": 3.71282517028868e-05,
      "loss": 2.8513,
      "step": 398000
    },
    {
      "epoch": 129.04376012965963,
      "grad_norm": 1.151758074760437,
      "learning_rate": 3.712500810898476e-05,
      "loss": 2.8653,
      "step": 398100
    },
    {
      "epoch": 129.07617504051865,
      "grad_norm": 1.2206677198410034,
      "learning_rate": 3.7121764515082716e-05,
      "loss": 2.8368,
      "step": 398200
    },
    {
      "epoch": 129.10858995137764,
      "grad_norm": 1.0412344932556152,
      "learning_rate": 3.711852092118067e-05,
      "loss": 2.8433,
      "step": 398300
    },
    {
      "epoch": 129.14100486223663,
      "grad_norm": 1.0453141927719116,
      "learning_rate": 3.711527732727863e-05,
      "loss": 2.8393,
      "step": 398400
    },
    {
      "epoch": 129.17341977309562,
      "grad_norm": 1.254723310470581,
      "learning_rate": 3.7112033733376586e-05,
      "loss": 2.8433,
      "step": 398500
    },
    {
      "epoch": 129.2058346839546,
      "grad_norm": 1.105700969696045,
      "learning_rate": 3.7108790139474545e-05,
      "loss": 2.8521,
      "step": 398600
    },
    {
      "epoch": 129.23824959481362,
      "grad_norm": 0.9444253444671631,
      "learning_rate": 3.7105546545572497e-05,
      "loss": 2.8479,
      "step": 398700
    },
    {
      "epoch": 129.2706645056726,
      "grad_norm": 1.1540215015411377,
      "learning_rate": 3.7102335387609474e-05,
      "loss": 2.8465,
      "step": 398800
    },
    {
      "epoch": 129.3030794165316,
      "grad_norm": 1.0400512218475342,
      "learning_rate": 3.709909179370743e-05,
      "loss": 2.8411,
      "step": 398900
    },
    {
      "epoch": 129.3354943273906,
      "grad_norm": 1.1019072532653809,
      "learning_rate": 3.709584819980539e-05,
      "loss": 2.8279,
      "step": 399000
    },
    {
      "epoch": 129.36790923824958,
      "grad_norm": 1.0452029705047607,
      "learning_rate": 3.7092604605903344e-05,
      "loss": 2.8561,
      "step": 399100
    },
    {
      "epoch": 129.4003241491086,
      "grad_norm": 0.9424725770950317,
      "learning_rate": 3.70893610120013e-05,
      "loss": 2.8767,
      "step": 399200
    },
    {
      "epoch": 129.4327390599676,
      "grad_norm": 0.9536263346672058,
      "learning_rate": 3.7086117418099254e-05,
      "loss": 2.8393,
      "step": 399300
    },
    {
      "epoch": 129.46515397082658,
      "grad_norm": 1.027751088142395,
      "learning_rate": 3.708287382419721e-05,
      "loss": 2.8256,
      "step": 399400
    },
    {
      "epoch": 129.49756888168557,
      "grad_norm": 1.0225037336349487,
      "learning_rate": 3.707963023029517e-05,
      "loss": 2.8439,
      "step": 399500
    },
    {
      "epoch": 129.52998379254458,
      "grad_norm": 1.1091376543045044,
      "learning_rate": 3.7076386636393124e-05,
      "loss": 2.8353,
      "step": 399600
    },
    {
      "epoch": 129.56239870340357,
      "grad_norm": 1.2414199113845825,
      "learning_rate": 3.70731754784301e-05,
      "loss": 2.8584,
      "step": 399700
    },
    {
      "epoch": 129.59481361426256,
      "grad_norm": 1.003097653388977,
      "learning_rate": 3.706993188452806e-05,
      "loss": 2.8494,
      "step": 399800
    },
    {
      "epoch": 129.62722852512155,
      "grad_norm": 1.1336499452590942,
      "learning_rate": 3.706668829062602e-05,
      "loss": 2.8686,
      "step": 399900
    },
    {
      "epoch": 129.65964343598054,
      "grad_norm": 1.2327324151992798,
      "learning_rate": 3.706344469672397e-05,
      "loss": 2.8671,
      "step": 400000
    },
    {
      "epoch": 129.69205834683956,
      "grad_norm": 1.021756887435913,
      "learning_rate": 3.706020110282193e-05,
      "loss": 2.8559,
      "step": 400100
    },
    {
      "epoch": 129.72447325769855,
      "grad_norm": 1.297681212425232,
      "learning_rate": 3.705695750891989e-05,
      "loss": 2.8597,
      "step": 400200
    },
    {
      "epoch": 129.75688816855754,
      "grad_norm": 1.1173195838928223,
      "learning_rate": 3.705371391501784e-05,
      "loss": 2.8354,
      "step": 400300
    },
    {
      "epoch": 129.78930307941653,
      "grad_norm": 1.2967593669891357,
      "learning_rate": 3.70504703211158e-05,
      "loss": 2.8464,
      "step": 400400
    },
    {
      "epoch": 129.82171799027552,
      "grad_norm": 1.11530339717865,
      "learning_rate": 3.704722672721375e-05,
      "loss": 2.848,
      "step": 400500
    },
    {
      "epoch": 129.85413290113453,
      "grad_norm": 1.1079341173171997,
      "learning_rate": 3.704398313331171e-05,
      "loss": 2.8395,
      "step": 400600
    },
    {
      "epoch": 129.88654781199352,
      "grad_norm": 1.134265661239624,
      "learning_rate": 3.704073953940967e-05,
      "loss": 2.866,
      "step": 400700
    },
    {
      "epoch": 129.9189627228525,
      "grad_norm": 1.3275600671768188,
      "learning_rate": 3.703749594550762e-05,
      "loss": 2.83,
      "step": 400800
    },
    {
      "epoch": 129.9513776337115,
      "grad_norm": 1.0498182773590088,
      "learning_rate": 3.703425235160558e-05,
      "loss": 2.8543,
      "step": 400900
    },
    {
      "epoch": 129.9837925445705,
      "grad_norm": 1.004542350769043,
      "learning_rate": 3.703100875770354e-05,
      "loss": 2.8505,
      "step": 401000
    },
    {
      "epoch": 130.0,
      "eval_bleu": 1.1258427668400723,
      "eval_loss": 3.9023995399475098,
      "eval_runtime": 5.8281,
      "eval_samples_per_second": 84.419,
      "eval_steps_per_second": 1.373,
      "step": 401050
    },
    {
      "epoch": 130.0162074554295,
      "grad_norm": 1.1719779968261719,
      "learning_rate": 3.702776516380149e-05,
      "loss": 2.8527,
      "step": 401100
    },
    {
      "epoch": 130.0486223662885,
      "grad_norm": 1.0407954454421997,
      "learning_rate": 3.702452156989945e-05,
      "loss": 2.8316,
      "step": 401200
    },
    {
      "epoch": 130.0810372771475,
      "grad_norm": 1.0970592498779297,
      "learning_rate": 3.702127797599741e-05,
      "loss": 2.8538,
      "step": 401300
    },
    {
      "epoch": 130.11345218800648,
      "grad_norm": 1.1036581993103027,
      "learning_rate": 3.701803438209536e-05,
      "loss": 2.8405,
      "step": 401400
    },
    {
      "epoch": 130.14586709886547,
      "grad_norm": 1.0372234582901,
      "learning_rate": 3.701479078819332e-05,
      "loss": 2.8444,
      "step": 401500
    },
    {
      "epoch": 130.17828200972448,
      "grad_norm": 1.168461561203003,
      "learning_rate": 3.701154719429128e-05,
      "loss": 2.8389,
      "step": 401600
    },
    {
      "epoch": 130.21069692058347,
      "grad_norm": 1.053789496421814,
      "learning_rate": 3.700830360038923e-05,
      "loss": 2.8318,
      "step": 401700
    },
    {
      "epoch": 130.24311183144246,
      "grad_norm": 1.1848770380020142,
      "learning_rate": 3.700506000648719e-05,
      "loss": 2.8318,
      "step": 401800
    },
    {
      "epoch": 130.27552674230145,
      "grad_norm": 1.0989108085632324,
      "learning_rate": 3.700181641258515e-05,
      "loss": 2.8646,
      "step": 401900
    },
    {
      "epoch": 130.30794165316044,
      "grad_norm": 1.0001304149627686,
      "learning_rate": 3.6998572818683105e-05,
      "loss": 2.8382,
      "step": 402000
    },
    {
      "epoch": 130.34035656401946,
      "grad_norm": 1.1670331954956055,
      "learning_rate": 3.6995329224781064e-05,
      "loss": 2.8567,
      "step": 402100
    },
    {
      "epoch": 130.37277147487845,
      "grad_norm": 1.057155966758728,
      "learning_rate": 3.6992085630879016e-05,
      "loss": 2.8645,
      "step": 402200
    },
    {
      "epoch": 130.40518638573744,
      "grad_norm": 1.0680086612701416,
      "learning_rate": 3.6988842036976975e-05,
      "loss": 2.8284,
      "step": 402300
    },
    {
      "epoch": 130.43760129659643,
      "grad_norm": 1.1373382806777954,
      "learning_rate": 3.6985598443074934e-05,
      "loss": 2.8457,
      "step": 402400
    },
    {
      "epoch": 130.47001620745542,
      "grad_norm": 1.0656073093414307,
      "learning_rate": 3.6982354849172886e-05,
      "loss": 2.8386,
      "step": 402500
    },
    {
      "epoch": 130.50243111831443,
      "grad_norm": 1.0907633304595947,
      "learning_rate": 3.6979111255270844e-05,
      "loss": 2.8378,
      "step": 402600
    },
    {
      "epoch": 130.53484602917342,
      "grad_norm": 1.0951299667358398,
      "learning_rate": 3.6975867661368796e-05,
      "loss": 2.8537,
      "step": 402700
    },
    {
      "epoch": 130.5672609400324,
      "grad_norm": 1.1710692644119263,
      "learning_rate": 3.6972624067466755e-05,
      "loss": 2.8363,
      "step": 402800
    },
    {
      "epoch": 130.5996758508914,
      "grad_norm": 1.2620302438735962,
      "learning_rate": 3.6969380473564714e-05,
      "loss": 2.8518,
      "step": 402900
    },
    {
      "epoch": 130.63209076175042,
      "grad_norm": 1.038233995437622,
      "learning_rate": 3.6966136879662666e-05,
      "loss": 2.8476,
      "step": 403000
    },
    {
      "epoch": 130.6645056726094,
      "grad_norm": 1.1994112730026245,
      "learning_rate": 3.6962893285760624e-05,
      "loss": 2.8517,
      "step": 403100
    },
    {
      "epoch": 130.6969205834684,
      "grad_norm": 1.0598143339157104,
      "learning_rate": 3.695964969185858e-05,
      "loss": 2.8479,
      "step": 403200
    },
    {
      "epoch": 130.7293354943274,
      "grad_norm": 1.2116336822509766,
      "learning_rate": 3.6956406097956535e-05,
      "loss": 2.8542,
      "step": 403300
    },
    {
      "epoch": 130.76175040518638,
      "grad_norm": 1.1271920204162598,
      "learning_rate": 3.6953162504054494e-05,
      "loss": 2.8502,
      "step": 403400
    },
    {
      "epoch": 130.7941653160454,
      "grad_norm": 1.0721745491027832,
      "learning_rate": 3.694991891015245e-05,
      "loss": 2.8584,
      "step": 403500
    },
    {
      "epoch": 130.82658022690438,
      "grad_norm": 1.120123028755188,
      "learning_rate": 3.6946675316250405e-05,
      "loss": 2.8477,
      "step": 403600
    },
    {
      "epoch": 130.85899513776337,
      "grad_norm": 1.1450767517089844,
      "learning_rate": 3.694346415828738e-05,
      "loss": 2.8565,
      "step": 403700
    },
    {
      "epoch": 130.89141004862236,
      "grad_norm": 1.0246564149856567,
      "learning_rate": 3.694022056438534e-05,
      "loss": 2.8562,
      "step": 403800
    },
    {
      "epoch": 130.92382495948135,
      "grad_norm": 1.1752218008041382,
      "learning_rate": 3.693697697048329e-05,
      "loss": 2.8316,
      "step": 403900
    },
    {
      "epoch": 130.95623987034037,
      "grad_norm": 1.157132625579834,
      "learning_rate": 3.693376581252028e-05,
      "loss": 2.8521,
      "step": 404000
    },
    {
      "epoch": 130.98865478119936,
      "grad_norm": 1.039039969444275,
      "learning_rate": 3.693052221861823e-05,
      "loss": 2.8409,
      "step": 404100
    },
    {
      "epoch": 131.0,
      "eval_bleu": 1.062321432988095,
      "eval_loss": 3.8989429473876953,
      "eval_runtime": 4.1588,
      "eval_samples_per_second": 118.302,
      "eval_steps_per_second": 1.924,
      "step": 404135
    },
    {
      "epoch": 131.02106969205835,
      "grad_norm": 1.2522709369659424,
      "learning_rate": 3.692727862471619e-05,
      "loss": 2.837,
      "step": 404200
    },
    {
      "epoch": 131.05348460291734,
      "grad_norm": 1.1356548070907593,
      "learning_rate": 3.692403503081414e-05,
      "loss": 2.8331,
      "step": 404300
    },
    {
      "epoch": 131.08589951377633,
      "grad_norm": 1.1264169216156006,
      "learning_rate": 3.69207914369121e-05,
      "loss": 2.8522,
      "step": 404400
    },
    {
      "epoch": 131.11831442463534,
      "grad_norm": 1.074060320854187,
      "learning_rate": 3.691754784301006e-05,
      "loss": 2.8402,
      "step": 404500
    },
    {
      "epoch": 131.15072933549433,
      "grad_norm": 1.2137378454208374,
      "learning_rate": 3.691430424910801e-05,
      "loss": 2.8411,
      "step": 404600
    },
    {
      "epoch": 131.18314424635332,
      "grad_norm": 1.0535213947296143,
      "learning_rate": 3.691106065520597e-05,
      "loss": 2.8357,
      "step": 404700
    },
    {
      "epoch": 131.2155591572123,
      "grad_norm": 1.159856915473938,
      "learning_rate": 3.690781706130393e-05,
      "loss": 2.8272,
      "step": 404800
    },
    {
      "epoch": 131.2479740680713,
      "grad_norm": 1.157634973526001,
      "learning_rate": 3.690457346740188e-05,
      "loss": 2.8433,
      "step": 404900
    },
    {
      "epoch": 131.28038897893032,
      "grad_norm": 1.1534391641616821,
      "learning_rate": 3.690132987349984e-05,
      "loss": 2.8273,
      "step": 405000
    },
    {
      "epoch": 131.3128038897893,
      "grad_norm": 0.9919010996818542,
      "learning_rate": 3.68980862795978e-05,
      "loss": 2.8429,
      "step": 405100
    },
    {
      "epoch": 131.3452188006483,
      "grad_norm": 1.2205597162246704,
      "learning_rate": 3.689484268569575e-05,
      "loss": 2.84,
      "step": 405200
    },
    {
      "epoch": 131.3776337115073,
      "grad_norm": 1.2044117450714111,
      "learning_rate": 3.689159909179371e-05,
      "loss": 2.8448,
      "step": 405300
    },
    {
      "epoch": 131.41004862236628,
      "grad_norm": 1.1451263427734375,
      "learning_rate": 3.6888355497891666e-05,
      "loss": 2.8328,
      "step": 405400
    },
    {
      "epoch": 131.4424635332253,
      "grad_norm": 1.0443115234375,
      "learning_rate": 3.6885111903989625e-05,
      "loss": 2.8431,
      "step": 405500
    },
    {
      "epoch": 131.47487844408428,
      "grad_norm": 1.3335380554199219,
      "learning_rate": 3.6881868310087584e-05,
      "loss": 2.8373,
      "step": 405600
    },
    {
      "epoch": 131.50729335494327,
      "grad_norm": 1.092299461364746,
      "learning_rate": 3.6878624716185536e-05,
      "loss": 2.8353,
      "step": 405700
    },
    {
      "epoch": 131.53970826580226,
      "grad_norm": 1.0126043558120728,
      "learning_rate": 3.6875381122283494e-05,
      "loss": 2.8419,
      "step": 405800
    },
    {
      "epoch": 131.57212317666125,
      "grad_norm": 1.2320595979690552,
      "learning_rate": 3.687213752838145e-05,
      "loss": 2.8416,
      "step": 405900
    },
    {
      "epoch": 131.60453808752027,
      "grad_norm": 1.0821706056594849,
      "learning_rate": 3.6868893934479405e-05,
      "loss": 2.8324,
      "step": 406000
    },
    {
      "epoch": 131.63695299837926,
      "grad_norm": 1.2901784181594849,
      "learning_rate": 3.6865650340577364e-05,
      "loss": 2.8524,
      "step": 406100
    },
    {
      "epoch": 131.66936790923825,
      "grad_norm": 1.0404901504516602,
      "learning_rate": 3.6862406746675316e-05,
      "loss": 2.8534,
      "step": 406200
    },
    {
      "epoch": 131.70178282009724,
      "grad_norm": 0.9514108896255493,
      "learning_rate": 3.6859163152773275e-05,
      "loss": 2.8642,
      "step": 406300
    },
    {
      "epoch": 131.73419773095625,
      "grad_norm": 0.9246245622634888,
      "learning_rate": 3.685591955887123e-05,
      "loss": 2.8197,
      "step": 406400
    },
    {
      "epoch": 131.76661264181524,
      "grad_norm": 1.1687990427017212,
      "learning_rate": 3.6852675964969185e-05,
      "loss": 2.8443,
      "step": 406500
    },
    {
      "epoch": 131.79902755267423,
      "grad_norm": 1.3324410915374756,
      "learning_rate": 3.6849432371067144e-05,
      "loss": 2.8542,
      "step": 406600
    },
    {
      "epoch": 131.83144246353322,
      "grad_norm": 1.0535091161727905,
      "learning_rate": 3.68461887771651e-05,
      "loss": 2.8788,
      "step": 406700
    },
    {
      "epoch": 131.8638573743922,
      "grad_norm": 1.2237656116485596,
      "learning_rate": 3.6842945183263055e-05,
      "loss": 2.8463,
      "step": 406800
    },
    {
      "epoch": 131.89627228525123,
      "grad_norm": 1.2396740913391113,
      "learning_rate": 3.6839701589361013e-05,
      "loss": 2.8466,
      "step": 406900
    },
    {
      "epoch": 131.92868719611022,
      "grad_norm": 1.0996922254562378,
      "learning_rate": 3.683645799545897e-05,
      "loss": 2.8633,
      "step": 407000
    },
    {
      "epoch": 131.9611021069692,
      "grad_norm": 1.025376796722412,
      "learning_rate": 3.6833214401556924e-05,
      "loss": 2.857,
      "step": 407100
    },
    {
      "epoch": 131.9935170178282,
      "grad_norm": 1.1480036973953247,
      "learning_rate": 3.682997080765488e-05,
      "loss": 2.8386,
      "step": 407200
    },
    {
      "epoch": 132.0,
      "eval_bleu": 1.1481875020298393,
      "eval_loss": 3.9065299034118652,
      "eval_runtime": 4.4422,
      "eval_samples_per_second": 110.755,
      "eval_steps_per_second": 1.801,
      "step": 407220
    },
    {
      "epoch": 132.02593192868719,
      "grad_norm": 1.0581153631210327,
      "learning_rate": 3.6826727213752835e-05,
      "loss": 2.8339,
      "step": 407300
    },
    {
      "epoch": 132.0583468395462,
      "grad_norm": 1.1426923274993896,
      "learning_rate": 3.6823483619850794e-05,
      "loss": 2.8269,
      "step": 407400
    },
    {
      "epoch": 132.0907617504052,
      "grad_norm": 1.0432568788528442,
      "learning_rate": 3.682024002594875e-05,
      "loss": 2.8223,
      "step": 407500
    },
    {
      "epoch": 132.12317666126418,
      "grad_norm": 0.9815314412117004,
      "learning_rate": 3.681702886798573e-05,
      "loss": 2.8296,
      "step": 407600
    },
    {
      "epoch": 132.15559157212317,
      "grad_norm": 1.034397006034851,
      "learning_rate": 3.681378527408368e-05,
      "loss": 2.8416,
      "step": 407700
    },
    {
      "epoch": 132.18800648298216,
      "grad_norm": 1.2921251058578491,
      "learning_rate": 3.681054168018164e-05,
      "loss": 2.8155,
      "step": 407800
    },
    {
      "epoch": 132.22042139384118,
      "grad_norm": 1.1209336519241333,
      "learning_rate": 3.68072980862796e-05,
      "loss": 2.8468,
      "step": 407900
    },
    {
      "epoch": 132.25283630470017,
      "grad_norm": 0.9474596977233887,
      "learning_rate": 3.680405449237755e-05,
      "loss": 2.8511,
      "step": 408000
    },
    {
      "epoch": 132.28525121555916,
      "grad_norm": 1.091302752494812,
      "learning_rate": 3.680081089847551e-05,
      "loss": 2.8108,
      "step": 408100
    },
    {
      "epoch": 132.31766612641815,
      "grad_norm": 1.2696208953857422,
      "learning_rate": 3.679756730457347e-05,
      "loss": 2.8506,
      "step": 408200
    },
    {
      "epoch": 132.35008103727714,
      "grad_norm": 0.9762433767318726,
      "learning_rate": 3.679432371067143e-05,
      "loss": 2.8309,
      "step": 408300
    },
    {
      "epoch": 132.38249594813615,
      "grad_norm": 1.3059111833572388,
      "learning_rate": 3.679108011676938e-05,
      "loss": 2.8418,
      "step": 408400
    },
    {
      "epoch": 132.41491085899514,
      "grad_norm": 1.0886539220809937,
      "learning_rate": 3.678783652286734e-05,
      "loss": 2.8144,
      "step": 408500
    },
    {
      "epoch": 132.44732576985413,
      "grad_norm": 1.1299152374267578,
      "learning_rate": 3.67845929289653e-05,
      "loss": 2.8673,
      "step": 408600
    },
    {
      "epoch": 132.47974068071312,
      "grad_norm": 1.1597288846969604,
      "learning_rate": 3.6781349335063256e-05,
      "loss": 2.8483,
      "step": 408700
    },
    {
      "epoch": 132.5121555915721,
      "grad_norm": 1.146395206451416,
      "learning_rate": 3.677810574116121e-05,
      "loss": 2.854,
      "step": 408800
    },
    {
      "epoch": 132.54457050243113,
      "grad_norm": 1.0652329921722412,
      "learning_rate": 3.677486214725917e-05,
      "loss": 2.846,
      "step": 408900
    },
    {
      "epoch": 132.57698541329012,
      "grad_norm": 0.9626764059066772,
      "learning_rate": 3.6771618553357125e-05,
      "loss": 2.8613,
      "step": 409000
    },
    {
      "epoch": 132.6094003241491,
      "grad_norm": 1.0292742252349854,
      "learning_rate": 3.676837495945508e-05,
      "loss": 2.8457,
      "step": 409100
    },
    {
      "epoch": 132.6418152350081,
      "grad_norm": 1.3296802043914795,
      "learning_rate": 3.6765131365553036e-05,
      "loss": 2.8398,
      "step": 409200
    },
    {
      "epoch": 132.67423014586709,
      "grad_norm": 1.0280646085739136,
      "learning_rate": 3.6761887771650995e-05,
      "loss": 2.837,
      "step": 409300
    },
    {
      "epoch": 132.7066450567261,
      "grad_norm": 1.099046230316162,
      "learning_rate": 3.675864417774895e-05,
      "loss": 2.834,
      "step": 409400
    },
    {
      "epoch": 132.7390599675851,
      "grad_norm": 1.196387767791748,
      "learning_rate": 3.6755400583846906e-05,
      "loss": 2.837,
      "step": 409500
    },
    {
      "epoch": 132.77147487844408,
      "grad_norm": 1.2095569372177124,
      "learning_rate": 3.675215698994486e-05,
      "loss": 2.8413,
      "step": 409600
    },
    {
      "epoch": 132.80388978930307,
      "grad_norm": 1.2064863443374634,
      "learning_rate": 3.6748913396042816e-05,
      "loss": 2.8474,
      "step": 409700
    },
    {
      "epoch": 132.8363047001621,
      "grad_norm": 1.1925002336502075,
      "learning_rate": 3.6745669802140775e-05,
      "loss": 2.8569,
      "step": 409800
    },
    {
      "epoch": 132.86871961102108,
      "grad_norm": 1.1740455627441406,
      "learning_rate": 3.674242620823873e-05,
      "loss": 2.8518,
      "step": 409900
    },
    {
      "epoch": 132.90113452188007,
      "grad_norm": 1.0856083631515503,
      "learning_rate": 3.6739182614336686e-05,
      "loss": 2.8529,
      "step": 410000
    },
    {
      "epoch": 132.93354943273906,
      "grad_norm": 1.266635775566101,
      "learning_rate": 3.6735939020434645e-05,
      "loss": 2.8634,
      "step": 410100
    },
    {
      "epoch": 132.96596434359805,
      "grad_norm": 1.0923471450805664,
      "learning_rate": 3.6732695426532597e-05,
      "loss": 2.8359,
      "step": 410200
    },
    {
      "epoch": 132.99837925445706,
      "grad_norm": 1.1441861391067505,
      "learning_rate": 3.6729451832630555e-05,
      "loss": 2.8425,
      "step": 410300
    },
    {
      "epoch": 133.0,
      "eval_bleu": 0.9788878446475665,
      "eval_loss": 3.9012510776519775,
      "eval_runtime": 4.0093,
      "eval_samples_per_second": 122.715,
      "eval_steps_per_second": 1.995,
      "step": 410305
    },
    {
      "epoch": 133.03079416531605,
      "grad_norm": 1.1472370624542236,
      "learning_rate": 3.6726208238728514e-05,
      "loss": 2.8354,
      "step": 410400
    },
    {
      "epoch": 133.06320907617504,
      "grad_norm": 1.0933328866958618,
      "learning_rate": 3.6722964644826466e-05,
      "loss": 2.8378,
      "step": 410500
    },
    {
      "epoch": 133.09562398703403,
      "grad_norm": 1.426116704940796,
      "learning_rate": 3.6719721050924425e-05,
      "loss": 2.8431,
      "step": 410600
    },
    {
      "epoch": 133.12803889789302,
      "grad_norm": 1.234071135520935,
      "learning_rate": 3.6716477457022383e-05,
      "loss": 2.8436,
      "step": 410700
    },
    {
      "epoch": 133.16045380875204,
      "grad_norm": 1.2361619472503662,
      "learning_rate": 3.6713233863120335e-05,
      "loss": 2.8437,
      "step": 410800
    },
    {
      "epoch": 133.19286871961103,
      "grad_norm": 0.9601580500602722,
      "learning_rate": 3.6709990269218294e-05,
      "loss": 2.8278,
      "step": 410900
    },
    {
      "epoch": 133.22528363047002,
      "grad_norm": 1.1883481740951538,
      "learning_rate": 3.670674667531625e-05,
      "loss": 2.8499,
      "step": 411000
    },
    {
      "epoch": 133.257698541329,
      "grad_norm": 1.2487238645553589,
      "learning_rate": 3.670350308141421e-05,
      "loss": 2.8276,
      "step": 411100
    },
    {
      "epoch": 133.290113452188,
      "grad_norm": 1.099259376525879,
      "learning_rate": 3.670025948751217e-05,
      "loss": 2.8338,
      "step": 411200
    },
    {
      "epoch": 133.322528363047,
      "grad_norm": 1.3310561180114746,
      "learning_rate": 3.669701589361012e-05,
      "loss": 2.8456,
      "step": 411300
    },
    {
      "epoch": 133.354943273906,
      "grad_norm": 1.1205337047576904,
      "learning_rate": 3.669377229970808e-05,
      "loss": 2.843,
      "step": 411400
    },
    {
      "epoch": 133.387358184765,
      "grad_norm": 1.1188421249389648,
      "learning_rate": 3.669052870580603e-05,
      "loss": 2.8345,
      "step": 411500
    },
    {
      "epoch": 133.41977309562398,
      "grad_norm": 0.9842931032180786,
      "learning_rate": 3.668731754784301e-05,
      "loss": 2.8277,
      "step": 411600
    },
    {
      "epoch": 133.45218800648297,
      "grad_norm": 1.1077537536621094,
      "learning_rate": 3.668407395394097e-05,
      "loss": 2.8293,
      "step": 411700
    },
    {
      "epoch": 133.484602917342,
      "grad_norm": 1.0706371068954468,
      "learning_rate": 3.668083036003893e-05,
      "loss": 2.8333,
      "step": 411800
    },
    {
      "epoch": 133.51701782820098,
      "grad_norm": 1.096474051475525,
      "learning_rate": 3.667758676613688e-05,
      "loss": 2.831,
      "step": 411900
    },
    {
      "epoch": 133.54943273905997,
      "grad_norm": 1.013900637626648,
      "learning_rate": 3.667434317223484e-05,
      "loss": 2.8489,
      "step": 412000
    },
    {
      "epoch": 133.58184764991896,
      "grad_norm": 1.3744889497756958,
      "learning_rate": 3.66710995783328e-05,
      "loss": 2.851,
      "step": 412100
    },
    {
      "epoch": 133.61426256077795,
      "grad_norm": 1.0810694694519043,
      "learning_rate": 3.666785598443075e-05,
      "loss": 2.8409,
      "step": 412200
    },
    {
      "epoch": 133.64667747163696,
      "grad_norm": 1.208662986755371,
      "learning_rate": 3.666461239052871e-05,
      "loss": 2.8412,
      "step": 412300
    },
    {
      "epoch": 133.67909238249595,
      "grad_norm": 1.2135974168777466,
      "learning_rate": 3.666136879662667e-05,
      "loss": 2.8237,
      "step": 412400
    },
    {
      "epoch": 133.71150729335494,
      "grad_norm": 1.0675370693206787,
      "learning_rate": 3.665812520272462e-05,
      "loss": 2.8358,
      "step": 412500
    },
    {
      "epoch": 133.74392220421393,
      "grad_norm": 1.245113492012024,
      "learning_rate": 3.665488160882258e-05,
      "loss": 2.8432,
      "step": 412600
    },
    {
      "epoch": 133.77633711507292,
      "grad_norm": 1.137446403503418,
      "learning_rate": 3.665163801492054e-05,
      "loss": 2.8402,
      "step": 412700
    },
    {
      "epoch": 133.80875202593194,
      "grad_norm": 1.2192201614379883,
      "learning_rate": 3.664839442101849e-05,
      "loss": 2.8432,
      "step": 412800
    },
    {
      "epoch": 133.84116693679093,
      "grad_norm": 1.0534926652908325,
      "learning_rate": 3.664515082711645e-05,
      "loss": 2.849,
      "step": 412900
    },
    {
      "epoch": 133.87358184764992,
      "grad_norm": 1.0847052335739136,
      "learning_rate": 3.66419072332144e-05,
      "loss": 2.8335,
      "step": 413000
    },
    {
      "epoch": 133.9059967585089,
      "grad_norm": 1.1072485446929932,
      "learning_rate": 3.663866363931236e-05,
      "loss": 2.8391,
      "step": 413100
    },
    {
      "epoch": 133.93841166936792,
      "grad_norm": 1.1522568464279175,
      "learning_rate": 3.663542004541032e-05,
      "loss": 2.8283,
      "step": 413200
    },
    {
      "epoch": 133.9708265802269,
      "grad_norm": 1.1346662044525146,
      "learning_rate": 3.663217645150827e-05,
      "loss": 2.8374,
      "step": 413300
    },
    {
      "epoch": 134.0,
      "eval_bleu": 1.107369332899459,
      "eval_loss": 3.9040699005126953,
      "eval_runtime": 6.161,
      "eval_samples_per_second": 79.857,
      "eval_steps_per_second": 1.298,
      "step": 413390
    },
    {
      "epoch": 134.0032414910859,
      "grad_norm": 1.1837635040283203,
      "learning_rate": 3.662893285760623e-05,
      "loss": 2.8394,
      "step": 413400
    },
    {
      "epoch": 134.0356564019449,
      "grad_norm": 1.0932466983795166,
      "learning_rate": 3.6625689263704186e-05,
      "loss": 2.8246,
      "step": 413500
    },
    {
      "epoch": 134.06807131280388,
      "grad_norm": 1.0694254636764526,
      "learning_rate": 3.6622478105741164e-05,
      "loss": 2.8371,
      "step": 413600
    },
    {
      "epoch": 134.1004862236629,
      "grad_norm": 1.0676606893539429,
      "learning_rate": 3.6619234511839116e-05,
      "loss": 2.8082,
      "step": 413700
    },
    {
      "epoch": 134.1329011345219,
      "grad_norm": 1.157153844833374,
      "learning_rate": 3.6615990917937075e-05,
      "loss": 2.8441,
      "step": 413800
    },
    {
      "epoch": 134.16531604538088,
      "grad_norm": 1.4604825973510742,
      "learning_rate": 3.6612747324035034e-05,
      "loss": 2.8361,
      "step": 413900
    },
    {
      "epoch": 134.19773095623987,
      "grad_norm": 1.0597927570343018,
      "learning_rate": 3.6609503730132986e-05,
      "loss": 2.837,
      "step": 414000
    },
    {
      "epoch": 134.23014586709886,
      "grad_norm": 1.1721935272216797,
      "learning_rate": 3.6606260136230944e-05,
      "loss": 2.8191,
      "step": 414100
    },
    {
      "epoch": 134.26256077795787,
      "grad_norm": 1.1437064409255981,
      "learning_rate": 3.660304897826792e-05,
      "loss": 2.8154,
      "step": 414200
    },
    {
      "epoch": 134.29497568881686,
      "grad_norm": 1.195184588432312,
      "learning_rate": 3.659980538436588e-05,
      "loss": 2.8251,
      "step": 414300
    },
    {
      "epoch": 134.32739059967585,
      "grad_norm": 1.2712029218673706,
      "learning_rate": 3.659656179046383e-05,
      "loss": 2.8409,
      "step": 414400
    },
    {
      "epoch": 134.35980551053484,
      "grad_norm": 0.9592558145523071,
      "learning_rate": 3.659331819656179e-05,
      "loss": 2.8479,
      "step": 414500
    },
    {
      "epoch": 134.39222042139383,
      "grad_norm": 1.094181776046753,
      "learning_rate": 3.6590074602659743e-05,
      "loss": 2.8454,
      "step": 414600
    },
    {
      "epoch": 134.42463533225285,
      "grad_norm": 1.1540871858596802,
      "learning_rate": 3.65868310087577e-05,
      "loss": 2.8254,
      "step": 414700
    },
    {
      "epoch": 134.45705024311184,
      "grad_norm": 1.1185483932495117,
      "learning_rate": 3.658358741485566e-05,
      "loss": 2.8389,
      "step": 414800
    },
    {
      "epoch": 134.48946515397083,
      "grad_norm": 1.0935004949569702,
      "learning_rate": 3.658034382095362e-05,
      "loss": 2.8405,
      "step": 414900
    },
    {
      "epoch": 134.52188006482982,
      "grad_norm": 1.2454710006713867,
      "learning_rate": 3.657710022705157e-05,
      "loss": 2.8289,
      "step": 415000
    },
    {
      "epoch": 134.5542949756888,
      "grad_norm": 0.985811173915863,
      "learning_rate": 3.657385663314953e-05,
      "loss": 2.825,
      "step": 415100
    },
    {
      "epoch": 134.58670988654782,
      "grad_norm": 1.2819401025772095,
      "learning_rate": 3.657061303924749e-05,
      "loss": 2.84,
      "step": 415200
    },
    {
      "epoch": 134.6191247974068,
      "grad_norm": 1.1488431692123413,
      "learning_rate": 3.656736944534545e-05,
      "loss": 2.8318,
      "step": 415300
    },
    {
      "epoch": 134.6515397082658,
      "grad_norm": 1.1441526412963867,
      "learning_rate": 3.656412585144341e-05,
      "loss": 2.8381,
      "step": 415400
    },
    {
      "epoch": 134.6839546191248,
      "grad_norm": 1.2669974565505981,
      "learning_rate": 3.656088225754136e-05,
      "loss": 2.8613,
      "step": 415500
    },
    {
      "epoch": 134.71636952998378,
      "grad_norm": 1.16535484790802,
      "learning_rate": 3.655763866363932e-05,
      "loss": 2.8496,
      "step": 415600
    },
    {
      "epoch": 134.7487844408428,
      "grad_norm": 1.261466383934021,
      "learning_rate": 3.655439506973727e-05,
      "loss": 2.83,
      "step": 415700
    },
    {
      "epoch": 134.7811993517018,
      "grad_norm": 1.2940119504928589,
      "learning_rate": 3.655115147583523e-05,
      "loss": 2.8317,
      "step": 415800
    },
    {
      "epoch": 134.81361426256078,
      "grad_norm": 1.0468980073928833,
      "learning_rate": 3.654790788193319e-05,
      "loss": 2.8509,
      "step": 415900
    },
    {
      "epoch": 134.84602917341977,
      "grad_norm": 1.2196612358093262,
      "learning_rate": 3.654466428803114e-05,
      "loss": 2.8333,
      "step": 416000
    },
    {
      "epoch": 134.87844408427875,
      "grad_norm": 1.0324245691299438,
      "learning_rate": 3.65414206941291e-05,
      "loss": 2.8512,
      "step": 416100
    },
    {
      "epoch": 134.91085899513777,
      "grad_norm": 1.1416807174682617,
      "learning_rate": 3.6538177100227056e-05,
      "loss": 2.8134,
      "step": 416200
    },
    {
      "epoch": 134.94327390599676,
      "grad_norm": 0.9973235726356506,
      "learning_rate": 3.653493350632501e-05,
      "loss": 2.8345,
      "step": 416300
    },
    {
      "epoch": 134.97568881685575,
      "grad_norm": 1.0276141166687012,
      "learning_rate": 3.653168991242297e-05,
      "loss": 2.8422,
      "step": 416400
    },
    {
      "epoch": 135.0,
      "eval_bleu": 1.1403616222730086,
      "eval_loss": 3.8995766639709473,
      "eval_runtime": 4.2275,
      "eval_samples_per_second": 116.382,
      "eval_steps_per_second": 1.892,
      "step": 416475
    },
    {
      "epoch": 135.00810372771474,
      "grad_norm": 0.9898662567138672,
      "learning_rate": 3.652844631852092e-05,
      "loss": 2.8559,
      "step": 416500
    },
    {
      "epoch": 135.04051863857376,
      "grad_norm": 1.154178500175476,
      "learning_rate": 3.652520272461888e-05,
      "loss": 2.8246,
      "step": 416600
    },
    {
      "epoch": 135.07293354943275,
      "grad_norm": 0.9977815747261047,
      "learning_rate": 3.6521959130716836e-05,
      "loss": 2.8266,
      "step": 416700
    },
    {
      "epoch": 135.10534846029174,
      "grad_norm": 1.1153963804244995,
      "learning_rate": 3.651871553681479e-05,
      "loss": 2.8245,
      "step": 416800
    },
    {
      "epoch": 135.13776337115073,
      "grad_norm": 1.0720833539962769,
      "learning_rate": 3.651547194291275e-05,
      "loss": 2.8299,
      "step": 416900
    },
    {
      "epoch": 135.17017828200972,
      "grad_norm": 1.0313987731933594,
      "learning_rate": 3.6512228349010706e-05,
      "loss": 2.8148,
      "step": 417000
    },
    {
      "epoch": 135.20259319286873,
      "grad_norm": 1.0956376791000366,
      "learning_rate": 3.650898475510866e-05,
      "loss": 2.814,
      "step": 417100
    },
    {
      "epoch": 135.23500810372772,
      "grad_norm": 1.1881099939346313,
      "learning_rate": 3.6505741161206617e-05,
      "loss": 2.8258,
      "step": 417200
    },
    {
      "epoch": 135.2674230145867,
      "grad_norm": 1.2566665410995483,
      "learning_rate": 3.6502497567304575e-05,
      "loss": 2.8221,
      "step": 417300
    },
    {
      "epoch": 135.2998379254457,
      "grad_norm": 1.2083886861801147,
      "learning_rate": 3.6499253973402534e-05,
      "loss": 2.8356,
      "step": 417400
    },
    {
      "epoch": 135.3322528363047,
      "grad_norm": 1.0695658922195435,
      "learning_rate": 3.6496010379500486e-05,
      "loss": 2.8326,
      "step": 417500
    },
    {
      "epoch": 135.3646677471637,
      "grad_norm": 1.0433988571166992,
      "learning_rate": 3.6492766785598445e-05,
      "loss": 2.8309,
      "step": 417600
    },
    {
      "epoch": 135.3970826580227,
      "grad_norm": 1.1927025318145752,
      "learning_rate": 3.6489523191696404e-05,
      "loss": 2.8462,
      "step": 417700
    },
    {
      "epoch": 135.4294975688817,
      "grad_norm": 1.1290768384933472,
      "learning_rate": 3.648627959779436e-05,
      "loss": 2.8186,
      "step": 417800
    },
    {
      "epoch": 135.46191247974068,
      "grad_norm": 1.1442469358444214,
      "learning_rate": 3.6483036003892314e-05,
      "loss": 2.8303,
      "step": 417900
    },
    {
      "epoch": 135.49432739059966,
      "grad_norm": 1.2654212713241577,
      "learning_rate": 3.647979240999027e-05,
      "loss": 2.8352,
      "step": 418000
    },
    {
      "epoch": 135.52674230145868,
      "grad_norm": 1.1575605869293213,
      "learning_rate": 3.647654881608823e-05,
      "loss": 2.8221,
      "step": 418100
    },
    {
      "epoch": 135.55915721231767,
      "grad_norm": 1.073972225189209,
      "learning_rate": 3.6473305222186184e-05,
      "loss": 2.8405,
      "step": 418200
    },
    {
      "epoch": 135.59157212317666,
      "grad_norm": 0.9640296697616577,
      "learning_rate": 3.647009406422316e-05,
      "loss": 2.8292,
      "step": 418300
    },
    {
      "epoch": 135.62398703403565,
      "grad_norm": 1.0811761617660522,
      "learning_rate": 3.646685047032112e-05,
      "loss": 2.8375,
      "step": 418400
    },
    {
      "epoch": 135.65640194489464,
      "grad_norm": 1.0603749752044678,
      "learning_rate": 3.646360687641908e-05,
      "loss": 2.837,
      "step": 418500
    },
    {
      "epoch": 135.68881685575366,
      "grad_norm": 1.1237218379974365,
      "learning_rate": 3.646036328251703e-05,
      "loss": 2.8537,
      "step": 418600
    },
    {
      "epoch": 135.72123176661265,
      "grad_norm": 1.0285582542419434,
      "learning_rate": 3.645711968861499e-05,
      "loss": 2.8418,
      "step": 418700
    },
    {
      "epoch": 135.75364667747164,
      "grad_norm": 1.0612051486968994,
      "learning_rate": 3.645387609471294e-05,
      "loss": 2.8289,
      "step": 418800
    },
    {
      "epoch": 135.78606158833063,
      "grad_norm": 1.1830089092254639,
      "learning_rate": 3.64506325008109e-05,
      "loss": 2.8481,
      "step": 418900
    },
    {
      "epoch": 135.81847649918961,
      "grad_norm": 1.1583744287490845,
      "learning_rate": 3.644738890690886e-05,
      "loss": 2.8395,
      "step": 419000
    },
    {
      "epoch": 135.85089141004863,
      "grad_norm": 1.1561306715011597,
      "learning_rate": 3.644414531300681e-05,
      "loss": 2.841,
      "step": 419100
    },
    {
      "epoch": 135.88330632090762,
      "grad_norm": 1.1150354146957397,
      "learning_rate": 3.644090171910477e-05,
      "loss": 2.8625,
      "step": 419200
    },
    {
      "epoch": 135.9157212317666,
      "grad_norm": 1.2324331998825073,
      "learning_rate": 3.643765812520273e-05,
      "loss": 2.8494,
      "step": 419300
    },
    {
      "epoch": 135.9481361426256,
      "grad_norm": 1.0550626516342163,
      "learning_rate": 3.643441453130068e-05,
      "loss": 2.836,
      "step": 419400
    },
    {
      "epoch": 135.9805510534846,
      "grad_norm": 1.1120506525039673,
      "learning_rate": 3.643117093739864e-05,
      "loss": 2.837,
      "step": 419500
    },
    {
      "epoch": 136.0,
      "eval_bleu": 1.2793498436619746,
      "eval_loss": 3.9098124504089355,
      "eval_runtime": 3.9772,
      "eval_samples_per_second": 123.704,
      "eval_steps_per_second": 2.011,
      "step": 419560
    },
    {
      "epoch": 136.0129659643436,
      "grad_norm": 1.1224735975265503,
      "learning_rate": 3.64279273434966e-05,
      "loss": 2.8447,
      "step": 419600
    },
    {
      "epoch": 136.0453808752026,
      "grad_norm": 1.122841238975525,
      "learning_rate": 3.642468374959455e-05,
      "loss": 2.8306,
      "step": 419700
    },
    {
      "epoch": 136.07779578606159,
      "grad_norm": 1.264032006263733,
      "learning_rate": 3.642144015569251e-05,
      "loss": 2.8238,
      "step": 419800
    },
    {
      "epoch": 136.11021069692057,
      "grad_norm": 1.2172489166259766,
      "learning_rate": 3.641819656179046e-05,
      "loss": 2.8278,
      "step": 419900
    },
    {
      "epoch": 136.1426256077796,
      "grad_norm": 1.3604110479354858,
      "learning_rate": 3.641495296788842e-05,
      "loss": 2.802,
      "step": 420000
    },
    {
      "epoch": 136.17504051863858,
      "grad_norm": 1.1927791833877563,
      "learning_rate": 3.641170937398638e-05,
      "loss": 2.8305,
      "step": 420100
    },
    {
      "epoch": 136.20745542949757,
      "grad_norm": 1.1306977272033691,
      "learning_rate": 3.640846578008433e-05,
      "loss": 2.8246,
      "step": 420200
    },
    {
      "epoch": 136.23987034035656,
      "grad_norm": 1.335029125213623,
      "learning_rate": 3.640525462212131e-05,
      "loss": 2.8283,
      "step": 420300
    },
    {
      "epoch": 136.27228525121555,
      "grad_norm": 1.1331799030303955,
      "learning_rate": 3.640201102821927e-05,
      "loss": 2.8563,
      "step": 420400
    },
    {
      "epoch": 136.30470016207457,
      "grad_norm": 1.1258915662765503,
      "learning_rate": 3.6398767434317225e-05,
      "loss": 2.8235,
      "step": 420500
    },
    {
      "epoch": 136.33711507293356,
      "grad_norm": 1.0528991222381592,
      "learning_rate": 3.639552384041518e-05,
      "loss": 2.8544,
      "step": 420600
    },
    {
      "epoch": 136.36952998379255,
      "grad_norm": 1.1232130527496338,
      "learning_rate": 3.6392280246513136e-05,
      "loss": 2.8366,
      "step": 420700
    },
    {
      "epoch": 136.40194489465154,
      "grad_norm": 1.4411331415176392,
      "learning_rate": 3.6389036652611095e-05,
      "loss": 2.8434,
      "step": 420800
    },
    {
      "epoch": 136.43435980551052,
      "grad_norm": 1.1621884107589722,
      "learning_rate": 3.638579305870905e-05,
      "loss": 2.8326,
      "step": 420900
    },
    {
      "epoch": 136.46677471636954,
      "grad_norm": 1.0692541599273682,
      "learning_rate": 3.6382549464807006e-05,
      "loss": 2.8374,
      "step": 421000
    },
    {
      "epoch": 136.49918962722853,
      "grad_norm": 1.1571918725967407,
      "learning_rate": 3.6379305870904964e-05,
      "loss": 2.8373,
      "step": 421100
    },
    {
      "epoch": 136.53160453808752,
      "grad_norm": 1.2380383014678955,
      "learning_rate": 3.637606227700292e-05,
      "loss": 2.8359,
      "step": 421200
    },
    {
      "epoch": 136.5640194489465,
      "grad_norm": 1.0501505136489868,
      "learning_rate": 3.637281868310088e-05,
      "loss": 2.8338,
      "step": 421300
    },
    {
      "epoch": 136.5964343598055,
      "grad_norm": 1.0701570510864258,
      "learning_rate": 3.6369575089198834e-05,
      "loss": 2.8454,
      "step": 421400
    },
    {
      "epoch": 136.62884927066452,
      "grad_norm": 1.2667276859283447,
      "learning_rate": 3.636633149529679e-05,
      "loss": 2.8211,
      "step": 421500
    },
    {
      "epoch": 136.6612641815235,
      "grad_norm": 1.144281268119812,
      "learning_rate": 3.636308790139475e-05,
      "loss": 2.8471,
      "step": 421600
    },
    {
      "epoch": 136.6936790923825,
      "grad_norm": 1.123802661895752,
      "learning_rate": 3.63598443074927e-05,
      "loss": 2.8316,
      "step": 421700
    },
    {
      "epoch": 136.72609400324149,
      "grad_norm": 1.1715675592422485,
      "learning_rate": 3.635660071359066e-05,
      "loss": 2.8359,
      "step": 421800
    },
    {
      "epoch": 136.75850891410047,
      "grad_norm": 1.0184239149093628,
      "learning_rate": 3.635335711968862e-05,
      "loss": 2.8386,
      "step": 421900
    },
    {
      "epoch": 136.7909238249595,
      "grad_norm": 1.1224384307861328,
      "learning_rate": 3.635011352578657e-05,
      "loss": 2.8047,
      "step": 422000
    },
    {
      "epoch": 136.82333873581848,
      "grad_norm": 1.1133828163146973,
      "learning_rate": 3.634686993188453e-05,
      "loss": 2.8153,
      "step": 422100
    },
    {
      "epoch": 136.85575364667747,
      "grad_norm": 1.2007558345794678,
      "learning_rate": 3.6343626337982483e-05,
      "loss": 2.8459,
      "step": 422200
    },
    {
      "epoch": 136.88816855753646,
      "grad_norm": 1.194412350654602,
      "learning_rate": 3.634041518001947e-05,
      "loss": 2.8495,
      "step": 422300
    },
    {
      "epoch": 136.92058346839545,
      "grad_norm": 1.2083396911621094,
      "learning_rate": 3.633717158611742e-05,
      "loss": 2.8259,
      "step": 422400
    },
    {
      "epoch": 136.95299837925447,
      "grad_norm": 1.0626544952392578,
      "learning_rate": 3.633392799221538e-05,
      "loss": 2.8421,
      "step": 422500
    },
    {
      "epoch": 136.98541329011346,
      "grad_norm": 1.0063642263412476,
      "learning_rate": 3.633068439831333e-05,
      "loss": 2.8002,
      "step": 422600
    },
    {
      "epoch": 137.0,
      "eval_bleu": 1.0068265878306124,
      "eval_loss": 3.9141671657562256,
      "eval_runtime": 4.1766,
      "eval_samples_per_second": 117.799,
      "eval_steps_per_second": 1.915,
      "step": 422645
    },
    {
      "epoch": 137.01782820097245,
      "grad_norm": 1.2535781860351562,
      "learning_rate": 3.632744080441129e-05,
      "loss": 2.819,
      "step": 422700
    },
    {
      "epoch": 137.05024311183143,
      "grad_norm": 1.2327977418899536,
      "learning_rate": 3.632419721050925e-05,
      "loss": 2.8112,
      "step": 422800
    },
    {
      "epoch": 137.08265802269042,
      "grad_norm": 1.2484886646270752,
      "learning_rate": 3.63209536166072e-05,
      "loss": 2.8287,
      "step": 422900
    },
    {
      "epoch": 137.11507293354944,
      "grad_norm": 1.2406527996063232,
      "learning_rate": 3.631771002270516e-05,
      "loss": 2.8188,
      "step": 423000
    },
    {
      "epoch": 137.14748784440843,
      "grad_norm": 1.0840116739273071,
      "learning_rate": 3.631446642880312e-05,
      "loss": 2.8208,
      "step": 423100
    },
    {
      "epoch": 137.17990275526742,
      "grad_norm": 1.1695984601974487,
      "learning_rate": 3.6311255270840095e-05,
      "loss": 2.8018,
      "step": 423200
    },
    {
      "epoch": 137.2123176661264,
      "grad_norm": 1.1445804834365845,
      "learning_rate": 3.630801167693805e-05,
      "loss": 2.8374,
      "step": 423300
    },
    {
      "epoch": 137.24473257698543,
      "grad_norm": 1.0648725032806396,
      "learning_rate": 3.6304768083036006e-05,
      "loss": 2.8264,
      "step": 423400
    },
    {
      "epoch": 137.27714748784442,
      "grad_norm": 1.033307671546936,
      "learning_rate": 3.6301524489133965e-05,
      "loss": 2.8216,
      "step": 423500
    },
    {
      "epoch": 137.3095623987034,
      "grad_norm": 1.0572603940963745,
      "learning_rate": 3.629828089523192e-05,
      "loss": 2.8198,
      "step": 423600
    },
    {
      "epoch": 137.3419773095624,
      "grad_norm": 1.098222255706787,
      "learning_rate": 3.6295037301329876e-05,
      "loss": 2.8404,
      "step": 423700
    },
    {
      "epoch": 137.37439222042138,
      "grad_norm": 1.122434139251709,
      "learning_rate": 3.629179370742783e-05,
      "loss": 2.8347,
      "step": 423800
    },
    {
      "epoch": 137.4068071312804,
      "grad_norm": 1.0531859397888184,
      "learning_rate": 3.6288550113525786e-05,
      "loss": 2.8416,
      "step": 423900
    },
    {
      "epoch": 137.4392220421394,
      "grad_norm": 0.9934291243553162,
      "learning_rate": 3.6285306519623745e-05,
      "loss": 2.828,
      "step": 424000
    },
    {
      "epoch": 137.47163695299838,
      "grad_norm": 1.3708770275115967,
      "learning_rate": 3.62820629257217e-05,
      "loss": 2.8243,
      "step": 424100
    },
    {
      "epoch": 137.50405186385737,
      "grad_norm": 1.162531852722168,
      "learning_rate": 3.6278819331819656e-05,
      "loss": 2.8314,
      "step": 424200
    },
    {
      "epoch": 137.53646677471636,
      "grad_norm": 1.4110610485076904,
      "learning_rate": 3.6275575737917614e-05,
      "loss": 2.8456,
      "step": 424300
    },
    {
      "epoch": 137.56888168557538,
      "grad_norm": 1.1940733194351196,
      "learning_rate": 3.6272332144015566e-05,
      "loss": 2.8143,
      "step": 424400
    },
    {
      "epoch": 137.60129659643437,
      "grad_norm": 1.2396364212036133,
      "learning_rate": 3.6269088550113525e-05,
      "loss": 2.8324,
      "step": 424500
    },
    {
      "epoch": 137.63371150729336,
      "grad_norm": 1.097796082496643,
      "learning_rate": 3.6265844956211484e-05,
      "loss": 2.8523,
      "step": 424600
    },
    {
      "epoch": 137.66612641815234,
      "grad_norm": 1.215923547744751,
      "learning_rate": 3.626260136230944e-05,
      "loss": 2.8333,
      "step": 424700
    },
    {
      "epoch": 137.69854132901133,
      "grad_norm": 1.0557061433792114,
      "learning_rate": 3.62593577684074e-05,
      "loss": 2.8292,
      "step": 424800
    },
    {
      "epoch": 137.73095623987035,
      "grad_norm": 1.070590615272522,
      "learning_rate": 3.6256114174505353e-05,
      "loss": 2.8326,
      "step": 424900
    },
    {
      "epoch": 137.76337115072934,
      "grad_norm": 1.1332473754882812,
      "learning_rate": 3.625287058060331e-05,
      "loss": 2.8208,
      "step": 425000
    },
    {
      "epoch": 137.79578606158833,
      "grad_norm": 1.0860366821289062,
      "learning_rate": 3.624962698670127e-05,
      "loss": 2.8223,
      "step": 425100
    },
    {
      "epoch": 137.82820097244732,
      "grad_norm": 1.058937668800354,
      "learning_rate": 3.624641582873824e-05,
      "loss": 2.8529,
      "step": 425200
    },
    {
      "epoch": 137.8606158833063,
      "grad_norm": 1.0881423950195312,
      "learning_rate": 3.62431722348362e-05,
      "loss": 2.8386,
      "step": 425300
    },
    {
      "epoch": 137.89303079416533,
      "grad_norm": 1.0486595630645752,
      "learning_rate": 3.623992864093416e-05,
      "loss": 2.8428,
      "step": 425400
    },
    {
      "epoch": 137.92544570502432,
      "grad_norm": 1.058239221572876,
      "learning_rate": 3.623668504703212e-05,
      "loss": 2.8332,
      "step": 425500
    },
    {
      "epoch": 137.9578606158833,
      "grad_norm": 1.1648589372634888,
      "learning_rate": 3.623344145313007e-05,
      "loss": 2.8439,
      "step": 425600
    },
    {
      "epoch": 137.9902755267423,
      "grad_norm": 1.2319492101669312,
      "learning_rate": 3.623019785922803e-05,
      "loss": 2.8177,
      "step": 425700
    },
    {
      "epoch": 138.0,
      "eval_bleu": 1.0887617652643724,
      "eval_loss": 3.9164865016937256,
      "eval_runtime": 4.3483,
      "eval_samples_per_second": 113.147,
      "eval_steps_per_second": 1.84,
      "step": 425730
    },
    {
      "epoch": 138.02269043760128,
      "grad_norm": 1.3098338842391968,
      "learning_rate": 3.622695426532599e-05,
      "loss": 2.8424,
      "step": 425800
    },
    {
      "epoch": 138.0551053484603,
      "grad_norm": 1.1267094612121582,
      "learning_rate": 3.622371067142394e-05,
      "loss": 2.8305,
      "step": 425900
    },
    {
      "epoch": 138.0875202593193,
      "grad_norm": 1.2123643159866333,
      "learning_rate": 3.62204670775219e-05,
      "loss": 2.8211,
      "step": 426000
    },
    {
      "epoch": 138.11993517017828,
      "grad_norm": 1.208940863609314,
      "learning_rate": 3.621722348361985e-05,
      "loss": 2.8203,
      "step": 426100
    },
    {
      "epoch": 138.15235008103727,
      "grad_norm": 1.116938829421997,
      "learning_rate": 3.621397988971781e-05,
      "loss": 2.8415,
      "step": 426200
    },
    {
      "epoch": 138.18476499189626,
      "grad_norm": 1.0207582712173462,
      "learning_rate": 3.621073629581577e-05,
      "loss": 2.8143,
      "step": 426300
    },
    {
      "epoch": 138.21717990275528,
      "grad_norm": 1.2351536750793457,
      "learning_rate": 3.620749270191372e-05,
      "loss": 2.8333,
      "step": 426400
    },
    {
      "epoch": 138.24959481361427,
      "grad_norm": 1.2358033657073975,
      "learning_rate": 3.620424910801168e-05,
      "loss": 2.817,
      "step": 426500
    },
    {
      "epoch": 138.28200972447326,
      "grad_norm": 1.1110395193099976,
      "learning_rate": 3.620100551410964e-05,
      "loss": 2.8314,
      "step": 426600
    },
    {
      "epoch": 138.31442463533224,
      "grad_norm": 1.0458260774612427,
      "learning_rate": 3.619776192020759e-05,
      "loss": 2.8244,
      "step": 426700
    },
    {
      "epoch": 138.34683954619126,
      "grad_norm": 1.0784398317337036,
      "learning_rate": 3.619451832630555e-05,
      "loss": 2.8078,
      "step": 426800
    },
    {
      "epoch": 138.37925445705025,
      "grad_norm": 1.1521832942962646,
      "learning_rate": 3.619127473240351e-05,
      "loss": 2.8247,
      "step": 426900
    },
    {
      "epoch": 138.41166936790924,
      "grad_norm": 1.2090171575546265,
      "learning_rate": 3.618803113850146e-05,
      "loss": 2.8289,
      "step": 427000
    },
    {
      "epoch": 138.44408427876823,
      "grad_norm": 1.0201356410980225,
      "learning_rate": 3.618478754459942e-05,
      "loss": 2.8296,
      "step": 427100
    },
    {
      "epoch": 138.47649918962722,
      "grad_norm": 1.104089617729187,
      "learning_rate": 3.618154395069737e-05,
      "loss": 2.8088,
      "step": 427200
    },
    {
      "epoch": 138.50891410048624,
      "grad_norm": 1.1158645153045654,
      "learning_rate": 3.617830035679533e-05,
      "loss": 2.8298,
      "step": 427300
    },
    {
      "epoch": 138.54132901134523,
      "grad_norm": 1.076130986213684,
      "learning_rate": 3.617505676289329e-05,
      "loss": 2.8119,
      "step": 427400
    },
    {
      "epoch": 138.57374392220422,
      "grad_norm": 1.0065362453460693,
      "learning_rate": 3.617181316899124e-05,
      "loss": 2.8305,
      "step": 427500
    },
    {
      "epoch": 138.6061588330632,
      "grad_norm": 1.193415880203247,
      "learning_rate": 3.61685695750892e-05,
      "loss": 2.8205,
      "step": 427600
    },
    {
      "epoch": 138.6385737439222,
      "grad_norm": 1.2311389446258545,
      "learning_rate": 3.6165325981187156e-05,
      "loss": 2.823,
      "step": 427700
    },
    {
      "epoch": 138.6709886547812,
      "grad_norm": 1.135576605796814,
      "learning_rate": 3.6162082387285115e-05,
      "loss": 2.8352,
      "step": 427800
    },
    {
      "epoch": 138.7034035656402,
      "grad_norm": 1.0200154781341553,
      "learning_rate": 3.6158838793383074e-05,
      "loss": 2.8282,
      "step": 427900
    },
    {
      "epoch": 138.7358184764992,
      "grad_norm": 0.9124623537063599,
      "learning_rate": 3.615559519948103e-05,
      "loss": 2.8285,
      "step": 428000
    },
    {
      "epoch": 138.76823338735818,
      "grad_norm": 1.2482075691223145,
      "learning_rate": 3.6152351605578984e-05,
      "loss": 2.8398,
      "step": 428100
    },
    {
      "epoch": 138.80064829821717,
      "grad_norm": 1.3228998184204102,
      "learning_rate": 3.614910801167694e-05,
      "loss": 2.8378,
      "step": 428200
    },
    {
      "epoch": 138.8330632090762,
      "grad_norm": 0.9461922645568848,
      "learning_rate": 3.6145864417774895e-05,
      "loss": 2.8278,
      "step": 428300
    },
    {
      "epoch": 138.86547811993518,
      "grad_norm": 1.2302780151367188,
      "learning_rate": 3.6142620823872854e-05,
      "loss": 2.8368,
      "step": 428400
    },
    {
      "epoch": 138.89789303079417,
      "grad_norm": 1.0503700971603394,
      "learning_rate": 3.613937722997081e-05,
      "loss": 2.8436,
      "step": 428500
    },
    {
      "epoch": 138.93030794165315,
      "grad_norm": 1.0061390399932861,
      "learning_rate": 3.6136133636068765e-05,
      "loss": 2.8442,
      "step": 428600
    },
    {
      "epoch": 138.96272285251214,
      "grad_norm": 1.1939183473587036,
      "learning_rate": 3.613289004216672e-05,
      "loss": 2.8298,
      "step": 428700
    },
    {
      "epoch": 138.99513776337116,
      "grad_norm": 1.2178401947021484,
      "learning_rate": 3.612964644826468e-05,
      "loss": 2.8324,
      "step": 428800
    },
    {
      "epoch": 139.0,
      "eval_bleu": 1.1262103770664231,
      "eval_loss": 3.9190187454223633,
      "eval_runtime": 4.2123,
      "eval_samples_per_second": 116.8,
      "eval_steps_per_second": 1.899,
      "step": 428815
    },
    {
      "epoch": 139.02755267423015,
      "grad_norm": 1.0846014022827148,
      "learning_rate": 3.6126402854362634e-05,
      "loss": 2.8093,
      "step": 428900
    },
    {
      "epoch": 139.05996758508914,
      "grad_norm": 1.322775959968567,
      "learning_rate": 3.612315926046059e-05,
      "loss": 2.8159,
      "step": 429000
    },
    {
      "epoch": 139.09238249594813,
      "grad_norm": 1.1908907890319824,
      "learning_rate": 3.6119915666558545e-05,
      "loss": 2.8306,
      "step": 429100
    },
    {
      "epoch": 139.12479740680712,
      "grad_norm": 1.1528122425079346,
      "learning_rate": 3.611670450859553e-05,
      "loss": 2.8185,
      "step": 429200
    },
    {
      "epoch": 139.15721231766614,
      "grad_norm": 1.0222805738449097,
      "learning_rate": 3.611349335063251e-05,
      "loss": 2.8019,
      "step": 429300
    },
    {
      "epoch": 139.18962722852513,
      "grad_norm": 1.041013240814209,
      "learning_rate": 3.611024975673046e-05,
      "loss": 2.8487,
      "step": 429400
    },
    {
      "epoch": 139.22204213938411,
      "grad_norm": 1.1548125743865967,
      "learning_rate": 3.610700616282842e-05,
      "loss": 2.8227,
      "step": 429500
    },
    {
      "epoch": 139.2544570502431,
      "grad_norm": 1.1383744478225708,
      "learning_rate": 3.610376256892638e-05,
      "loss": 2.8103,
      "step": 429600
    },
    {
      "epoch": 139.2868719611021,
      "grad_norm": 1.132495641708374,
      "learning_rate": 3.610051897502433e-05,
      "loss": 2.8057,
      "step": 429700
    },
    {
      "epoch": 139.3192868719611,
      "grad_norm": 1.0300525426864624,
      "learning_rate": 3.609727538112229e-05,
      "loss": 2.8195,
      "step": 429800
    },
    {
      "epoch": 139.3517017828201,
      "grad_norm": 1.1408889293670654,
      "learning_rate": 3.609403178722024e-05,
      "loss": 2.8251,
      "step": 429900
    },
    {
      "epoch": 139.3841166936791,
      "grad_norm": 0.9600631594657898,
      "learning_rate": 3.60907881933182e-05,
      "loss": 2.8301,
      "step": 430000
    },
    {
      "epoch": 139.41653160453808,
      "grad_norm": 0.9586990475654602,
      "learning_rate": 3.608754459941616e-05,
      "loss": 2.8187,
      "step": 430100
    },
    {
      "epoch": 139.4489465153971,
      "grad_norm": 0.981425404548645,
      "learning_rate": 3.608430100551411e-05,
      "loss": 2.8208,
      "step": 430200
    },
    {
      "epoch": 139.4813614262561,
      "grad_norm": 1.2683355808258057,
      "learning_rate": 3.608105741161207e-05,
      "loss": 2.8109,
      "step": 430300
    },
    {
      "epoch": 139.51377633711508,
      "grad_norm": 1.179417610168457,
      "learning_rate": 3.6077813817710026e-05,
      "loss": 2.8139,
      "step": 430400
    },
    {
      "epoch": 139.54619124797406,
      "grad_norm": 1.0539352893829346,
      "learning_rate": 3.607457022380798e-05,
      "loss": 2.8369,
      "step": 430500
    },
    {
      "epoch": 139.57860615883305,
      "grad_norm": 1.0092310905456543,
      "learning_rate": 3.607132662990594e-05,
      "loss": 2.833,
      "step": 430600
    },
    {
      "epoch": 139.61102106969207,
      "grad_norm": 1.1866577863693237,
      "learning_rate": 3.606808303600389e-05,
      "loss": 2.8107,
      "step": 430700
    },
    {
      "epoch": 139.64343598055106,
      "grad_norm": 1.2589507102966309,
      "learning_rate": 3.606483944210185e-05,
      "loss": 2.8198,
      "step": 430800
    },
    {
      "epoch": 139.67585089141005,
      "grad_norm": 1.2566086053848267,
      "learning_rate": 3.6061595848199806e-05,
      "loss": 2.8454,
      "step": 430900
    },
    {
      "epoch": 139.70826580226904,
      "grad_norm": 1.1288715600967407,
      "learning_rate": 3.605835225429776e-05,
      "loss": 2.8148,
      "step": 431000
    },
    {
      "epoch": 139.74068071312803,
      "grad_norm": 1.064104437828064,
      "learning_rate": 3.605510866039572e-05,
      "loss": 2.8158,
      "step": 431100
    },
    {
      "epoch": 139.77309562398705,
      "grad_norm": 1.3354382514953613,
      "learning_rate": 3.6051865066493676e-05,
      "loss": 2.8252,
      "step": 431200
    },
    {
      "epoch": 139.80551053484604,
      "grad_norm": 1.1923266649246216,
      "learning_rate": 3.6048621472591635e-05,
      "loss": 2.8114,
      "step": 431300
    },
    {
      "epoch": 139.83792544570503,
      "grad_norm": 1.2944939136505127,
      "learning_rate": 3.604537787868959e-05,
      "loss": 2.8493,
      "step": 431400
    },
    {
      "epoch": 139.87034035656401,
      "grad_norm": 0.9194449782371521,
      "learning_rate": 3.604213428478755e-05,
      "loss": 2.832,
      "step": 431500
    },
    {
      "epoch": 139.902755267423,
      "grad_norm": 1.2893770933151245,
      "learning_rate": 3.6038890690885504e-05,
      "loss": 2.825,
      "step": 431600
    },
    {
      "epoch": 139.93517017828202,
      "grad_norm": 1.261492133140564,
      "learning_rate": 3.603564709698346e-05,
      "loss": 2.8503,
      "step": 431700
    },
    {
      "epoch": 139.967585089141,
      "grad_norm": 1.11946439743042,
      "learning_rate": 3.6032403503081415e-05,
      "loss": 2.8348,
      "step": 431800
    },
    {
      "epoch": 140.0,
      "grad_norm": 1.0710221529006958,
      "learning_rate": 3.6029159909179373e-05,
      "loss": 2.8401,
      "step": 431900
    },
    {
      "epoch": 140.0,
      "eval_bleu": 1.1469878935809734,
      "eval_loss": 3.914172649383545,
      "eval_runtime": 4.1757,
      "eval_samples_per_second": 117.826,
      "eval_steps_per_second": 1.916,
      "step": 431900
    },
    {
      "epoch": 140.032414910859,
      "grad_norm": 1.1708837747573853,
      "learning_rate": 3.602591631527733e-05,
      "loss": 2.7982,
      "step": 432000
    },
    {
      "epoch": 140.06482982171798,
      "grad_norm": 1.1574629545211792,
      "learning_rate": 3.6022672721375284e-05,
      "loss": 2.8147,
      "step": 432100
    },
    {
      "epoch": 140.097244732577,
      "grad_norm": 1.2689017057418823,
      "learning_rate": 3.601942912747324e-05,
      "loss": 2.8219,
      "step": 432200
    },
    {
      "epoch": 140.12965964343599,
      "grad_norm": 1.2057275772094727,
      "learning_rate": 3.60161855335712e-05,
      "loss": 2.8184,
      "step": 432300
    },
    {
      "epoch": 140.16207455429497,
      "grad_norm": 1.2449764013290405,
      "learning_rate": 3.6012941939669154e-05,
      "loss": 2.8199,
      "step": 432400
    },
    {
      "epoch": 140.19448946515396,
      "grad_norm": 1.285077452659607,
      "learning_rate": 3.600969834576711e-05,
      "loss": 2.809,
      "step": 432500
    },
    {
      "epoch": 140.22690437601295,
      "grad_norm": 0.9927593469619751,
      "learning_rate": 3.600648718780409e-05,
      "loss": 2.8243,
      "step": 432600
    },
    {
      "epoch": 140.25931928687197,
      "grad_norm": 0.9933475852012634,
      "learning_rate": 3.600324359390205e-05,
      "loss": 2.8268,
      "step": 432700
    },
    {
      "epoch": 140.29173419773096,
      "grad_norm": 1.227668046951294,
      "learning_rate": 3.6e-05,
      "loss": 2.8169,
      "step": 432800
    },
    {
      "epoch": 140.32414910858995,
      "grad_norm": 1.1639989614486694,
      "learning_rate": 3.599675640609796e-05,
      "loss": 2.8101,
      "step": 432900
    },
    {
      "epoch": 140.35656401944894,
      "grad_norm": 1.2580246925354004,
      "learning_rate": 3.599351281219592e-05,
      "loss": 2.8318,
      "step": 433000
    },
    {
      "epoch": 140.38897893030793,
      "grad_norm": 1.1749777793884277,
      "learning_rate": 3.599026921829387e-05,
      "loss": 2.8208,
      "step": 433100
    },
    {
      "epoch": 140.42139384116695,
      "grad_norm": 1.2184422016143799,
      "learning_rate": 3.598702562439183e-05,
      "loss": 2.8132,
      "step": 433200
    },
    {
      "epoch": 140.45380875202594,
      "grad_norm": 1.1905224323272705,
      "learning_rate": 3.598378203048978e-05,
      "loss": 2.8062,
      "step": 433300
    },
    {
      "epoch": 140.48622366288492,
      "grad_norm": 1.2327497005462646,
      "learning_rate": 3.598053843658774e-05,
      "loss": 2.8108,
      "step": 433400
    },
    {
      "epoch": 140.5186385737439,
      "grad_norm": 1.205543875694275,
      "learning_rate": 3.59772948426857e-05,
      "loss": 2.8322,
      "step": 433500
    },
    {
      "epoch": 140.5510534846029,
      "grad_norm": 1.1404107809066772,
      "learning_rate": 3.597405124878365e-05,
      "loss": 2.8248,
      "step": 433600
    },
    {
      "epoch": 140.58346839546192,
      "grad_norm": 1.1714516878128052,
      "learning_rate": 3.597080765488161e-05,
      "loss": 2.8213,
      "step": 433700
    },
    {
      "epoch": 140.6158833063209,
      "grad_norm": 1.1863274574279785,
      "learning_rate": 3.596756406097957e-05,
      "loss": 2.8314,
      "step": 433800
    },
    {
      "epoch": 140.6482982171799,
      "grad_norm": 1.129440426826477,
      "learning_rate": 3.596432046707752e-05,
      "loss": 2.8544,
      "step": 433900
    },
    {
      "epoch": 140.6807131280389,
      "grad_norm": 1.1695317029953003,
      "learning_rate": 3.596107687317548e-05,
      "loss": 2.8261,
      "step": 434000
    },
    {
      "epoch": 140.7131280388979,
      "grad_norm": 1.1835060119628906,
      "learning_rate": 3.5957865715212457e-05,
      "loss": 2.8287,
      "step": 434100
    },
    {
      "epoch": 140.7455429497569,
      "grad_norm": 1.154855489730835,
      "learning_rate": 3.5954622121310415e-05,
      "loss": 2.8155,
      "step": 434200
    },
    {
      "epoch": 140.77795786061589,
      "grad_norm": 1.053505539894104,
      "learning_rate": 3.595137852740837e-05,
      "loss": 2.8277,
      "step": 434300
    },
    {
      "epoch": 140.81037277147487,
      "grad_norm": 1.026811122894287,
      "learning_rate": 3.5948134933506326e-05,
      "loss": 2.8305,
      "step": 434400
    },
    {
      "epoch": 140.84278768233386,
      "grad_norm": 1.0882207155227661,
      "learning_rate": 3.594489133960428e-05,
      "loss": 2.846,
      "step": 434500
    },
    {
      "epoch": 140.87520259319288,
      "grad_norm": 1.0536562204360962,
      "learning_rate": 3.594164774570224e-05,
      "loss": 2.84,
      "step": 434600
    },
    {
      "epoch": 140.90761750405187,
      "grad_norm": 1.0638291835784912,
      "learning_rate": 3.5938404151800195e-05,
      "loss": 2.8329,
      "step": 434700
    },
    {
      "epoch": 140.94003241491086,
      "grad_norm": 1.1214923858642578,
      "learning_rate": 3.5935160557898154e-05,
      "loss": 2.8191,
      "step": 434800
    },
    {
      "epoch": 140.97244732576985,
      "grad_norm": 1.2312122583389282,
      "learning_rate": 3.5931916963996106e-05,
      "loss": 2.8235,
      "step": 434900
    },
    {
      "epoch": 141.0,
      "eval_bleu": 1.0341474625679887,
      "eval_loss": 3.9185683727264404,
      "eval_runtime": 4.1108,
      "eval_samples_per_second": 119.686,
      "eval_steps_per_second": 1.946,
      "step": 434985
    },
    {
      "epoch": 141.00486223662884,
      "grad_norm": 1.1173080205917358,
      "learning_rate": 3.5928673370094065e-05,
      "loss": 2.8045,
      "step": 435000
    },
    {
      "epoch": 141.03727714748786,
      "grad_norm": 1.0672388076782227,
      "learning_rate": 3.5925429776192024e-05,
      "loss": 2.814,
      "step": 435100
    },
    {
      "epoch": 141.06969205834685,
      "grad_norm": 1.2639414072036743,
      "learning_rate": 3.592218618228998e-05,
      "loss": 2.8161,
      "step": 435200
    },
    {
      "epoch": 141.10210696920583,
      "grad_norm": 1.1642894744873047,
      "learning_rate": 3.591894258838794e-05,
      "loss": 2.8097,
      "step": 435300
    },
    {
      "epoch": 141.13452188006482,
      "grad_norm": 1.1866923570632935,
      "learning_rate": 3.591569899448589e-05,
      "loss": 2.8211,
      "step": 435400
    },
    {
      "epoch": 141.1669367909238,
      "grad_norm": 1.3702057600021362,
      "learning_rate": 3.591245540058385e-05,
      "loss": 2.8105,
      "step": 435500
    },
    {
      "epoch": 141.19935170178283,
      "grad_norm": 1.241310954093933,
      "learning_rate": 3.5909211806681804e-05,
      "loss": 2.8078,
      "step": 435600
    },
    {
      "epoch": 141.23176661264182,
      "grad_norm": 1.1783143281936646,
      "learning_rate": 3.590596821277976e-05,
      "loss": 2.8032,
      "step": 435700
    },
    {
      "epoch": 141.2641815235008,
      "grad_norm": 1.0827692747116089,
      "learning_rate": 3.590272461887772e-05,
      "loss": 2.8122,
      "step": 435800
    },
    {
      "epoch": 141.2965964343598,
      "grad_norm": 1.1241766214370728,
      "learning_rate": 3.589948102497567e-05,
      "loss": 2.7936,
      "step": 435900
    },
    {
      "epoch": 141.3290113452188,
      "grad_norm": 1.1868436336517334,
      "learning_rate": 3.589623743107363e-05,
      "loss": 2.8076,
      "step": 436000
    },
    {
      "epoch": 141.3614262560778,
      "grad_norm": 1.1272581815719604,
      "learning_rate": 3.589299383717159e-05,
      "loss": 2.8298,
      "step": 436100
    },
    {
      "epoch": 141.3938411669368,
      "grad_norm": 1.0530134439468384,
      "learning_rate": 3.588975024326954e-05,
      "loss": 2.8296,
      "step": 436200
    },
    {
      "epoch": 141.42625607779578,
      "grad_norm": 1.144349217414856,
      "learning_rate": 3.58865066493675e-05,
      "loss": 2.8251,
      "step": 436300
    },
    {
      "epoch": 141.45867098865477,
      "grad_norm": 0.9882986545562744,
      "learning_rate": 3.5883263055465453e-05,
      "loss": 2.807,
      "step": 436400
    },
    {
      "epoch": 141.49108589951376,
      "grad_norm": 1.2071733474731445,
      "learning_rate": 3.588001946156341e-05,
      "loss": 2.8249,
      "step": 436500
    },
    {
      "epoch": 141.52350081037278,
      "grad_norm": 1.1716469526290894,
      "learning_rate": 3.587677586766137e-05,
      "loss": 2.8312,
      "step": 436600
    },
    {
      "epoch": 141.55591572123177,
      "grad_norm": 1.0725798606872559,
      "learning_rate": 3.587353227375932e-05,
      "loss": 2.8269,
      "step": 436700
    },
    {
      "epoch": 141.58833063209076,
      "grad_norm": 1.2090325355529785,
      "learning_rate": 3.587028867985728e-05,
      "loss": 2.8095,
      "step": 436800
    },
    {
      "epoch": 141.62074554294975,
      "grad_norm": 1.2102687358856201,
      "learning_rate": 3.586704508595524e-05,
      "loss": 2.8215,
      "step": 436900
    },
    {
      "epoch": 141.65316045380874,
      "grad_norm": 1.2171638011932373,
      "learning_rate": 3.586380149205319e-05,
      "loss": 2.8414,
      "step": 437000
    },
    {
      "epoch": 141.68557536466776,
      "grad_norm": 1.1123212575912476,
      "learning_rate": 3.586055789815115e-05,
      "loss": 2.8355,
      "step": 437100
    },
    {
      "epoch": 141.71799027552674,
      "grad_norm": 1.1375528573989868,
      "learning_rate": 3.585731430424911e-05,
      "loss": 2.8295,
      "step": 437200
    },
    {
      "epoch": 141.75040518638573,
      "grad_norm": 1.0753345489501953,
      "learning_rate": 3.585407071034707e-05,
      "loss": 2.8218,
      "step": 437300
    },
    {
      "epoch": 141.78282009724472,
      "grad_norm": 1.075228214263916,
      "learning_rate": 3.585082711644502e-05,
      "loss": 2.8238,
      "step": 437400
    },
    {
      "epoch": 141.81523500810374,
      "grad_norm": 0.9967745542526245,
      "learning_rate": 3.584758352254298e-05,
      "loss": 2.8202,
      "step": 437500
    },
    {
      "epoch": 141.84764991896273,
      "grad_norm": 0.9572752118110657,
      "learning_rate": 3.584433992864094e-05,
      "loss": 2.821,
      "step": 437600
    },
    {
      "epoch": 141.88006482982172,
      "grad_norm": 1.271497368812561,
      "learning_rate": 3.58410963347389e-05,
      "loss": 2.8054,
      "step": 437700
    },
    {
      "epoch": 141.9124797406807,
      "grad_norm": 1.1679692268371582,
      "learning_rate": 3.583785274083685e-05,
      "loss": 2.821,
      "step": 437800
    },
    {
      "epoch": 141.9448946515397,
      "grad_norm": 1.1140528917312622,
      "learning_rate": 3.583460914693481e-05,
      "loss": 2.8306,
      "step": 437900
    },
    {
      "epoch": 141.97730956239872,
      "grad_norm": 1.027775764465332,
      "learning_rate": 3.5831365553032766e-05,
      "loss": 2.8224,
      "step": 438000
    },
    {
      "epoch": 142.0,
      "eval_bleu": 1.0219324824751082,
      "eval_loss": 3.917123794555664,
      "eval_runtime": 3.8248,
      "eval_samples_per_second": 128.635,
      "eval_steps_per_second": 2.092,
      "step": 438070
    },
    {
      "epoch": 142.0097244732577,
      "grad_norm": 1.167482614517212,
      "learning_rate": 3.582812195913072e-05,
      "loss": 2.8555,
      "step": 438100
    },
    {
      "epoch": 142.0421393841167,
      "grad_norm": 0.95273357629776,
      "learning_rate": 3.582487836522868e-05,
      "loss": 2.7985,
      "step": 438200
    },
    {
      "epoch": 142.07455429497568,
      "grad_norm": 0.9527717232704163,
      "learning_rate": 3.5821634771326636e-05,
      "loss": 2.8227,
      "step": 438300
    },
    {
      "epoch": 142.10696920583467,
      "grad_norm": 1.1205979585647583,
      "learning_rate": 3.581839117742459e-05,
      "loss": 2.8117,
      "step": 438400
    },
    {
      "epoch": 142.1393841166937,
      "grad_norm": 1.0264066457748413,
      "learning_rate": 3.5815147583522546e-05,
      "loss": 2.8229,
      "step": 438500
    },
    {
      "epoch": 142.17179902755268,
      "grad_norm": 1.150719404220581,
      "learning_rate": 3.58119039896205e-05,
      "loss": 2.7937,
      "step": 438600
    },
    {
      "epoch": 142.20421393841167,
      "grad_norm": 1.2067108154296875,
      "learning_rate": 3.580866039571846e-05,
      "loss": 2.8275,
      "step": 438700
    },
    {
      "epoch": 142.23662884927066,
      "grad_norm": 1.1803133487701416,
      "learning_rate": 3.5805416801816416e-05,
      "loss": 2.8006,
      "step": 438800
    },
    {
      "epoch": 142.26904376012965,
      "grad_norm": 1.21170973777771,
      "learning_rate": 3.580217320791437e-05,
      "loss": 2.8152,
      "step": 438900
    },
    {
      "epoch": 142.30145867098867,
      "grad_norm": 1.3279118537902832,
      "learning_rate": 3.5798929614012327e-05,
      "loss": 2.8352,
      "step": 439000
    },
    {
      "epoch": 142.33387358184766,
      "grad_norm": 1.1720805168151855,
      "learning_rate": 3.5795686020110285e-05,
      "loss": 2.825,
      "step": 439100
    },
    {
      "epoch": 142.36628849270664,
      "grad_norm": 1.0466513633728027,
      "learning_rate": 3.579244242620824e-05,
      "loss": 2.8257,
      "step": 439200
    },
    {
      "epoch": 142.39870340356563,
      "grad_norm": 1.0843381881713867,
      "learning_rate": 3.5789198832306196e-05,
      "loss": 2.812,
      "step": 439300
    },
    {
      "epoch": 142.43111831442462,
      "grad_norm": 1.1817936897277832,
      "learning_rate": 3.578595523840415e-05,
      "loss": 2.8184,
      "step": 439400
    },
    {
      "epoch": 142.46353322528364,
      "grad_norm": 1.0493160486221313,
      "learning_rate": 3.578271164450211e-05,
      "loss": 2.8196,
      "step": 439500
    },
    {
      "epoch": 142.49594813614263,
      "grad_norm": 1.104068636894226,
      "learning_rate": 3.5779468050600065e-05,
      "loss": 2.8246,
      "step": 439600
    },
    {
      "epoch": 142.52836304700162,
      "grad_norm": 1.0761961936950684,
      "learning_rate": 3.5776224456698024e-05,
      "loss": 2.8097,
      "step": 439700
    },
    {
      "epoch": 142.5607779578606,
      "grad_norm": 1.144729495048523,
      "learning_rate": 3.5772980862795976e-05,
      "loss": 2.8282,
      "step": 439800
    },
    {
      "epoch": 142.5931928687196,
      "grad_norm": 1.476270079612732,
      "learning_rate": 3.5769737268893935e-05,
      "loss": 2.8218,
      "step": 439900
    },
    {
      "epoch": 142.62560777957862,
      "grad_norm": 1.0626672506332397,
      "learning_rate": 3.5766493674991894e-05,
      "loss": 2.8066,
      "step": 440000
    },
    {
      "epoch": 142.6580226904376,
      "grad_norm": 1.0893354415893555,
      "learning_rate": 3.576325008108985e-05,
      "loss": 2.8108,
      "step": 440100
    },
    {
      "epoch": 142.6904376012966,
      "grad_norm": 1.168938159942627,
      "learning_rate": 3.576003892312682e-05,
      "loss": 2.8294,
      "step": 440200
    },
    {
      "epoch": 142.72285251215558,
      "grad_norm": 1.3195865154266357,
      "learning_rate": 3.575679532922478e-05,
      "loss": 2.8397,
      "step": 440300
    },
    {
      "epoch": 142.75526742301457,
      "grad_norm": 1.1975654363632202,
      "learning_rate": 3.575355173532274e-05,
      "loss": 2.828,
      "step": 440400
    },
    {
      "epoch": 142.7876823338736,
      "grad_norm": 1.1240304708480835,
      "learning_rate": 3.57503081414207e-05,
      "loss": 2.8107,
      "step": 440500
    },
    {
      "epoch": 142.82009724473258,
      "grad_norm": 1.0356467962265015,
      "learning_rate": 3.574706454751866e-05,
      "loss": 2.8253,
      "step": 440600
    },
    {
      "epoch": 142.85251215559157,
      "grad_norm": 1.0755189657211304,
      "learning_rate": 3.574382095361661e-05,
      "loss": 2.8019,
      "step": 440700
    },
    {
      "epoch": 142.88492706645056,
      "grad_norm": 1.1711657047271729,
      "learning_rate": 3.574057735971457e-05,
      "loss": 2.8425,
      "step": 440800
    },
    {
      "epoch": 142.91734197730958,
      "grad_norm": 1.057437539100647,
      "learning_rate": 3.573733376581252e-05,
      "loss": 2.8108,
      "step": 440900
    },
    {
      "epoch": 142.94975688816857,
      "grad_norm": 1.1808747053146362,
      "learning_rate": 3.573409017191048e-05,
      "loss": 2.8111,
      "step": 441000
    },
    {
      "epoch": 142.98217179902755,
      "grad_norm": 1.1965683698654175,
      "learning_rate": 3.573084657800844e-05,
      "loss": 2.8066,
      "step": 441100
    },
    {
      "epoch": 143.0,
      "eval_bleu": 1.1272604464154674,
      "eval_loss": 3.9201314449310303,
      "eval_runtime": 4.4413,
      "eval_samples_per_second": 110.779,
      "eval_steps_per_second": 1.801,
      "step": 441155
    },
    {
      "epoch": 143.01458670988654,
      "grad_norm": 1.1631996631622314,
      "learning_rate": 3.572760298410639e-05,
      "loss": 2.8108,
      "step": 441200
    },
    {
      "epoch": 143.04700162074553,
      "grad_norm": 1.1878572702407837,
      "learning_rate": 3.572435939020435e-05,
      "loss": 2.8068,
      "step": 441300
    },
    {
      "epoch": 143.07941653160455,
      "grad_norm": 1.0917942523956299,
      "learning_rate": 3.572111579630231e-05,
      "loss": 2.8284,
      "step": 441400
    },
    {
      "epoch": 143.11183144246354,
      "grad_norm": 1.2742352485656738,
      "learning_rate": 3.571787220240026e-05,
      "loss": 2.805,
      "step": 441500
    },
    {
      "epoch": 143.14424635332253,
      "grad_norm": 1.0701030492782593,
      "learning_rate": 3.571462860849822e-05,
      "loss": 2.8079,
      "step": 441600
    },
    {
      "epoch": 143.17666126418152,
      "grad_norm": 1.1944389343261719,
      "learning_rate": 3.571138501459618e-05,
      "loss": 2.8124,
      "step": 441700
    },
    {
      "epoch": 143.2090761750405,
      "grad_norm": 1.2186431884765625,
      "learning_rate": 3.570814142069413e-05,
      "loss": 2.8148,
      "step": 441800
    },
    {
      "epoch": 143.24149108589953,
      "grad_norm": 1.0969531536102295,
      "learning_rate": 3.570489782679209e-05,
      "loss": 2.8142,
      "step": 441900
    },
    {
      "epoch": 143.27390599675851,
      "grad_norm": 1.17122483253479,
      "learning_rate": 3.570165423289004e-05,
      "loss": 2.8142,
      "step": 442000
    },
    {
      "epoch": 143.3063209076175,
      "grad_norm": 1.1241203546524048,
      "learning_rate": 3.5698410638988e-05,
      "loss": 2.8298,
      "step": 442100
    },
    {
      "epoch": 143.3387358184765,
      "grad_norm": 1.2438334226608276,
      "learning_rate": 3.569519948102498e-05,
      "loss": 2.8126,
      "step": 442200
    },
    {
      "epoch": 143.37115072933548,
      "grad_norm": 1.2142844200134277,
      "learning_rate": 3.5691955887122935e-05,
      "loss": 2.8197,
      "step": 442300
    },
    {
      "epoch": 143.4035656401945,
      "grad_norm": 1.2356029748916626,
      "learning_rate": 3.568871229322089e-05,
      "loss": 2.812,
      "step": 442400
    },
    {
      "epoch": 143.4359805510535,
      "grad_norm": 0.9484562873840332,
      "learning_rate": 3.5685468699318846e-05,
      "loss": 2.8305,
      "step": 442500
    },
    {
      "epoch": 143.46839546191248,
      "grad_norm": 1.270434021949768,
      "learning_rate": 3.5682225105416805e-05,
      "loss": 2.8187,
      "step": 442600
    },
    {
      "epoch": 143.50081037277147,
      "grad_norm": 1.1929134130477905,
      "learning_rate": 3.567898151151476e-05,
      "loss": 2.7993,
      "step": 442700
    },
    {
      "epoch": 143.53322528363046,
      "grad_norm": 1.1878012418746948,
      "learning_rate": 3.5675737917612716e-05,
      "loss": 2.8355,
      "step": 442800
    },
    {
      "epoch": 143.56564019448948,
      "grad_norm": 1.2670888900756836,
      "learning_rate": 3.5672494323710674e-05,
      "loss": 2.8154,
      "step": 442900
    },
    {
      "epoch": 143.59805510534846,
      "grad_norm": 1.1186492443084717,
      "learning_rate": 3.5669250729808626e-05,
      "loss": 2.8097,
      "step": 443000
    },
    {
      "epoch": 143.63047001620745,
      "grad_norm": 1.2711125612258911,
      "learning_rate": 3.5666007135906585e-05,
      "loss": 2.814,
      "step": 443100
    },
    {
      "epoch": 143.66288492706644,
      "grad_norm": 1.1916424036026,
      "learning_rate": 3.566276354200454e-05,
      "loss": 2.8024,
      "step": 443200
    },
    {
      "epoch": 143.69529983792543,
      "grad_norm": 1.290497064590454,
      "learning_rate": 3.5659519948102496e-05,
      "loss": 2.816,
      "step": 443300
    },
    {
      "epoch": 143.72771474878445,
      "grad_norm": 1.2927879095077515,
      "learning_rate": 3.5656276354200454e-05,
      "loss": 2.8371,
      "step": 443400
    },
    {
      "epoch": 143.76012965964344,
      "grad_norm": 1.0474042892456055,
      "learning_rate": 3.565303276029841e-05,
      "loss": 2.8298,
      "step": 443500
    },
    {
      "epoch": 143.79254457050243,
      "grad_norm": 1.0362063646316528,
      "learning_rate": 3.564978916639637e-05,
      "loss": 2.813,
      "step": 443600
    },
    {
      "epoch": 143.82495948136142,
      "grad_norm": 1.2233597040176392,
      "learning_rate": 3.564654557249433e-05,
      "loss": 2.7923,
      "step": 443700
    },
    {
      "epoch": 143.8573743922204,
      "grad_norm": 1.0685728788375854,
      "learning_rate": 3.564330197859228e-05,
      "loss": 2.8306,
      "step": 443800
    },
    {
      "epoch": 143.88978930307943,
      "grad_norm": 1.0372428894042969,
      "learning_rate": 3.564005838469024e-05,
      "loss": 2.8181,
      "step": 443900
    },
    {
      "epoch": 143.92220421393841,
      "grad_norm": 1.4157133102416992,
      "learning_rate": 3.56368147907882e-05,
      "loss": 2.8151,
      "step": 444000
    },
    {
      "epoch": 143.9546191247974,
      "grad_norm": 0.9885478019714355,
      "learning_rate": 3.563357119688615e-05,
      "loss": 2.8144,
      "step": 444100
    },
    {
      "epoch": 143.9870340356564,
      "grad_norm": 1.196514368057251,
      "learning_rate": 3.563036003892313e-05,
      "loss": 2.8174,
      "step": 444200
    },
    {
      "epoch": 144.0,
      "eval_bleu": 1.1826273086591905,
      "eval_loss": 3.9191997051239014,
      "eval_runtime": 4.4748,
      "eval_samples_per_second": 109.949,
      "eval_steps_per_second": 1.788,
      "step": 444240
    },
    {
      "epoch": 144.0194489465154,
      "grad_norm": 1.0018337965011597,
      "learning_rate": 3.562711644502109e-05,
      "loss": 2.807,
      "step": 444300
    },
    {
      "epoch": 144.0518638573744,
      "grad_norm": 1.1846253871917725,
      "learning_rate": 3.562387285111904e-05,
      "loss": 2.8166,
      "step": 444400
    },
    {
      "epoch": 144.0842787682334,
      "grad_norm": 1.034058928489685,
      "learning_rate": 3.5620629257217e-05,
      "loss": 2.8079,
      "step": 444500
    },
    {
      "epoch": 144.11669367909238,
      "grad_norm": 1.1099179983139038,
      "learning_rate": 3.561738566331496e-05,
      "loss": 2.8162,
      "step": 444600
    },
    {
      "epoch": 144.14910858995137,
      "grad_norm": 1.0994807481765747,
      "learning_rate": 3.561414206941291e-05,
      "loss": 2.7987,
      "step": 444700
    },
    {
      "epoch": 144.18152350081039,
      "grad_norm": 1.0128884315490723,
      "learning_rate": 3.561089847551087e-05,
      "loss": 2.8127,
      "step": 444800
    },
    {
      "epoch": 144.21393841166937,
      "grad_norm": 1.0951439142227173,
      "learning_rate": 3.560765488160883e-05,
      "loss": 2.8063,
      "step": 444900
    },
    {
      "epoch": 144.24635332252836,
      "grad_norm": 1.0388559103012085,
      "learning_rate": 3.560441128770678e-05,
      "loss": 2.8115,
      "step": 445000
    },
    {
      "epoch": 144.27876823338735,
      "grad_norm": 1.1445192098617554,
      "learning_rate": 3.560116769380474e-05,
      "loss": 2.8172,
      "step": 445100
    },
    {
      "epoch": 144.31118314424634,
      "grad_norm": 1.0861274003982544,
      "learning_rate": 3.55979240999027e-05,
      "loss": 2.8279,
      "step": 445200
    },
    {
      "epoch": 144.34359805510536,
      "grad_norm": 1.1462962627410889,
      "learning_rate": 3.559468050600065e-05,
      "loss": 2.7918,
      "step": 445300
    },
    {
      "epoch": 144.37601296596435,
      "grad_norm": 1.112865924835205,
      "learning_rate": 3.559143691209861e-05,
      "loss": 2.8283,
      "step": 445400
    },
    {
      "epoch": 144.40842787682334,
      "grad_norm": 1.0750479698181152,
      "learning_rate": 3.558819331819656e-05,
      "loss": 2.8174,
      "step": 445500
    },
    {
      "epoch": 144.44084278768233,
      "grad_norm": 1.138580083847046,
      "learning_rate": 3.558494972429452e-05,
      "loss": 2.8169,
      "step": 445600
    },
    {
      "epoch": 144.47325769854132,
      "grad_norm": 1.0925103425979614,
      "learning_rate": 3.558170613039248e-05,
      "loss": 2.8019,
      "step": 445700
    },
    {
      "epoch": 144.50567260940034,
      "grad_norm": 1.0238537788391113,
      "learning_rate": 3.557846253649043e-05,
      "loss": 2.7944,
      "step": 445800
    },
    {
      "epoch": 144.53808752025932,
      "grad_norm": 1.2338664531707764,
      "learning_rate": 3.557521894258839e-05,
      "loss": 2.8103,
      "step": 445900
    },
    {
      "epoch": 144.5705024311183,
      "grad_norm": 1.2920395135879517,
      "learning_rate": 3.5571975348686347e-05,
      "loss": 2.8086,
      "step": 446000
    },
    {
      "epoch": 144.6029173419773,
      "grad_norm": 1.0178250074386597,
      "learning_rate": 3.55687317547843e-05,
      "loss": 2.8232,
      "step": 446100
    },
    {
      "epoch": 144.6353322528363,
      "grad_norm": 1.1600513458251953,
      "learning_rate": 3.5565520596821276e-05,
      "loss": 2.8114,
      "step": 446200
    },
    {
      "epoch": 144.6677471636953,
      "grad_norm": 1.1120003461837769,
      "learning_rate": 3.5562277002919235e-05,
      "loss": 2.8306,
      "step": 446300
    },
    {
      "epoch": 144.7001620745543,
      "grad_norm": 1.2303557395935059,
      "learning_rate": 3.5559033409017194e-05,
      "loss": 2.8491,
      "step": 446400
    },
    {
      "epoch": 144.7325769854133,
      "grad_norm": 1.338727355003357,
      "learning_rate": 3.5555789815115146e-05,
      "loss": 2.7968,
      "step": 446500
    },
    {
      "epoch": 144.76499189627228,
      "grad_norm": 0.9896644949913025,
      "learning_rate": 3.5552546221213105e-05,
      "loss": 2.8202,
      "step": 446600
    },
    {
      "epoch": 144.79740680713127,
      "grad_norm": 1.0699807405471802,
      "learning_rate": 3.5549302627311057e-05,
      "loss": 2.8107,
      "step": 446700
    },
    {
      "epoch": 144.82982171799028,
      "grad_norm": 1.1305551528930664,
      "learning_rate": 3.5546059033409015e-05,
      "loss": 2.8269,
      "step": 446800
    },
    {
      "epoch": 144.86223662884927,
      "grad_norm": 1.210593819618225,
      "learning_rate": 3.5542815439506974e-05,
      "loss": 2.8154,
      "step": 446900
    },
    {
      "epoch": 144.89465153970826,
      "grad_norm": 1.0699992179870605,
      "learning_rate": 3.553957184560493e-05,
      "loss": 2.805,
      "step": 447000
    },
    {
      "epoch": 144.92706645056725,
      "grad_norm": 1.1918375492095947,
      "learning_rate": 3.553632825170289e-05,
      "loss": 2.8164,
      "step": 447100
    },
    {
      "epoch": 144.95948136142624,
      "grad_norm": 1.1378757953643799,
      "learning_rate": 3.553308465780085e-05,
      "loss": 2.8085,
      "step": 447200
    },
    {
      "epoch": 144.99189627228526,
      "grad_norm": 1.2220629453659058,
      "learning_rate": 3.55298410638988e-05,
      "loss": 2.8232,
      "step": 447300
    },
    {
      "epoch": 145.0,
      "eval_bleu": 1.0741297363802262,
      "eval_loss": 3.925769090652466,
      "eval_runtime": 4.3905,
      "eval_samples_per_second": 112.061,
      "eval_steps_per_second": 1.822,
      "step": 447325
    },
    {
      "epoch": 145.02431118314425,
      "grad_norm": 1.0217989683151245,
      "learning_rate": 3.552659746999676e-05,
      "loss": 2.8024,
      "step": 447400
    },
    {
      "epoch": 145.05672609400324,
      "grad_norm": 1.2637462615966797,
      "learning_rate": 3.552335387609472e-05,
      "loss": 2.8092,
      "step": 447500
    },
    {
      "epoch": 145.08914100486223,
      "grad_norm": 1.066858172416687,
      "learning_rate": 3.552011028219267e-05,
      "loss": 2.7963,
      "step": 447600
    },
    {
      "epoch": 145.12155591572125,
      "grad_norm": 1.180326223373413,
      "learning_rate": 3.551686668829063e-05,
      "loss": 2.7916,
      "step": 447700
    },
    {
      "epoch": 145.15397082658023,
      "grad_norm": 1.2421002388000488,
      "learning_rate": 3.551362309438858e-05,
      "loss": 2.8117,
      "step": 447800
    },
    {
      "epoch": 145.18638573743922,
      "grad_norm": 1.0749698877334595,
      "learning_rate": 3.551037950048654e-05,
      "loss": 2.7836,
      "step": 447900
    },
    {
      "epoch": 145.2188006482982,
      "grad_norm": 1.2142494916915894,
      "learning_rate": 3.55071359065845e-05,
      "loss": 2.8074,
      "step": 448000
    },
    {
      "epoch": 145.2512155591572,
      "grad_norm": 0.9042927622795105,
      "learning_rate": 3.550389231268245e-05,
      "loss": 2.8094,
      "step": 448100
    },
    {
      "epoch": 145.28363047001622,
      "grad_norm": 1.1737489700317383,
      "learning_rate": 3.550068115471943e-05,
      "loss": 2.8021,
      "step": 448200
    },
    {
      "epoch": 145.3160453808752,
      "grad_norm": 1.1731950044631958,
      "learning_rate": 3.549743756081739e-05,
      "loss": 2.8062,
      "step": 448300
    },
    {
      "epoch": 145.3484602917342,
      "grad_norm": 1.1094712018966675,
      "learning_rate": 3.549419396691535e-05,
      "loss": 2.8231,
      "step": 448400
    },
    {
      "epoch": 145.3808752025932,
      "grad_norm": 1.1570113897323608,
      "learning_rate": 3.54909503730133e-05,
      "loss": 2.8094,
      "step": 448500
    },
    {
      "epoch": 145.41329011345218,
      "grad_norm": 1.0254169702529907,
      "learning_rate": 3.548770677911126e-05,
      "loss": 2.8042,
      "step": 448600
    },
    {
      "epoch": 145.4457050243112,
      "grad_norm": 1.0596107244491577,
      "learning_rate": 3.5484463185209217e-05,
      "loss": 2.8383,
      "step": 448700
    },
    {
      "epoch": 145.47811993517018,
      "grad_norm": 1.1800589561462402,
      "learning_rate": 3.548121959130717e-05,
      "loss": 2.814,
      "step": 448800
    },
    {
      "epoch": 145.51053484602917,
      "grad_norm": 1.070259928703308,
      "learning_rate": 3.547797599740513e-05,
      "loss": 2.8158,
      "step": 448900
    },
    {
      "epoch": 145.54294975688816,
      "grad_norm": 1.110918402671814,
      "learning_rate": 3.547473240350308e-05,
      "loss": 2.7909,
      "step": 449000
    },
    {
      "epoch": 145.57536466774715,
      "grad_norm": 1.1920815706253052,
      "learning_rate": 3.547148880960104e-05,
      "loss": 2.8228,
      "step": 449100
    },
    {
      "epoch": 145.60777957860617,
      "grad_norm": 1.1851812601089478,
      "learning_rate": 3.5468245215699e-05,
      "loss": 2.8215,
      "step": 449200
    },
    {
      "epoch": 145.64019448946516,
      "grad_norm": 1.0216017961502075,
      "learning_rate": 3.546500162179695e-05,
      "loss": 2.835,
      "step": 449300
    },
    {
      "epoch": 145.67260940032415,
      "grad_norm": 1.2746416330337524,
      "learning_rate": 3.546175802789491e-05,
      "loss": 2.8154,
      "step": 449400
    },
    {
      "epoch": 145.70502431118314,
      "grad_norm": 1.0317046642303467,
      "learning_rate": 3.5458514433992866e-05,
      "loss": 2.8016,
      "step": 449500
    },
    {
      "epoch": 145.73743922204213,
      "grad_norm": 1.401568055152893,
      "learning_rate": 3.545527084009082e-05,
      "loss": 2.8256,
      "step": 449600
    },
    {
      "epoch": 145.76985413290114,
      "grad_norm": 1.319624423980713,
      "learning_rate": 3.545202724618878e-05,
      "loss": 2.8016,
      "step": 449700
    },
    {
      "epoch": 145.80226904376013,
      "grad_norm": 1.1591452360153198,
      "learning_rate": 3.5448783652286736e-05,
      "loss": 2.7988,
      "step": 449800
    },
    {
      "epoch": 145.83468395461912,
      "grad_norm": 1.1178967952728271,
      "learning_rate": 3.544554005838469e-05,
      "loss": 2.8203,
      "step": 449900
    },
    {
      "epoch": 145.8670988654781,
      "grad_norm": 1.1006251573562622,
      "learning_rate": 3.5442296464482646e-05,
      "loss": 2.8228,
      "step": 450000
    },
    {
      "epoch": 145.8995137763371,
      "grad_norm": 1.0120009183883667,
      "learning_rate": 3.5439052870580605e-05,
      "loss": 2.8104,
      "step": 450100
    },
    {
      "epoch": 145.93192868719612,
      "grad_norm": 1.143804669380188,
      "learning_rate": 3.543584171261758e-05,
      "loss": 2.8102,
      "step": 450200
    },
    {
      "epoch": 145.9643435980551,
      "grad_norm": 1.0529046058654785,
      "learning_rate": 3.5432598118715535e-05,
      "loss": 2.8259,
      "step": 450300
    },
    {
      "epoch": 145.9967585089141,
      "grad_norm": 1.1055461168289185,
      "learning_rate": 3.5429354524813494e-05,
      "loss": 2.8076,
      "step": 450400
    },
    {
      "epoch": 146.0,
      "eval_bleu": 1.0465420320258045,
      "eval_loss": 3.9289615154266357,
      "eval_runtime": 4.0633,
      "eval_samples_per_second": 121.083,
      "eval_steps_per_second": 1.969,
      "step": 450410
    },
    {
      "epoch": 146.0291734197731,
      "grad_norm": 1.208180546760559,
      "learning_rate": 3.542611093091145e-05,
      "loss": 2.8041,
      "step": 450500
    },
    {
      "epoch": 146.06158833063208,
      "grad_norm": 1.0131264925003052,
      "learning_rate": 3.542286733700941e-05,
      "loss": 2.8096,
      "step": 450600
    },
    {
      "epoch": 146.0940032414911,
      "grad_norm": 1.1249324083328247,
      "learning_rate": 3.541962374310736e-05,
      "loss": 2.79,
      "step": 450700
    },
    {
      "epoch": 146.12641815235008,
      "grad_norm": 1.1091184616088867,
      "learning_rate": 3.541641258514434e-05,
      "loss": 2.7944,
      "step": 450800
    },
    {
      "epoch": 146.15883306320907,
      "grad_norm": 1.3827494382858276,
      "learning_rate": 3.541320142718132e-05,
      "loss": 2.8079,
      "step": 450900
    },
    {
      "epoch": 146.19124797406806,
      "grad_norm": 1.137339472770691,
      "learning_rate": 3.540995783327927e-05,
      "loss": 2.8247,
      "step": 451000
    },
    {
      "epoch": 146.22366288492708,
      "grad_norm": 1.2491506338119507,
      "learning_rate": 3.540671423937723e-05,
      "loss": 2.8258,
      "step": 451100
    },
    {
      "epoch": 146.25607779578607,
      "grad_norm": 1.1693058013916016,
      "learning_rate": 3.540347064547519e-05,
      "loss": 2.8037,
      "step": 451200
    },
    {
      "epoch": 146.28849270664506,
      "grad_norm": 1.2930437326431274,
      "learning_rate": 3.540022705157314e-05,
      "loss": 2.8122,
      "step": 451300
    },
    {
      "epoch": 146.32090761750405,
      "grad_norm": 0.9735614657402039,
      "learning_rate": 3.53969834576711e-05,
      "loss": 2.8121,
      "step": 451400
    },
    {
      "epoch": 146.35332252836304,
      "grad_norm": 1.0836213827133179,
      "learning_rate": 3.539373986376906e-05,
      "loss": 2.7943,
      "step": 451500
    },
    {
      "epoch": 146.38573743922205,
      "grad_norm": 1.1373423337936401,
      "learning_rate": 3.539049626986701e-05,
      "loss": 2.7868,
      "step": 451600
    },
    {
      "epoch": 146.41815235008104,
      "grad_norm": 1.1938737630844116,
      "learning_rate": 3.538725267596497e-05,
      "loss": 2.8183,
      "step": 451700
    },
    {
      "epoch": 146.45056726094003,
      "grad_norm": 0.9940919876098633,
      "learning_rate": 3.538400908206293e-05,
      "loss": 2.795,
      "step": 451800
    },
    {
      "epoch": 146.48298217179902,
      "grad_norm": 1.2427256107330322,
      "learning_rate": 3.5380765488160886e-05,
      "loss": 2.8191,
      "step": 451900
    },
    {
      "epoch": 146.515397082658,
      "grad_norm": 1.1934194564819336,
      "learning_rate": 3.5377521894258844e-05,
      "loss": 2.7999,
      "step": 452000
    },
    {
      "epoch": 146.54781199351703,
      "grad_norm": 1.0928741693496704,
      "learning_rate": 3.5374278300356796e-05,
      "loss": 2.8181,
      "step": 452100
    },
    {
      "epoch": 146.58022690437602,
      "grad_norm": 1.2930412292480469,
      "learning_rate": 3.5371034706454755e-05,
      "loss": 2.8102,
      "step": 452200
    },
    {
      "epoch": 146.612641815235,
      "grad_norm": 1.444161057472229,
      "learning_rate": 3.5367791112552714e-05,
      "loss": 2.8369,
      "step": 452300
    },
    {
      "epoch": 146.645056726094,
      "grad_norm": 1.2219003438949585,
      "learning_rate": 3.5364547518650666e-05,
      "loss": 2.8184,
      "step": 452400
    },
    {
      "epoch": 146.677471636953,
      "grad_norm": 1.0074435472488403,
      "learning_rate": 3.5361303924748625e-05,
      "loss": 2.8132,
      "step": 452500
    },
    {
      "epoch": 146.709886547812,
      "grad_norm": 1.134843111038208,
      "learning_rate": 3.535806033084658e-05,
      "loss": 2.7993,
      "step": 452600
    },
    {
      "epoch": 146.742301458671,
      "grad_norm": 1.00823175907135,
      "learning_rate": 3.5354816736944535e-05,
      "loss": 2.8172,
      "step": 452700
    },
    {
      "epoch": 146.77471636952998,
      "grad_norm": 1.251204252243042,
      "learning_rate": 3.5351573143042494e-05,
      "loss": 2.8162,
      "step": 452800
    },
    {
      "epoch": 146.80713128038897,
      "grad_norm": 1.2711561918258667,
      "learning_rate": 3.534832954914045e-05,
      "loss": 2.8072,
      "step": 452900
    },
    {
      "epoch": 146.83954619124796,
      "grad_norm": 1.1387040615081787,
      "learning_rate": 3.5345085955238405e-05,
      "loss": 2.7956,
      "step": 453000
    },
    {
      "epoch": 146.87196110210698,
      "grad_norm": 1.1461981534957886,
      "learning_rate": 3.5341842361336364e-05,
      "loss": 2.8003,
      "step": 453100
    },
    {
      "epoch": 146.90437601296597,
      "grad_norm": 1.1042184829711914,
      "learning_rate": 3.5338598767434316e-05,
      "loss": 2.8022,
      "step": 453200
    },
    {
      "epoch": 146.93679092382496,
      "grad_norm": 1.256004810333252,
      "learning_rate": 3.5335355173532274e-05,
      "loss": 2.8172,
      "step": 453300
    },
    {
      "epoch": 146.96920583468395,
      "grad_norm": 1.2542299032211304,
      "learning_rate": 3.533211157963023e-05,
      "loss": 2.8088,
      "step": 453400
    },
    {
      "epoch": 147.0,
      "eval_bleu": 1.195750157306043,
      "eval_loss": 3.9283668994903564,
      "eval_runtime": 4.7214,
      "eval_samples_per_second": 104.206,
      "eval_steps_per_second": 1.694,
      "step": 453495
    },
    {
      "epoch": 147.00162074554294,
      "grad_norm": 1.0719877481460571,
      "learning_rate": 3.532890042166721e-05,
      "loss": 2.801,
      "step": 453500
    },
    {
      "epoch": 147.03403565640195,
      "grad_norm": 1.079304814338684,
      "learning_rate": 3.532565682776516e-05,
      "loss": 2.8075,
      "step": 453600
    },
    {
      "epoch": 147.06645056726094,
      "grad_norm": 1.039174199104309,
      "learning_rate": 3.532241323386312e-05,
      "loss": 2.8046,
      "step": 453700
    },
    {
      "epoch": 147.09886547811993,
      "grad_norm": 1.2435545921325684,
      "learning_rate": 3.531916963996108e-05,
      "loss": 2.7951,
      "step": 453800
    },
    {
      "epoch": 147.13128038897892,
      "grad_norm": 1.0563771724700928,
      "learning_rate": 3.531592604605903e-05,
      "loss": 2.7927,
      "step": 453900
    },
    {
      "epoch": 147.1636952998379,
      "grad_norm": 1.1057475805282593,
      "learning_rate": 3.531268245215699e-05,
      "loss": 2.7857,
      "step": 454000
    },
    {
      "epoch": 147.19611021069693,
      "grad_norm": 1.3097164630889893,
      "learning_rate": 3.530943885825495e-05,
      "loss": 2.7991,
      "step": 454100
    },
    {
      "epoch": 147.22852512155592,
      "grad_norm": 1.0122333765029907,
      "learning_rate": 3.53061952643529e-05,
      "loss": 2.8159,
      "step": 454200
    },
    {
      "epoch": 147.2609400324149,
      "grad_norm": 1.1606756448745728,
      "learning_rate": 3.530295167045086e-05,
      "loss": 2.7928,
      "step": 454300
    },
    {
      "epoch": 147.2933549432739,
      "grad_norm": 1.0381064414978027,
      "learning_rate": 3.529970807654881e-05,
      "loss": 2.8015,
      "step": 454400
    },
    {
      "epoch": 147.32576985413291,
      "grad_norm": 1.3581244945526123,
      "learning_rate": 3.529646448264677e-05,
      "loss": 2.8194,
      "step": 454500
    },
    {
      "epoch": 147.3581847649919,
      "grad_norm": 1.1266714334487915,
      "learning_rate": 3.529322088874473e-05,
      "loss": 2.8004,
      "step": 454600
    },
    {
      "epoch": 147.3905996758509,
      "grad_norm": 1.194649577140808,
      "learning_rate": 3.528997729484269e-05,
      "loss": 2.8025,
      "step": 454700
    },
    {
      "epoch": 147.42301458670988,
      "grad_norm": 1.0766397714614868,
      "learning_rate": 3.528673370094065e-05,
      "loss": 2.8068,
      "step": 454800
    },
    {
      "epoch": 147.45542949756887,
      "grad_norm": 1.0643107891082764,
      "learning_rate": 3.52834901070386e-05,
      "loss": 2.8064,
      "step": 454900
    },
    {
      "epoch": 147.4878444084279,
      "grad_norm": 1.0610251426696777,
      "learning_rate": 3.528024651313656e-05,
      "loss": 2.8166,
      "step": 455000
    },
    {
      "epoch": 147.52025931928688,
      "grad_norm": 1.3822035789489746,
      "learning_rate": 3.527700291923452e-05,
      "loss": 2.7974,
      "step": 455100
    },
    {
      "epoch": 147.55267423014587,
      "grad_norm": 1.10780930519104,
      "learning_rate": 3.5273759325332476e-05,
      "loss": 2.8216,
      "step": 455200
    },
    {
      "epoch": 147.58508914100486,
      "grad_norm": 1.1108461618423462,
      "learning_rate": 3.527051573143043e-05,
      "loss": 2.7886,
      "step": 455300
    },
    {
      "epoch": 147.61750405186385,
      "grad_norm": 1.2672145366668701,
      "learning_rate": 3.5267272137528386e-05,
      "loss": 2.7939,
      "step": 455400
    },
    {
      "epoch": 147.64991896272286,
      "grad_norm": 1.0346511602401733,
      "learning_rate": 3.526402854362634e-05,
      "loss": 2.8027,
      "step": 455500
    },
    {
      "epoch": 147.68233387358185,
      "grad_norm": 0.9906768202781677,
      "learning_rate": 3.52607849497243e-05,
      "loss": 2.8189,
      "step": 455600
    },
    {
      "epoch": 147.71474878444084,
      "grad_norm": 0.9254412055015564,
      "learning_rate": 3.5257541355822256e-05,
      "loss": 2.8055,
      "step": 455700
    },
    {
      "epoch": 147.74716369529983,
      "grad_norm": 1.19801926612854,
      "learning_rate": 3.525429776192021e-05,
      "loss": 2.8011,
      "step": 455800
    },
    {
      "epoch": 147.77957860615882,
      "grad_norm": 1.0845751762390137,
      "learning_rate": 3.5251054168018166e-05,
      "loss": 2.8108,
      "step": 455900
    },
    {
      "epoch": 147.81199351701784,
      "grad_norm": 1.1476730108261108,
      "learning_rate": 3.5247810574116125e-05,
      "loss": 2.8159,
      "step": 456000
    },
    {
      "epoch": 147.84440842787683,
      "grad_norm": 1.0299447774887085,
      "learning_rate": 3.524456698021408e-05,
      "loss": 2.823,
      "step": 456100
    },
    {
      "epoch": 147.87682333873582,
      "grad_norm": 1.3988703489303589,
      "learning_rate": 3.5241323386312036e-05,
      "loss": 2.8146,
      "step": 456200
    },
    {
      "epoch": 147.9092382495948,
      "grad_norm": 1.171420931816101,
      "learning_rate": 3.523807979240999e-05,
      "loss": 2.8148,
      "step": 456300
    },
    {
      "epoch": 147.9416531604538,
      "grad_norm": 1.0616891384124756,
      "learning_rate": 3.5234836198507947e-05,
      "loss": 2.8136,
      "step": 456400
    },
    {
      "epoch": 147.97406807131281,
      "grad_norm": 1.22385573387146,
      "learning_rate": 3.5231592604605905e-05,
      "loss": 2.8268,
      "step": 456500
    },
    {
      "epoch": 148.0,
      "eval_bleu": 1.1612350947389094,
      "eval_loss": 3.931208848953247,
      "eval_runtime": 4.4536,
      "eval_samples_per_second": 110.471,
      "eval_steps_per_second": 1.796,
      "step": 456580
    },
    {
      "epoch": 148.0064829821718,
      "grad_norm": 1.0978435277938843,
      "learning_rate": 3.522834901070386e-05,
      "loss": 2.796,
      "step": 456600
    },
    {
      "epoch": 148.0388978930308,
      "grad_norm": 1.1001348495483398,
      "learning_rate": 3.5225105416801816e-05,
      "loss": 2.797,
      "step": 456700
    },
    {
      "epoch": 148.07131280388978,
      "grad_norm": 1.1083823442459106,
      "learning_rate": 3.5221861822899775e-05,
      "loss": 2.8115,
      "step": 456800
    },
    {
      "epoch": 148.10372771474877,
      "grad_norm": 1.086926817893982,
      "learning_rate": 3.521861822899773e-05,
      "loss": 2.7962,
      "step": 456900
    },
    {
      "epoch": 148.1361426256078,
      "grad_norm": 1.1650846004486084,
      "learning_rate": 3.5215374635095685e-05,
      "loss": 2.8063,
      "step": 457000
    },
    {
      "epoch": 148.16855753646678,
      "grad_norm": 1.1391435861587524,
      "learning_rate": 3.5212131041193644e-05,
      "loss": 2.8118,
      "step": 457100
    },
    {
      "epoch": 148.20097244732577,
      "grad_norm": 1.1437311172485352,
      "learning_rate": 3.52088874472916e-05,
      "loss": 2.7908,
      "step": 457200
    },
    {
      "epoch": 148.23338735818476,
      "grad_norm": 1.157402515411377,
      "learning_rate": 3.5205643853389555e-05,
      "loss": 2.799,
      "step": 457300
    },
    {
      "epoch": 148.26580226904375,
      "grad_norm": 1.1390783786773682,
      "learning_rate": 3.5202400259487514e-05,
      "loss": 2.8201,
      "step": 457400
    },
    {
      "epoch": 148.29821717990276,
      "grad_norm": 1.100798487663269,
      "learning_rate": 3.519915666558547e-05,
      "loss": 2.7885,
      "step": 457500
    },
    {
      "epoch": 148.33063209076175,
      "grad_norm": 1.1206746101379395,
      "learning_rate": 3.519591307168343e-05,
      "loss": 2.8012,
      "step": 457600
    },
    {
      "epoch": 148.36304700162074,
      "grad_norm": 1.1175931692123413,
      "learning_rate": 3.519266947778138e-05,
      "loss": 2.7965,
      "step": 457700
    },
    {
      "epoch": 148.39546191247973,
      "grad_norm": 1.5803685188293457,
      "learning_rate": 3.518942588387934e-05,
      "loss": 2.8004,
      "step": 457800
    },
    {
      "epoch": 148.42787682333875,
      "grad_norm": 1.1197426319122314,
      "learning_rate": 3.51861822899773e-05,
      "loss": 2.7991,
      "step": 457900
    },
    {
      "epoch": 148.46029173419774,
      "grad_norm": 1.1571002006530762,
      "learning_rate": 3.518293869607525e-05,
      "loss": 2.8018,
      "step": 458000
    },
    {
      "epoch": 148.49270664505673,
      "grad_norm": 1.0094976425170898,
      "learning_rate": 3.517969510217321e-05,
      "loss": 2.8116,
      "step": 458100
    },
    {
      "epoch": 148.52512155591572,
      "grad_norm": 0.9721211194992065,
      "learning_rate": 3.517645150827117e-05,
      "loss": 2.8094,
      "step": 458200
    },
    {
      "epoch": 148.5575364667747,
      "grad_norm": 1.1291671991348267,
      "learning_rate": 3.517320791436912e-05,
      "loss": 2.803,
      "step": 458300
    },
    {
      "epoch": 148.58995137763372,
      "grad_norm": 1.118980884552002,
      "learning_rate": 3.516996432046708e-05,
      "loss": 2.7913,
      "step": 458400
    },
    {
      "epoch": 148.6223662884927,
      "grad_norm": 1.3666975498199463,
      "learning_rate": 3.516672072656503e-05,
      "loss": 2.809,
      "step": 458500
    },
    {
      "epoch": 148.6547811993517,
      "grad_norm": 1.1671085357666016,
      "learning_rate": 3.516347713266299e-05,
      "loss": 2.8052,
      "step": 458600
    },
    {
      "epoch": 148.6871961102107,
      "grad_norm": 1.0033987760543823,
      "learning_rate": 3.516023353876095e-05,
      "loss": 2.7911,
      "step": 458700
    },
    {
      "epoch": 148.71961102106968,
      "grad_norm": 1.0507761240005493,
      "learning_rate": 3.51569899448589e-05,
      "loss": 2.7954,
      "step": 458800
    },
    {
      "epoch": 148.7520259319287,
      "grad_norm": 1.104712963104248,
      "learning_rate": 3.515374635095686e-05,
      "loss": 2.788,
      "step": 458900
    },
    {
      "epoch": 148.7844408427877,
      "grad_norm": 1.236865758895874,
      "learning_rate": 3.515050275705482e-05,
      "loss": 2.8215,
      "step": 459000
    },
    {
      "epoch": 148.81685575364668,
      "grad_norm": 1.1934583187103271,
      "learning_rate": 3.514725916315277e-05,
      "loss": 2.8087,
      "step": 459100
    },
    {
      "epoch": 148.84927066450567,
      "grad_norm": 1.0900019407272339,
      "learning_rate": 3.514401556925073e-05,
      "loss": 2.8013,
      "step": 459200
    },
    {
      "epoch": 148.88168557536466,
      "grad_norm": 1.0274940729141235,
      "learning_rate": 3.514077197534868e-05,
      "loss": 2.8144,
      "step": 459300
    },
    {
      "epoch": 148.91410048622367,
      "grad_norm": 0.9973649382591248,
      "learning_rate": 3.513752838144664e-05,
      "loss": 2.8058,
      "step": 459400
    },
    {
      "epoch": 148.94651539708266,
      "grad_norm": 1.022094964981079,
      "learning_rate": 3.513431722348362e-05,
      "loss": 2.8133,
      "step": 459500
    },
    {
      "epoch": 148.97893030794165,
      "grad_norm": 1.0810844898223877,
      "learning_rate": 3.513107362958158e-05,
      "loss": 2.8163,
      "step": 459600
    },
    {
      "epoch": 149.0,
      "eval_bleu": 1.0144466880652487,
      "eval_loss": 3.9333252906799316,
      "eval_runtime": 4.165,
      "eval_samples_per_second": 118.127,
      "eval_steps_per_second": 1.921,
      "step": 459665
    },
    {
      "epoch": 149.01134521880064,
      "grad_norm": 1.0271753072738647,
      "learning_rate": 3.512783003567953e-05,
      "loss": 2.8187,
      "step": 459700
    },
    {
      "epoch": 149.04376012965963,
      "grad_norm": 1.0730410814285278,
      "learning_rate": 3.512458644177749e-05,
      "loss": 2.8057,
      "step": 459800
    },
    {
      "epoch": 149.07617504051865,
      "grad_norm": 1.1213864088058472,
      "learning_rate": 3.512134284787545e-05,
      "loss": 2.7838,
      "step": 459900
    },
    {
      "epoch": 149.10858995137764,
      "grad_norm": 1.1883158683776855,
      "learning_rate": 3.51180992539734e-05,
      "loss": 2.8082,
      "step": 460000
    },
    {
      "epoch": 149.14100486223663,
      "grad_norm": 1.1575757265090942,
      "learning_rate": 3.511485566007136e-05,
      "loss": 2.8096,
      "step": 460100
    },
    {
      "epoch": 149.17341977309562,
      "grad_norm": 1.2605177164077759,
      "learning_rate": 3.5111612066169317e-05,
      "loss": 2.7809,
      "step": 460200
    },
    {
      "epoch": 149.2058346839546,
      "grad_norm": 1.182368516921997,
      "learning_rate": 3.5108368472267275e-05,
      "loss": 2.7813,
      "step": 460300
    },
    {
      "epoch": 149.23824959481362,
      "grad_norm": 1.0521087646484375,
      "learning_rate": 3.5105124878365234e-05,
      "loss": 2.7905,
      "step": 460400
    },
    {
      "epoch": 149.2706645056726,
      "grad_norm": 1.242576241493225,
      "learning_rate": 3.510188128446319e-05,
      "loss": 2.8262,
      "step": 460500
    },
    {
      "epoch": 149.3030794165316,
      "grad_norm": 1.0751758813858032,
      "learning_rate": 3.5098637690561145e-05,
      "loss": 2.8108,
      "step": 460600
    },
    {
      "epoch": 149.3354943273906,
      "grad_norm": 1.2796803712844849,
      "learning_rate": 3.5095394096659103e-05,
      "loss": 2.7992,
      "step": 460700
    },
    {
      "epoch": 149.36790923824958,
      "grad_norm": 1.101172924041748,
      "learning_rate": 3.5092150502757055e-05,
      "loss": 2.8129,
      "step": 460800
    },
    {
      "epoch": 149.4003241491086,
      "grad_norm": 1.051766037940979,
      "learning_rate": 3.5088906908855014e-05,
      "loss": 2.8081,
      "step": 460900
    },
    {
      "epoch": 149.4327390599676,
      "grad_norm": 1.0375914573669434,
      "learning_rate": 3.508566331495297e-05,
      "loss": 2.7963,
      "step": 461000
    },
    {
      "epoch": 149.46515397082658,
      "grad_norm": 1.0262397527694702,
      "learning_rate": 3.5082419721050925e-05,
      "loss": 2.8061,
      "step": 461100
    },
    {
      "epoch": 149.49756888168557,
      "grad_norm": 1.2323102951049805,
      "learning_rate": 3.5079176127148884e-05,
      "loss": 2.789,
      "step": 461200
    },
    {
      "epoch": 149.52998379254458,
      "grad_norm": 1.3043478727340698,
      "learning_rate": 3.507593253324684e-05,
      "loss": 2.7893,
      "step": 461300
    },
    {
      "epoch": 149.56239870340357,
      "grad_norm": 0.9759541153907776,
      "learning_rate": 3.5072688939344794e-05,
      "loss": 2.8044,
      "step": 461400
    },
    {
      "epoch": 149.59481361426256,
      "grad_norm": 1.2248483896255493,
      "learning_rate": 3.506944534544275e-05,
      "loss": 2.814,
      "step": 461500
    },
    {
      "epoch": 149.62722852512155,
      "grad_norm": 1.2597637176513672,
      "learning_rate": 3.506620175154071e-05,
      "loss": 2.8116,
      "step": 461600
    },
    {
      "epoch": 149.65964343598054,
      "grad_norm": 0.9766963124275208,
      "learning_rate": 3.5062958157638664e-05,
      "loss": 2.7953,
      "step": 461700
    },
    {
      "epoch": 149.69205834683956,
      "grad_norm": 1.140568494796753,
      "learning_rate": 3.505971456373662e-05,
      "loss": 2.8352,
      "step": 461800
    },
    {
      "epoch": 149.72447325769855,
      "grad_norm": 1.0811004638671875,
      "learning_rate": 3.5056470969834575e-05,
      "loss": 2.7999,
      "step": 461900
    },
    {
      "epoch": 149.75688816855754,
      "grad_norm": 1.271752119064331,
      "learning_rate": 3.505322737593253e-05,
      "loss": 2.7963,
      "step": 462000
    },
    {
      "epoch": 149.78930307941653,
      "grad_norm": 1.074920654296875,
      "learning_rate": 3.504998378203049e-05,
      "loss": 2.7942,
      "step": 462100
    },
    {
      "epoch": 149.82171799027552,
      "grad_norm": 1.0621163845062256,
      "learning_rate": 3.5046740188128444e-05,
      "loss": 2.7914,
      "step": 462200
    },
    {
      "epoch": 149.85413290113453,
      "grad_norm": 1.2071071863174438,
      "learning_rate": 3.50434965942264e-05,
      "loss": 2.8117,
      "step": 462300
    },
    {
      "epoch": 149.88654781199352,
      "grad_norm": 1.2467116117477417,
      "learning_rate": 3.504025300032436e-05,
      "loss": 2.8058,
      "step": 462400
    },
    {
      "epoch": 149.9189627228525,
      "grad_norm": 1.2726973295211792,
      "learning_rate": 3.5037009406422313e-05,
      "loss": 2.8253,
      "step": 462500
    },
    {
      "epoch": 149.9513776337115,
      "grad_norm": 1.1955487728118896,
      "learning_rate": 3.503376581252027e-05,
      "loss": 2.7741,
      "step": 462600
    },
    {
      "epoch": 149.9837925445705,
      "grad_norm": 1.1451112031936646,
      "learning_rate": 3.503052221861823e-05,
      "loss": 2.8052,
      "step": 462700
    },
    {
      "epoch": 150.0,
      "eval_bleu": 0.8860613367161085,
      "eval_loss": 3.9293723106384277,
      "eval_runtime": 3.8883,
      "eval_samples_per_second": 126.533,
      "eval_steps_per_second": 2.057,
      "step": 462750
    },
    {
      "epoch": 150.0162074554295,
      "grad_norm": 0.965965211391449,
      "learning_rate": 3.502727862471619e-05,
      "loss": 2.7851,
      "step": 462800
    },
    {
      "epoch": 150.0486223662885,
      "grad_norm": 0.9933935403823853,
      "learning_rate": 3.502403503081415e-05,
      "loss": 2.7853,
      "step": 462900
    },
    {
      "epoch": 150.0810372771475,
      "grad_norm": 1.1553806066513062,
      "learning_rate": 3.50207914369121e-05,
      "loss": 2.7786,
      "step": 463000
    },
    {
      "epoch": 150.11345218800648,
      "grad_norm": 1.1043905019760132,
      "learning_rate": 3.501754784301006e-05,
      "loss": 2.8021,
      "step": 463100
    },
    {
      "epoch": 150.14586709886547,
      "grad_norm": 1.3789422512054443,
      "learning_rate": 3.501430424910802e-05,
      "loss": 2.7874,
      "step": 463200
    },
    {
      "epoch": 150.17828200972448,
      "grad_norm": 1.1314983367919922,
      "learning_rate": 3.501106065520597e-05,
      "loss": 2.7882,
      "step": 463300
    },
    {
      "epoch": 150.21069692058347,
      "grad_norm": 1.0444334745407104,
      "learning_rate": 3.500781706130393e-05,
      "loss": 2.8194,
      "step": 463400
    },
    {
      "epoch": 150.24311183144246,
      "grad_norm": 1.1726125478744507,
      "learning_rate": 3.5004605903340906e-05,
      "loss": 2.804,
      "step": 463500
    },
    {
      "epoch": 150.27552674230145,
      "grad_norm": 1.2400500774383545,
      "learning_rate": 3.5001362309438865e-05,
      "loss": 2.7824,
      "step": 463600
    },
    {
      "epoch": 150.30794165316044,
      "grad_norm": 1.1679624319076538,
      "learning_rate": 3.499811871553682e-05,
      "loss": 2.788,
      "step": 463700
    },
    {
      "epoch": 150.34035656401946,
      "grad_norm": 1.2390245199203491,
      "learning_rate": 3.4994875121634776e-05,
      "loss": 2.7847,
      "step": 463800
    },
    {
      "epoch": 150.37277147487845,
      "grad_norm": 1.3215867280960083,
      "learning_rate": 3.4991631527732735e-05,
      "loss": 2.8048,
      "step": 463900
    },
    {
      "epoch": 150.40518638573744,
      "grad_norm": 1.3648462295532227,
      "learning_rate": 3.4988387933830687e-05,
      "loss": 2.8054,
      "step": 464000
    },
    {
      "epoch": 150.43760129659643,
      "grad_norm": 1.2737853527069092,
      "learning_rate": 3.4985144339928645e-05,
      "loss": 2.8029,
      "step": 464100
    },
    {
      "epoch": 150.47001620745542,
      "grad_norm": 1.0593479871749878,
      "learning_rate": 3.49819007460266e-05,
      "loss": 2.8034,
      "step": 464200
    },
    {
      "epoch": 150.50243111831443,
      "grad_norm": 1.1129528284072876,
      "learning_rate": 3.4978657152124556e-05,
      "loss": 2.795,
      "step": 464300
    },
    {
      "epoch": 150.53484602917342,
      "grad_norm": 1.17212975025177,
      "learning_rate": 3.4975413558222515e-05,
      "loss": 2.7999,
      "step": 464400
    },
    {
      "epoch": 150.5672609400324,
      "grad_norm": 1.1068776845932007,
      "learning_rate": 3.497216996432047e-05,
      "loss": 2.7992,
      "step": 464500
    },
    {
      "epoch": 150.5996758508914,
      "grad_norm": 1.3323454856872559,
      "learning_rate": 3.4968926370418425e-05,
      "loss": 2.8046,
      "step": 464600
    },
    {
      "epoch": 150.63209076175042,
      "grad_norm": 1.1914395093917847,
      "learning_rate": 3.4965682776516384e-05,
      "loss": 2.7986,
      "step": 464700
    },
    {
      "epoch": 150.6645056726094,
      "grad_norm": 1.1612545251846313,
      "learning_rate": 3.4962439182614336e-05,
      "loss": 2.8133,
      "step": 464800
    },
    {
      "epoch": 150.6969205834684,
      "grad_norm": 1.0913190841674805,
      "learning_rate": 3.4959195588712295e-05,
      "loss": 2.8122,
      "step": 464900
    },
    {
      "epoch": 150.7293354943274,
      "grad_norm": 1.1375489234924316,
      "learning_rate": 3.495595199481025e-05,
      "loss": 2.7975,
      "step": 465000
    },
    {
      "epoch": 150.76175040518638,
      "grad_norm": 1.048002004623413,
      "learning_rate": 3.4952708400908206e-05,
      "loss": 2.8191,
      "step": 465100
    },
    {
      "epoch": 150.7941653160454,
      "grad_norm": 1.0866166353225708,
      "learning_rate": 3.4949464807006164e-05,
      "loss": 2.812,
      "step": 465200
    },
    {
      "epoch": 150.82658022690438,
      "grad_norm": 1.1408085823059082,
      "learning_rate": 3.4946221213104116e-05,
      "loss": 2.8207,
      "step": 465300
    },
    {
      "epoch": 150.85899513776337,
      "grad_norm": 1.1435719728469849,
      "learning_rate": 3.4942977619202075e-05,
      "loss": 2.8162,
      "step": 465400
    },
    {
      "epoch": 150.89141004862236,
      "grad_norm": 1.2551027536392212,
      "learning_rate": 3.493976646123905e-05,
      "loss": 2.8098,
      "step": 465500
    },
    {
      "epoch": 150.92382495948135,
      "grad_norm": 1.2699165344238281,
      "learning_rate": 3.493652286733701e-05,
      "loss": 2.7972,
      "step": 465600
    },
    {
      "epoch": 150.95623987034037,
      "grad_norm": 1.089074730873108,
      "learning_rate": 3.4933279273434964e-05,
      "loss": 2.805,
      "step": 465700
    },
    {
      "epoch": 150.98865478119936,
      "grad_norm": 1.0417710542678833,
      "learning_rate": 3.493003567953292e-05,
      "loss": 2.7786,
      "step": 465800
    },
    {
      "epoch": 151.0,
      "eval_bleu": 0.9991596053578887,
      "eval_loss": 3.9336061477661133,
      "eval_runtime": 4.4844,
      "eval_samples_per_second": 109.713,
      "eval_steps_per_second": 1.784,
      "step": 465835
    },
    {
      "epoch": 151.02106969205835,
      "grad_norm": 0.9531643986701965,
      "learning_rate": 3.492679208563088e-05,
      "loss": 2.799,
      "step": 465900
    },
    {
      "epoch": 151.05348460291734,
      "grad_norm": 1.134361743927002,
      "learning_rate": 3.492354849172883e-05,
      "loss": 2.793,
      "step": 466000
    },
    {
      "epoch": 151.08589951377633,
      "grad_norm": 1.2435581684112549,
      "learning_rate": 3.492030489782679e-05,
      "loss": 2.7752,
      "step": 466100
    },
    {
      "epoch": 151.11831442463534,
      "grad_norm": 1.226054310798645,
      "learning_rate": 3.491706130392475e-05,
      "loss": 2.783,
      "step": 466200
    },
    {
      "epoch": 151.15072933549433,
      "grad_norm": 1.0533021688461304,
      "learning_rate": 3.491381771002271e-05,
      "loss": 2.7955,
      "step": 466300
    },
    {
      "epoch": 151.18314424635332,
      "grad_norm": 0.9805105924606323,
      "learning_rate": 3.491057411612066e-05,
      "loss": 2.7975,
      "step": 466400
    },
    {
      "epoch": 151.2155591572123,
      "grad_norm": 1.223760724067688,
      "learning_rate": 3.490733052221862e-05,
      "loss": 2.7853,
      "step": 466500
    },
    {
      "epoch": 151.2479740680713,
      "grad_norm": 1.1547889709472656,
      "learning_rate": 3.490408692831658e-05,
      "loss": 2.8083,
      "step": 466600
    },
    {
      "epoch": 151.28038897893032,
      "grad_norm": 1.1183443069458008,
      "learning_rate": 3.490084333441454e-05,
      "loss": 2.7853,
      "step": 466700
    },
    {
      "epoch": 151.3128038897893,
      "grad_norm": 1.1612414121627808,
      "learning_rate": 3.489759974051249e-05,
      "loss": 2.8146,
      "step": 466800
    },
    {
      "epoch": 151.3452188006483,
      "grad_norm": 1.2599865198135376,
      "learning_rate": 3.489435614661045e-05,
      "loss": 2.7807,
      "step": 466900
    },
    {
      "epoch": 151.3776337115073,
      "grad_norm": 1.1643884181976318,
      "learning_rate": 3.489111255270841e-05,
      "loss": 2.78,
      "step": 467000
    },
    {
      "epoch": 151.41004862236628,
      "grad_norm": 1.1926416158676147,
      "learning_rate": 3.488786895880636e-05,
      "loss": 2.8022,
      "step": 467100
    },
    {
      "epoch": 151.4424635332253,
      "grad_norm": 1.0816303491592407,
      "learning_rate": 3.488462536490432e-05,
      "loss": 2.8054,
      "step": 467200
    },
    {
      "epoch": 151.47487844408428,
      "grad_norm": 1.2098623514175415,
      "learning_rate": 3.488138177100227e-05,
      "loss": 2.7925,
      "step": 467300
    },
    {
      "epoch": 151.50729335494327,
      "grad_norm": 1.2038308382034302,
      "learning_rate": 3.487813817710023e-05,
      "loss": 2.8086,
      "step": 467400
    },
    {
      "epoch": 151.53970826580226,
      "grad_norm": 1.1694083213806152,
      "learning_rate": 3.4874927019137206e-05,
      "loss": 2.8002,
      "step": 467500
    },
    {
      "epoch": 151.57212317666125,
      "grad_norm": 1.3359581232070923,
      "learning_rate": 3.4871683425235165e-05,
      "loss": 2.7922,
      "step": 467600
    },
    {
      "epoch": 151.60453808752027,
      "grad_norm": 1.1427966356277466,
      "learning_rate": 3.486843983133312e-05,
      "loss": 2.799,
      "step": 467700
    },
    {
      "epoch": 151.63695299837926,
      "grad_norm": 1.2216805219650269,
      "learning_rate": 3.4865196237431076e-05,
      "loss": 2.7908,
      "step": 467800
    },
    {
      "epoch": 151.66936790923825,
      "grad_norm": 1.2206083536148071,
      "learning_rate": 3.4861952643529034e-05,
      "loss": 2.7994,
      "step": 467900
    },
    {
      "epoch": 151.70178282009724,
      "grad_norm": 1.1373711824417114,
      "learning_rate": 3.4858709049626986e-05,
      "loss": 2.8018,
      "step": 468000
    },
    {
      "epoch": 151.73419773095625,
      "grad_norm": 1.1118149757385254,
      "learning_rate": 3.4855465455724945e-05,
      "loss": 2.8054,
      "step": 468100
    },
    {
      "epoch": 151.76661264181524,
      "grad_norm": 1.223574161529541,
      "learning_rate": 3.4852221861822904e-05,
      "loss": 2.8131,
      "step": 468200
    },
    {
      "epoch": 151.79902755267423,
      "grad_norm": 0.9910532832145691,
      "learning_rate": 3.4848978267920856e-05,
      "loss": 2.8017,
      "step": 468300
    },
    {
      "epoch": 151.83144246353322,
      "grad_norm": 1.1284358501434326,
      "learning_rate": 3.4845734674018814e-05,
      "loss": 2.8001,
      "step": 468400
    },
    {
      "epoch": 151.8638573743922,
      "grad_norm": 1.1196314096450806,
      "learning_rate": 3.484249108011677e-05,
      "loss": 2.8112,
      "step": 468500
    },
    {
      "epoch": 151.89627228525123,
      "grad_norm": 1.310085654258728,
      "learning_rate": 3.4839247486214725e-05,
      "loss": 2.7916,
      "step": 468600
    },
    {
      "epoch": 151.92868719611022,
      "grad_norm": 0.9827989935874939,
      "learning_rate": 3.4836003892312684e-05,
      "loss": 2.8054,
      "step": 468700
    },
    {
      "epoch": 151.9611021069692,
      "grad_norm": 1.1431143283843994,
      "learning_rate": 3.4832760298410636e-05,
      "loss": 2.8135,
      "step": 468800
    },
    {
      "epoch": 151.9935170178282,
      "grad_norm": 0.9912487864494324,
      "learning_rate": 3.4829516704508595e-05,
      "loss": 2.7949,
      "step": 468900
    },
    {
      "epoch": 152.0,
      "eval_bleu": 1.1389986889818702,
      "eval_loss": 3.9405579566955566,
      "eval_runtime": 4.403,
      "eval_samples_per_second": 111.741,
      "eval_steps_per_second": 1.817,
      "step": 468920
    },
    {
      "epoch": 152.02593192868719,
      "grad_norm": 1.0766377449035645,
      "learning_rate": 3.482627311060655e-05,
      "loss": 2.7906,
      "step": 469000
    },
    {
      "epoch": 152.0583468395462,
      "grad_norm": 1.180317759513855,
      "learning_rate": 3.4823029516704505e-05,
      "loss": 2.7772,
      "step": 469100
    },
    {
      "epoch": 152.0907617504052,
      "grad_norm": 0.9929601550102234,
      "learning_rate": 3.4819785922802464e-05,
      "loss": 2.7974,
      "step": 469200
    },
    {
      "epoch": 152.12317666126418,
      "grad_norm": 1.2945743799209595,
      "learning_rate": 3.481654232890042e-05,
      "loss": 2.7979,
      "step": 469300
    },
    {
      "epoch": 152.15559157212317,
      "grad_norm": 1.274967074394226,
      "learning_rate": 3.481329873499838e-05,
      "loss": 2.7916,
      "step": 469400
    },
    {
      "epoch": 152.18800648298216,
      "grad_norm": 1.2750850915908813,
      "learning_rate": 3.481008757703535e-05,
      "loss": 2.8022,
      "step": 469500
    },
    {
      "epoch": 152.22042139384118,
      "grad_norm": 1.1805589199066162,
      "learning_rate": 3.480684398313331e-05,
      "loss": 2.7865,
      "step": 469600
    },
    {
      "epoch": 152.25283630470017,
      "grad_norm": 1.4898124933242798,
      "learning_rate": 3.480360038923127e-05,
      "loss": 2.7867,
      "step": 469700
    },
    {
      "epoch": 152.28525121555916,
      "grad_norm": 1.022800326347351,
      "learning_rate": 3.480035679532922e-05,
      "loss": 2.8033,
      "step": 469800
    },
    {
      "epoch": 152.31766612641815,
      "grad_norm": 1.2476691007614136,
      "learning_rate": 3.479711320142718e-05,
      "loss": 2.7796,
      "step": 469900
    },
    {
      "epoch": 152.35008103727714,
      "grad_norm": 1.1733512878417969,
      "learning_rate": 3.479386960752514e-05,
      "loss": 2.8134,
      "step": 470000
    },
    {
      "epoch": 152.38249594813615,
      "grad_norm": 1.0772693157196045,
      "learning_rate": 3.47906260136231e-05,
      "loss": 2.7946,
      "step": 470100
    },
    {
      "epoch": 152.41491085899514,
      "grad_norm": 1.1110786199569702,
      "learning_rate": 3.478738241972106e-05,
      "loss": 2.8094,
      "step": 470200
    },
    {
      "epoch": 152.44732576985413,
      "grad_norm": 1.0942165851593018,
      "learning_rate": 3.478417126175803e-05,
      "loss": 2.8254,
      "step": 470300
    },
    {
      "epoch": 152.47974068071312,
      "grad_norm": 1.2575417757034302,
      "learning_rate": 3.478092766785599e-05,
      "loss": 2.7936,
      "step": 470400
    },
    {
      "epoch": 152.5121555915721,
      "grad_norm": 1.3332512378692627,
      "learning_rate": 3.4777684073953946e-05,
      "loss": 2.7787,
      "step": 470500
    },
    {
      "epoch": 152.54457050243113,
      "grad_norm": 1.142091155052185,
      "learning_rate": 3.47744404800519e-05,
      "loss": 2.7792,
      "step": 470600
    },
    {
      "epoch": 152.57698541329012,
      "grad_norm": 1.4124737977981567,
      "learning_rate": 3.4771196886149856e-05,
      "loss": 2.7933,
      "step": 470700
    },
    {
      "epoch": 152.6094003241491,
      "grad_norm": 1.2365288734436035,
      "learning_rate": 3.4767953292247815e-05,
      "loss": 2.7917,
      "step": 470800
    },
    {
      "epoch": 152.6418152350081,
      "grad_norm": 1.1728792190551758,
      "learning_rate": 3.4764709698345774e-05,
      "loss": 2.7773,
      "step": 470900
    },
    {
      "epoch": 152.67423014586709,
      "grad_norm": 1.1664750576019287,
      "learning_rate": 3.4761466104443726e-05,
      "loss": 2.8109,
      "step": 471000
    },
    {
      "epoch": 152.7066450567261,
      "grad_norm": 1.0976814031600952,
      "learning_rate": 3.4758222510541684e-05,
      "loss": 2.7841,
      "step": 471100
    },
    {
      "epoch": 152.7390599675851,
      "grad_norm": 1.2442408800125122,
      "learning_rate": 3.475497891663964e-05,
      "loss": 2.7964,
      "step": 471200
    },
    {
      "epoch": 152.77147487844408,
      "grad_norm": 1.109460473060608,
      "learning_rate": 3.4751735322737595e-05,
      "loss": 2.797,
      "step": 471300
    },
    {
      "epoch": 152.80388978930307,
      "grad_norm": 0.9809220433235168,
      "learning_rate": 3.4748491728835554e-05,
      "loss": 2.8143,
      "step": 471400
    },
    {
      "epoch": 152.8363047001621,
      "grad_norm": 1.053362488746643,
      "learning_rate": 3.4745248134933506e-05,
      "loss": 2.7794,
      "step": 471500
    },
    {
      "epoch": 152.86871961102108,
      "grad_norm": 1.2009519338607788,
      "learning_rate": 3.4742004541031465e-05,
      "loss": 2.791,
      "step": 471600
    },
    {
      "epoch": 152.90113452188007,
      "grad_norm": 1.1670050621032715,
      "learning_rate": 3.473876094712942e-05,
      "loss": 2.7905,
      "step": 471700
    },
    {
      "epoch": 152.93354943273906,
      "grad_norm": 1.030860185623169,
      "learning_rate": 3.4735517353227375e-05,
      "loss": 2.8205,
      "step": 471800
    },
    {
      "epoch": 152.96596434359805,
      "grad_norm": 1.10513174533844,
      "learning_rate": 3.4732273759325334e-05,
      "loss": 2.788,
      "step": 471900
    },
    {
      "epoch": 152.99837925445706,
      "grad_norm": 1.320930004119873,
      "learning_rate": 3.472903016542329e-05,
      "loss": 2.81,
      "step": 472000
    },
    {
      "epoch": 153.0,
      "eval_bleu": 1.2208149176600636,
      "eval_loss": 3.93918776512146,
      "eval_runtime": 4.0852,
      "eval_samples_per_second": 120.434,
      "eval_steps_per_second": 1.958,
      "step": 472005
    },
    {
      "epoch": 153.03079416531605,
      "grad_norm": 1.2383098602294922,
      "learning_rate": 3.4725786571521245e-05,
      "loss": 2.7963,
      "step": 472100
    },
    {
      "epoch": 153.06320907617504,
      "grad_norm": 1.0808409452438354,
      "learning_rate": 3.4722542977619203e-05,
      "loss": 2.7723,
      "step": 472200
    },
    {
      "epoch": 153.09562398703403,
      "grad_norm": 1.1170812845230103,
      "learning_rate": 3.4719299383717155e-05,
      "loss": 2.8096,
      "step": 472300
    },
    {
      "epoch": 153.12803889789302,
      "grad_norm": 1.0983469486236572,
      "learning_rate": 3.4716055789815114e-05,
      "loss": 2.7938,
      "step": 472400
    },
    {
      "epoch": 153.16045380875204,
      "grad_norm": 1.2432420253753662,
      "learning_rate": 3.471281219591307e-05,
      "loss": 2.7964,
      "step": 472500
    },
    {
      "epoch": 153.19286871961103,
      "grad_norm": 1.1158829927444458,
      "learning_rate": 3.470960103795005e-05,
      "loss": 2.7901,
      "step": 472600
    },
    {
      "epoch": 153.22528363047002,
      "grad_norm": 1.3178266286849976,
      "learning_rate": 3.4706357444048e-05,
      "loss": 2.8007,
      "step": 472700
    },
    {
      "epoch": 153.257698541329,
      "grad_norm": 1.22951078414917,
      "learning_rate": 3.470311385014596e-05,
      "loss": 2.7954,
      "step": 472800
    },
    {
      "epoch": 153.290113452188,
      "grad_norm": 1.178513526916504,
      "learning_rate": 3.469987025624392e-05,
      "loss": 2.7782,
      "step": 472900
    },
    {
      "epoch": 153.322528363047,
      "grad_norm": 1.2951633930206299,
      "learning_rate": 3.469662666234187e-05,
      "loss": 2.7931,
      "step": 473000
    },
    {
      "epoch": 153.354943273906,
      "grad_norm": 1.1315934658050537,
      "learning_rate": 3.469338306843983e-05,
      "loss": 2.7846,
      "step": 473100
    },
    {
      "epoch": 153.387358184765,
      "grad_norm": 1.3100031614303589,
      "learning_rate": 3.469013947453779e-05,
      "loss": 2.7762,
      "step": 473200
    },
    {
      "epoch": 153.41977309562398,
      "grad_norm": 1.2355207204818726,
      "learning_rate": 3.468689588063574e-05,
      "loss": 2.7907,
      "step": 473300
    },
    {
      "epoch": 153.45218800648297,
      "grad_norm": 1.1203755140304565,
      "learning_rate": 3.46836522867337e-05,
      "loss": 2.786,
      "step": 473400
    },
    {
      "epoch": 153.484602917342,
      "grad_norm": 1.0407862663269043,
      "learning_rate": 3.468040869283166e-05,
      "loss": 2.8058,
      "step": 473500
    },
    {
      "epoch": 153.51701782820098,
      "grad_norm": 1.0912712812423706,
      "learning_rate": 3.467716509892962e-05,
      "loss": 2.7961,
      "step": 473600
    },
    {
      "epoch": 153.54943273905997,
      "grad_norm": 1.1451976299285889,
      "learning_rate": 3.4673921505027577e-05,
      "loss": 2.801,
      "step": 473700
    },
    {
      "epoch": 153.58184764991896,
      "grad_norm": 1.1718254089355469,
      "learning_rate": 3.467067791112553e-05,
      "loss": 2.7949,
      "step": 473800
    },
    {
      "epoch": 153.61426256077795,
      "grad_norm": 1.1222854852676392,
      "learning_rate": 3.466743431722349e-05,
      "loss": 2.7839,
      "step": 473900
    },
    {
      "epoch": 153.64667747163696,
      "grad_norm": 1.1470448970794678,
      "learning_rate": 3.4664190723321446e-05,
      "loss": 2.8227,
      "step": 474000
    },
    {
      "epoch": 153.67909238249595,
      "grad_norm": 1.062511682510376,
      "learning_rate": 3.46609471294194e-05,
      "loss": 2.7844,
      "step": 474100
    },
    {
      "epoch": 153.71150729335494,
      "grad_norm": 1.2120939493179321,
      "learning_rate": 3.465770353551736e-05,
      "loss": 2.7948,
      "step": 474200
    },
    {
      "epoch": 153.74392220421393,
      "grad_norm": 1.1358236074447632,
      "learning_rate": 3.4654459941615315e-05,
      "loss": 2.7908,
      "step": 474300
    },
    {
      "epoch": 153.77633711507292,
      "grad_norm": 1.107682704925537,
      "learning_rate": 3.465121634771327e-05,
      "loss": 2.7874,
      "step": 474400
    },
    {
      "epoch": 153.80875202593194,
      "grad_norm": 1.1179710626602173,
      "learning_rate": 3.4647972753811226e-05,
      "loss": 2.7904,
      "step": 474500
    },
    {
      "epoch": 153.84116693679093,
      "grad_norm": 1.1363929510116577,
      "learning_rate": 3.464472915990918e-05,
      "loss": 2.8028,
      "step": 474600
    },
    {
      "epoch": 153.87358184764992,
      "grad_norm": 1.034023642539978,
      "learning_rate": 3.464148556600714e-05,
      "loss": 2.7795,
      "step": 474700
    },
    {
      "epoch": 153.9059967585089,
      "grad_norm": 1.2379539012908936,
      "learning_rate": 3.4638241972105096e-05,
      "loss": 2.7976,
      "step": 474800
    },
    {
      "epoch": 153.93841166936792,
      "grad_norm": 1.1160101890563965,
      "learning_rate": 3.463499837820305e-05,
      "loss": 2.7955,
      "step": 474900
    },
    {
      "epoch": 153.9708265802269,
      "grad_norm": 1.50838041305542,
      "learning_rate": 3.4631754784301006e-05,
      "loss": 2.8027,
      "step": 475000
    },
    {
      "epoch": 154.0,
      "eval_bleu": 0.9441030365694053,
      "eval_loss": 3.945159673690796,
      "eval_runtime": 3.9921,
      "eval_samples_per_second": 123.242,
      "eval_steps_per_second": 2.004,
      "step": 475090
    },
    {
      "epoch": 154.0032414910859,
      "grad_norm": 1.0805622339248657,
      "learning_rate": 3.4628511190398965e-05,
      "loss": 2.7943,
      "step": 475100
    },
    {
      "epoch": 154.0356564019449,
      "grad_norm": 1.082582950592041,
      "learning_rate": 3.462526759649692e-05,
      "loss": 2.7707,
      "step": 475200
    },
    {
      "epoch": 154.06807131280388,
      "grad_norm": 1.329748272895813,
      "learning_rate": 3.4622024002594876e-05,
      "loss": 2.7909,
      "step": 475300
    },
    {
      "epoch": 154.1004862236629,
      "grad_norm": 1.0143203735351562,
      "learning_rate": 3.4618780408692835e-05,
      "loss": 2.7728,
      "step": 475400
    },
    {
      "epoch": 154.1329011345219,
      "grad_norm": 1.157785177230835,
      "learning_rate": 3.4615536814790787e-05,
      "loss": 2.7945,
      "step": 475500
    },
    {
      "epoch": 154.16531604538088,
      "grad_norm": 1.024765133857727,
      "learning_rate": 3.4612293220888745e-05,
      "loss": 2.8139,
      "step": 475600
    },
    {
      "epoch": 154.19773095623987,
      "grad_norm": 1.2050590515136719,
      "learning_rate": 3.46090496269867e-05,
      "loss": 2.7819,
      "step": 475700
    },
    {
      "epoch": 154.23014586709886,
      "grad_norm": 1.046534538269043,
      "learning_rate": 3.4605806033084656e-05,
      "loss": 2.7913,
      "step": 475800
    },
    {
      "epoch": 154.26256077795787,
      "grad_norm": 1.1006172895431519,
      "learning_rate": 3.4602562439182615e-05,
      "loss": 2.7836,
      "step": 475900
    },
    {
      "epoch": 154.29497568881686,
      "grad_norm": 0.9870970845222473,
      "learning_rate": 3.4599318845280573e-05,
      "loss": 2.779,
      "step": 476000
    },
    {
      "epoch": 154.32739059967585,
      "grad_norm": 1.1861149072647095,
      "learning_rate": 3.459607525137853e-05,
      "loss": 2.7967,
      "step": 476100
    },
    {
      "epoch": 154.35980551053484,
      "grad_norm": 1.0458799600601196,
      "learning_rate": 3.459283165747649e-05,
      "loss": 2.7923,
      "step": 476200
    },
    {
      "epoch": 154.39222042139383,
      "grad_norm": 1.1890919208526611,
      "learning_rate": 3.458958806357444e-05,
      "loss": 2.8021,
      "step": 476300
    },
    {
      "epoch": 154.42463533225285,
      "grad_norm": 1.147521734237671,
      "learning_rate": 3.45863444696724e-05,
      "loss": 2.7996,
      "step": 476400
    },
    {
      "epoch": 154.45705024311184,
      "grad_norm": 1.0695419311523438,
      "learning_rate": 3.458310087577036e-05,
      "loss": 2.7889,
      "step": 476500
    },
    {
      "epoch": 154.48946515397083,
      "grad_norm": 1.0430537462234497,
      "learning_rate": 3.457988971780733e-05,
      "loss": 2.7798,
      "step": 476600
    },
    {
      "epoch": 154.52188006482982,
      "grad_norm": 1.4497212171554565,
      "learning_rate": 3.457664612390529e-05,
      "loss": 2.8097,
      "step": 476700
    },
    {
      "epoch": 154.5542949756888,
      "grad_norm": 1.1888575553894043,
      "learning_rate": 3.457340253000325e-05,
      "loss": 2.7942,
      "step": 476800
    },
    {
      "epoch": 154.58670988654782,
      "grad_norm": 1.4051567316055298,
      "learning_rate": 3.45701589361012e-05,
      "loss": 2.7717,
      "step": 476900
    },
    {
      "epoch": 154.6191247974068,
      "grad_norm": 1.0902680158615112,
      "learning_rate": 3.456691534219916e-05,
      "loss": 2.7889,
      "step": 477000
    },
    {
      "epoch": 154.6515397082658,
      "grad_norm": 1.1639355421066284,
      "learning_rate": 3.456367174829712e-05,
      "loss": 2.7905,
      "step": 477100
    },
    {
      "epoch": 154.6839546191248,
      "grad_norm": 1.2352930307388306,
      "learning_rate": 3.456042815439507e-05,
      "loss": 2.7789,
      "step": 477200
    },
    {
      "epoch": 154.71636952998378,
      "grad_norm": 1.0801646709442139,
      "learning_rate": 3.455718456049303e-05,
      "loss": 2.8025,
      "step": 477300
    },
    {
      "epoch": 154.7487844408428,
      "grad_norm": 1.1079638004302979,
      "learning_rate": 3.455394096659099e-05,
      "loss": 2.7829,
      "step": 477400
    },
    {
      "epoch": 154.7811993517018,
      "grad_norm": 1.0466183423995972,
      "learning_rate": 3.455069737268894e-05,
      "loss": 2.7865,
      "step": 477500
    },
    {
      "epoch": 154.81361426256078,
      "grad_norm": 1.1033281087875366,
      "learning_rate": 3.45474537787869e-05,
      "loss": 2.7712,
      "step": 477600
    },
    {
      "epoch": 154.84602917341977,
      "grad_norm": 1.1620222330093384,
      "learning_rate": 3.454421018488486e-05,
      "loss": 2.7915,
      "step": 477700
    },
    {
      "epoch": 154.87844408427875,
      "grad_norm": 1.0983091592788696,
      "learning_rate": 3.454096659098281e-05,
      "loss": 2.8046,
      "step": 477800
    },
    {
      "epoch": 154.91085899513777,
      "grad_norm": 1.0434601306915283,
      "learning_rate": 3.453772299708077e-05,
      "loss": 2.7983,
      "step": 477900
    },
    {
      "epoch": 154.94327390599676,
      "grad_norm": 1.3456971645355225,
      "learning_rate": 3.453447940317872e-05,
      "loss": 2.815,
      "step": 478000
    },
    {
      "epoch": 154.97568881685575,
      "grad_norm": 1.2940821647644043,
      "learning_rate": 3.453123580927668e-05,
      "loss": 2.7955,
      "step": 478100
    },
    {
      "epoch": 155.0,
      "eval_bleu": 0.91807106391746,
      "eval_loss": 3.932871103286743,
      "eval_runtime": 4.0366,
      "eval_samples_per_second": 121.884,
      "eval_steps_per_second": 1.982,
      "step": 478175
    },
    {
      "epoch": 155.00810372771474,
      "grad_norm": 1.2787169218063354,
      "learning_rate": 3.452799221537464e-05,
      "loss": 2.7951,
      "step": 478200
    },
    {
      "epoch": 155.04051863857376,
      "grad_norm": 1.0574367046356201,
      "learning_rate": 3.452474862147259e-05,
      "loss": 2.7912,
      "step": 478300
    },
    {
      "epoch": 155.07293354943275,
      "grad_norm": 1.2896808385849,
      "learning_rate": 3.452150502757055e-05,
      "loss": 2.7856,
      "step": 478400
    },
    {
      "epoch": 155.10534846029174,
      "grad_norm": 1.2487348318099976,
      "learning_rate": 3.451826143366851e-05,
      "loss": 2.7825,
      "step": 478500
    },
    {
      "epoch": 155.13776337115073,
      "grad_norm": 1.1902000904083252,
      "learning_rate": 3.4515050275705485e-05,
      "loss": 2.7902,
      "step": 478600
    },
    {
      "epoch": 155.17017828200972,
      "grad_norm": 1.1974610090255737,
      "learning_rate": 3.451180668180344e-05,
      "loss": 2.7767,
      "step": 478700
    },
    {
      "epoch": 155.20259319286873,
      "grad_norm": 0.9954658150672913,
      "learning_rate": 3.4508563087901395e-05,
      "loss": 2.7861,
      "step": 478800
    },
    {
      "epoch": 155.23500810372772,
      "grad_norm": 1.5322774648666382,
      "learning_rate": 3.4505319493999354e-05,
      "loss": 2.7883,
      "step": 478900
    },
    {
      "epoch": 155.2674230145867,
      "grad_norm": 1.0983935594558716,
      "learning_rate": 3.4502075900097306e-05,
      "loss": 2.8233,
      "step": 479000
    },
    {
      "epoch": 155.2998379254457,
      "grad_norm": 0.9637154936790466,
      "learning_rate": 3.4498832306195265e-05,
      "loss": 2.7898,
      "step": 479100
    },
    {
      "epoch": 155.3322528363047,
      "grad_norm": 1.1142196655273438,
      "learning_rate": 3.4495588712293224e-05,
      "loss": 2.7796,
      "step": 479200
    },
    {
      "epoch": 155.3646677471637,
      "grad_norm": 1.1667252779006958,
      "learning_rate": 3.4492345118391176e-05,
      "loss": 2.7833,
      "step": 479300
    },
    {
      "epoch": 155.3970826580227,
      "grad_norm": 0.985945463180542,
      "learning_rate": 3.4489101524489134e-05,
      "loss": 2.7848,
      "step": 479400
    },
    {
      "epoch": 155.4294975688817,
      "grad_norm": 1.1355501413345337,
      "learning_rate": 3.448585793058709e-05,
      "loss": 2.8031,
      "step": 479500
    },
    {
      "epoch": 155.46191247974068,
      "grad_norm": 1.1162769794464111,
      "learning_rate": 3.448261433668505e-05,
      "loss": 2.7599,
      "step": 479600
    },
    {
      "epoch": 155.49432739059966,
      "grad_norm": 1.1059623956680298,
      "learning_rate": 3.4479370742783004e-05,
      "loss": 2.7881,
      "step": 479700
    },
    {
      "epoch": 155.52674230145868,
      "grad_norm": 1.3426183462142944,
      "learning_rate": 3.447612714888096e-05,
      "loss": 2.7767,
      "step": 479800
    },
    {
      "epoch": 155.55915721231767,
      "grad_norm": 1.3029414415359497,
      "learning_rate": 3.447288355497892e-05,
      "loss": 2.7963,
      "step": 479900
    },
    {
      "epoch": 155.59157212317666,
      "grad_norm": 1.027564525604248,
      "learning_rate": 3.446963996107688e-05,
      "loss": 2.7759,
      "step": 480000
    },
    {
      "epoch": 155.62398703403565,
      "grad_norm": 1.318723201751709,
      "learning_rate": 3.446639636717483e-05,
      "loss": 2.7943,
      "step": 480100
    },
    {
      "epoch": 155.65640194489464,
      "grad_norm": 0.9802390336990356,
      "learning_rate": 3.446315277327279e-05,
      "loss": 2.7875,
      "step": 480200
    },
    {
      "epoch": 155.68881685575366,
      "grad_norm": 1.4384217262268066,
      "learning_rate": 3.445990917937074e-05,
      "loss": 2.7854,
      "step": 480300
    },
    {
      "epoch": 155.72123176661265,
      "grad_norm": 1.0909448862075806,
      "learning_rate": 3.44566655854687e-05,
      "loss": 2.7957,
      "step": 480400
    },
    {
      "epoch": 155.75364667747164,
      "grad_norm": 1.0057674646377563,
      "learning_rate": 3.445342199156666e-05,
      "loss": 2.7864,
      "step": 480500
    },
    {
      "epoch": 155.78606158833063,
      "grad_norm": 1.1828539371490479,
      "learning_rate": 3.445021083360364e-05,
      "loss": 2.8058,
      "step": 480600
    },
    {
      "epoch": 155.81847649918961,
      "grad_norm": 1.1925095319747925,
      "learning_rate": 3.444696723970159e-05,
      "loss": 2.7969,
      "step": 480700
    },
    {
      "epoch": 155.85089141004863,
      "grad_norm": 1.0890097618103027,
      "learning_rate": 3.444372364579955e-05,
      "loss": 2.8103,
      "step": 480800
    },
    {
      "epoch": 155.88330632090762,
      "grad_norm": 1.1207382678985596,
      "learning_rate": 3.444048005189751e-05,
      "loss": 2.7848,
      "step": 480900
    },
    {
      "epoch": 155.9157212317666,
      "grad_norm": 1.2676024436950684,
      "learning_rate": 3.443723645799546e-05,
      "loss": 2.7974,
      "step": 481000
    },
    {
      "epoch": 155.9481361426256,
      "grad_norm": 1.159745216369629,
      "learning_rate": 3.443399286409342e-05,
      "loss": 2.7822,
      "step": 481100
    },
    {
      "epoch": 155.9805510534846,
      "grad_norm": 1.1303809881210327,
      "learning_rate": 3.443074927019138e-05,
      "loss": 2.8122,
      "step": 481200
    },
    {
      "epoch": 156.0,
      "eval_bleu": 1.1216109984586824,
      "eval_loss": 3.9481053352355957,
      "eval_runtime": 4.327,
      "eval_samples_per_second": 113.706,
      "eval_steps_per_second": 1.849,
      "step": 481260
    },
    {
      "epoch": 156.0129659643436,
      "grad_norm": 1.1831461191177368,
      "learning_rate": 3.442750567628933e-05,
      "loss": 2.7993,
      "step": 481300
    },
    {
      "epoch": 156.0453808752026,
      "grad_norm": 1.0231366157531738,
      "learning_rate": 3.442426208238729e-05,
      "loss": 2.7588,
      "step": 481400
    },
    {
      "epoch": 156.07779578606159,
      "grad_norm": 1.1576491594314575,
      "learning_rate": 3.4421018488485246e-05,
      "loss": 2.7844,
      "step": 481500
    },
    {
      "epoch": 156.11021069692057,
      "grad_norm": 1.1215767860412598,
      "learning_rate": 3.44177748945832e-05,
      "loss": 2.8076,
      "step": 481600
    },
    {
      "epoch": 156.1426256077796,
      "grad_norm": 1.1501384973526,
      "learning_rate": 3.441453130068116e-05,
      "loss": 2.7712,
      "step": 481700
    },
    {
      "epoch": 156.17504051863858,
      "grad_norm": 1.012840986251831,
      "learning_rate": 3.441128770677911e-05,
      "loss": 2.7795,
      "step": 481800
    },
    {
      "epoch": 156.20745542949757,
      "grad_norm": 1.0665580034255981,
      "learning_rate": 3.440804411287707e-05,
      "loss": 2.7788,
      "step": 481900
    },
    {
      "epoch": 156.23987034035656,
      "grad_norm": 1.2039884328842163,
      "learning_rate": 3.4404800518975026e-05,
      "loss": 2.762,
      "step": 482000
    },
    {
      "epoch": 156.27228525121555,
      "grad_norm": 1.3580023050308228,
      "learning_rate": 3.4401589361012004e-05,
      "loss": 2.7907,
      "step": 482100
    },
    {
      "epoch": 156.30470016207457,
      "grad_norm": 1.2807430028915405,
      "learning_rate": 3.4398345767109956e-05,
      "loss": 2.7915,
      "step": 482200
    },
    {
      "epoch": 156.33711507293356,
      "grad_norm": 1.198038935661316,
      "learning_rate": 3.4395102173207915e-05,
      "loss": 2.7858,
      "step": 482300
    },
    {
      "epoch": 156.36952998379255,
      "grad_norm": 1.4747157096862793,
      "learning_rate": 3.4391858579305874e-05,
      "loss": 2.7694,
      "step": 482400
    },
    {
      "epoch": 156.40194489465154,
      "grad_norm": 1.2129465341567993,
      "learning_rate": 3.4388614985403826e-05,
      "loss": 2.7883,
      "step": 482500
    },
    {
      "epoch": 156.43435980551052,
      "grad_norm": 0.9860017895698547,
      "learning_rate": 3.4385371391501784e-05,
      "loss": 2.8055,
      "step": 482600
    },
    {
      "epoch": 156.46677471636954,
      "grad_norm": 1.228938341140747,
      "learning_rate": 3.438212779759974e-05,
      "loss": 2.7827,
      "step": 482700
    },
    {
      "epoch": 156.49918962722853,
      "grad_norm": 1.265073299407959,
      "learning_rate": 3.4378884203697695e-05,
      "loss": 2.7843,
      "step": 482800
    },
    {
      "epoch": 156.53160453808752,
      "grad_norm": 1.1987906694412231,
      "learning_rate": 3.4375640609795654e-05,
      "loss": 2.7856,
      "step": 482900
    },
    {
      "epoch": 156.5640194489465,
      "grad_norm": 1.0489475727081299,
      "learning_rate": 3.437239701589361e-05,
      "loss": 2.795,
      "step": 483000
    },
    {
      "epoch": 156.5964343598055,
      "grad_norm": 1.1791287660598755,
      "learning_rate": 3.4369153421991565e-05,
      "loss": 2.7831,
      "step": 483100
    },
    {
      "epoch": 156.62884927066452,
      "grad_norm": 1.1805485486984253,
      "learning_rate": 3.436590982808952e-05,
      "loss": 2.7988,
      "step": 483200
    },
    {
      "epoch": 156.6612641815235,
      "grad_norm": 1.1884877681732178,
      "learning_rate": 3.436266623418748e-05,
      "loss": 2.798,
      "step": 483300
    },
    {
      "epoch": 156.6936790923825,
      "grad_norm": 1.1723664999008179,
      "learning_rate": 3.435942264028544e-05,
      "loss": 2.7849,
      "step": 483400
    },
    {
      "epoch": 156.72609400324149,
      "grad_norm": 1.1476447582244873,
      "learning_rate": 3.43561790463834e-05,
      "loss": 2.7743,
      "step": 483500
    },
    {
      "epoch": 156.75850891410047,
      "grad_norm": 1.2562284469604492,
      "learning_rate": 3.435293545248135e-05,
      "loss": 2.7719,
      "step": 483600
    },
    {
      "epoch": 156.7909238249595,
      "grad_norm": 1.3897860050201416,
      "learning_rate": 3.434969185857931e-05,
      "loss": 2.8017,
      "step": 483700
    },
    {
      "epoch": 156.82333873581848,
      "grad_norm": 1.1138558387756348,
      "learning_rate": 3.434644826467727e-05,
      "loss": 2.7937,
      "step": 483800
    },
    {
      "epoch": 156.85575364667747,
      "grad_norm": 1.1108239889144897,
      "learning_rate": 3.434320467077522e-05,
      "loss": 2.7888,
      "step": 483900
    },
    {
      "epoch": 156.88816855753646,
      "grad_norm": 1.2938008308410645,
      "learning_rate": 3.433996107687318e-05,
      "loss": 2.7892,
      "step": 484000
    },
    {
      "epoch": 156.92058346839545,
      "grad_norm": 1.0591418743133545,
      "learning_rate": 3.433671748297113e-05,
      "loss": 2.8077,
      "step": 484100
    },
    {
      "epoch": 156.95299837925447,
      "grad_norm": 1.1932674646377563,
      "learning_rate": 3.433347388906909e-05,
      "loss": 2.7876,
      "step": 484200
    },
    {
      "epoch": 156.98541329011346,
      "grad_norm": 1.079364538192749,
      "learning_rate": 3.433023029516705e-05,
      "loss": 2.7992,
      "step": 484300
    },
    {
      "epoch": 157.0,
      "eval_bleu": 1.0733160290592572,
      "eval_loss": 3.94075870513916,
      "eval_runtime": 4.2423,
      "eval_samples_per_second": 115.974,
      "eval_steps_per_second": 1.886,
      "step": 484345
    },
    {
      "epoch": 157.01782820097245,
      "grad_norm": 1.1526477336883545,
      "learning_rate": 3.4326986701265e-05,
      "loss": 2.7671,
      "step": 484400
    },
    {
      "epoch": 157.05024311183143,
      "grad_norm": 1.1840903759002686,
      "learning_rate": 3.432374310736296e-05,
      "loss": 2.7777,
      "step": 484500
    },
    {
      "epoch": 157.08265802269042,
      "grad_norm": 1.1160959005355835,
      "learning_rate": 3.432049951346092e-05,
      "loss": 2.7753,
      "step": 484600
    },
    {
      "epoch": 157.11507293354944,
      "grad_norm": 1.104995846748352,
      "learning_rate": 3.431725591955887e-05,
      "loss": 2.7754,
      "step": 484700
    },
    {
      "epoch": 157.14748784440843,
      "grad_norm": 1.13181471824646,
      "learning_rate": 3.431401232565683e-05,
      "loss": 2.7727,
      "step": 484800
    },
    {
      "epoch": 157.17990275526742,
      "grad_norm": 1.2598423957824707,
      "learning_rate": 3.431076873175478e-05,
      "loss": 2.789,
      "step": 484900
    },
    {
      "epoch": 157.2123176661264,
      "grad_norm": 1.1969338655471802,
      "learning_rate": 3.430752513785274e-05,
      "loss": 2.7637,
      "step": 485000
    },
    {
      "epoch": 157.24473257698543,
      "grad_norm": 1.2554973363876343,
      "learning_rate": 3.43042815439507e-05,
      "loss": 2.7821,
      "step": 485100
    },
    {
      "epoch": 157.27714748784442,
      "grad_norm": 1.0923454761505127,
      "learning_rate": 3.430103795004865e-05,
      "loss": 2.7793,
      "step": 485200
    },
    {
      "epoch": 157.3095623987034,
      "grad_norm": 1.2747875452041626,
      "learning_rate": 3.429779435614661e-05,
      "loss": 2.7907,
      "step": 485300
    },
    {
      "epoch": 157.3419773095624,
      "grad_norm": 1.1407599449157715,
      "learning_rate": 3.429458319818359e-05,
      "loss": 2.7879,
      "step": 485400
    },
    {
      "epoch": 157.37439222042138,
      "grad_norm": 1.0154646635055542,
      "learning_rate": 3.4291339604281546e-05,
      "loss": 2.792,
      "step": 485500
    },
    {
      "epoch": 157.4068071312804,
      "grad_norm": 1.101078748703003,
      "learning_rate": 3.42880960103795e-05,
      "loss": 2.7928,
      "step": 485600
    },
    {
      "epoch": 157.4392220421394,
      "grad_norm": 1.1534371376037598,
      "learning_rate": 3.428485241647746e-05,
      "loss": 2.7774,
      "step": 485700
    },
    {
      "epoch": 157.47163695299838,
      "grad_norm": 1.1990305185317993,
      "learning_rate": 3.4281608822575415e-05,
      "loss": 2.7681,
      "step": 485800
    },
    {
      "epoch": 157.50405186385737,
      "grad_norm": 1.0846481323242188,
      "learning_rate": 3.427836522867337e-05,
      "loss": 2.7961,
      "step": 485900
    },
    {
      "epoch": 157.53646677471636,
      "grad_norm": 1.0269591808319092,
      "learning_rate": 3.4275121634771326e-05,
      "loss": 2.7829,
      "step": 486000
    },
    {
      "epoch": 157.56888168557538,
      "grad_norm": 1.1522716283798218,
      "learning_rate": 3.4271878040869285e-05,
      "loss": 2.786,
      "step": 486100
    },
    {
      "epoch": 157.60129659643437,
      "grad_norm": 1.088824987411499,
      "learning_rate": 3.4268634446967244e-05,
      "loss": 2.8019,
      "step": 486200
    },
    {
      "epoch": 157.63371150729336,
      "grad_norm": 1.2392351627349854,
      "learning_rate": 3.4265390853065196e-05,
      "loss": 2.7786,
      "step": 486300
    },
    {
      "epoch": 157.66612641815234,
      "grad_norm": 1.4105300903320312,
      "learning_rate": 3.4262147259163154e-05,
      "loss": 2.7968,
      "step": 486400
    },
    {
      "epoch": 157.69854132901133,
      "grad_norm": 1.2620021104812622,
      "learning_rate": 3.425890366526111e-05,
      "loss": 2.7907,
      "step": 486500
    },
    {
      "epoch": 157.73095623987035,
      "grad_norm": 1.13748037815094,
      "learning_rate": 3.425566007135907e-05,
      "loss": 2.7717,
      "step": 486600
    },
    {
      "epoch": 157.76337115072934,
      "grad_norm": 1.2910271883010864,
      "learning_rate": 3.4252416477457024e-05,
      "loss": 2.7803,
      "step": 486700
    },
    {
      "epoch": 157.79578606158833,
      "grad_norm": 1.160672903060913,
      "learning_rate": 3.424917288355498e-05,
      "loss": 2.7839,
      "step": 486800
    },
    {
      "epoch": 157.82820097244732,
      "grad_norm": 1.0040079355239868,
      "learning_rate": 3.424592928965294e-05,
      "loss": 2.7867,
      "step": 486900
    },
    {
      "epoch": 157.8606158833063,
      "grad_norm": 1.2139021158218384,
      "learning_rate": 3.424268569575089e-05,
      "loss": 2.7875,
      "step": 487000
    },
    {
      "epoch": 157.89303079416533,
      "grad_norm": 1.4612821340560913,
      "learning_rate": 3.423944210184885e-05,
      "loss": 2.8024,
      "step": 487100
    },
    {
      "epoch": 157.92544570502432,
      "grad_norm": 1.3803138732910156,
      "learning_rate": 3.4236198507946804e-05,
      "loss": 2.7965,
      "step": 487200
    },
    {
      "epoch": 157.9578606158833,
      "grad_norm": 1.0678143501281738,
      "learning_rate": 3.423295491404476e-05,
      "loss": 2.7963,
      "step": 487300
    },
    {
      "epoch": 157.9902755267423,
      "grad_norm": 1.0684881210327148,
      "learning_rate": 3.422971132014272e-05,
      "loss": 2.8072,
      "step": 487400
    },
    {
      "epoch": 158.0,
      "eval_bleu": 0.9332073441739105,
      "eval_loss": 3.9433658123016357,
      "eval_runtime": 4.3143,
      "eval_samples_per_second": 114.039,
      "eval_steps_per_second": 1.854,
      "step": 487430
    },
    {
      "epoch": 158.02269043760128,
      "grad_norm": 1.1797397136688232,
      "learning_rate": 3.4226467726240673e-05,
      "loss": 2.7885,
      "step": 487500
    },
    {
      "epoch": 158.0551053484603,
      "grad_norm": 0.9923670291900635,
      "learning_rate": 3.422322413233863e-05,
      "loss": 2.7979,
      "step": 487600
    },
    {
      "epoch": 158.0875202593193,
      "grad_norm": 1.2643377780914307,
      "learning_rate": 3.421998053843659e-05,
      "loss": 2.7877,
      "step": 487700
    },
    {
      "epoch": 158.11993517017828,
      "grad_norm": 1.1689749956130981,
      "learning_rate": 3.421673694453454e-05,
      "loss": 2.7859,
      "step": 487800
    },
    {
      "epoch": 158.15235008103727,
      "grad_norm": 1.2803438901901245,
      "learning_rate": 3.42134933506325e-05,
      "loss": 2.7668,
      "step": 487900
    },
    {
      "epoch": 158.18476499189626,
      "grad_norm": 1.3157362937927246,
      "learning_rate": 3.421024975673046e-05,
      "loss": 2.7945,
      "step": 488000
    },
    {
      "epoch": 158.21717990275528,
      "grad_norm": 1.0195871591567993,
      "learning_rate": 3.420700616282841e-05,
      "loss": 2.774,
      "step": 488100
    },
    {
      "epoch": 158.24959481361427,
      "grad_norm": 1.1993757486343384,
      "learning_rate": 3.420376256892637e-05,
      "loss": 2.7824,
      "step": 488200
    },
    {
      "epoch": 158.28200972447326,
      "grad_norm": 1.3151822090148926,
      "learning_rate": 3.420051897502432e-05,
      "loss": 2.7863,
      "step": 488300
    },
    {
      "epoch": 158.31442463533224,
      "grad_norm": 1.068784236907959,
      "learning_rate": 3.419727538112228e-05,
      "loss": 2.7874,
      "step": 488400
    },
    {
      "epoch": 158.34683954619126,
      "grad_norm": 1.406107783317566,
      "learning_rate": 3.419403178722024e-05,
      "loss": 2.7756,
      "step": 488500
    },
    {
      "epoch": 158.37925445705025,
      "grad_norm": 1.1510717868804932,
      "learning_rate": 3.41907881933182e-05,
      "loss": 2.7795,
      "step": 488600
    },
    {
      "epoch": 158.41166936790924,
      "grad_norm": 1.4894212484359741,
      "learning_rate": 3.418754459941616e-05,
      "loss": 2.7918,
      "step": 488700
    },
    {
      "epoch": 158.44408427876823,
      "grad_norm": 1.2136774063110352,
      "learning_rate": 3.418430100551411e-05,
      "loss": 2.7629,
      "step": 488800
    },
    {
      "epoch": 158.47649918962722,
      "grad_norm": 1.4831537008285522,
      "learning_rate": 3.418105741161207e-05,
      "loss": 2.7733,
      "step": 488900
    },
    {
      "epoch": 158.50891410048624,
      "grad_norm": 1.0910701751708984,
      "learning_rate": 3.417781381771003e-05,
      "loss": 2.7768,
      "step": 489000
    },
    {
      "epoch": 158.54132901134523,
      "grad_norm": 1.1912789344787598,
      "learning_rate": 3.4174570223807986e-05,
      "loss": 2.7754,
      "step": 489100
    },
    {
      "epoch": 158.57374392220422,
      "grad_norm": 1.1957398653030396,
      "learning_rate": 3.417132662990594e-05,
      "loss": 2.7894,
      "step": 489200
    },
    {
      "epoch": 158.6061588330632,
      "grad_norm": 1.0914514064788818,
      "learning_rate": 3.41680830360039e-05,
      "loss": 2.7635,
      "step": 489300
    },
    {
      "epoch": 158.6385737439222,
      "grad_norm": 1.098025918006897,
      "learning_rate": 3.4164871878040875e-05,
      "loss": 2.7625,
      "step": 489400
    },
    {
      "epoch": 158.6709886547812,
      "grad_norm": 1.2231119871139526,
      "learning_rate": 3.4161628284138833e-05,
      "loss": 2.7986,
      "step": 489500
    },
    {
      "epoch": 158.7034035656402,
      "grad_norm": 1.1476244926452637,
      "learning_rate": 3.4158384690236785e-05,
      "loss": 2.7614,
      "step": 489600
    },
    {
      "epoch": 158.7358184764992,
      "grad_norm": 1.1553086042404175,
      "learning_rate": 3.4155141096334744e-05,
      "loss": 2.7944,
      "step": 489700
    },
    {
      "epoch": 158.76823338735818,
      "grad_norm": 1.183228850364685,
      "learning_rate": 3.4151897502432696e-05,
      "loss": 2.7868,
      "step": 489800
    },
    {
      "epoch": 158.80064829821717,
      "grad_norm": 1.1173224449157715,
      "learning_rate": 3.4148653908530655e-05,
      "loss": 2.7727,
      "step": 489900
    },
    {
      "epoch": 158.8330632090762,
      "grad_norm": 1.1240766048431396,
      "learning_rate": 3.4145410314628614e-05,
      "loss": 2.7887,
      "step": 490000
    },
    {
      "epoch": 158.86547811993518,
      "grad_norm": 1.2078968286514282,
      "learning_rate": 3.4142166720726566e-05,
      "loss": 2.7938,
      "step": 490100
    },
    {
      "epoch": 158.89789303079417,
      "grad_norm": 1.1345171928405762,
      "learning_rate": 3.4138923126824524e-05,
      "loss": 2.7747,
      "step": 490200
    },
    {
      "epoch": 158.93030794165315,
      "grad_norm": 1.092660665512085,
      "learning_rate": 3.413567953292248e-05,
      "loss": 2.7784,
      "step": 490300
    },
    {
      "epoch": 158.96272285251214,
      "grad_norm": 1.2699183225631714,
      "learning_rate": 3.4132435939020435e-05,
      "loss": 2.7849,
      "step": 490400
    },
    {
      "epoch": 158.99513776337116,
      "grad_norm": 1.030239462852478,
      "learning_rate": 3.4129192345118394e-05,
      "loss": 2.8105,
      "step": 490500
    },
    {
      "epoch": 159.0,
      "eval_bleu": 0.9473722944159151,
      "eval_loss": 3.9516687393188477,
      "eval_runtime": 4.2152,
      "eval_samples_per_second": 116.72,
      "eval_steps_per_second": 1.898,
      "step": 490515
    },
    {
      "epoch": 159.02755267423015,
      "grad_norm": 1.233407735824585,
      "learning_rate": 3.4125948751216346e-05,
      "loss": 2.782,
      "step": 490600
    },
    {
      "epoch": 159.05996758508914,
      "grad_norm": 1.091558814048767,
      "learning_rate": 3.4122705157314305e-05,
      "loss": 2.7818,
      "step": 490700
    },
    {
      "epoch": 159.09238249594813,
      "grad_norm": 1.2540574073791504,
      "learning_rate": 3.411946156341226e-05,
      "loss": 2.7818,
      "step": 490800
    },
    {
      "epoch": 159.12479740680712,
      "grad_norm": 1.1286087036132812,
      "learning_rate": 3.4116217969510215e-05,
      "loss": 2.7815,
      "step": 490900
    },
    {
      "epoch": 159.15721231766614,
      "grad_norm": 0.9543218612670898,
      "learning_rate": 3.4112974375608174e-05,
      "loss": 2.7571,
      "step": 491000
    },
    {
      "epoch": 159.18962722852513,
      "grad_norm": 1.0919967889785767,
      "learning_rate": 3.410973078170613e-05,
      "loss": 2.7697,
      "step": 491100
    },
    {
      "epoch": 159.22204213938411,
      "grad_norm": 1.1414203643798828,
      "learning_rate": 3.4106487187804085e-05,
      "loss": 2.801,
      "step": 491200
    },
    {
      "epoch": 159.2544570502431,
      "grad_norm": 0.9955601692199707,
      "learning_rate": 3.4103243593902043e-05,
      "loss": 2.7864,
      "step": 491300
    },
    {
      "epoch": 159.2868719611021,
      "grad_norm": 1.2160547971725464,
      "learning_rate": 3.410003243593902e-05,
      "loss": 2.7744,
      "step": 491400
    },
    {
      "epoch": 159.3192868719611,
      "grad_norm": 1.1467106342315674,
      "learning_rate": 3.409678884203698e-05,
      "loss": 2.7755,
      "step": 491500
    },
    {
      "epoch": 159.3517017828201,
      "grad_norm": 1.1691670417785645,
      "learning_rate": 3.409354524813493e-05,
      "loss": 2.7706,
      "step": 491600
    },
    {
      "epoch": 159.3841166936791,
      "grad_norm": 1.1469749212265015,
      "learning_rate": 3.409030165423289e-05,
      "loss": 2.7994,
      "step": 491700
    },
    {
      "epoch": 159.41653160453808,
      "grad_norm": 1.1686735153198242,
      "learning_rate": 3.408705806033085e-05,
      "loss": 2.7811,
      "step": 491800
    },
    {
      "epoch": 159.4489465153971,
      "grad_norm": 1.0181041955947876,
      "learning_rate": 3.40838144664288e-05,
      "loss": 2.7863,
      "step": 491900
    },
    {
      "epoch": 159.4813614262561,
      "grad_norm": 1.1317378282546997,
      "learning_rate": 3.408057087252676e-05,
      "loss": 2.7916,
      "step": 492000
    },
    {
      "epoch": 159.51377633711508,
      "grad_norm": 1.1348754167556763,
      "learning_rate": 3.407732727862472e-05,
      "loss": 2.7802,
      "step": 492100
    },
    {
      "epoch": 159.54619124797406,
      "grad_norm": 1.212905764579773,
      "learning_rate": 3.407408368472267e-05,
      "loss": 2.7538,
      "step": 492200
    },
    {
      "epoch": 159.57860615883305,
      "grad_norm": 1.293260931968689,
      "learning_rate": 3.407084009082063e-05,
      "loss": 2.767,
      "step": 492300
    },
    {
      "epoch": 159.61102106969207,
      "grad_norm": 1.1024059057235718,
      "learning_rate": 3.406759649691859e-05,
      "loss": 2.7579,
      "step": 492400
    },
    {
      "epoch": 159.64343598055106,
      "grad_norm": 1.0793073177337646,
      "learning_rate": 3.406435290301655e-05,
      "loss": 2.7837,
      "step": 492500
    },
    {
      "epoch": 159.67585089141005,
      "grad_norm": 1.1477659940719604,
      "learning_rate": 3.4061109309114506e-05,
      "loss": 2.7977,
      "step": 492600
    },
    {
      "epoch": 159.70826580226904,
      "grad_norm": 1.1385289430618286,
      "learning_rate": 3.405786571521246e-05,
      "loss": 2.7597,
      "step": 492700
    },
    {
      "epoch": 159.74068071312803,
      "grad_norm": 1.1301634311676025,
      "learning_rate": 3.4054622121310417e-05,
      "loss": 2.7876,
      "step": 492800
    },
    {
      "epoch": 159.77309562398705,
      "grad_norm": 1.0744353532791138,
      "learning_rate": 3.405137852740837e-05,
      "loss": 2.7729,
      "step": 492900
    },
    {
      "epoch": 159.80551053484604,
      "grad_norm": 1.1259053945541382,
      "learning_rate": 3.404813493350633e-05,
      "loss": 2.7808,
      "step": 493000
    },
    {
      "epoch": 159.83792544570503,
      "grad_norm": 1.205384612083435,
      "learning_rate": 3.4044891339604286e-05,
      "loss": 2.7806,
      "step": 493100
    },
    {
      "epoch": 159.87034035656401,
      "grad_norm": 1.344244360923767,
      "learning_rate": 3.404164774570224e-05,
      "loss": 2.7916,
      "step": 493200
    },
    {
      "epoch": 159.902755267423,
      "grad_norm": 1.1391741037368774,
      "learning_rate": 3.40384041518002e-05,
      "loss": 2.7785,
      "step": 493300
    },
    {
      "epoch": 159.93517017828202,
      "grad_norm": 1.301936149597168,
      "learning_rate": 3.4035192993837174e-05,
      "loss": 2.7844,
      "step": 493400
    },
    {
      "epoch": 159.967585089141,
      "grad_norm": 1.266937017440796,
      "learning_rate": 3.403194939993513e-05,
      "loss": 2.7998,
      "step": 493500
    },
    {
      "epoch": 160.0,
      "grad_norm": 1.0684467554092407,
      "learning_rate": 3.4028705806033085e-05,
      "loss": 2.7929,
      "step": 493600
    },
    {
      "epoch": 160.0,
      "eval_bleu": 1.0666043530001312,
      "eval_loss": 3.949741840362549,
      "eval_runtime": 4.2273,
      "eval_samples_per_second": 116.386,
      "eval_steps_per_second": 1.892,
      "step": 493600
    },
    {
      "epoch": 160.032414910859,
      "grad_norm": 1.0181517601013184,
      "learning_rate": 3.402549464807006e-05,
      "loss": 2.7756,
      "step": 493700
    },
    {
      "epoch": 160.06482982171798,
      "grad_norm": 1.1756242513656616,
      "learning_rate": 3.402225105416802e-05,
      "loss": 2.7644,
      "step": 493800
    },
    {
      "epoch": 160.097244732577,
      "grad_norm": 1.2671862840652466,
      "learning_rate": 3.401900746026598e-05,
      "loss": 2.7708,
      "step": 493900
    },
    {
      "epoch": 160.12965964343599,
      "grad_norm": 1.5633717775344849,
      "learning_rate": 3.401576386636393e-05,
      "loss": 2.7767,
      "step": 494000
    },
    {
      "epoch": 160.16207455429497,
      "grad_norm": 1.145187497138977,
      "learning_rate": 3.401252027246189e-05,
      "loss": 2.7775,
      "step": 494100
    },
    {
      "epoch": 160.19448946515396,
      "grad_norm": 1.1289459466934204,
      "learning_rate": 3.400927667855985e-05,
      "loss": 2.776,
      "step": 494200
    },
    {
      "epoch": 160.22690437601295,
      "grad_norm": 1.2698283195495605,
      "learning_rate": 3.40060330846578e-05,
      "loss": 2.7777,
      "step": 494300
    },
    {
      "epoch": 160.25931928687197,
      "grad_norm": 1.1082795858383179,
      "learning_rate": 3.400278949075576e-05,
      "loss": 2.773,
      "step": 494400
    },
    {
      "epoch": 160.29173419773096,
      "grad_norm": 1.3118419647216797,
      "learning_rate": 3.399954589685371e-05,
      "loss": 2.7779,
      "step": 494500
    },
    {
      "epoch": 160.32414910858995,
      "grad_norm": 1.331737995147705,
      "learning_rate": 3.399630230295167e-05,
      "loss": 2.7651,
      "step": 494600
    },
    {
      "epoch": 160.35656401944894,
      "grad_norm": 1.0925252437591553,
      "learning_rate": 3.399305870904963e-05,
      "loss": 2.7898,
      "step": 494700
    },
    {
      "epoch": 160.38897893030793,
      "grad_norm": 1.215635061264038,
      "learning_rate": 3.398981511514758e-05,
      "loss": 2.7725,
      "step": 494800
    },
    {
      "epoch": 160.42139384116695,
      "grad_norm": 1.1762666702270508,
      "learning_rate": 3.398657152124554e-05,
      "loss": 2.7627,
      "step": 494900
    },
    {
      "epoch": 160.45380875202594,
      "grad_norm": 1.2576422691345215,
      "learning_rate": 3.39833279273435e-05,
      "loss": 2.7728,
      "step": 495000
    },
    {
      "epoch": 160.48622366288492,
      "grad_norm": 1.3238774538040161,
      "learning_rate": 3.398008433344145e-05,
      "loss": 2.7753,
      "step": 495100
    },
    {
      "epoch": 160.5186385737439,
      "grad_norm": 2.1720046997070312,
      "learning_rate": 3.397684073953941e-05,
      "loss": 2.7858,
      "step": 495200
    },
    {
      "epoch": 160.5510534846029,
      "grad_norm": 1.167275071144104,
      "learning_rate": 3.397359714563737e-05,
      "loss": 2.7892,
      "step": 495300
    },
    {
      "epoch": 160.58346839546192,
      "grad_norm": 1.4360560178756714,
      "learning_rate": 3.397035355173532e-05,
      "loss": 2.7993,
      "step": 495400
    },
    {
      "epoch": 160.6158833063209,
      "grad_norm": 1.2406152486801147,
      "learning_rate": 3.396710995783328e-05,
      "loss": 2.7926,
      "step": 495500
    },
    {
      "epoch": 160.6482982171799,
      "grad_norm": 1.090662956237793,
      "learning_rate": 3.396386636393123e-05,
      "loss": 2.7919,
      "step": 495600
    },
    {
      "epoch": 160.6807131280389,
      "grad_norm": 1.0865731239318848,
      "learning_rate": 3.396062277002919e-05,
      "loss": 2.7792,
      "step": 495700
    },
    {
      "epoch": 160.7131280388979,
      "grad_norm": 1.3313950300216675,
      "learning_rate": 3.395737917612715e-05,
      "loss": 2.7711,
      "step": 495800
    },
    {
      "epoch": 160.7455429497569,
      "grad_norm": 1.1167558431625366,
      "learning_rate": 3.395413558222511e-05,
      "loss": 2.7853,
      "step": 495900
    },
    {
      "epoch": 160.77795786061589,
      "grad_norm": 1.2904796600341797,
      "learning_rate": 3.395089198832307e-05,
      "loss": 2.7885,
      "step": 496000
    },
    {
      "epoch": 160.81037277147487,
      "grad_norm": 1.3192988634109497,
      "learning_rate": 3.3947648394421025e-05,
      "loss": 2.7734,
      "step": 496100
    },
    {
      "epoch": 160.84278768233386,
      "grad_norm": 1.3975452184677124,
      "learning_rate": 3.394440480051898e-05,
      "loss": 2.7959,
      "step": 496200
    },
    {
      "epoch": 160.87520259319288,
      "grad_norm": 1.2366236448287964,
      "learning_rate": 3.3941161206616936e-05,
      "loss": 2.7674,
      "step": 496300
    },
    {
      "epoch": 160.90761750405187,
      "grad_norm": 1.0891684293746948,
      "learning_rate": 3.3937917612714895e-05,
      "loss": 2.7686,
      "step": 496400
    },
    {
      "epoch": 160.94003241491086,
      "grad_norm": 1.2026596069335938,
      "learning_rate": 3.393467401881285e-05,
      "loss": 2.7906,
      "step": 496500
    },
    {
      "epoch": 160.97244732576985,
      "grad_norm": 1.2239595651626587,
      "learning_rate": 3.3931430424910806e-05,
      "loss": 2.7771,
      "step": 496600
    },
    {
      "epoch": 161.0,
      "eval_bleu": 1.0284981008357748,
      "eval_loss": 3.953524351119995,
      "eval_runtime": 4.0988,
      "eval_samples_per_second": 120.036,
      "eval_steps_per_second": 1.952,
      "step": 496685
    },
    {
      "epoch": 161.00486223662884,
      "grad_norm": 1.242099404335022,
      "learning_rate": 3.392818683100876e-05,
      "loss": 2.7922,
      "step": 496700
    },
    {
      "epoch": 161.03727714748786,
      "grad_norm": 1.161162257194519,
      "learning_rate": 3.3924943237106716e-05,
      "loss": 2.7704,
      "step": 496800
    },
    {
      "epoch": 161.06969205834685,
      "grad_norm": 1.3248244524002075,
      "learning_rate": 3.3921699643204675e-05,
      "loss": 2.7684,
      "step": 496900
    },
    {
      "epoch": 161.10210696920583,
      "grad_norm": 1.2123961448669434,
      "learning_rate": 3.391845604930263e-05,
      "loss": 2.7743,
      "step": 497000
    },
    {
      "epoch": 161.13452188006482,
      "grad_norm": 1.2350260019302368,
      "learning_rate": 3.3915212455400586e-05,
      "loss": 2.7553,
      "step": 497100
    },
    {
      "epoch": 161.1669367909238,
      "grad_norm": 1.2445989847183228,
      "learning_rate": 3.3911968861498544e-05,
      "loss": 2.79,
      "step": 497200
    },
    {
      "epoch": 161.19935170178283,
      "grad_norm": 1.125373363494873,
      "learning_rate": 3.3908725267596496e-05,
      "loss": 2.7758,
      "step": 497300
    },
    {
      "epoch": 161.23176661264182,
      "grad_norm": 1.054612398147583,
      "learning_rate": 3.3905481673694455e-05,
      "loss": 2.7726,
      "step": 497400
    },
    {
      "epoch": 161.2641815235008,
      "grad_norm": 1.0780123472213745,
      "learning_rate": 3.390223807979241e-05,
      "loss": 2.8069,
      "step": 497500
    },
    {
      "epoch": 161.2965964343598,
      "grad_norm": 1.1443376541137695,
      "learning_rate": 3.3898994485890366e-05,
      "loss": 2.7843,
      "step": 497600
    },
    {
      "epoch": 161.3290113452188,
      "grad_norm": 1.372611403465271,
      "learning_rate": 3.3895750891988325e-05,
      "loss": 2.7846,
      "step": 497700
    },
    {
      "epoch": 161.3614262560778,
      "grad_norm": 1.0153648853302002,
      "learning_rate": 3.38925397340253e-05,
      "loss": 2.7808,
      "step": 497800
    },
    {
      "epoch": 161.3938411669368,
      "grad_norm": 1.1667286157608032,
      "learning_rate": 3.3889296140123254e-05,
      "loss": 2.7725,
      "step": 497900
    },
    {
      "epoch": 161.42625607779578,
      "grad_norm": 1.314462423324585,
      "learning_rate": 3.388605254622121e-05,
      "loss": 2.7492,
      "step": 498000
    },
    {
      "epoch": 161.45867098865477,
      "grad_norm": 1.2400047779083252,
      "learning_rate": 3.388280895231917e-05,
      "loss": 2.7904,
      "step": 498100
    },
    {
      "epoch": 161.49108589951376,
      "grad_norm": 1.128872275352478,
      "learning_rate": 3.3879565358417124e-05,
      "loss": 2.7699,
      "step": 498200
    },
    {
      "epoch": 161.52350081037278,
      "grad_norm": 1.1780409812927246,
      "learning_rate": 3.387632176451508e-05,
      "loss": 2.7734,
      "step": 498300
    },
    {
      "epoch": 161.55591572123177,
      "grad_norm": 1.225730299949646,
      "learning_rate": 3.387307817061304e-05,
      "loss": 2.7664,
      "step": 498400
    },
    {
      "epoch": 161.58833063209076,
      "grad_norm": 1.155808448791504,
      "learning_rate": 3.386983457671099e-05,
      "loss": 2.7693,
      "step": 498500
    },
    {
      "epoch": 161.62074554294975,
      "grad_norm": 1.2758017778396606,
      "learning_rate": 3.386659098280895e-05,
      "loss": 2.7777,
      "step": 498600
    },
    {
      "epoch": 161.65316045380874,
      "grad_norm": 1.2414127588272095,
      "learning_rate": 3.386334738890691e-05,
      "loss": 2.7693,
      "step": 498700
    },
    {
      "epoch": 161.68557536466776,
      "grad_norm": 1.0727812051773071,
      "learning_rate": 3.386010379500486e-05,
      "loss": 2.76,
      "step": 498800
    },
    {
      "epoch": 161.71799027552674,
      "grad_norm": 0.9527783393859863,
      "learning_rate": 3.385686020110282e-05,
      "loss": 2.7935,
      "step": 498900
    },
    {
      "epoch": 161.75040518638573,
      "grad_norm": 1.138922929763794,
      "learning_rate": 3.385361660720078e-05,
      "loss": 2.768,
      "step": 499000
    },
    {
      "epoch": 161.78282009724472,
      "grad_norm": 1.1799737215042114,
      "learning_rate": 3.385037301329874e-05,
      "loss": 2.798,
      "step": 499100
    },
    {
      "epoch": 161.81523500810374,
      "grad_norm": 1.0104769468307495,
      "learning_rate": 3.38471294193967e-05,
      "loss": 2.7519,
      "step": 499200
    },
    {
      "epoch": 161.84764991896273,
      "grad_norm": 1.1494495868682861,
      "learning_rate": 3.384388582549465e-05,
      "loss": 2.7584,
      "step": 499300
    },
    {
      "epoch": 161.88006482982172,
      "grad_norm": 1.206508755683899,
      "learning_rate": 3.384064223159261e-05,
      "loss": 2.7821,
      "step": 499400
    },
    {
      "epoch": 161.9124797406807,
      "grad_norm": 1.4138494729995728,
      "learning_rate": 3.383739863769057e-05,
      "loss": 2.8003,
      "step": 499500
    },
    {
      "epoch": 161.9448946515397,
      "grad_norm": 1.2393752336502075,
      "learning_rate": 3.383415504378852e-05,
      "loss": 2.776,
      "step": 499600
    },
    {
      "epoch": 161.97730956239872,
      "grad_norm": 1.1556897163391113,
      "learning_rate": 3.383091144988648e-05,
      "loss": 2.7978,
      "step": 499700
    },
    {
      "epoch": 162.0,
      "eval_bleu": 0.9500616324315889,
      "eval_loss": 3.9546549320220947,
      "eval_runtime": 4.1895,
      "eval_samples_per_second": 117.436,
      "eval_steps_per_second": 1.91,
      "step": 499770
    },
    {
      "epoch": 162.0097244732577,
      "grad_norm": 1.222874402999878,
      "learning_rate": 3.3827700291923456e-05,
      "loss": 2.7636,
      "step": 499800
    },
    {
      "epoch": 162.0421393841167,
      "grad_norm": 1.2026970386505127,
      "learning_rate": 3.382448913396043e-05,
      "loss": 2.7631,
      "step": 499900
    },
    {
      "epoch": 162.07455429497568,
      "grad_norm": 1.0710675716400146,
      "learning_rate": 3.3821245540058385e-05,
      "loss": 2.7738,
      "step": 500000
    },
    {
      "epoch": 162.10696920583467,
      "grad_norm": 1.0688236951828003,
      "learning_rate": 3.3818001946156344e-05,
      "loss": 2.7668,
      "step": 500100
    },
    {
      "epoch": 162.1393841166937,
      "grad_norm": 1.1383026838302612,
      "learning_rate": 3.38147583522543e-05,
      "loss": 2.784,
      "step": 500200
    },
    {
      "epoch": 162.17179902755268,
      "grad_norm": 1.4492801427841187,
      "learning_rate": 3.381151475835226e-05,
      "loss": 2.7891,
      "step": 500300
    },
    {
      "epoch": 162.20421393841167,
      "grad_norm": 1.181091070175171,
      "learning_rate": 3.3808271164450214e-05,
      "loss": 2.7981,
      "step": 500400
    },
    {
      "epoch": 162.23662884927066,
      "grad_norm": 1.2051466703414917,
      "learning_rate": 3.380502757054817e-05,
      "loss": 2.7733,
      "step": 500500
    },
    {
      "epoch": 162.26904376012965,
      "grad_norm": 1.1647886037826538,
      "learning_rate": 3.3801783976646124e-05,
      "loss": 2.7805,
      "step": 500600
    },
    {
      "epoch": 162.30145867098867,
      "grad_norm": 0.9409080743789673,
      "learning_rate": 3.379854038274408e-05,
      "loss": 2.7699,
      "step": 500700
    },
    {
      "epoch": 162.33387358184766,
      "grad_norm": 1.276930332183838,
      "learning_rate": 3.379529678884204e-05,
      "loss": 2.7756,
      "step": 500800
    },
    {
      "epoch": 162.36628849270664,
      "grad_norm": 1.046770691871643,
      "learning_rate": 3.3792053194939994e-05,
      "loss": 2.7641,
      "step": 500900
    },
    {
      "epoch": 162.39870340356563,
      "grad_norm": 1.1135587692260742,
      "learning_rate": 3.378880960103795e-05,
      "loss": 2.7736,
      "step": 501000
    },
    {
      "epoch": 162.43111831442462,
      "grad_norm": 1.3664884567260742,
      "learning_rate": 3.378556600713591e-05,
      "loss": 2.7709,
      "step": 501100
    },
    {
      "epoch": 162.46353322528364,
      "grad_norm": 1.104353427886963,
      "learning_rate": 3.378232241323386e-05,
      "loss": 2.7795,
      "step": 501200
    },
    {
      "epoch": 162.49594813614263,
      "grad_norm": 1.0887579917907715,
      "learning_rate": 3.377907881933182e-05,
      "loss": 2.7589,
      "step": 501300
    },
    {
      "epoch": 162.52836304700162,
      "grad_norm": 1.2230830192565918,
      "learning_rate": 3.377583522542978e-05,
      "loss": 2.7847,
      "step": 501400
    },
    {
      "epoch": 162.5607779578606,
      "grad_norm": 1.0118839740753174,
      "learning_rate": 3.377259163152773e-05,
      "loss": 2.7815,
      "step": 501500
    },
    {
      "epoch": 162.5931928687196,
      "grad_norm": 1.1703662872314453,
      "learning_rate": 3.376934803762569e-05,
      "loss": 2.7683,
      "step": 501600
    },
    {
      "epoch": 162.62560777957862,
      "grad_norm": 1.3431637287139893,
      "learning_rate": 3.3766104443723643e-05,
      "loss": 2.7559,
      "step": 501700
    },
    {
      "epoch": 162.6580226904376,
      "grad_norm": 1.1981395483016968,
      "learning_rate": 3.37628608498216e-05,
      "loss": 2.7689,
      "step": 501800
    },
    {
      "epoch": 162.6904376012966,
      "grad_norm": 1.0998423099517822,
      "learning_rate": 3.375961725591956e-05,
      "loss": 2.789,
      "step": 501900
    },
    {
      "epoch": 162.72285251215558,
      "grad_norm": 1.262751817703247,
      "learning_rate": 3.375637366201751e-05,
      "loss": 2.7652,
      "step": 502000
    },
    {
      "epoch": 162.75526742301457,
      "grad_norm": 1.1189981698989868,
      "learning_rate": 3.375313006811547e-05,
      "loss": 2.7745,
      "step": 502100
    },
    {
      "epoch": 162.7876823338736,
      "grad_norm": 1.088083028793335,
      "learning_rate": 3.374988647421343e-05,
      "loss": 2.771,
      "step": 502200
    },
    {
      "epoch": 162.82009724473258,
      "grad_norm": 1.0930571556091309,
      "learning_rate": 3.374664288031138e-05,
      "loss": 2.7618,
      "step": 502300
    },
    {
      "epoch": 162.85251215559157,
      "grad_norm": 1.0989500284194946,
      "learning_rate": 3.374339928640934e-05,
      "loss": 2.7852,
      "step": 502400
    },
    {
      "epoch": 162.88492706645056,
      "grad_norm": 1.1206588745117188,
      "learning_rate": 3.37401556925073e-05,
      "loss": 2.7964,
      "step": 502500
    },
    {
      "epoch": 162.91734197730958,
      "grad_norm": 1.1393067836761475,
      "learning_rate": 3.373691209860526e-05,
      "loss": 2.7708,
      "step": 502600
    },
    {
      "epoch": 162.94975688816857,
      "grad_norm": 1.1778533458709717,
      "learning_rate": 3.373366850470322e-05,
      "loss": 2.7899,
      "step": 502700
    },
    {
      "epoch": 162.98217179902755,
      "grad_norm": 1.2011644840240479,
      "learning_rate": 3.373042491080117e-05,
      "loss": 2.7636,
      "step": 502800
    },
    {
      "epoch": 163.0,
      "eval_bleu": 1.0789409872141538,
      "eval_loss": 3.95648455619812,
      "eval_runtime": 4.4174,
      "eval_samples_per_second": 111.378,
      "eval_steps_per_second": 1.811,
      "step": 502855
    },
    {
      "epoch": 163.01458670988654,
      "grad_norm": 1.1719874143600464,
      "learning_rate": 3.372718131689913e-05,
      "loss": 2.7897,
      "step": 502900
    },
    {
      "epoch": 163.04700162074553,
      "grad_norm": 1.337440848350525,
      "learning_rate": 3.372393772299709e-05,
      "loss": 2.7473,
      "step": 503000
    },
    {
      "epoch": 163.07941653160455,
      "grad_norm": 1.2928807735443115,
      "learning_rate": 3.372069412909504e-05,
      "loss": 2.7668,
      "step": 503100
    },
    {
      "epoch": 163.11183144246354,
      "grad_norm": 1.1570062637329102,
      "learning_rate": 3.3717450535193e-05,
      "loss": 2.7603,
      "step": 503200
    },
    {
      "epoch": 163.14424635332253,
      "grad_norm": 1.102218747138977,
      "learning_rate": 3.3714206941290956e-05,
      "loss": 2.7569,
      "step": 503300
    },
    {
      "epoch": 163.17666126418152,
      "grad_norm": 1.025834321975708,
      "learning_rate": 3.371096334738891e-05,
      "loss": 2.7696,
      "step": 503400
    },
    {
      "epoch": 163.2090761750405,
      "grad_norm": 1.1484657526016235,
      "learning_rate": 3.370771975348687e-05,
      "loss": 2.7708,
      "step": 503500
    },
    {
      "epoch": 163.24149108589953,
      "grad_norm": 1.110182285308838,
      "learning_rate": 3.370447615958482e-05,
      "loss": 2.7846,
      "step": 503600
    },
    {
      "epoch": 163.27390599675851,
      "grad_norm": 1.226143479347229,
      "learning_rate": 3.370123256568278e-05,
      "loss": 2.7709,
      "step": 503700
    },
    {
      "epoch": 163.3063209076175,
      "grad_norm": 1.492910623550415,
      "learning_rate": 3.3697988971780736e-05,
      "loss": 2.7826,
      "step": 503800
    },
    {
      "epoch": 163.3387358184765,
      "grad_norm": 1.1788917779922485,
      "learning_rate": 3.369474537787869e-05,
      "loss": 2.7945,
      "step": 503900
    },
    {
      "epoch": 163.37115072933548,
      "grad_norm": 1.1041921377182007,
      "learning_rate": 3.369150178397665e-05,
      "loss": 2.7705,
      "step": 504000
    },
    {
      "epoch": 163.4035656401945,
      "grad_norm": 1.220629334449768,
      "learning_rate": 3.3688290626013625e-05,
      "loss": 2.7791,
      "step": 504100
    },
    {
      "epoch": 163.4359805510535,
      "grad_norm": 1.1170703172683716,
      "learning_rate": 3.3685047032111584e-05,
      "loss": 2.7794,
      "step": 504200
    },
    {
      "epoch": 163.46839546191248,
      "grad_norm": 1.1008256673812866,
      "learning_rate": 3.3681803438209536e-05,
      "loss": 2.7849,
      "step": 504300
    },
    {
      "epoch": 163.50081037277147,
      "grad_norm": 1.240424394607544,
      "learning_rate": 3.3678559844307494e-05,
      "loss": 2.7514,
      "step": 504400
    },
    {
      "epoch": 163.53322528363046,
      "grad_norm": 1.1604363918304443,
      "learning_rate": 3.367531625040545e-05,
      "loss": 2.7584,
      "step": 504500
    },
    {
      "epoch": 163.56564019448948,
      "grad_norm": 1.1466904878616333,
      "learning_rate": 3.3672072656503405e-05,
      "loss": 2.7739,
      "step": 504600
    },
    {
      "epoch": 163.59805510534846,
      "grad_norm": 1.2976540327072144,
      "learning_rate": 3.3668829062601364e-05,
      "loss": 2.765,
      "step": 504700
    },
    {
      "epoch": 163.63047001620745,
      "grad_norm": 1.2467947006225586,
      "learning_rate": 3.3665585468699316e-05,
      "loss": 2.7635,
      "step": 504800
    },
    {
      "epoch": 163.66288492706644,
      "grad_norm": 1.0821964740753174,
      "learning_rate": 3.3662341874797274e-05,
      "loss": 2.7764,
      "step": 504900
    },
    {
      "epoch": 163.69529983792543,
      "grad_norm": 1.1442203521728516,
      "learning_rate": 3.365909828089523e-05,
      "loss": 2.7827,
      "step": 505000
    },
    {
      "epoch": 163.72771474878445,
      "grad_norm": 1.082479476928711,
      "learning_rate": 3.3655854686993185e-05,
      "loss": 2.7836,
      "step": 505100
    },
    {
      "epoch": 163.76012965964344,
      "grad_norm": 1.4660691022872925,
      "learning_rate": 3.3652611093091144e-05,
      "loss": 2.7869,
      "step": 505200
    },
    {
      "epoch": 163.79254457050243,
      "grad_norm": 1.0613853931427002,
      "learning_rate": 3.36493674991891e-05,
      "loss": 2.7748,
      "step": 505300
    },
    {
      "epoch": 163.82495948136142,
      "grad_norm": 1.203816294670105,
      "learning_rate": 3.364612390528706e-05,
      "loss": 2.7717,
      "step": 505400
    },
    {
      "epoch": 163.8573743922204,
      "grad_norm": 1.2103288173675537,
      "learning_rate": 3.364288031138501e-05,
      "loss": 2.7811,
      "step": 505500
    },
    {
      "epoch": 163.88978930307943,
      "grad_norm": 1.1705496311187744,
      "learning_rate": 3.363963671748297e-05,
      "loss": 2.7721,
      "step": 505600
    },
    {
      "epoch": 163.92220421393841,
      "grad_norm": 1.3001164197921753,
      "learning_rate": 3.363639312358093e-05,
      "loss": 2.7638,
      "step": 505700
    },
    {
      "epoch": 163.9546191247974,
      "grad_norm": 1.2665798664093018,
      "learning_rate": 3.363314952967889e-05,
      "loss": 2.7808,
      "step": 505800
    },
    {
      "epoch": 163.9870340356564,
      "grad_norm": 0.9939581751823425,
      "learning_rate": 3.362990593577684e-05,
      "loss": 2.7579,
      "step": 505900
    },
    {
      "epoch": 164.0,
      "eval_bleu": 0.9493272671185308,
      "eval_loss": 3.9615609645843506,
      "eval_runtime": 4.4311,
      "eval_samples_per_second": 111.033,
      "eval_steps_per_second": 1.805,
      "step": 505940
    },
    {
      "epoch": 164.0194489465154,
      "grad_norm": 1.0554461479187012,
      "learning_rate": 3.36266623418748e-05,
      "loss": 2.7773,
      "step": 506000
    },
    {
      "epoch": 164.0518638573744,
      "grad_norm": 1.1950266361236572,
      "learning_rate": 3.362341874797276e-05,
      "loss": 2.77,
      "step": 506100
    },
    {
      "epoch": 164.0842787682334,
      "grad_norm": 1.4313647747039795,
      "learning_rate": 3.362017515407071e-05,
      "loss": 2.7598,
      "step": 506200
    },
    {
      "epoch": 164.11669367909238,
      "grad_norm": 1.3635197877883911,
      "learning_rate": 3.361693156016867e-05,
      "loss": 2.7579,
      "step": 506300
    },
    {
      "epoch": 164.14910858995137,
      "grad_norm": 1.1007020473480225,
      "learning_rate": 3.361368796626663e-05,
      "loss": 2.7629,
      "step": 506400
    },
    {
      "epoch": 164.18152350081039,
      "grad_norm": 1.212822675704956,
      "learning_rate": 3.361044437236458e-05,
      "loss": 2.7752,
      "step": 506500
    },
    {
      "epoch": 164.21393841166937,
      "grad_norm": 1.2024009227752686,
      "learning_rate": 3.360720077846254e-05,
      "loss": 2.765,
      "step": 506600
    },
    {
      "epoch": 164.24635332252836,
      "grad_norm": 1.064875841140747,
      "learning_rate": 3.36039571845605e-05,
      "loss": 2.766,
      "step": 506700
    },
    {
      "epoch": 164.27876823338735,
      "grad_norm": 1.2559136152267456,
      "learning_rate": 3.360071359065845e-05,
      "loss": 2.7729,
      "step": 506800
    },
    {
      "epoch": 164.31118314424634,
      "grad_norm": 1.225775122642517,
      "learning_rate": 3.359746999675641e-05,
      "loss": 2.7588,
      "step": 506900
    },
    {
      "epoch": 164.34359805510536,
      "grad_norm": 1.2283252477645874,
      "learning_rate": 3.359422640285436e-05,
      "loss": 2.7793,
      "step": 507000
    },
    {
      "epoch": 164.37601296596435,
      "grad_norm": 1.3308824300765991,
      "learning_rate": 3.359098280895232e-05,
      "loss": 2.7534,
      "step": 507100
    },
    {
      "epoch": 164.40842787682334,
      "grad_norm": 1.1654752492904663,
      "learning_rate": 3.358773921505028e-05,
      "loss": 2.7637,
      "step": 507200
    },
    {
      "epoch": 164.44084278768233,
      "grad_norm": 1.1382774114608765,
      "learning_rate": 3.358449562114823e-05,
      "loss": 2.7839,
      "step": 507300
    },
    {
      "epoch": 164.47325769854132,
      "grad_norm": 1.1569279432296753,
      "learning_rate": 3.358125202724619e-05,
      "loss": 2.7695,
      "step": 507400
    },
    {
      "epoch": 164.50567260940034,
      "grad_norm": 1.1996574401855469,
      "learning_rate": 3.357800843334415e-05,
      "loss": 2.779,
      "step": 507500
    },
    {
      "epoch": 164.53808752025932,
      "grad_norm": 1.0400111675262451,
      "learning_rate": 3.35747648394421e-05,
      "loss": 2.7666,
      "step": 507600
    },
    {
      "epoch": 164.5705024311183,
      "grad_norm": 1.283888339996338,
      "learning_rate": 3.357152124554006e-05,
      "loss": 2.7696,
      "step": 507700
    },
    {
      "epoch": 164.6029173419773,
      "grad_norm": 1.2710177898406982,
      "learning_rate": 3.356827765163802e-05,
      "loss": 2.7954,
      "step": 507800
    },
    {
      "epoch": 164.6353322528363,
      "grad_norm": 1.1272907257080078,
      "learning_rate": 3.356503405773597e-05,
      "loss": 2.7578,
      "step": 507900
    },
    {
      "epoch": 164.6677471636953,
      "grad_norm": 1.1117017269134521,
      "learning_rate": 3.356179046383393e-05,
      "loss": 2.7603,
      "step": 508000
    },
    {
      "epoch": 164.7001620745543,
      "grad_norm": 1.212924599647522,
      "learning_rate": 3.3558579305870906e-05,
      "loss": 2.7664,
      "step": 508100
    },
    {
      "epoch": 164.7325769854133,
      "grad_norm": 1.1617214679718018,
      "learning_rate": 3.355533571196886e-05,
      "loss": 2.7612,
      "step": 508200
    },
    {
      "epoch": 164.76499189627228,
      "grad_norm": 1.1056668758392334,
      "learning_rate": 3.3552092118066816e-05,
      "loss": 2.7484,
      "step": 508300
    },
    {
      "epoch": 164.79740680713127,
      "grad_norm": 1.2721354961395264,
      "learning_rate": 3.3548848524164775e-05,
      "loss": 2.7677,
      "step": 508400
    },
    {
      "epoch": 164.82982171799028,
      "grad_norm": 1.1546496152877808,
      "learning_rate": 3.3545604930262734e-05,
      "loss": 2.7698,
      "step": 508500
    },
    {
      "epoch": 164.86223662884927,
      "grad_norm": 1.2147438526153564,
      "learning_rate": 3.3542393772299705e-05,
      "loss": 2.784,
      "step": 508600
    },
    {
      "epoch": 164.89465153970826,
      "grad_norm": 1.0973122119903564,
      "learning_rate": 3.3539150178397663e-05,
      "loss": 2.7938,
      "step": 508700
    },
    {
      "epoch": 164.92706645056725,
      "grad_norm": 1.2794519662857056,
      "learning_rate": 3.353590658449562e-05,
      "loss": 2.7786,
      "step": 508800
    },
    {
      "epoch": 164.95948136142624,
      "grad_norm": 1.1203179359436035,
      "learning_rate": 3.3532662990593574e-05,
      "loss": 2.7976,
      "step": 508900
    },
    {
      "epoch": 164.99189627228526,
      "grad_norm": 1.2917804718017578,
      "learning_rate": 3.352941939669153e-05,
      "loss": 2.7912,
      "step": 509000
    },
    {
      "epoch": 165.0,
      "eval_bleu": 1.1413635481491173,
      "eval_loss": 3.9594714641571045,
      "eval_runtime": 4.2349,
      "eval_samples_per_second": 116.177,
      "eval_steps_per_second": 1.889,
      "step": 509025
    },
    {
      "epoch": 165.02431118314425,
      "grad_norm": 1.1250028610229492,
      "learning_rate": 3.352617580278949e-05,
      "loss": 2.7638,
      "step": 509100
    },
    {
      "epoch": 165.05672609400324,
      "grad_norm": 1.146214485168457,
      "learning_rate": 3.352293220888745e-05,
      "loss": 2.7646,
      "step": 509200
    },
    {
      "epoch": 165.08914100486223,
      "grad_norm": 1.1823279857635498,
      "learning_rate": 3.351968861498541e-05,
      "loss": 2.7528,
      "step": 509300
    },
    {
      "epoch": 165.12155591572125,
      "grad_norm": 1.2573471069335938,
      "learning_rate": 3.351644502108337e-05,
      "loss": 2.7652,
      "step": 509400
    },
    {
      "epoch": 165.15397082658023,
      "grad_norm": 1.222644329071045,
      "learning_rate": 3.351320142718132e-05,
      "loss": 2.7717,
      "step": 509500
    },
    {
      "epoch": 165.18638573743922,
      "grad_norm": 1.1549395322799683,
      "learning_rate": 3.350995783327928e-05,
      "loss": 2.7604,
      "step": 509600
    },
    {
      "epoch": 165.2188006482982,
      "grad_norm": 1.2850247621536255,
      "learning_rate": 3.350671423937723e-05,
      "loss": 2.7812,
      "step": 509700
    },
    {
      "epoch": 165.2512155591572,
      "grad_norm": 1.011406660079956,
      "learning_rate": 3.350347064547519e-05,
      "loss": 2.7398,
      "step": 509800
    },
    {
      "epoch": 165.28363047001622,
      "grad_norm": 1.218449354171753,
      "learning_rate": 3.350022705157315e-05,
      "loss": 2.7824,
      "step": 509900
    },
    {
      "epoch": 165.3160453808752,
      "grad_norm": 1.1301367282867432,
      "learning_rate": 3.34969834576711e-05,
      "loss": 2.7662,
      "step": 510000
    },
    {
      "epoch": 165.3484602917342,
      "grad_norm": 0.9980381727218628,
      "learning_rate": 3.349373986376906e-05,
      "loss": 2.7659,
      "step": 510100
    },
    {
      "epoch": 165.3808752025932,
      "grad_norm": 1.229457139968872,
      "learning_rate": 3.349049626986702e-05,
      "loss": 2.7805,
      "step": 510200
    },
    {
      "epoch": 165.41329011345218,
      "grad_norm": 1.325693964958191,
      "learning_rate": 3.348725267596497e-05,
      "loss": 2.7709,
      "step": 510300
    },
    {
      "epoch": 165.4457050243112,
      "grad_norm": 1.5736664533615112,
      "learning_rate": 3.348400908206293e-05,
      "loss": 2.7911,
      "step": 510400
    },
    {
      "epoch": 165.47811993517018,
      "grad_norm": 1.1427643299102783,
      "learning_rate": 3.348076548816088e-05,
      "loss": 2.7948,
      "step": 510500
    },
    {
      "epoch": 165.51053484602917,
      "grad_norm": 1.0960053205490112,
      "learning_rate": 3.347752189425884e-05,
      "loss": 2.7638,
      "step": 510600
    },
    {
      "epoch": 165.54294975688816,
      "grad_norm": 1.2717138528823853,
      "learning_rate": 3.34742783003568e-05,
      "loss": 2.7547,
      "step": 510700
    },
    {
      "epoch": 165.57536466774715,
      "grad_norm": 1.0488827228546143,
      "learning_rate": 3.347103470645475e-05,
      "loss": 2.7639,
      "step": 510800
    },
    {
      "epoch": 165.60777957860617,
      "grad_norm": 1.27761971950531,
      "learning_rate": 3.346779111255271e-05,
      "loss": 2.7559,
      "step": 510900
    },
    {
      "epoch": 165.64019448946516,
      "grad_norm": 1.0827622413635254,
      "learning_rate": 3.346454751865067e-05,
      "loss": 2.7803,
      "step": 511000
    },
    {
      "epoch": 165.67260940032415,
      "grad_norm": 1.2393443584442139,
      "learning_rate": 3.346130392474862e-05,
      "loss": 2.7569,
      "step": 511100
    },
    {
      "epoch": 165.70502431118314,
      "grad_norm": 1.1764687299728394,
      "learning_rate": 3.345806033084658e-05,
      "loss": 2.7572,
      "step": 511200
    },
    {
      "epoch": 165.73743922204213,
      "grad_norm": 1.1343984603881836,
      "learning_rate": 3.3454816736944537e-05,
      "loss": 2.7869,
      "step": 511300
    },
    {
      "epoch": 165.76985413290114,
      "grad_norm": 1.155748963356018,
      "learning_rate": 3.345157314304249e-05,
      "loss": 2.7727,
      "step": 511400
    },
    {
      "epoch": 165.80226904376013,
      "grad_norm": 1.1975209712982178,
      "learning_rate": 3.344832954914045e-05,
      "loss": 2.7333,
      "step": 511500
    },
    {
      "epoch": 165.83468395461912,
      "grad_norm": 1.2766609191894531,
      "learning_rate": 3.3445085955238406e-05,
      "loss": 2.7679,
      "step": 511600
    },
    {
      "epoch": 165.8670988654781,
      "grad_norm": 1.1759933233261108,
      "learning_rate": 3.3441874797275384e-05,
      "loss": 2.7837,
      "step": 511700
    },
    {
      "epoch": 165.8995137763371,
      "grad_norm": 1.2279711961746216,
      "learning_rate": 3.3438631203373336e-05,
      "loss": 2.7713,
      "step": 511800
    },
    {
      "epoch": 165.93192868719612,
      "grad_norm": 1.017182469367981,
      "learning_rate": 3.3435387609471295e-05,
      "loss": 2.7721,
      "step": 511900
    },
    {
      "epoch": 165.9643435980551,
      "grad_norm": 0.9758544564247131,
      "learning_rate": 3.343214401556925e-05,
      "loss": 2.7617,
      "step": 512000
    },
    {
      "epoch": 165.9967585089141,
      "grad_norm": 1.178148865699768,
      "learning_rate": 3.3428900421667205e-05,
      "loss": 2.7674,
      "step": 512100
    },
    {
      "epoch": 166.0,
      "eval_bleu": 0.9991742868587951,
      "eval_loss": 3.96431827545166,
      "eval_runtime": 4.2736,
      "eval_samples_per_second": 115.126,
      "eval_steps_per_second": 1.872,
      "step": 512110
    },
    {
      "epoch": 166.0291734197731,
      "grad_norm": 1.1254916191101074,
      "learning_rate": 3.3425656827765164e-05,
      "loss": 2.7498,
      "step": 512200
    },
    {
      "epoch": 166.06158833063208,
      "grad_norm": 1.2325825691223145,
      "learning_rate": 3.342241323386312e-05,
      "loss": 2.7649,
      "step": 512300
    },
    {
      "epoch": 166.0940032414911,
      "grad_norm": 1.1408603191375732,
      "learning_rate": 3.341916963996108e-05,
      "loss": 2.773,
      "step": 512400
    },
    {
      "epoch": 166.12641815235008,
      "grad_norm": 1.313611388206482,
      "learning_rate": 3.341592604605904e-05,
      "loss": 2.7596,
      "step": 512500
    },
    {
      "epoch": 166.15883306320907,
      "grad_norm": 1.2760485410690308,
      "learning_rate": 3.341268245215699e-05,
      "loss": 2.7532,
      "step": 512600
    },
    {
      "epoch": 166.19124797406806,
      "grad_norm": 1.1264206171035767,
      "learning_rate": 3.340943885825495e-05,
      "loss": 2.7599,
      "step": 512700
    },
    {
      "epoch": 166.22366288492708,
      "grad_norm": 1.365756869316101,
      "learning_rate": 3.34061952643529e-05,
      "loss": 2.7511,
      "step": 512800
    },
    {
      "epoch": 166.25607779578607,
      "grad_norm": 1.2351535558700562,
      "learning_rate": 3.340295167045086e-05,
      "loss": 2.7676,
      "step": 512900
    },
    {
      "epoch": 166.28849270664506,
      "grad_norm": 1.1263985633850098,
      "learning_rate": 3.339970807654882e-05,
      "loss": 2.7671,
      "step": 513000
    },
    {
      "epoch": 166.32090761750405,
      "grad_norm": 1.2891392707824707,
      "learning_rate": 3.339646448264677e-05,
      "loss": 2.7694,
      "step": 513100
    },
    {
      "epoch": 166.35332252836304,
      "grad_norm": 1.2481305599212646,
      "learning_rate": 3.339322088874473e-05,
      "loss": 2.7559,
      "step": 513200
    },
    {
      "epoch": 166.38573743922205,
      "grad_norm": 1.1363731622695923,
      "learning_rate": 3.338997729484269e-05,
      "loss": 2.7644,
      "step": 513300
    },
    {
      "epoch": 166.41815235008104,
      "grad_norm": 1.4688481092453003,
      "learning_rate": 3.338673370094064e-05,
      "loss": 2.7755,
      "step": 513400
    },
    {
      "epoch": 166.45056726094003,
      "grad_norm": 1.203856110572815,
      "learning_rate": 3.33834901070386e-05,
      "loss": 2.7726,
      "step": 513500
    },
    {
      "epoch": 166.48298217179902,
      "grad_norm": 1.3477967977523804,
      "learning_rate": 3.338024651313656e-05,
      "loss": 2.7734,
      "step": 513600
    },
    {
      "epoch": 166.515397082658,
      "grad_norm": 1.033296823501587,
      "learning_rate": 3.337700291923451e-05,
      "loss": 2.7381,
      "step": 513700
    },
    {
      "epoch": 166.54781199351703,
      "grad_norm": 1.2107967138290405,
      "learning_rate": 3.337375932533247e-05,
      "loss": 2.7621,
      "step": 513800
    },
    {
      "epoch": 166.58022690437602,
      "grad_norm": 1.1924428939819336,
      "learning_rate": 3.337051573143042e-05,
      "loss": 2.7675,
      "step": 513900
    },
    {
      "epoch": 166.612641815235,
      "grad_norm": 1.0879796743392944,
      "learning_rate": 3.336727213752838e-05,
      "loss": 2.7711,
      "step": 514000
    },
    {
      "epoch": 166.645056726094,
      "grad_norm": 1.1644421815872192,
      "learning_rate": 3.336402854362634e-05,
      "loss": 2.7819,
      "step": 514100
    },
    {
      "epoch": 166.677471636953,
      "grad_norm": 1.063625693321228,
      "learning_rate": 3.336078494972429e-05,
      "loss": 2.7782,
      "step": 514200
    },
    {
      "epoch": 166.709886547812,
      "grad_norm": 1.2913498878479004,
      "learning_rate": 3.335754135582225e-05,
      "loss": 2.7593,
      "step": 514300
    },
    {
      "epoch": 166.742301458671,
      "grad_norm": 1.237044095993042,
      "learning_rate": 3.335429776192021e-05,
      "loss": 2.7784,
      "step": 514400
    },
    {
      "epoch": 166.77471636952998,
      "grad_norm": 1.078375220298767,
      "learning_rate": 3.335105416801817e-05,
      "loss": 2.768,
      "step": 514500
    },
    {
      "epoch": 166.80713128038897,
      "grad_norm": 1.235365629196167,
      "learning_rate": 3.334781057411612e-05,
      "loss": 2.7758,
      "step": 514600
    },
    {
      "epoch": 166.83954619124796,
      "grad_norm": 1.2675756216049194,
      "learning_rate": 3.334456698021408e-05,
      "loss": 2.754,
      "step": 514700
    },
    {
      "epoch": 166.87196110210698,
      "grad_norm": 1.1172447204589844,
      "learning_rate": 3.334132338631204e-05,
      "loss": 2.758,
      "step": 514800
    },
    {
      "epoch": 166.90437601296597,
      "grad_norm": 1.1080178022384644,
      "learning_rate": 3.3338079792409996e-05,
      "loss": 2.7683,
      "step": 514900
    },
    {
      "epoch": 166.93679092382496,
      "grad_norm": 1.0808700323104858,
      "learning_rate": 3.333483619850795e-05,
      "loss": 2.7768,
      "step": 515000
    },
    {
      "epoch": 166.96920583468395,
      "grad_norm": 1.2186594009399414,
      "learning_rate": 3.3331592604605907e-05,
      "loss": 2.7678,
      "step": 515100
    },
    {
      "epoch": 167.0,
      "eval_bleu": 1.0661084051136638,
      "eval_loss": 3.9565649032592773,
      "eval_runtime": 3.7835,
      "eval_samples_per_second": 130.037,
      "eval_steps_per_second": 2.114,
      "step": 515195
    },
    {
      "epoch": 167.00162074554294,
      "grad_norm": 1.0906535387039185,
      "learning_rate": 3.3328349010703865e-05,
      "loss": 2.779,
      "step": 515200
    },
    {
      "epoch": 167.03403565640195,
      "grad_norm": 1.2454997301101685,
      "learning_rate": 3.332510541680182e-05,
      "loss": 2.7515,
      "step": 515300
    },
    {
      "epoch": 167.06645056726094,
      "grad_norm": 1.174056053161621,
      "learning_rate": 3.3321861822899776e-05,
      "loss": 2.7688,
      "step": 515400
    },
    {
      "epoch": 167.09886547811993,
      "grad_norm": 1.2440786361694336,
      "learning_rate": 3.3318618228997735e-05,
      "loss": 2.7519,
      "step": 515500
    },
    {
      "epoch": 167.13128038897892,
      "grad_norm": 1.0644614696502686,
      "learning_rate": 3.331537463509569e-05,
      "loss": 2.7646,
      "step": 515600
    },
    {
      "epoch": 167.1636952998379,
      "grad_norm": 1.0257006883621216,
      "learning_rate": 3.3312131041193645e-05,
      "loss": 2.7557,
      "step": 515700
    },
    {
      "epoch": 167.19611021069693,
      "grad_norm": 1.145316481590271,
      "learning_rate": 3.330891988323062e-05,
      "loss": 2.7869,
      "step": 515800
    },
    {
      "epoch": 167.22852512155592,
      "grad_norm": 1.2059910297393799,
      "learning_rate": 3.330567628932858e-05,
      "loss": 2.7638,
      "step": 515900
    },
    {
      "epoch": 167.2609400324149,
      "grad_norm": 1.1068482398986816,
      "learning_rate": 3.3302432695426534e-05,
      "loss": 2.762,
      "step": 516000
    },
    {
      "epoch": 167.2933549432739,
      "grad_norm": 1.392621636390686,
      "learning_rate": 3.329918910152449e-05,
      "loss": 2.7682,
      "step": 516100
    },
    {
      "epoch": 167.32576985413291,
      "grad_norm": 1.2948373556137085,
      "learning_rate": 3.3295945507622445e-05,
      "loss": 2.7577,
      "step": 516200
    },
    {
      "epoch": 167.3581847649919,
      "grad_norm": 1.263249158859253,
      "learning_rate": 3.3292701913720403e-05,
      "loss": 2.7618,
      "step": 516300
    },
    {
      "epoch": 167.3905996758509,
      "grad_norm": 1.439741849899292,
      "learning_rate": 3.328945831981836e-05,
      "loss": 2.7595,
      "step": 516400
    },
    {
      "epoch": 167.42301458670988,
      "grad_norm": 1.1850645542144775,
      "learning_rate": 3.3286214725916314e-05,
      "loss": 2.7573,
      "step": 516500
    },
    {
      "epoch": 167.45542949756887,
      "grad_norm": 1.183361530303955,
      "learning_rate": 3.328297113201427e-05,
      "loss": 2.7878,
      "step": 516600
    },
    {
      "epoch": 167.4878444084279,
      "grad_norm": 1.22909414768219,
      "learning_rate": 3.327972753811223e-05,
      "loss": 2.7594,
      "step": 516700
    },
    {
      "epoch": 167.52025931928688,
      "grad_norm": 1.1216493844985962,
      "learning_rate": 3.3276483944210184e-05,
      "loss": 2.7617,
      "step": 516800
    },
    {
      "epoch": 167.55267423014587,
      "grad_norm": 1.2204687595367432,
      "learning_rate": 3.327324035030814e-05,
      "loss": 2.7541,
      "step": 516900
    },
    {
      "epoch": 167.58508914100486,
      "grad_norm": 1.2373450994491577,
      "learning_rate": 3.32699967564061e-05,
      "loss": 2.7743,
      "step": 517000
    },
    {
      "epoch": 167.61750405186385,
      "grad_norm": 0.9967663884162903,
      "learning_rate": 3.326675316250405e-05,
      "loss": 2.7909,
      "step": 517100
    },
    {
      "epoch": 167.64991896272286,
      "grad_norm": 1.082125186920166,
      "learning_rate": 3.326350956860201e-05,
      "loss": 2.7771,
      "step": 517200
    },
    {
      "epoch": 167.68233387358185,
      "grad_norm": 1.1828335523605347,
      "learning_rate": 3.3260265974699964e-05,
      "loss": 2.7479,
      "step": 517300
    },
    {
      "epoch": 167.71474878444084,
      "grad_norm": 1.1666637659072876,
      "learning_rate": 3.325702238079792e-05,
      "loss": 2.7482,
      "step": 517400
    },
    {
      "epoch": 167.74716369529983,
      "grad_norm": 1.1447283029556274,
      "learning_rate": 3.325377878689588e-05,
      "loss": 2.788,
      "step": 517500
    },
    {
      "epoch": 167.77957860615882,
      "grad_norm": 1.302106261253357,
      "learning_rate": 3.325053519299384e-05,
      "loss": 2.762,
      "step": 517600
    },
    {
      "epoch": 167.81199351701784,
      "grad_norm": 1.3224214315414429,
      "learning_rate": 3.32472915990918e-05,
      "loss": 2.7657,
      "step": 517700
    },
    {
      "epoch": 167.84440842787683,
      "grad_norm": 1.1153886318206787,
      "learning_rate": 3.324408044112877e-05,
      "loss": 2.7591,
      "step": 517800
    },
    {
      "epoch": 167.87682333873582,
      "grad_norm": 1.0935707092285156,
      "learning_rate": 3.324083684722673e-05,
      "loss": 2.7643,
      "step": 517900
    },
    {
      "epoch": 167.9092382495948,
      "grad_norm": 1.2159579992294312,
      "learning_rate": 3.323759325332468e-05,
      "loss": 2.7658,
      "step": 518000
    },
    {
      "epoch": 167.9416531604538,
      "grad_norm": 1.1100934743881226,
      "learning_rate": 3.323434965942264e-05,
      "loss": 2.7704,
      "step": 518100
    },
    {
      "epoch": 167.97406807131281,
      "grad_norm": 1.272452473640442,
      "learning_rate": 3.32311060655206e-05,
      "loss": 2.7508,
      "step": 518200
    },
    {
      "epoch": 168.0,
      "eval_bleu": 1.2602855313874206,
      "eval_loss": 3.9649221897125244,
      "eval_runtime": 4.0332,
      "eval_samples_per_second": 121.989,
      "eval_steps_per_second": 1.984,
      "step": 518280
    },
    {
      "epoch": 168.0064829821718,
      "grad_norm": 1.1231238842010498,
      "learning_rate": 3.322786247161856e-05,
      "loss": 2.7813,
      "step": 518300
    },
    {
      "epoch": 168.0388978930308,
      "grad_norm": 1.097157597541809,
      "learning_rate": 3.3224618877716515e-05,
      "loss": 2.7759,
      "step": 518400
    },
    {
      "epoch": 168.07131280388978,
      "grad_norm": 1.3427965641021729,
      "learning_rate": 3.322137528381447e-05,
      "loss": 2.7714,
      "step": 518500
    },
    {
      "epoch": 168.10372771474877,
      "grad_norm": 1.000043272972107,
      "learning_rate": 3.3218131689912426e-05,
      "loss": 2.7498,
      "step": 518600
    },
    {
      "epoch": 168.1361426256078,
      "grad_norm": 1.2725955247879028,
      "learning_rate": 3.3214888096010385e-05,
      "loss": 2.7563,
      "step": 518700
    },
    {
      "epoch": 168.16855753646678,
      "grad_norm": 1.130880355834961,
      "learning_rate": 3.321164450210834e-05,
      "loss": 2.7546,
      "step": 518800
    },
    {
      "epoch": 168.20097244732577,
      "grad_norm": 1.1276416778564453,
      "learning_rate": 3.3208400908206296e-05,
      "loss": 2.7654,
      "step": 518900
    },
    {
      "epoch": 168.23338735818476,
      "grad_norm": 1.3714361190795898,
      "learning_rate": 3.3205157314304254e-05,
      "loss": 2.7312,
      "step": 519000
    },
    {
      "epoch": 168.26580226904375,
      "grad_norm": 1.3834245204925537,
      "learning_rate": 3.3201913720402206e-05,
      "loss": 2.778,
      "step": 519100
    },
    {
      "epoch": 168.29821717990276,
      "grad_norm": 0.9596172571182251,
      "learning_rate": 3.3198670126500165e-05,
      "loss": 2.7633,
      "step": 519200
    },
    {
      "epoch": 168.33063209076175,
      "grad_norm": 1.2254008054733276,
      "learning_rate": 3.3195426532598124e-05,
      "loss": 2.7509,
      "step": 519300
    },
    {
      "epoch": 168.36304700162074,
      "grad_norm": 1.1729280948638916,
      "learning_rate": 3.3192182938696076e-05,
      "loss": 2.7664,
      "step": 519400
    },
    {
      "epoch": 168.39546191247973,
      "grad_norm": 1.0887720584869385,
      "learning_rate": 3.3188939344794035e-05,
      "loss": 2.7665,
      "step": 519500
    },
    {
      "epoch": 168.42787682333875,
      "grad_norm": 1.0808748006820679,
      "learning_rate": 3.3185695750891986e-05,
      "loss": 2.7578,
      "step": 519600
    },
    {
      "epoch": 168.46029173419774,
      "grad_norm": 1.223649501800537,
      "learning_rate": 3.3182452156989945e-05,
      "loss": 2.7499,
      "step": 519700
    },
    {
      "epoch": 168.49270664505673,
      "grad_norm": 1.21547269821167,
      "learning_rate": 3.317927343496595e-05,
      "loss": 2.7803,
      "step": 519800
    },
    {
      "epoch": 168.52512155591572,
      "grad_norm": 1.0090885162353516,
      "learning_rate": 3.31760298410639e-05,
      "loss": 2.7293,
      "step": 519900
    },
    {
      "epoch": 168.5575364667747,
      "grad_norm": 1.1015762090682983,
      "learning_rate": 3.317278624716186e-05,
      "loss": 2.7549,
      "step": 520000
    },
    {
      "epoch": 168.58995137763372,
      "grad_norm": 1.1614601612091064,
      "learning_rate": 3.316954265325981e-05,
      "loss": 2.762,
      "step": 520100
    },
    {
      "epoch": 168.6223662884927,
      "grad_norm": 1.024348497390747,
      "learning_rate": 3.316629905935777e-05,
      "loss": 2.7415,
      "step": 520200
    },
    {
      "epoch": 168.6547811993517,
      "grad_norm": 1.0343430042266846,
      "learning_rate": 3.316305546545573e-05,
      "loss": 2.7478,
      "step": 520300
    },
    {
      "epoch": 168.6871961102107,
      "grad_norm": 0.9994091987609863,
      "learning_rate": 3.315981187155368e-05,
      "loss": 2.7637,
      "step": 520400
    },
    {
      "epoch": 168.71961102106968,
      "grad_norm": 0.9376221299171448,
      "learning_rate": 3.315656827765164e-05,
      "loss": 2.7663,
      "step": 520500
    },
    {
      "epoch": 168.7520259319287,
      "grad_norm": 1.2817214727401733,
      "learning_rate": 3.31533246837496e-05,
      "loss": 2.7737,
      "step": 520600
    },
    {
      "epoch": 168.7844408427877,
      "grad_norm": 1.2161980867385864,
      "learning_rate": 3.315008108984755e-05,
      "loss": 2.7804,
      "step": 520700
    },
    {
      "epoch": 168.81685575364668,
      "grad_norm": 1.2482494115829468,
      "learning_rate": 3.314683749594551e-05,
      "loss": 2.7425,
      "step": 520800
    },
    {
      "epoch": 168.84927066450567,
      "grad_norm": 1.401917576789856,
      "learning_rate": 3.314359390204347e-05,
      "loss": 2.7703,
      "step": 520900
    },
    {
      "epoch": 168.88168557536466,
      "grad_norm": 1.0803545713424683,
      "learning_rate": 3.314035030814142e-05,
      "loss": 2.7688,
      "step": 521000
    },
    {
      "epoch": 168.91410048622367,
      "grad_norm": 1.1410375833511353,
      "learning_rate": 3.313710671423938e-05,
      "loss": 2.7588,
      "step": 521100
    },
    {
      "epoch": 168.94651539708266,
      "grad_norm": 1.1162142753601074,
      "learning_rate": 3.313386312033733e-05,
      "loss": 2.7858,
      "step": 521200
    },
    {
      "epoch": 168.97893030794165,
      "grad_norm": 1.5647876262664795,
      "learning_rate": 3.313061952643529e-05,
      "loss": 2.7825,
      "step": 521300
    },
    {
      "epoch": 169.0,
      "eval_bleu": 1.0712876065484176,
      "eval_loss": 3.967297077178955,
      "eval_runtime": 3.8954,
      "eval_samples_per_second": 126.304,
      "eval_steps_per_second": 2.054,
      "step": 521365
    },
    {
      "epoch": 169.01134521880064,
      "grad_norm": 1.165574550628662,
      "learning_rate": 3.312737593253325e-05,
      "loss": 2.7498,
      "step": 521400
    },
    {
      "epoch": 169.04376012965963,
      "grad_norm": 1.3193564414978027,
      "learning_rate": 3.31241323386312e-05,
      "loss": 2.7433,
      "step": 521500
    },
    {
      "epoch": 169.07617504051865,
      "grad_norm": 1.2040235996246338,
      "learning_rate": 3.312088874472916e-05,
      "loss": 2.759,
      "step": 521600
    },
    {
      "epoch": 169.10858995137764,
      "grad_norm": 1.09091055393219,
      "learning_rate": 3.311764515082712e-05,
      "loss": 2.7651,
      "step": 521700
    },
    {
      "epoch": 169.14100486223663,
      "grad_norm": 1.0606991052627563,
      "learning_rate": 3.3114401556925076e-05,
      "loss": 2.758,
      "step": 521800
    },
    {
      "epoch": 169.17341977309562,
      "grad_norm": 1.1712896823883057,
      "learning_rate": 3.3111157963023035e-05,
      "loss": 2.7504,
      "step": 521900
    },
    {
      "epoch": 169.2058346839546,
      "grad_norm": 1.4150723218917847,
      "learning_rate": 3.310791436912099e-05,
      "loss": 2.7488,
      "step": 522000
    },
    {
      "epoch": 169.23824959481362,
      "grad_norm": 1.3041332960128784,
      "learning_rate": 3.3104670775218946e-05,
      "loss": 2.7741,
      "step": 522100
    },
    {
      "epoch": 169.2706645056726,
      "grad_norm": 1.3962780237197876,
      "learning_rate": 3.3101427181316904e-05,
      "loss": 2.7394,
      "step": 522200
    },
    {
      "epoch": 169.3030794165316,
      "grad_norm": 1.1684672832489014,
      "learning_rate": 3.3098183587414856e-05,
      "loss": 2.7525,
      "step": 522300
    },
    {
      "epoch": 169.3354943273906,
      "grad_norm": 1.130739688873291,
      "learning_rate": 3.3094939993512815e-05,
      "loss": 2.7269,
      "step": 522400
    },
    {
      "epoch": 169.36790923824958,
      "grad_norm": 1.2026609182357788,
      "learning_rate": 3.3091696399610774e-05,
      "loss": 2.7618,
      "step": 522500
    },
    {
      "epoch": 169.4003241491086,
      "grad_norm": 1.1704914569854736,
      "learning_rate": 3.3088452805708726e-05,
      "loss": 2.7628,
      "step": 522600
    },
    {
      "epoch": 169.4327390599676,
      "grad_norm": 1.2491334676742554,
      "learning_rate": 3.3085209211806685e-05,
      "loss": 2.7489,
      "step": 522700
    },
    {
      "epoch": 169.46515397082658,
      "grad_norm": 1.0080780982971191,
      "learning_rate": 3.308196561790464e-05,
      "loss": 2.7631,
      "step": 522800
    },
    {
      "epoch": 169.49756888168557,
      "grad_norm": 1.1934361457824707,
      "learning_rate": 3.3078722024002595e-05,
      "loss": 2.7592,
      "step": 522900
    },
    {
      "epoch": 169.52998379254458,
      "grad_norm": 1.1258087158203125,
      "learning_rate": 3.3075478430100554e-05,
      "loss": 2.7809,
      "step": 523000
    },
    {
      "epoch": 169.56239870340357,
      "grad_norm": 1.0377238988876343,
      "learning_rate": 3.3072234836198506e-05,
      "loss": 2.7638,
      "step": 523100
    },
    {
      "epoch": 169.59481361426256,
      "grad_norm": 1.262830376625061,
      "learning_rate": 3.3068991242296465e-05,
      "loss": 2.7797,
      "step": 523200
    },
    {
      "epoch": 169.62722852512155,
      "grad_norm": 1.1418383121490479,
      "learning_rate": 3.3065747648394424e-05,
      "loss": 2.7624,
      "step": 523300
    },
    {
      "epoch": 169.65964343598054,
      "grad_norm": 1.0840092897415161,
      "learning_rate": 3.3062504054492376e-05,
      "loss": 2.7656,
      "step": 523400
    },
    {
      "epoch": 169.69205834683956,
      "grad_norm": 1.202232003211975,
      "learning_rate": 3.3059260460590334e-05,
      "loss": 2.7477,
      "step": 523500
    },
    {
      "epoch": 169.72447325769855,
      "grad_norm": 1.2688335180282593,
      "learning_rate": 3.305601686668829e-05,
      "loss": 2.7628,
      "step": 523600
    },
    {
      "epoch": 169.75688816855754,
      "grad_norm": 1.1491446495056152,
      "learning_rate": 3.3052773272786245e-05,
      "loss": 2.7632,
      "step": 523700
    },
    {
      "epoch": 169.78930307941653,
      "grad_norm": 1.1098427772521973,
      "learning_rate": 3.3049529678884204e-05,
      "loss": 2.7585,
      "step": 523800
    },
    {
      "epoch": 169.82171799027552,
      "grad_norm": 1.297227144241333,
      "learning_rate": 3.304631852092118e-05,
      "loss": 2.7489,
      "step": 523900
    },
    {
      "epoch": 169.85413290113453,
      "grad_norm": 1.457557201385498,
      "learning_rate": 3.304307492701914e-05,
      "loss": 2.761,
      "step": 524000
    },
    {
      "epoch": 169.88654781199352,
      "grad_norm": 1.0642977952957153,
      "learning_rate": 3.303983133311709e-05,
      "loss": 2.7624,
      "step": 524100
    },
    {
      "epoch": 169.9189627228525,
      "grad_norm": 1.2390190362930298,
      "learning_rate": 3.303662017515407e-05,
      "loss": 2.8021,
      "step": 524200
    },
    {
      "epoch": 169.9513776337115,
      "grad_norm": 1.1716760396957397,
      "learning_rate": 3.303337658125203e-05,
      "loss": 2.7566,
      "step": 524300
    },
    {
      "epoch": 169.9837925445705,
      "grad_norm": 1.195861577987671,
      "learning_rate": 3.303013298734999e-05,
      "loss": 2.7676,
      "step": 524400
    },
    {
      "epoch": 170.0,
      "eval_bleu": 1.1923699970322637,
      "eval_loss": 3.9681291580200195,
      "eval_runtime": 4.0095,
      "eval_samples_per_second": 122.71,
      "eval_steps_per_second": 1.995,
      "step": 524450
    },
    {
      "epoch": 170.0162074554295,
      "grad_norm": 1.2430132627487183,
      "learning_rate": 3.302688939344794e-05,
      "loss": 2.7537,
      "step": 524500
    },
    {
      "epoch": 170.0486223662885,
      "grad_norm": 1.1701290607452393,
      "learning_rate": 3.30236457995459e-05,
      "loss": 2.7434,
      "step": 524600
    },
    {
      "epoch": 170.0810372771475,
      "grad_norm": 1.2718600034713745,
      "learning_rate": 3.302040220564385e-05,
      "loss": 2.7385,
      "step": 524700
    },
    {
      "epoch": 170.11345218800648,
      "grad_norm": 1.1788227558135986,
      "learning_rate": 3.301715861174181e-05,
      "loss": 2.7298,
      "step": 524800
    },
    {
      "epoch": 170.14586709886547,
      "grad_norm": 1.0421971082687378,
      "learning_rate": 3.301391501783977e-05,
      "loss": 2.7724,
      "step": 524900
    },
    {
      "epoch": 170.17828200972448,
      "grad_norm": 1.1811115741729736,
      "learning_rate": 3.301067142393772e-05,
      "loss": 2.7667,
      "step": 525000
    },
    {
      "epoch": 170.21069692058347,
      "grad_norm": 1.1794260740280151,
      "learning_rate": 3.300742783003568e-05,
      "loss": 2.7648,
      "step": 525100
    },
    {
      "epoch": 170.24311183144246,
      "grad_norm": 1.2737592458724976,
      "learning_rate": 3.300418423613364e-05,
      "loss": 2.746,
      "step": 525200
    },
    {
      "epoch": 170.27552674230145,
      "grad_norm": 1.292336106300354,
      "learning_rate": 3.3000940642231596e-05,
      "loss": 2.7791,
      "step": 525300
    },
    {
      "epoch": 170.30794165316044,
      "grad_norm": 1.259831428527832,
      "learning_rate": 3.299769704832955e-05,
      "loss": 2.753,
      "step": 525400
    },
    {
      "epoch": 170.34035656401946,
      "grad_norm": 1.060091495513916,
      "learning_rate": 3.2994453454427507e-05,
      "loss": 2.7668,
      "step": 525500
    },
    {
      "epoch": 170.37277147487845,
      "grad_norm": 1.1098226308822632,
      "learning_rate": 3.2991209860525465e-05,
      "loss": 2.7522,
      "step": 525600
    },
    {
      "epoch": 170.40518638573744,
      "grad_norm": 1.1782830953598022,
      "learning_rate": 3.2987966266623424e-05,
      "loss": 2.7441,
      "step": 525700
    },
    {
      "epoch": 170.43760129659643,
      "grad_norm": 1.1781123876571655,
      "learning_rate": 3.2984722672721376e-05,
      "loss": 2.7554,
      "step": 525800
    },
    {
      "epoch": 170.47001620745542,
      "grad_norm": 1.2420164346694946,
      "learning_rate": 3.2981479078819335e-05,
      "loss": 2.7579,
      "step": 525900
    },
    {
      "epoch": 170.50243111831443,
      "grad_norm": 1.0882277488708496,
      "learning_rate": 3.2978235484917293e-05,
      "loss": 2.7641,
      "step": 526000
    },
    {
      "epoch": 170.53484602917342,
      "grad_norm": 1.2845935821533203,
      "learning_rate": 3.2974991891015245e-05,
      "loss": 2.7606,
      "step": 526100
    },
    {
      "epoch": 170.5672609400324,
      "grad_norm": 1.1752879619598389,
      "learning_rate": 3.2971748297113204e-05,
      "loss": 2.7543,
      "step": 526200
    },
    {
      "epoch": 170.5996758508914,
      "grad_norm": 1.3194904327392578,
      "learning_rate": 3.296850470321116e-05,
      "loss": 2.7661,
      "step": 526300
    },
    {
      "epoch": 170.63209076175042,
      "grad_norm": 1.2926114797592163,
      "learning_rate": 3.2965261109309115e-05,
      "loss": 2.7657,
      "step": 526400
    },
    {
      "epoch": 170.6645056726094,
      "grad_norm": 1.147534728050232,
      "learning_rate": 3.296204995134609e-05,
      "loss": 2.7471,
      "step": 526500
    },
    {
      "epoch": 170.6969205834684,
      "grad_norm": 1.2167785167694092,
      "learning_rate": 3.295880635744405e-05,
      "loss": 2.7458,
      "step": 526600
    },
    {
      "epoch": 170.7293354943274,
      "grad_norm": 1.0638030767440796,
      "learning_rate": 3.295556276354201e-05,
      "loss": 2.747,
      "step": 526700
    },
    {
      "epoch": 170.76175040518638,
      "grad_norm": 1.1715337038040161,
      "learning_rate": 3.295231916963996e-05,
      "loss": 2.765,
      "step": 526800
    },
    {
      "epoch": 170.7941653160454,
      "grad_norm": 1.135608196258545,
      "learning_rate": 3.294907557573792e-05,
      "loss": 2.7738,
      "step": 526900
    },
    {
      "epoch": 170.82658022690438,
      "grad_norm": 1.2440705299377441,
      "learning_rate": 3.294583198183588e-05,
      "loss": 2.76,
      "step": 527000
    },
    {
      "epoch": 170.85899513776337,
      "grad_norm": 1.182482123374939,
      "learning_rate": 3.294258838793383e-05,
      "loss": 2.7657,
      "step": 527100
    },
    {
      "epoch": 170.89141004862236,
      "grad_norm": 1.2256957292556763,
      "learning_rate": 3.293934479403179e-05,
      "loss": 2.766,
      "step": 527200
    },
    {
      "epoch": 170.92382495948135,
      "grad_norm": 1.2127294540405273,
      "learning_rate": 3.293610120012974e-05,
      "loss": 2.7551,
      "step": 527300
    },
    {
      "epoch": 170.95623987034037,
      "grad_norm": 1.234728217124939,
      "learning_rate": 3.29328576062277e-05,
      "loss": 2.7727,
      "step": 527400
    },
    {
      "epoch": 170.98865478119936,
      "grad_norm": 1.1378158330917358,
      "learning_rate": 3.292961401232566e-05,
      "loss": 2.7678,
      "step": 527500
    },
    {
      "epoch": 171.0,
      "eval_bleu": 1.1862525642495048,
      "eval_loss": 3.9629416465759277,
      "eval_runtime": 3.9553,
      "eval_samples_per_second": 124.389,
      "eval_steps_per_second": 2.023,
      "step": 527535
    },
    {
      "epoch": 171.02106969205835,
      "grad_norm": 0.9736645817756653,
      "learning_rate": 3.292637041842361e-05,
      "loss": 2.7643,
      "step": 527600
    },
    {
      "epoch": 171.05348460291734,
      "grad_norm": 1.3160582780838013,
      "learning_rate": 3.292312682452157e-05,
      "loss": 2.7564,
      "step": 527700
    },
    {
      "epoch": 171.08589951377633,
      "grad_norm": 1.16706383228302,
      "learning_rate": 3.291988323061953e-05,
      "loss": 2.739,
      "step": 527800
    },
    {
      "epoch": 171.11831442463534,
      "grad_norm": 1.0485241413116455,
      "learning_rate": 3.291663963671748e-05,
      "loss": 2.7396,
      "step": 527900
    },
    {
      "epoch": 171.15072933549433,
      "grad_norm": 1.0284568071365356,
      "learning_rate": 3.291339604281544e-05,
      "loss": 2.7452,
      "step": 528000
    },
    {
      "epoch": 171.18314424635332,
      "grad_norm": 1.1074618101119995,
      "learning_rate": 3.291015244891339e-05,
      "loss": 2.751,
      "step": 528100
    },
    {
      "epoch": 171.2155591572123,
      "grad_norm": 1.1605815887451172,
      "learning_rate": 3.290690885501135e-05,
      "loss": 2.777,
      "step": 528200
    },
    {
      "epoch": 171.2479740680713,
      "grad_norm": 1.2534257173538208,
      "learning_rate": 3.290369769704833e-05,
      "loss": 2.7459,
      "step": 528300
    },
    {
      "epoch": 171.28038897893032,
      "grad_norm": 1.187184453010559,
      "learning_rate": 3.290045410314629e-05,
      "loss": 2.7459,
      "step": 528400
    },
    {
      "epoch": 171.3128038897893,
      "grad_norm": 1.2068607807159424,
      "learning_rate": 3.289721050924424e-05,
      "loss": 2.7642,
      "step": 528500
    },
    {
      "epoch": 171.3452188006483,
      "grad_norm": 1.0691598653793335,
      "learning_rate": 3.28939669153422e-05,
      "loss": 2.7466,
      "step": 528600
    },
    {
      "epoch": 171.3776337115073,
      "grad_norm": 1.1842695474624634,
      "learning_rate": 3.289072332144016e-05,
      "loss": 2.761,
      "step": 528700
    },
    {
      "epoch": 171.41004862236628,
      "grad_norm": 1.197702407836914,
      "learning_rate": 3.288747972753811e-05,
      "loss": 2.7596,
      "step": 528800
    },
    {
      "epoch": 171.4424635332253,
      "grad_norm": 1.0500128269195557,
      "learning_rate": 3.288423613363607e-05,
      "loss": 2.7655,
      "step": 528900
    },
    {
      "epoch": 171.47487844408428,
      "grad_norm": 1.2908258438110352,
      "learning_rate": 3.2880992539734026e-05,
      "loss": 2.7438,
      "step": 529000
    },
    {
      "epoch": 171.50729335494327,
      "grad_norm": 0.9840950965881348,
      "learning_rate": 3.2877748945831985e-05,
      "loss": 2.7533,
      "step": 529100
    },
    {
      "epoch": 171.53970826580226,
      "grad_norm": 1.239725947380066,
      "learning_rate": 3.2874505351929944e-05,
      "loss": 2.7648,
      "step": 529200
    },
    {
      "epoch": 171.57212317666125,
      "grad_norm": 1.1497515439987183,
      "learning_rate": 3.28712617580279e-05,
      "loss": 2.7444,
      "step": 529300
    },
    {
      "epoch": 171.60453808752027,
      "grad_norm": 1.1494717597961426,
      "learning_rate": 3.2868018164125854e-05,
      "loss": 2.7755,
      "step": 529400
    },
    {
      "epoch": 171.63695299837926,
      "grad_norm": 1.174626111984253,
      "learning_rate": 3.286477457022381e-05,
      "loss": 2.7663,
      "step": 529500
    },
    {
      "epoch": 171.66936790923825,
      "grad_norm": 1.1877185106277466,
      "learning_rate": 3.2861530976321765e-05,
      "loss": 2.753,
      "step": 529600
    },
    {
      "epoch": 171.70178282009724,
      "grad_norm": 1.3683130741119385,
      "learning_rate": 3.2858287382419724e-05,
      "loss": 2.7497,
      "step": 529700
    },
    {
      "epoch": 171.73419773095625,
      "grad_norm": 1.407679796218872,
      "learning_rate": 3.285504378851768e-05,
      "loss": 2.7664,
      "step": 529800
    },
    {
      "epoch": 171.76661264181524,
      "grad_norm": 1.2264235019683838,
      "learning_rate": 3.2851800194615634e-05,
      "loss": 2.7523,
      "step": 529900
    },
    {
      "epoch": 171.79902755267423,
      "grad_norm": 1.182289481163025,
      "learning_rate": 3.284855660071359e-05,
      "loss": 2.7578,
      "step": 530000
    },
    {
      "epoch": 171.83144246353322,
      "grad_norm": 1.335977554321289,
      "learning_rate": 3.284531300681155e-05,
      "loss": 2.7586,
      "step": 530100
    },
    {
      "epoch": 171.8638573743922,
      "grad_norm": 1.1973183155059814,
      "learning_rate": 3.2842069412909504e-05,
      "loss": 2.7776,
      "step": 530200
    },
    {
      "epoch": 171.89627228525123,
      "grad_norm": 1.1879926919937134,
      "learning_rate": 3.283882581900746e-05,
      "loss": 2.7518,
      "step": 530300
    },
    {
      "epoch": 171.92868719611022,
      "grad_norm": 1.1621155738830566,
      "learning_rate": 3.2835582225105415e-05,
      "loss": 2.7585,
      "step": 530400
    },
    {
      "epoch": 171.9611021069692,
      "grad_norm": 1.067285418510437,
      "learning_rate": 3.28323710671424e-05,
      "loss": 2.755,
      "step": 530500
    },
    {
      "epoch": 171.9935170178282,
      "grad_norm": 1.2603062391281128,
      "learning_rate": 3.282912747324035e-05,
      "loss": 2.787,
      "step": 530600
    },
    {
      "epoch": 172.0,
      "eval_bleu": 1.218732531742786,
      "eval_loss": 3.9666569232940674,
      "eval_runtime": 3.9949,
      "eval_samples_per_second": 123.156,
      "eval_steps_per_second": 2.003,
      "step": 530620
    },
    {
      "epoch": 172.02593192868719,
      "grad_norm": 1.184648871421814,
      "learning_rate": 3.282588387933831e-05,
      "loss": 2.7495,
      "step": 530700
    },
    {
      "epoch": 172.0583468395462,
      "grad_norm": 1.132544994354248,
      "learning_rate": 3.282264028543626e-05,
      "loss": 2.7268,
      "step": 530800
    },
    {
      "epoch": 172.0907617504052,
      "grad_norm": 1.126116394996643,
      "learning_rate": 3.281939669153422e-05,
      "loss": 2.7635,
      "step": 530900
    },
    {
      "epoch": 172.12317666126418,
      "grad_norm": 1.3032010793685913,
      "learning_rate": 3.281615309763218e-05,
      "loss": 2.7583,
      "step": 531000
    },
    {
      "epoch": 172.15559157212317,
      "grad_norm": 1.0720961093902588,
      "learning_rate": 3.281290950373013e-05,
      "loss": 2.7326,
      "step": 531100
    },
    {
      "epoch": 172.18800648298216,
      "grad_norm": 1.3127343654632568,
      "learning_rate": 3.280966590982809e-05,
      "loss": 2.758,
      "step": 531200
    },
    {
      "epoch": 172.22042139384118,
      "grad_norm": 1.0692214965820312,
      "learning_rate": 3.280642231592605e-05,
      "loss": 2.7616,
      "step": 531300
    },
    {
      "epoch": 172.25283630470017,
      "grad_norm": 1.1444847583770752,
      "learning_rate": 3.2803178722024e-05,
      "loss": 2.7506,
      "step": 531400
    },
    {
      "epoch": 172.28525121555916,
      "grad_norm": 1.0653403997421265,
      "learning_rate": 3.279993512812196e-05,
      "loss": 2.7287,
      "step": 531500
    },
    {
      "epoch": 172.31766612641815,
      "grad_norm": 1.218814730644226,
      "learning_rate": 3.279669153421992e-05,
      "loss": 2.7371,
      "step": 531600
    },
    {
      "epoch": 172.35008103727714,
      "grad_norm": 1.2581455707550049,
      "learning_rate": 3.279344794031787e-05,
      "loss": 2.7637,
      "step": 531700
    },
    {
      "epoch": 172.38249594813615,
      "grad_norm": 1.22402024269104,
      "learning_rate": 3.279020434641583e-05,
      "loss": 2.7527,
      "step": 531800
    },
    {
      "epoch": 172.41491085899514,
      "grad_norm": 1.036104679107666,
      "learning_rate": 3.278696075251379e-05,
      "loss": 2.7506,
      "step": 531900
    },
    {
      "epoch": 172.44732576985413,
      "grad_norm": 1.0037904977798462,
      "learning_rate": 3.278371715861174e-05,
      "loss": 2.7437,
      "step": 532000
    },
    {
      "epoch": 172.47974068071312,
      "grad_norm": 1.11306631565094,
      "learning_rate": 3.27804735647097e-05,
      "loss": 2.7588,
      "step": 532100
    },
    {
      "epoch": 172.5121555915721,
      "grad_norm": 1.3955219984054565,
      "learning_rate": 3.277722997080766e-05,
      "loss": 2.7555,
      "step": 532200
    },
    {
      "epoch": 172.54457050243113,
      "grad_norm": 1.3824703693389893,
      "learning_rate": 3.2773986376905616e-05,
      "loss": 2.758,
      "step": 532300
    },
    {
      "epoch": 172.57698541329012,
      "grad_norm": 1.2396130561828613,
      "learning_rate": 3.2770742783003575e-05,
      "loss": 2.7836,
      "step": 532400
    },
    {
      "epoch": 172.6094003241491,
      "grad_norm": 1.1855671405792236,
      "learning_rate": 3.276749918910153e-05,
      "loss": 2.7576,
      "step": 532500
    },
    {
      "epoch": 172.6418152350081,
      "grad_norm": 1.3452165126800537,
      "learning_rate": 3.2764255595199485e-05,
      "loss": 2.7516,
      "step": 532600
    },
    {
      "epoch": 172.67423014586709,
      "grad_norm": 1.3359062671661377,
      "learning_rate": 3.276101200129744e-05,
      "loss": 2.7513,
      "step": 532700
    },
    {
      "epoch": 172.7066450567261,
      "grad_norm": 1.197618007659912,
      "learning_rate": 3.2757768407395396e-05,
      "loss": 2.7362,
      "step": 532800
    },
    {
      "epoch": 172.7390599675851,
      "grad_norm": 1.2101635932922363,
      "learning_rate": 3.2754524813493355e-05,
      "loss": 2.762,
      "step": 532900
    },
    {
      "epoch": 172.77147487844408,
      "grad_norm": 1.231170654296875,
      "learning_rate": 3.275128121959131e-05,
      "loss": 2.7588,
      "step": 533000
    },
    {
      "epoch": 172.80388978930307,
      "grad_norm": 1.14462149143219,
      "learning_rate": 3.2748037625689266e-05,
      "loss": 2.7636,
      "step": 533100
    },
    {
      "epoch": 172.8363047001621,
      "grad_norm": 1.3405221700668335,
      "learning_rate": 3.2744794031787224e-05,
      "loss": 2.7667,
      "step": 533200
    },
    {
      "epoch": 172.86871961102108,
      "grad_norm": 1.1712911128997803,
      "learning_rate": 3.2741550437885176e-05,
      "loss": 2.7659,
      "step": 533300
    },
    {
      "epoch": 172.90113452188007,
      "grad_norm": 1.547755479812622,
      "learning_rate": 3.2738306843983135e-05,
      "loss": 2.7652,
      "step": 533400
    },
    {
      "epoch": 172.93354943273906,
      "grad_norm": 1.300185203552246,
      "learning_rate": 3.2735063250081094e-05,
      "loss": 2.7491,
      "step": 533500
    },
    {
      "epoch": 172.96596434359805,
      "grad_norm": 1.1971758604049683,
      "learning_rate": 3.2731819656179046e-05,
      "loss": 2.7609,
      "step": 533600
    },
    {
      "epoch": 172.99837925445706,
      "grad_norm": 1.147243618965149,
      "learning_rate": 3.2728576062277004e-05,
      "loss": 2.7628,
      "step": 533700
    },
    {
      "epoch": 173.0,
      "eval_bleu": 1.0583735552895974,
      "eval_loss": 3.971653938293457,
      "eval_runtime": 4.2937,
      "eval_samples_per_second": 114.588,
      "eval_steps_per_second": 1.863,
      "step": 533705
    },
    {
      "epoch": 173.03079416531605,
      "grad_norm": 1.2258397340774536,
      "learning_rate": 3.2725332468374956e-05,
      "loss": 2.7358,
      "step": 533800
    },
    {
      "epoch": 173.06320907617504,
      "grad_norm": 1.3096795082092285,
      "learning_rate": 3.2722088874472915e-05,
      "loss": 2.7241,
      "step": 533900
    },
    {
      "epoch": 173.09562398703403,
      "grad_norm": 1.0935735702514648,
      "learning_rate": 3.2718845280570874e-05,
      "loss": 2.7483,
      "step": 534000
    },
    {
      "epoch": 173.12803889789302,
      "grad_norm": 1.112894892692566,
      "learning_rate": 3.2715601686668826e-05,
      "loss": 2.7538,
      "step": 534100
    },
    {
      "epoch": 173.16045380875204,
      "grad_norm": 1.2650974988937378,
      "learning_rate": 3.2712358092766785e-05,
      "loss": 2.7538,
      "step": 534200
    },
    {
      "epoch": 173.19286871961103,
      "grad_norm": 1.3950276374816895,
      "learning_rate": 3.270911449886474e-05,
      "loss": 2.7525,
      "step": 534300
    },
    {
      "epoch": 173.22528363047002,
      "grad_norm": 1.1441638469696045,
      "learning_rate": 3.27058709049627e-05,
      "loss": 2.7541,
      "step": 534400
    },
    {
      "epoch": 173.257698541329,
      "grad_norm": 1.1917951107025146,
      "learning_rate": 3.2702627311060654e-05,
      "loss": 2.762,
      "step": 534500
    },
    {
      "epoch": 173.290113452188,
      "grad_norm": 1.0088621377944946,
      "learning_rate": 3.269938371715861e-05,
      "loss": 2.7413,
      "step": 534600
    },
    {
      "epoch": 173.322528363047,
      "grad_norm": 1.1637227535247803,
      "learning_rate": 3.269614012325657e-05,
      "loss": 2.7392,
      "step": 534700
    },
    {
      "epoch": 173.354943273906,
      "grad_norm": 1.0094547271728516,
      "learning_rate": 3.269289652935453e-05,
      "loss": 2.7472,
      "step": 534800
    },
    {
      "epoch": 173.387358184765,
      "grad_norm": 1.0051624774932861,
      "learning_rate": 3.268965293545248e-05,
      "loss": 2.7524,
      "step": 534900
    },
    {
      "epoch": 173.41977309562398,
      "grad_norm": 1.073358178138733,
      "learning_rate": 3.268640934155044e-05,
      "loss": 2.7458,
      "step": 535000
    },
    {
      "epoch": 173.45218800648297,
      "grad_norm": 1.3339372873306274,
      "learning_rate": 3.26831657476484e-05,
      "loss": 2.7258,
      "step": 535100
    },
    {
      "epoch": 173.484602917342,
      "grad_norm": 1.2623374462127686,
      "learning_rate": 3.267992215374635e-05,
      "loss": 2.7495,
      "step": 535200
    },
    {
      "epoch": 173.51701782820098,
      "grad_norm": 1.432754397392273,
      "learning_rate": 3.267667855984431e-05,
      "loss": 2.7593,
      "step": 535300
    },
    {
      "epoch": 173.54943273905997,
      "grad_norm": 1.2539570331573486,
      "learning_rate": 3.267343496594227e-05,
      "loss": 2.7506,
      "step": 535400
    },
    {
      "epoch": 173.58184764991896,
      "grad_norm": 1.1922552585601807,
      "learning_rate": 3.267019137204022e-05,
      "loss": 2.7488,
      "step": 535500
    },
    {
      "epoch": 173.61426256077795,
      "grad_norm": 1.1111499071121216,
      "learning_rate": 3.266694777813818e-05,
      "loss": 2.7636,
      "step": 535600
    },
    {
      "epoch": 173.64667747163696,
      "grad_norm": 1.2602330446243286,
      "learning_rate": 3.266370418423613e-05,
      "loss": 2.7549,
      "step": 535700
    },
    {
      "epoch": 173.67909238249595,
      "grad_norm": 1.1853814125061035,
      "learning_rate": 3.266046059033409e-05,
      "loss": 2.7539,
      "step": 535800
    },
    {
      "epoch": 173.71150729335494,
      "grad_norm": 1.3113818168640137,
      "learning_rate": 3.265721699643205e-05,
      "loss": 2.7451,
      "step": 535900
    },
    {
      "epoch": 173.74392220421393,
      "grad_norm": 1.1549690961837769,
      "learning_rate": 3.265400583846903e-05,
      "loss": 2.7587,
      "step": 536000
    },
    {
      "epoch": 173.77633711507292,
      "grad_norm": 1.1721092462539673,
      "learning_rate": 3.265076224456698e-05,
      "loss": 2.734,
      "step": 536100
    },
    {
      "epoch": 173.80875202593194,
      "grad_norm": 1.2306941747665405,
      "learning_rate": 3.264751865066494e-05,
      "loss": 2.7504,
      "step": 536200
    },
    {
      "epoch": 173.84116693679093,
      "grad_norm": 1.2881827354431152,
      "learning_rate": 3.26442750567629e-05,
      "loss": 2.7482,
      "step": 536300
    },
    {
      "epoch": 173.87358184764992,
      "grad_norm": 1.1566439867019653,
      "learning_rate": 3.264103146286085e-05,
      "loss": 2.777,
      "step": 536400
    },
    {
      "epoch": 173.9059967585089,
      "grad_norm": 1.121124029159546,
      "learning_rate": 3.263778786895881e-05,
      "loss": 2.7876,
      "step": 536500
    },
    {
      "epoch": 173.93841166936792,
      "grad_norm": 1.149612545967102,
      "learning_rate": 3.2634544275056766e-05,
      "loss": 2.7633,
      "step": 536600
    },
    {
      "epoch": 173.9708265802269,
      "grad_norm": 1.0014728307724,
      "learning_rate": 3.263130068115472e-05,
      "loss": 2.764,
      "step": 536700
    },
    {
      "epoch": 174.0,
      "eval_bleu": 1.2095165920746875,
      "eval_loss": 3.9748103618621826,
      "eval_runtime": 4.8964,
      "eval_samples_per_second": 100.483,
      "eval_steps_per_second": 1.634,
      "step": 536790
    },
    {
      "epoch": 174.0032414910859,
      "grad_norm": 1.34542715549469,
      "learning_rate": 3.262805708725268e-05,
      "loss": 2.7431,
      "step": 536800
    },
    {
      "epoch": 174.0356564019449,
      "grad_norm": 1.1894060373306274,
      "learning_rate": 3.2624813493350636e-05,
      "loss": 2.7527,
      "step": 536900
    },
    {
      "epoch": 174.06807131280388,
      "grad_norm": 1.1891663074493408,
      "learning_rate": 3.262156989944859e-05,
      "loss": 2.7413,
      "step": 537000
    },
    {
      "epoch": 174.1004862236629,
      "grad_norm": 1.0692871809005737,
      "learning_rate": 3.2618326305546546e-05,
      "loss": 2.7449,
      "step": 537100
    },
    {
      "epoch": 174.1329011345219,
      "grad_norm": 1.1593537330627441,
      "learning_rate": 3.26150827116445e-05,
      "loss": 2.7437,
      "step": 537200
    },
    {
      "epoch": 174.16531604538088,
      "grad_norm": 1.3544483184814453,
      "learning_rate": 3.261183911774246e-05,
      "loss": 2.7601,
      "step": 537300
    },
    {
      "epoch": 174.19773095623987,
      "grad_norm": 1.1197940111160278,
      "learning_rate": 3.2608595523840416e-05,
      "loss": 2.755,
      "step": 537400
    },
    {
      "epoch": 174.23014586709886,
      "grad_norm": 1.2906743288040161,
      "learning_rate": 3.2605351929938374e-05,
      "loss": 2.7406,
      "step": 537500
    },
    {
      "epoch": 174.26256077795787,
      "grad_norm": 1.2738559246063232,
      "learning_rate": 3.260210833603633e-05,
      "loss": 2.7458,
      "step": 537600
    },
    {
      "epoch": 174.29497568881686,
      "grad_norm": 1.1189360618591309,
      "learning_rate": 3.259886474213429e-05,
      "loss": 2.7457,
      "step": 537700
    },
    {
      "epoch": 174.32739059967585,
      "grad_norm": 1.3531506061553955,
      "learning_rate": 3.2595621148232244e-05,
      "loss": 2.75,
      "step": 537800
    },
    {
      "epoch": 174.35980551053484,
      "grad_norm": 1.1848546266555786,
      "learning_rate": 3.25923775543302e-05,
      "loss": 2.7417,
      "step": 537900
    },
    {
      "epoch": 174.39222042139383,
      "grad_norm": 1.2360926866531372,
      "learning_rate": 3.258913396042816e-05,
      "loss": 2.7631,
      "step": 538000
    },
    {
      "epoch": 174.42463533225285,
      "grad_norm": 1.1725419759750366,
      "learning_rate": 3.258589036652611e-05,
      "loss": 2.7372,
      "step": 538100
    },
    {
      "epoch": 174.45705024311184,
      "grad_norm": 1.191544771194458,
      "learning_rate": 3.258264677262407e-05,
      "loss": 2.7382,
      "step": 538200
    },
    {
      "epoch": 174.48946515397083,
      "grad_norm": 1.0857484340667725,
      "learning_rate": 3.2579403178722024e-05,
      "loss": 2.7299,
      "step": 538300
    },
    {
      "epoch": 174.52188006482982,
      "grad_norm": 1.264477252960205,
      "learning_rate": 3.257615958481998e-05,
      "loss": 2.7653,
      "step": 538400
    },
    {
      "epoch": 174.5542949756888,
      "grad_norm": 1.2877699136734009,
      "learning_rate": 3.257291599091794e-05,
      "loss": 2.7669,
      "step": 538500
    },
    {
      "epoch": 174.58670988654782,
      "grad_norm": 1.178157925605774,
      "learning_rate": 3.2569672397015894e-05,
      "loss": 2.7393,
      "step": 538600
    },
    {
      "epoch": 174.6191247974068,
      "grad_norm": 1.1161655187606812,
      "learning_rate": 3.256642880311385e-05,
      "loss": 2.7409,
      "step": 538700
    },
    {
      "epoch": 174.6515397082658,
      "grad_norm": 1.0929744243621826,
      "learning_rate": 3.256318520921181e-05,
      "loss": 2.7539,
      "step": 538800
    },
    {
      "epoch": 174.6839546191248,
      "grad_norm": 1.3406972885131836,
      "learning_rate": 3.255994161530976e-05,
      "loss": 2.7601,
      "step": 538900
    },
    {
      "epoch": 174.71636952998378,
      "grad_norm": 1.1290457248687744,
      "learning_rate": 3.255669802140772e-05,
      "loss": 2.7847,
      "step": 539000
    },
    {
      "epoch": 174.7487844408428,
      "grad_norm": 1.1254035234451294,
      "learning_rate": 3.2553454427505674e-05,
      "loss": 2.739,
      "step": 539100
    },
    {
      "epoch": 174.7811993517018,
      "grad_norm": 1.1199839115142822,
      "learning_rate": 3.255021083360363e-05,
      "loss": 2.7564,
      "step": 539200
    },
    {
      "epoch": 174.81361426256078,
      "grad_norm": 1.1248483657836914,
      "learning_rate": 3.254696723970159e-05,
      "loss": 2.7454,
      "step": 539300
    },
    {
      "epoch": 174.84602917341977,
      "grad_norm": 1.2190756797790527,
      "learning_rate": 3.254372364579954e-05,
      "loss": 2.7415,
      "step": 539400
    },
    {
      "epoch": 174.87844408427875,
      "grad_norm": 1.2517611980438232,
      "learning_rate": 3.25404800518975e-05,
      "loss": 2.7716,
      "step": 539500
    },
    {
      "epoch": 174.91085899513777,
      "grad_norm": 1.2175612449645996,
      "learning_rate": 3.253723645799546e-05,
      "loss": 2.7507,
      "step": 539600
    },
    {
      "epoch": 174.94327390599676,
      "grad_norm": 1.2084105014801025,
      "learning_rate": 3.253399286409341e-05,
      "loss": 2.7611,
      "step": 539700
    },
    {
      "epoch": 174.97568881685575,
      "grad_norm": 1.3919334411621094,
      "learning_rate": 3.253074927019137e-05,
      "loss": 2.7514,
      "step": 539800
    },
    {
      "epoch": 175.0,
      "eval_bleu": 1.3467397026585755,
      "eval_loss": 3.978630542755127,
      "eval_runtime": 3.864,
      "eval_samples_per_second": 127.328,
      "eval_steps_per_second": 2.07,
      "step": 539875
    },
    {
      "epoch": 175.00810372771474,
      "grad_norm": 1.0372854471206665,
      "learning_rate": 3.252750567628933e-05,
      "loss": 2.7464,
      "step": 539900
    },
    {
      "epoch": 175.04051863857376,
      "grad_norm": 1.37471604347229,
      "learning_rate": 3.252429451832631e-05,
      "loss": 2.7365,
      "step": 540000
    },
    {
      "epoch": 175.07293354943275,
      "grad_norm": 1.2426111698150635,
      "learning_rate": 3.252105092442426e-05,
      "loss": 2.7502,
      "step": 540100
    },
    {
      "epoch": 175.10534846029174,
      "grad_norm": 1.3264141082763672,
      "learning_rate": 3.251780733052222e-05,
      "loss": 2.731,
      "step": 540200
    },
    {
      "epoch": 175.13776337115073,
      "grad_norm": 1.1744463443756104,
      "learning_rate": 3.251456373662018e-05,
      "loss": 2.7596,
      "step": 540300
    },
    {
      "epoch": 175.17017828200972,
      "grad_norm": 1.0874842405319214,
      "learning_rate": 3.251132014271813e-05,
      "loss": 2.764,
      "step": 540400
    },
    {
      "epoch": 175.20259319286873,
      "grad_norm": 1.2213084697723389,
      "learning_rate": 3.250807654881609e-05,
      "loss": 2.7581,
      "step": 540500
    },
    {
      "epoch": 175.23500810372772,
      "grad_norm": 1.1773810386657715,
      "learning_rate": 3.250483295491405e-05,
      "loss": 2.7628,
      "step": 540600
    },
    {
      "epoch": 175.2674230145867,
      "grad_norm": 1.2931816577911377,
      "learning_rate": 3.2501589361012006e-05,
      "loss": 2.7481,
      "step": 540700
    },
    {
      "epoch": 175.2998379254457,
      "grad_norm": 1.2434619665145874,
      "learning_rate": 3.2498345767109964e-05,
      "loss": 2.7357,
      "step": 540800
    },
    {
      "epoch": 175.3322528363047,
      "grad_norm": 1.0619500875473022,
      "learning_rate": 3.2495102173207916e-05,
      "loss": 2.7367,
      "step": 540900
    },
    {
      "epoch": 175.3646677471637,
      "grad_norm": 1.1758522987365723,
      "learning_rate": 3.2491858579305875e-05,
      "loss": 2.7611,
      "step": 541000
    },
    {
      "epoch": 175.3970826580227,
      "grad_norm": 1.3157274723052979,
      "learning_rate": 3.2488614985403834e-05,
      "loss": 2.746,
      "step": 541100
    },
    {
      "epoch": 175.4294975688817,
      "grad_norm": 1.162490963935852,
      "learning_rate": 3.2485371391501786e-05,
      "loss": 2.7506,
      "step": 541200
    },
    {
      "epoch": 175.46191247974068,
      "grad_norm": 1.176520824432373,
      "learning_rate": 3.2482127797599744e-05,
      "loss": 2.7402,
      "step": 541300
    },
    {
      "epoch": 175.49432739059966,
      "grad_norm": 1.21426522731781,
      "learning_rate": 3.2478884203697696e-05,
      "loss": 2.7409,
      "step": 541400
    },
    {
      "epoch": 175.52674230145868,
      "grad_norm": 1.099550724029541,
      "learning_rate": 3.247567304573468e-05,
      "loss": 2.7416,
      "step": 541500
    },
    {
      "epoch": 175.55915721231767,
      "grad_norm": 1.2330130338668823,
      "learning_rate": 3.247242945183263e-05,
      "loss": 2.7665,
      "step": 541600
    },
    {
      "epoch": 175.59157212317666,
      "grad_norm": 1.177779197692871,
      "learning_rate": 3.246921829386961e-05,
      "loss": 2.7388,
      "step": 541700
    },
    {
      "epoch": 175.62398703403565,
      "grad_norm": 1.1961454153060913,
      "learning_rate": 3.246597469996757e-05,
      "loss": 2.7488,
      "step": 541800
    },
    {
      "epoch": 175.65640194489464,
      "grad_norm": 1.4159095287322998,
      "learning_rate": 3.246273110606552e-05,
      "loss": 2.7299,
      "step": 541900
    },
    {
      "epoch": 175.68881685575366,
      "grad_norm": 1.2236506938934326,
      "learning_rate": 3.245948751216348e-05,
      "loss": 2.7513,
      "step": 542000
    },
    {
      "epoch": 175.72123176661265,
      "grad_norm": 1.268119215965271,
      "learning_rate": 3.245624391826144e-05,
      "loss": 2.7572,
      "step": 542100
    },
    {
      "epoch": 175.75364667747164,
      "grad_norm": 1.1598753929138184,
      "learning_rate": 3.245300032435939e-05,
      "loss": 2.7545,
      "step": 542200
    },
    {
      "epoch": 175.78606158833063,
      "grad_norm": 1.2877576351165771,
      "learning_rate": 3.244975673045735e-05,
      "loss": 2.7373,
      "step": 542300
    },
    {
      "epoch": 175.81847649918961,
      "grad_norm": 1.2179620265960693,
      "learning_rate": 3.244651313655531e-05,
      "loss": 2.7567,
      "step": 542400
    },
    {
      "epoch": 175.85089141004863,
      "grad_norm": 1.3602755069732666,
      "learning_rate": 3.244326954265326e-05,
      "loss": 2.7333,
      "step": 542500
    },
    {
      "epoch": 175.88330632090762,
      "grad_norm": 1.118222713470459,
      "learning_rate": 3.244002594875122e-05,
      "loss": 2.7407,
      "step": 542600
    },
    {
      "epoch": 175.9157212317666,
      "grad_norm": 1.4926778078079224,
      "learning_rate": 3.243678235484918e-05,
      "loss": 2.7659,
      "step": 542700
    },
    {
      "epoch": 175.9481361426256,
      "grad_norm": 1.184552788734436,
      "learning_rate": 3.243353876094713e-05,
      "loss": 2.761,
      "step": 542800
    },
    {
      "epoch": 175.9805510534846,
      "grad_norm": 1.1230063438415527,
      "learning_rate": 3.243029516704509e-05,
      "loss": 2.7469,
      "step": 542900
    },
    {
      "epoch": 176.0,
      "eval_bleu": 1.209026209129921,
      "eval_loss": 3.9755702018737793,
      "eval_runtime": 3.9621,
      "eval_samples_per_second": 124.176,
      "eval_steps_per_second": 2.019,
      "step": 542960
    },
    {
      "epoch": 176.0129659643436,
      "grad_norm": 1.191348671913147,
      "learning_rate": 3.242705157314304e-05,
      "loss": 2.7628,
      "step": 543000
    },
    {
      "epoch": 176.0453808752026,
      "grad_norm": 1.1527653932571411,
      "learning_rate": 3.2423807979241e-05,
      "loss": 2.7228,
      "step": 543100
    },
    {
      "epoch": 176.07779578606159,
      "grad_norm": 1.0771452188491821,
      "learning_rate": 3.242056438533896e-05,
      "loss": 2.7495,
      "step": 543200
    },
    {
      "epoch": 176.11021069692057,
      "grad_norm": 1.1466612815856934,
      "learning_rate": 3.241732079143691e-05,
      "loss": 2.7197,
      "step": 543300
    },
    {
      "epoch": 176.1426256077796,
      "grad_norm": 1.1574389934539795,
      "learning_rate": 3.241407719753487e-05,
      "loss": 2.7456,
      "step": 543400
    },
    {
      "epoch": 176.17504051863858,
      "grad_norm": 1.298534870147705,
      "learning_rate": 3.241083360363283e-05,
      "loss": 2.7344,
      "step": 543500
    },
    {
      "epoch": 176.20745542949757,
      "grad_norm": 1.0776386260986328,
      "learning_rate": 3.240759000973078e-05,
      "loss": 2.7646,
      "step": 543600
    },
    {
      "epoch": 176.23987034035656,
      "grad_norm": 1.1003915071487427,
      "learning_rate": 3.240434641582874e-05,
      "loss": 2.7516,
      "step": 543700
    },
    {
      "epoch": 176.27228525121555,
      "grad_norm": 1.1060724258422852,
      "learning_rate": 3.24011028219267e-05,
      "loss": 2.7402,
      "step": 543800
    },
    {
      "epoch": 176.30470016207457,
      "grad_norm": 1.220045566558838,
      "learning_rate": 3.239785922802465e-05,
      "loss": 2.7364,
      "step": 543900
    },
    {
      "epoch": 176.33711507293356,
      "grad_norm": 1.2203397750854492,
      "learning_rate": 3.239461563412261e-05,
      "loss": 2.7541,
      "step": 544000
    },
    {
      "epoch": 176.36952998379255,
      "grad_norm": 1.0985008478164673,
      "learning_rate": 3.2391372040220566e-05,
      "loss": 2.7339,
      "step": 544100
    },
    {
      "epoch": 176.40194489465154,
      "grad_norm": 1.045508623123169,
      "learning_rate": 3.2388128446318525e-05,
      "loss": 2.7495,
      "step": 544200
    },
    {
      "epoch": 176.43435980551052,
      "grad_norm": 1.2177873849868774,
      "learning_rate": 3.2384884852416484e-05,
      "loss": 2.7699,
      "step": 544300
    },
    {
      "epoch": 176.46677471636954,
      "grad_norm": 1.3008629083633423,
      "learning_rate": 3.2381641258514436e-05,
      "loss": 2.7286,
      "step": 544400
    },
    {
      "epoch": 176.49918962722853,
      "grad_norm": 1.19183349609375,
      "learning_rate": 3.2378397664612395e-05,
      "loss": 2.7544,
      "step": 544500
    },
    {
      "epoch": 176.53160453808752,
      "grad_norm": 1.3604028224945068,
      "learning_rate": 3.237515407071035e-05,
      "loss": 2.7685,
      "step": 544600
    },
    {
      "epoch": 176.5640194489465,
      "grad_norm": 1.2692028284072876,
      "learning_rate": 3.2371910476808305e-05,
      "loss": 2.7313,
      "step": 544700
    },
    {
      "epoch": 176.5964343598055,
      "grad_norm": 1.2017360925674438,
      "learning_rate": 3.2368666882906264e-05,
      "loss": 2.7555,
      "step": 544800
    },
    {
      "epoch": 176.62884927066452,
      "grad_norm": 1.1807544231414795,
      "learning_rate": 3.236542328900422e-05,
      "loss": 2.7453,
      "step": 544900
    },
    {
      "epoch": 176.6612641815235,
      "grad_norm": 1.1409697532653809,
      "learning_rate": 3.2362179695102175e-05,
      "loss": 2.7508,
      "step": 545000
    },
    {
      "epoch": 176.6936790923825,
      "grad_norm": 1.3209240436553955,
      "learning_rate": 3.2358936101200133e-05,
      "loss": 2.7446,
      "step": 545100
    },
    {
      "epoch": 176.72609400324149,
      "grad_norm": 1.0401028394699097,
      "learning_rate": 3.2355692507298085e-05,
      "loss": 2.7532,
      "step": 545200
    },
    {
      "epoch": 176.75850891410047,
      "grad_norm": 1.1648948192596436,
      "learning_rate": 3.2352448913396044e-05,
      "loss": 2.7538,
      "step": 545300
    },
    {
      "epoch": 176.7909238249595,
      "grad_norm": 1.1768043041229248,
      "learning_rate": 3.2349205319494e-05,
      "loss": 2.7319,
      "step": 545400
    },
    {
      "epoch": 176.82333873581848,
      "grad_norm": 1.321796178817749,
      "learning_rate": 3.2345961725591955e-05,
      "loss": 2.7359,
      "step": 545500
    },
    {
      "epoch": 176.85575364667747,
      "grad_norm": 1.0715620517730713,
      "learning_rate": 3.2342718131689914e-05,
      "loss": 2.7315,
      "step": 545600
    },
    {
      "epoch": 176.88816855753646,
      "grad_norm": 1.2339224815368652,
      "learning_rate": 3.233947453778787e-05,
      "loss": 2.7475,
      "step": 545700
    },
    {
      "epoch": 176.92058346839545,
      "grad_norm": 1.3126499652862549,
      "learning_rate": 3.2336230943885824e-05,
      "loss": 2.7555,
      "step": 545800
    },
    {
      "epoch": 176.95299837925447,
      "grad_norm": 1.4038559198379517,
      "learning_rate": 3.233298734998378e-05,
      "loss": 2.7629,
      "step": 545900
    },
    {
      "epoch": 176.98541329011346,
      "grad_norm": 1.1505961418151855,
      "learning_rate": 3.232974375608174e-05,
      "loss": 2.7574,
      "step": 546000
    },
    {
      "epoch": 177.0,
      "eval_bleu": 1.1365350727967438,
      "eval_loss": 3.9758334159851074,
      "eval_runtime": 4.2775,
      "eval_samples_per_second": 115.022,
      "eval_steps_per_second": 1.87,
      "step": 546045
    },
    {
      "epoch": 177.01782820097245,
      "grad_norm": 1.0505818128585815,
      "learning_rate": 3.2326500162179694e-05,
      "loss": 2.7702,
      "step": 546100
    },
    {
      "epoch": 177.05024311183143,
      "grad_norm": 1.1001534461975098,
      "learning_rate": 3.232325656827765e-05,
      "loss": 2.7433,
      "step": 546200
    },
    {
      "epoch": 177.08265802269042,
      "grad_norm": 1.1763724088668823,
      "learning_rate": 3.2320012974375604e-05,
      "loss": 2.7474,
      "step": 546300
    },
    {
      "epoch": 177.11507293354944,
      "grad_norm": 1.273901343345642,
      "learning_rate": 3.231676938047356e-05,
      "loss": 2.7224,
      "step": 546400
    },
    {
      "epoch": 177.14748784440843,
      "grad_norm": 0.9738712906837463,
      "learning_rate": 3.231352578657152e-05,
      "loss": 2.7485,
      "step": 546500
    },
    {
      "epoch": 177.17990275526742,
      "grad_norm": 1.0430575609207153,
      "learning_rate": 3.231028219266948e-05,
      "loss": 2.7513,
      "step": 546600
    },
    {
      "epoch": 177.2123176661264,
      "grad_norm": 1.440658688545227,
      "learning_rate": 3.230703859876744e-05,
      "loss": 2.7357,
      "step": 546700
    },
    {
      "epoch": 177.24473257698543,
      "grad_norm": 1.1374778747558594,
      "learning_rate": 3.230379500486539e-05,
      "loss": 2.7464,
      "step": 546800
    },
    {
      "epoch": 177.27714748784442,
      "grad_norm": 1.4390676021575928,
      "learning_rate": 3.230055141096335e-05,
      "loss": 2.7207,
      "step": 546900
    },
    {
      "epoch": 177.3095623987034,
      "grad_norm": 1.1241904497146606,
      "learning_rate": 3.229730781706131e-05,
      "loss": 2.7336,
      "step": 547000
    },
    {
      "epoch": 177.3419773095624,
      "grad_norm": 1.0596674680709839,
      "learning_rate": 3.229406422315926e-05,
      "loss": 2.7387,
      "step": 547100
    },
    {
      "epoch": 177.37439222042138,
      "grad_norm": 1.082368016242981,
      "learning_rate": 3.229082062925722e-05,
      "loss": 2.7414,
      "step": 547200
    },
    {
      "epoch": 177.4068071312804,
      "grad_norm": 1.10103178024292,
      "learning_rate": 3.228757703535518e-05,
      "loss": 2.7344,
      "step": 547300
    },
    {
      "epoch": 177.4392220421394,
      "grad_norm": 1.3319058418273926,
      "learning_rate": 3.228433344145313e-05,
      "loss": 2.7524,
      "step": 547400
    },
    {
      "epoch": 177.47163695299838,
      "grad_norm": 1.3317348957061768,
      "learning_rate": 3.228108984755109e-05,
      "loss": 2.7407,
      "step": 547500
    },
    {
      "epoch": 177.50405186385737,
      "grad_norm": 1.3769094944000244,
      "learning_rate": 3.227784625364905e-05,
      "loss": 2.7513,
      "step": 547600
    },
    {
      "epoch": 177.53646677471636,
      "grad_norm": 1.0691709518432617,
      "learning_rate": 3.2274635095686026e-05,
      "loss": 2.7336,
      "step": 547700
    },
    {
      "epoch": 177.56888168557538,
      "grad_norm": 1.1752954721450806,
      "learning_rate": 3.227139150178398e-05,
      "loss": 2.7574,
      "step": 547800
    },
    {
      "epoch": 177.60129659643437,
      "grad_norm": 1.238294243812561,
      "learning_rate": 3.2268147907881936e-05,
      "loss": 2.7464,
      "step": 547900
    },
    {
      "epoch": 177.63371150729336,
      "grad_norm": 1.248053789138794,
      "learning_rate": 3.2264904313979895e-05,
      "loss": 2.7364,
      "step": 548000
    },
    {
      "epoch": 177.66612641815234,
      "grad_norm": 1.2538039684295654,
      "learning_rate": 3.226166072007785e-05,
      "loss": 2.7486,
      "step": 548100
    },
    {
      "epoch": 177.69854132901133,
      "grad_norm": 1.1066522598266602,
      "learning_rate": 3.2258417126175806e-05,
      "loss": 2.7332,
      "step": 548200
    },
    {
      "epoch": 177.73095623987035,
      "grad_norm": 1.2772042751312256,
      "learning_rate": 3.2255173532273764e-05,
      "loss": 2.7714,
      "step": 548300
    },
    {
      "epoch": 177.76337115072934,
      "grad_norm": 1.0131502151489258,
      "learning_rate": 3.2251929938371716e-05,
      "loss": 2.7547,
      "step": 548400
    },
    {
      "epoch": 177.79578606158833,
      "grad_norm": 1.0627938508987427,
      "learning_rate": 3.2248686344469675e-05,
      "loss": 2.7332,
      "step": 548500
    },
    {
      "epoch": 177.82820097244732,
      "grad_norm": 1.3038098812103271,
      "learning_rate": 3.224544275056763e-05,
      "loss": 2.7525,
      "step": 548600
    },
    {
      "epoch": 177.8606158833063,
      "grad_norm": 1.0980255603790283,
      "learning_rate": 3.2242199156665586e-05,
      "loss": 2.7372,
      "step": 548700
    },
    {
      "epoch": 177.89303079416533,
      "grad_norm": 1.1471056938171387,
      "learning_rate": 3.2238955562763545e-05,
      "loss": 2.7518,
      "step": 548800
    },
    {
      "epoch": 177.92544570502432,
      "grad_norm": 1.0651695728302002,
      "learning_rate": 3.22357119688615e-05,
      "loss": 2.728,
      "step": 548900
    },
    {
      "epoch": 177.9578606158833,
      "grad_norm": 1.098296880722046,
      "learning_rate": 3.2232468374959455e-05,
      "loss": 2.7592,
      "step": 549000
    },
    {
      "epoch": 177.9902755267423,
      "grad_norm": 1.309827208518982,
      "learning_rate": 3.2229224781057414e-05,
      "loss": 2.7647,
      "step": 549100
    },
    {
      "epoch": 178.0,
      "eval_bleu": 1.1019832788330357,
      "eval_loss": 3.975390911102295,
      "eval_runtime": 4.1425,
      "eval_samples_per_second": 118.769,
      "eval_steps_per_second": 1.931,
      "step": 549130
    },
    {
      "epoch": 178.02269043760128,
      "grad_norm": 1.1582121849060059,
      "learning_rate": 3.2225981187155366e-05,
      "loss": 2.7261,
      "step": 549200
    },
    {
      "epoch": 178.0551053484603,
      "grad_norm": 1.0788910388946533,
      "learning_rate": 3.2222737593253325e-05,
      "loss": 2.7157,
      "step": 549300
    },
    {
      "epoch": 178.0875202593193,
      "grad_norm": 1.3570945262908936,
      "learning_rate": 3.221949399935128e-05,
      "loss": 2.74,
      "step": 549400
    },
    {
      "epoch": 178.11993517017828,
      "grad_norm": 1.122442603111267,
      "learning_rate": 3.2216250405449236e-05,
      "loss": 2.7273,
      "step": 549500
    },
    {
      "epoch": 178.15235008103727,
      "grad_norm": 1.099816918373108,
      "learning_rate": 3.2213006811547194e-05,
      "loss": 2.7255,
      "step": 549600
    },
    {
      "epoch": 178.18476499189626,
      "grad_norm": 1.3241853713989258,
      "learning_rate": 3.220979565358417e-05,
      "loss": 2.7286,
      "step": 549700
    },
    {
      "epoch": 178.21717990275528,
      "grad_norm": 1.228011965751648,
      "learning_rate": 3.2206552059682124e-05,
      "loss": 2.7464,
      "step": 549800
    },
    {
      "epoch": 178.24959481361427,
      "grad_norm": 1.085498571395874,
      "learning_rate": 3.220330846578008e-05,
      "loss": 2.7397,
      "step": 549900
    },
    {
      "epoch": 178.28200972447326,
      "grad_norm": 1.1561328172683716,
      "learning_rate": 3.220006487187804e-05,
      "loss": 2.734,
      "step": 550000
    },
    {
      "epoch": 178.31442463533224,
      "grad_norm": 1.1839929819107056,
      "learning_rate": 3.2196821277976e-05,
      "loss": 2.7634,
      "step": 550100
    },
    {
      "epoch": 178.34683954619126,
      "grad_norm": 1.235796570777893,
      "learning_rate": 3.219357768407395e-05,
      "loss": 2.7532,
      "step": 550200
    },
    {
      "epoch": 178.37925445705025,
      "grad_norm": 1.0379822254180908,
      "learning_rate": 3.219033409017191e-05,
      "loss": 2.763,
      "step": 550300
    },
    {
      "epoch": 178.41166936790924,
      "grad_norm": 1.0898301601409912,
      "learning_rate": 3.218709049626987e-05,
      "loss": 2.7309,
      "step": 550400
    },
    {
      "epoch": 178.44408427876823,
      "grad_norm": 1.1411195993423462,
      "learning_rate": 3.218384690236783e-05,
      "loss": 2.7471,
      "step": 550500
    },
    {
      "epoch": 178.47649918962722,
      "grad_norm": 0.9533445835113525,
      "learning_rate": 3.218060330846579e-05,
      "loss": 2.7457,
      "step": 550600
    },
    {
      "epoch": 178.50891410048624,
      "grad_norm": 1.1389367580413818,
      "learning_rate": 3.217735971456374e-05,
      "loss": 2.7434,
      "step": 550700
    },
    {
      "epoch": 178.54132901134523,
      "grad_norm": 1.0992920398712158,
      "learning_rate": 3.21741161206617e-05,
      "loss": 2.7365,
      "step": 550800
    },
    {
      "epoch": 178.57374392220422,
      "grad_norm": 1.3625928163528442,
      "learning_rate": 3.217087252675965e-05,
      "loss": 2.7431,
      "step": 550900
    },
    {
      "epoch": 178.6061588330632,
      "grad_norm": 1.0742956399917603,
      "learning_rate": 3.216762893285761e-05,
      "loss": 2.7514,
      "step": 551000
    },
    {
      "epoch": 178.6385737439222,
      "grad_norm": 1.1646562814712524,
      "learning_rate": 3.216438533895557e-05,
      "loss": 2.7251,
      "step": 551100
    },
    {
      "epoch": 178.6709886547812,
      "grad_norm": 1.1547772884368896,
      "learning_rate": 3.216114174505352e-05,
      "loss": 2.7395,
      "step": 551200
    },
    {
      "epoch": 178.7034035656402,
      "grad_norm": 1.25716233253479,
      "learning_rate": 3.215789815115148e-05,
      "loss": 2.7493,
      "step": 551300
    },
    {
      "epoch": 178.7358184764992,
      "grad_norm": 1.2195841073989868,
      "learning_rate": 3.2154686993188456e-05,
      "loss": 2.7493,
      "step": 551400
    },
    {
      "epoch": 178.76823338735818,
      "grad_norm": 1.25400710105896,
      "learning_rate": 3.2151443399286415e-05,
      "loss": 2.7513,
      "step": 551500
    },
    {
      "epoch": 178.80064829821717,
      "grad_norm": 1.0970298051834106,
      "learning_rate": 3.2148199805384367e-05,
      "loss": 2.7547,
      "step": 551600
    },
    {
      "epoch": 178.8330632090762,
      "grad_norm": 1.1475967168807983,
      "learning_rate": 3.2144956211482325e-05,
      "loss": 2.7424,
      "step": 551700
    },
    {
      "epoch": 178.86547811993518,
      "grad_norm": 1.1285274028778076,
      "learning_rate": 3.2141712617580284e-05,
      "loss": 2.7487,
      "step": 551800
    },
    {
      "epoch": 178.89789303079417,
      "grad_norm": 1.2481956481933594,
      "learning_rate": 3.2138469023678236e-05,
      "loss": 2.7331,
      "step": 551900
    },
    {
      "epoch": 178.93030794165315,
      "grad_norm": 1.1819709539413452,
      "learning_rate": 3.2135225429776195e-05,
      "loss": 2.7532,
      "step": 552000
    },
    {
      "epoch": 178.96272285251214,
      "grad_norm": 1.0846683979034424,
      "learning_rate": 3.213198183587415e-05,
      "loss": 2.7561,
      "step": 552100
    },
    {
      "epoch": 178.99513776337116,
      "grad_norm": 1.291581392288208,
      "learning_rate": 3.2128738241972105e-05,
      "loss": 2.7558,
      "step": 552200
    },
    {
      "epoch": 179.0,
      "eval_bleu": 0.9525892061680641,
      "eval_loss": 3.977634906768799,
      "eval_runtime": 4.2182,
      "eval_samples_per_second": 116.637,
      "eval_steps_per_second": 1.897,
      "step": 552215
    },
    {
      "epoch": 179.02755267423015,
      "grad_norm": 1.3766437768936157,
      "learning_rate": 3.2125494648070064e-05,
      "loss": 2.7246,
      "step": 552300
    },
    {
      "epoch": 179.05996758508914,
      "grad_norm": 1.0689423084259033,
      "learning_rate": 3.2122251054168016e-05,
      "loss": 2.7224,
      "step": 552400
    },
    {
      "epoch": 179.09238249594813,
      "grad_norm": 1.0884758234024048,
      "learning_rate": 3.2119007460265975e-05,
      "loss": 2.7284,
      "step": 552500
    },
    {
      "epoch": 179.12479740680712,
      "grad_norm": 1.1830029487609863,
      "learning_rate": 3.2115763866363934e-05,
      "loss": 2.7512,
      "step": 552600
    },
    {
      "epoch": 179.15721231766614,
      "grad_norm": 1.1436318159103394,
      "learning_rate": 3.2112520272461886e-05,
      "loss": 2.7577,
      "step": 552700
    },
    {
      "epoch": 179.18962722852513,
      "grad_norm": 1.3576545715332031,
      "learning_rate": 3.2109276678559844e-05,
      "loss": 2.7357,
      "step": 552800
    },
    {
      "epoch": 179.22204213938411,
      "grad_norm": 1.1467218399047852,
      "learning_rate": 3.21060330846578e-05,
      "loss": 2.7212,
      "step": 552900
    },
    {
      "epoch": 179.2544570502431,
      "grad_norm": 1.0484414100646973,
      "learning_rate": 3.2102789490755755e-05,
      "loss": 2.731,
      "step": 553000
    },
    {
      "epoch": 179.2868719611021,
      "grad_norm": 1.2077851295471191,
      "learning_rate": 3.2099545896853714e-05,
      "loss": 2.7489,
      "step": 553100
    },
    {
      "epoch": 179.3192868719611,
      "grad_norm": 1.3435083627700806,
      "learning_rate": 3.209630230295167e-05,
      "loss": 2.7387,
      "step": 553200
    },
    {
      "epoch": 179.3517017828201,
      "grad_norm": 1.2253464460372925,
      "learning_rate": 3.209305870904963e-05,
      "loss": 2.7431,
      "step": 553300
    },
    {
      "epoch": 179.3841166936791,
      "grad_norm": 1.1590200662612915,
      "learning_rate": 3.208981511514759e-05,
      "loss": 2.7663,
      "step": 553400
    },
    {
      "epoch": 179.41653160453808,
      "grad_norm": 1.1782571077346802,
      "learning_rate": 3.208657152124554e-05,
      "loss": 2.747,
      "step": 553500
    },
    {
      "epoch": 179.4489465153971,
      "grad_norm": 1.1145875453948975,
      "learning_rate": 3.20833279273435e-05,
      "loss": 2.7515,
      "step": 553600
    },
    {
      "epoch": 179.4813614262561,
      "grad_norm": 1.1967381238937378,
      "learning_rate": 3.208008433344146e-05,
      "loss": 2.7258,
      "step": 553700
    },
    {
      "epoch": 179.51377633711508,
      "grad_norm": 1.252319574356079,
      "learning_rate": 3.207684073953941e-05,
      "loss": 2.7498,
      "step": 553800
    },
    {
      "epoch": 179.54619124797406,
      "grad_norm": 1.1575918197631836,
      "learning_rate": 3.207359714563737e-05,
      "loss": 2.73,
      "step": 553900
    },
    {
      "epoch": 179.57860615883305,
      "grad_norm": 1.1434342861175537,
      "learning_rate": 3.207035355173532e-05,
      "loss": 2.7496,
      "step": 554000
    },
    {
      "epoch": 179.61102106969207,
      "grad_norm": 1.1281163692474365,
      "learning_rate": 3.206710995783328e-05,
      "loss": 2.7401,
      "step": 554100
    },
    {
      "epoch": 179.64343598055106,
      "grad_norm": 1.129592776298523,
      "learning_rate": 3.206386636393124e-05,
      "loss": 2.7297,
      "step": 554200
    },
    {
      "epoch": 179.67585089141005,
      "grad_norm": 1.2562425136566162,
      "learning_rate": 3.206062277002919e-05,
      "loss": 2.7522,
      "step": 554300
    },
    {
      "epoch": 179.70826580226904,
      "grad_norm": 1.2364270687103271,
      "learning_rate": 3.205737917612715e-05,
      "loss": 2.7417,
      "step": 554400
    },
    {
      "epoch": 179.74068071312803,
      "grad_norm": 1.1075329780578613,
      "learning_rate": 3.205413558222511e-05,
      "loss": 2.7527,
      "step": 554500
    },
    {
      "epoch": 179.77309562398705,
      "grad_norm": 1.1245578527450562,
      "learning_rate": 3.205092442426209e-05,
      "loss": 2.7562,
      "step": 554600
    },
    {
      "epoch": 179.80551053484604,
      "grad_norm": 1.2342838048934937,
      "learning_rate": 3.204768083036004e-05,
      "loss": 2.7167,
      "step": 554700
    },
    {
      "epoch": 179.83792544570503,
      "grad_norm": 1.2050871849060059,
      "learning_rate": 3.2044437236458e-05,
      "loss": 2.7271,
      "step": 554800
    },
    {
      "epoch": 179.87034035656401,
      "grad_norm": 1.2698601484298706,
      "learning_rate": 3.2041193642555956e-05,
      "loss": 2.7579,
      "step": 554900
    },
    {
      "epoch": 179.902755267423,
      "grad_norm": 1.227792739868164,
      "learning_rate": 3.203795004865391e-05,
      "loss": 2.7432,
      "step": 555000
    },
    {
      "epoch": 179.93517017828202,
      "grad_norm": 1.2430633306503296,
      "learning_rate": 3.203470645475187e-05,
      "loss": 2.725,
      "step": 555100
    },
    {
      "epoch": 179.967585089141,
      "grad_norm": 1.2042042016983032,
      "learning_rate": 3.2031462860849826e-05,
      "loss": 2.757,
      "step": 555200
    },
    {
      "epoch": 180.0,
      "grad_norm": 1.156909704208374,
      "learning_rate": 3.202821926694778e-05,
      "loss": 2.7369,
      "step": 555300
    },
    {
      "epoch": 180.0,
      "eval_bleu": 0.9550843572734109,
      "eval_loss": 3.9771528244018555,
      "eval_runtime": 4.0315,
      "eval_samples_per_second": 122.04,
      "eval_steps_per_second": 1.984,
      "step": 555300
    },
    {
      "epoch": 180.032414910859,
      "grad_norm": 1.256977915763855,
      "learning_rate": 3.2024975673045737e-05,
      "loss": 2.7219,
      "step": 555400
    },
    {
      "epoch": 180.06482982171798,
      "grad_norm": 1.2097891569137573,
      "learning_rate": 3.202173207914369e-05,
      "loss": 2.7402,
      "step": 555500
    },
    {
      "epoch": 180.097244732577,
      "grad_norm": 1.1909929513931274,
      "learning_rate": 3.201848848524165e-05,
      "loss": 2.7303,
      "step": 555600
    },
    {
      "epoch": 180.12965964343599,
      "grad_norm": 1.0674022436141968,
      "learning_rate": 3.2015244891339606e-05,
      "loss": 2.7454,
      "step": 555700
    },
    {
      "epoch": 180.16207455429497,
      "grad_norm": 1.2616288661956787,
      "learning_rate": 3.201200129743756e-05,
      "loss": 2.7429,
      "step": 555800
    },
    {
      "epoch": 180.19448946515396,
      "grad_norm": 1.1146609783172607,
      "learning_rate": 3.200875770353552e-05,
      "loss": 2.7444,
      "step": 555900
    },
    {
      "epoch": 180.22690437601295,
      "grad_norm": 1.4110562801361084,
      "learning_rate": 3.2005514109633475e-05,
      "loss": 2.743,
      "step": 556000
    },
    {
      "epoch": 180.25931928687197,
      "grad_norm": 1.0773983001708984,
      "learning_rate": 3.200227051573143e-05,
      "loss": 2.7239,
      "step": 556100
    },
    {
      "epoch": 180.29173419773096,
      "grad_norm": 1.0377662181854248,
      "learning_rate": 3.1999026921829386e-05,
      "loss": 2.7385,
      "step": 556200
    },
    {
      "epoch": 180.32414910858995,
      "grad_norm": 1.355271339416504,
      "learning_rate": 3.1995783327927345e-05,
      "loss": 2.7443,
      "step": 556300
    },
    {
      "epoch": 180.35656401944894,
      "grad_norm": 1.1733059883117676,
      "learning_rate": 3.1992539734025304e-05,
      "loss": 2.756,
      "step": 556400
    },
    {
      "epoch": 180.38897893030793,
      "grad_norm": 1.2377650737762451,
      "learning_rate": 3.198929614012326e-05,
      "loss": 2.7295,
      "step": 556500
    },
    {
      "epoch": 180.42139384116695,
      "grad_norm": 1.2446506023406982,
      "learning_rate": 3.1986052546221214e-05,
      "loss": 2.7302,
      "step": 556600
    },
    {
      "epoch": 180.45380875202594,
      "grad_norm": 1.1138553619384766,
      "learning_rate": 3.198280895231917e-05,
      "loss": 2.7531,
      "step": 556700
    },
    {
      "epoch": 180.48622366288492,
      "grad_norm": 1.2443044185638428,
      "learning_rate": 3.197956535841713e-05,
      "loss": 2.728,
      "step": 556800
    },
    {
      "epoch": 180.5186385737439,
      "grad_norm": 1.0276880264282227,
      "learning_rate": 3.1976321764515084e-05,
      "loss": 2.7189,
      "step": 556900
    },
    {
      "epoch": 180.5510534846029,
      "grad_norm": 1.203855276107788,
      "learning_rate": 3.197307817061304e-05,
      "loss": 2.7413,
      "step": 557000
    },
    {
      "epoch": 180.58346839546192,
      "grad_norm": 1.2237333059310913,
      "learning_rate": 3.1969834576711e-05,
      "loss": 2.7384,
      "step": 557100
    },
    {
      "epoch": 180.6158833063209,
      "grad_norm": 1.0825703144073486,
      "learning_rate": 3.196659098280895e-05,
      "loss": 2.724,
      "step": 557200
    },
    {
      "epoch": 180.6482982171799,
      "grad_norm": 1.382082223892212,
      "learning_rate": 3.196334738890691e-05,
      "loss": 2.7382,
      "step": 557300
    },
    {
      "epoch": 180.6807131280389,
      "grad_norm": 1.0545071363449097,
      "learning_rate": 3.1960103795004864e-05,
      "loss": 2.7565,
      "step": 557400
    },
    {
      "epoch": 180.7131280388979,
      "grad_norm": 1.0408884286880493,
      "learning_rate": 3.195686020110282e-05,
      "loss": 2.7321,
      "step": 557500
    },
    {
      "epoch": 180.7455429497569,
      "grad_norm": 1.2180510759353638,
      "learning_rate": 3.195361660720078e-05,
      "loss": 2.7396,
      "step": 557600
    },
    {
      "epoch": 180.77795786061589,
      "grad_norm": 1.1718589067459106,
      "learning_rate": 3.1950373013298733e-05,
      "loss": 2.7472,
      "step": 557700
    },
    {
      "epoch": 180.81037277147487,
      "grad_norm": 1.3292227983474731,
      "learning_rate": 3.194712941939669e-05,
      "loss": 2.737,
      "step": 557800
    },
    {
      "epoch": 180.84278768233386,
      "grad_norm": 1.1361597776412964,
      "learning_rate": 3.194388582549465e-05,
      "loss": 2.7343,
      "step": 557900
    },
    {
      "epoch": 180.87520259319288,
      "grad_norm": 1.0333991050720215,
      "learning_rate": 3.19406422315926e-05,
      "loss": 2.7246,
      "step": 558000
    },
    {
      "epoch": 180.90761750405187,
      "grad_norm": 1.1022098064422607,
      "learning_rate": 3.193739863769056e-05,
      "loss": 2.7683,
      "step": 558100
    },
    {
      "epoch": 180.94003241491086,
      "grad_norm": 1.1624289751052856,
      "learning_rate": 3.193415504378852e-05,
      "loss": 2.7526,
      "step": 558200
    },
    {
      "epoch": 180.97244732576985,
      "grad_norm": 1.2556877136230469,
      "learning_rate": 3.193091144988647e-05,
      "loss": 2.7361,
      "step": 558300
    },
    {
      "epoch": 181.0,
      "eval_bleu": 1.0164556603204224,
      "eval_loss": 3.983135461807251,
      "eval_runtime": 4.2492,
      "eval_samples_per_second": 115.788,
      "eval_steps_per_second": 1.883,
      "step": 558385
    },
    {
      "epoch": 181.00486223662884,
      "grad_norm": 1.151505947113037,
      "learning_rate": 3.192766785598443e-05,
      "loss": 2.7305,
      "step": 558400
    },
    {
      "epoch": 181.03727714748786,
      "grad_norm": 1.3695528507232666,
      "learning_rate": 3.192442426208238e-05,
      "loss": 2.7425,
      "step": 558500
    },
    {
      "epoch": 181.06969205834685,
      "grad_norm": 1.2548675537109375,
      "learning_rate": 3.192121310411937e-05,
      "loss": 2.7165,
      "step": 558600
    },
    {
      "epoch": 181.10210696920583,
      "grad_norm": 1.5118343830108643,
      "learning_rate": 3.191796951021732e-05,
      "loss": 2.739,
      "step": 558700
    },
    {
      "epoch": 181.13452188006482,
      "grad_norm": 1.4589145183563232,
      "learning_rate": 3.191472591631528e-05,
      "loss": 2.724,
      "step": 558800
    },
    {
      "epoch": 181.1669367909238,
      "grad_norm": 1.0247732400894165,
      "learning_rate": 3.191148232241323e-05,
      "loss": 2.7278,
      "step": 558900
    },
    {
      "epoch": 181.19935170178283,
      "grad_norm": 1.3151655197143555,
      "learning_rate": 3.190823872851119e-05,
      "loss": 2.7302,
      "step": 559000
    },
    {
      "epoch": 181.23176661264182,
      "grad_norm": 1.251983404159546,
      "learning_rate": 3.190499513460915e-05,
      "loss": 2.7332,
      "step": 559100
    },
    {
      "epoch": 181.2641815235008,
      "grad_norm": 1.3146378993988037,
      "learning_rate": 3.1901783976646126e-05,
      "loss": 2.7169,
      "step": 559200
    },
    {
      "epoch": 181.2965964343598,
      "grad_norm": 1.341422200202942,
      "learning_rate": 3.189854038274408e-05,
      "loss": 2.7233,
      "step": 559300
    },
    {
      "epoch": 181.3290113452188,
      "grad_norm": 1.2036329507827759,
      "learning_rate": 3.1895296788842036e-05,
      "loss": 2.7297,
      "step": 559400
    },
    {
      "epoch": 181.3614262560778,
      "grad_norm": 1.1783069372177124,
      "learning_rate": 3.1892053194939995e-05,
      "loss": 2.7313,
      "step": 559500
    },
    {
      "epoch": 181.3938411669368,
      "grad_norm": 1.0356942415237427,
      "learning_rate": 3.188880960103795e-05,
      "loss": 2.7316,
      "step": 559600
    },
    {
      "epoch": 181.42625607779578,
      "grad_norm": 1.082788109779358,
      "learning_rate": 3.1885566007135906e-05,
      "loss": 2.7465,
      "step": 559700
    },
    {
      "epoch": 181.45867098865477,
      "grad_norm": 1.1532020568847656,
      "learning_rate": 3.1882322413233864e-05,
      "loss": 2.7531,
      "step": 559800
    },
    {
      "epoch": 181.49108589951376,
      "grad_norm": 1.2037564516067505,
      "learning_rate": 3.187907881933182e-05,
      "loss": 2.7365,
      "step": 559900
    },
    {
      "epoch": 181.52350081037278,
      "grad_norm": 1.0463752746582031,
      "learning_rate": 3.187583522542978e-05,
      "loss": 2.7425,
      "step": 560000
    },
    {
      "epoch": 181.55591572123177,
      "grad_norm": 1.1116875410079956,
      "learning_rate": 3.1872591631527734e-05,
      "loss": 2.7397,
      "step": 560100
    },
    {
      "epoch": 181.58833063209076,
      "grad_norm": 1.1776336431503296,
      "learning_rate": 3.186934803762569e-05,
      "loss": 2.7176,
      "step": 560200
    },
    {
      "epoch": 181.62074554294975,
      "grad_norm": 1.1408179998397827,
      "learning_rate": 3.186610444372365e-05,
      "loss": 2.7389,
      "step": 560300
    },
    {
      "epoch": 181.65316045380874,
      "grad_norm": 1.3045594692230225,
      "learning_rate": 3.1862860849821603e-05,
      "loss": 2.7404,
      "step": 560400
    },
    {
      "epoch": 181.68557536466776,
      "grad_norm": 1.2809479236602783,
      "learning_rate": 3.185961725591956e-05,
      "loss": 2.7343,
      "step": 560500
    },
    {
      "epoch": 181.71799027552674,
      "grad_norm": 1.552378535270691,
      "learning_rate": 3.185637366201752e-05,
      "loss": 2.7495,
      "step": 560600
    },
    {
      "epoch": 181.75040518638573,
      "grad_norm": 1.3106919527053833,
      "learning_rate": 3.185313006811547e-05,
      "loss": 2.73,
      "step": 560700
    },
    {
      "epoch": 181.78282009724472,
      "grad_norm": 1.319186806678772,
      "learning_rate": 3.184988647421343e-05,
      "loss": 2.7647,
      "step": 560800
    },
    {
      "epoch": 181.81523500810374,
      "grad_norm": 1.2970521450042725,
      "learning_rate": 3.184664288031139e-05,
      "loss": 2.7336,
      "step": 560900
    },
    {
      "epoch": 181.84764991896273,
      "grad_norm": 1.3162648677825928,
      "learning_rate": 3.184339928640934e-05,
      "loss": 2.7386,
      "step": 561000
    },
    {
      "epoch": 181.88006482982172,
      "grad_norm": 1.1396385431289673,
      "learning_rate": 3.18401556925073e-05,
      "loss": 2.7451,
      "step": 561100
    },
    {
      "epoch": 181.9124797406807,
      "grad_norm": 1.2081856727600098,
      "learning_rate": 3.183691209860525e-05,
      "loss": 2.7365,
      "step": 561200
    },
    {
      "epoch": 181.9448946515397,
      "grad_norm": 0.9732006788253784,
      "learning_rate": 3.183366850470321e-05,
      "loss": 2.737,
      "step": 561300
    },
    {
      "epoch": 181.97730956239872,
      "grad_norm": 1.2984024286270142,
      "learning_rate": 3.183042491080117e-05,
      "loss": 2.7582,
      "step": 561400
    },
    {
      "epoch": 182.0,
      "eval_bleu": 0.8808562143288946,
      "eval_loss": 3.9857537746429443,
      "eval_runtime": 4.2882,
      "eval_samples_per_second": 114.733,
      "eval_steps_per_second": 1.866,
      "step": 561470
    },
    {
      "epoch": 182.0097244732577,
      "grad_norm": 1.229989767074585,
      "learning_rate": 3.182718131689912e-05,
      "loss": 2.7321,
      "step": 561500
    },
    {
      "epoch": 182.0421393841167,
      "grad_norm": 1.484879732131958,
      "learning_rate": 3.182393772299708e-05,
      "loss": 2.7284,
      "step": 561600
    },
    {
      "epoch": 182.07455429497568,
      "grad_norm": 1.2827306985855103,
      "learning_rate": 3.182069412909504e-05,
      "loss": 2.7149,
      "step": 561700
    },
    {
      "epoch": 182.10696920583467,
      "grad_norm": 1.176137924194336,
      "learning_rate": 3.181745053519299e-05,
      "loss": 2.7234,
      "step": 561800
    },
    {
      "epoch": 182.1393841166937,
      "grad_norm": 1.3606832027435303,
      "learning_rate": 3.181420694129095e-05,
      "loss": 2.723,
      "step": 561900
    },
    {
      "epoch": 182.17179902755268,
      "grad_norm": 1.2037320137023926,
      "learning_rate": 3.18109633473889e-05,
      "loss": 2.7176,
      "step": 562000
    },
    {
      "epoch": 182.20421393841167,
      "grad_norm": 1.1303824186325073,
      "learning_rate": 3.180771975348686e-05,
      "loss": 2.7405,
      "step": 562100
    },
    {
      "epoch": 182.23662884927066,
      "grad_norm": 1.1870609521865845,
      "learning_rate": 3.180447615958482e-05,
      "loss": 2.7387,
      "step": 562200
    },
    {
      "epoch": 182.26904376012965,
      "grad_norm": 1.1926047801971436,
      "learning_rate": 3.180123256568278e-05,
      "loss": 2.7358,
      "step": 562300
    },
    {
      "epoch": 182.30145867098867,
      "grad_norm": 1.1412357091903687,
      "learning_rate": 3.179798897178074e-05,
      "loss": 2.7289,
      "step": 562400
    },
    {
      "epoch": 182.33387358184766,
      "grad_norm": 1.1911457777023315,
      "learning_rate": 3.1794745377878696e-05,
      "loss": 2.7423,
      "step": 562500
    },
    {
      "epoch": 182.36628849270664,
      "grad_norm": 1.2561619281768799,
      "learning_rate": 3.179150178397665e-05,
      "loss": 2.7168,
      "step": 562600
    },
    {
      "epoch": 182.39870340356563,
      "grad_norm": 1.1044248342514038,
      "learning_rate": 3.178825819007461e-05,
      "loss": 2.7393,
      "step": 562700
    },
    {
      "epoch": 182.43111831442462,
      "grad_norm": 1.4505178928375244,
      "learning_rate": 3.1785014596172566e-05,
      "loss": 2.7332,
      "step": 562800
    },
    {
      "epoch": 182.46353322528364,
      "grad_norm": 1.1301013231277466,
      "learning_rate": 3.178177100227052e-05,
      "loss": 2.7461,
      "step": 562900
    },
    {
      "epoch": 182.49594813614263,
      "grad_norm": 1.283976435661316,
      "learning_rate": 3.1778527408368477e-05,
      "loss": 2.7408,
      "step": 563000
    },
    {
      "epoch": 182.52836304700162,
      "grad_norm": 1.0020688772201538,
      "learning_rate": 3.177528381446643e-05,
      "loss": 2.7309,
      "step": 563100
    },
    {
      "epoch": 182.5607779578606,
      "grad_norm": 1.1198443174362183,
      "learning_rate": 3.177207265650341e-05,
      "loss": 2.7423,
      "step": 563200
    },
    {
      "epoch": 182.5931928687196,
      "grad_norm": 1.1743823289871216,
      "learning_rate": 3.1768829062601365e-05,
      "loss": 2.728,
      "step": 563300
    },
    {
      "epoch": 182.62560777957862,
      "grad_norm": 1.2821515798568726,
      "learning_rate": 3.1765585468699324e-05,
      "loss": 2.7413,
      "step": 563400
    },
    {
      "epoch": 182.6580226904376,
      "grad_norm": 1.1408271789550781,
      "learning_rate": 3.1762341874797276e-05,
      "loss": 2.7358,
      "step": 563500
    },
    {
      "epoch": 182.6904376012966,
      "grad_norm": 1.0815688371658325,
      "learning_rate": 3.1759098280895234e-05,
      "loss": 2.7428,
      "step": 563600
    },
    {
      "epoch": 182.72285251215558,
      "grad_norm": 1.1528136730194092,
      "learning_rate": 3.175585468699319e-05,
      "loss": 2.7188,
      "step": 563700
    },
    {
      "epoch": 182.75526742301457,
      "grad_norm": 1.178290843963623,
      "learning_rate": 3.1752611093091145e-05,
      "loss": 2.7448,
      "step": 563800
    },
    {
      "epoch": 182.7876823338736,
      "grad_norm": 1.2281198501586914,
      "learning_rate": 3.1749367499189104e-05,
      "loss": 2.7378,
      "step": 563900
    },
    {
      "epoch": 182.82009724473258,
      "grad_norm": 1.1696357727050781,
      "learning_rate": 3.174612390528706e-05,
      "loss": 2.737,
      "step": 564000
    },
    {
      "epoch": 182.85251215559157,
      "grad_norm": 1.0021483898162842,
      "learning_rate": 3.1742880311385015e-05,
      "loss": 2.7219,
      "step": 564100
    },
    {
      "epoch": 182.88492706645056,
      "grad_norm": 1.3461846113204956,
      "learning_rate": 3.173963671748297e-05,
      "loss": 2.7349,
      "step": 564200
    },
    {
      "epoch": 182.91734197730958,
      "grad_norm": 1.1557528972625732,
      "learning_rate": 3.1736393123580925e-05,
      "loss": 2.7517,
      "step": 564300
    },
    {
      "epoch": 182.94975688816857,
      "grad_norm": 1.3706496953964233,
      "learning_rate": 3.1733149529678884e-05,
      "loss": 2.7407,
      "step": 564400
    },
    {
      "epoch": 182.98217179902755,
      "grad_norm": 1.135745644569397,
      "learning_rate": 3.172990593577684e-05,
      "loss": 2.7379,
      "step": 564500
    },
    {
      "epoch": 183.0,
      "eval_bleu": 0.9371458609267603,
      "eval_loss": 3.987555503845215,
      "eval_runtime": 4.4273,
      "eval_samples_per_second": 111.128,
      "eval_steps_per_second": 1.807,
      "step": 564555
    },
    {
      "epoch": 183.01458670988654,
      "grad_norm": 1.2124295234680176,
      "learning_rate": 3.1726662341874795e-05,
      "loss": 2.7404,
      "step": 564600
    },
    {
      "epoch": 183.04700162074553,
      "grad_norm": 1.114685297012329,
      "learning_rate": 3.1723418747972754e-05,
      "loss": 2.7306,
      "step": 564700
    },
    {
      "epoch": 183.07941653160455,
      "grad_norm": 1.0669844150543213,
      "learning_rate": 3.172017515407071e-05,
      "loss": 2.7312,
      "step": 564800
    },
    {
      "epoch": 183.11183144246354,
      "grad_norm": 1.1652801036834717,
      "learning_rate": 3.1716931560168664e-05,
      "loss": 2.7545,
      "step": 564900
    },
    {
      "epoch": 183.14424635332253,
      "grad_norm": 1.3512388467788696,
      "learning_rate": 3.171368796626662e-05,
      "loss": 2.7293,
      "step": 565000
    },
    {
      "epoch": 183.17666126418152,
      "grad_norm": 1.1125638484954834,
      "learning_rate": 3.171044437236458e-05,
      "loss": 2.7284,
      "step": 565100
    },
    {
      "epoch": 183.2090761750405,
      "grad_norm": 1.2196128368377686,
      "learning_rate": 3.170723321440156e-05,
      "loss": 2.733,
      "step": 565200
    },
    {
      "epoch": 183.24149108589953,
      "grad_norm": 1.231083869934082,
      "learning_rate": 3.170398962049951e-05,
      "loss": 2.7106,
      "step": 565300
    },
    {
      "epoch": 183.27390599675851,
      "grad_norm": 1.2075940370559692,
      "learning_rate": 3.170074602659747e-05,
      "loss": 2.7431,
      "step": 565400
    },
    {
      "epoch": 183.3063209076175,
      "grad_norm": 1.2888392210006714,
      "learning_rate": 3.169750243269543e-05,
      "loss": 2.7306,
      "step": 565500
    },
    {
      "epoch": 183.3387358184765,
      "grad_norm": 1.3909366130828857,
      "learning_rate": 3.169425883879338e-05,
      "loss": 2.7363,
      "step": 565600
    },
    {
      "epoch": 183.37115072933548,
      "grad_norm": 1.0192928314208984,
      "learning_rate": 3.169101524489134e-05,
      "loss": 2.7301,
      "step": 565700
    },
    {
      "epoch": 183.4035656401945,
      "grad_norm": 1.2264777421951294,
      "learning_rate": 3.16877716509893e-05,
      "loss": 2.7441,
      "step": 565800
    },
    {
      "epoch": 183.4359805510535,
      "grad_norm": 1.1589468717575073,
      "learning_rate": 3.168452805708726e-05,
      "loss": 2.7312,
      "step": 565900
    },
    {
      "epoch": 183.46839546191248,
      "grad_norm": 1.141924262046814,
      "learning_rate": 3.168128446318521e-05,
      "loss": 2.7442,
      "step": 566000
    },
    {
      "epoch": 183.50081037277147,
      "grad_norm": 1.156373143196106,
      "learning_rate": 3.167804086928317e-05,
      "loss": 2.7191,
      "step": 566100
    },
    {
      "epoch": 183.53322528363046,
      "grad_norm": 1.166367530822754,
      "learning_rate": 3.167479727538113e-05,
      "loss": 2.7327,
      "step": 566200
    },
    {
      "epoch": 183.56564019448948,
      "grad_norm": 1.1374180316925049,
      "learning_rate": 3.1671553681479085e-05,
      "loss": 2.7261,
      "step": 566300
    },
    {
      "epoch": 183.59805510534846,
      "grad_norm": 1.3187612295150757,
      "learning_rate": 3.166831008757704e-05,
      "loss": 2.729,
      "step": 566400
    },
    {
      "epoch": 183.63047001620745,
      "grad_norm": 1.4065431356430054,
      "learning_rate": 3.1665066493674996e-05,
      "loss": 2.7332,
      "step": 566500
    },
    {
      "epoch": 183.66288492706644,
      "grad_norm": 1.1270724534988403,
      "learning_rate": 3.1661822899772955e-05,
      "loss": 2.7272,
      "step": 566600
    },
    {
      "epoch": 183.69529983792543,
      "grad_norm": 1.2442567348480225,
      "learning_rate": 3.165857930587091e-05,
      "loss": 2.7475,
      "step": 566700
    },
    {
      "epoch": 183.72771474878445,
      "grad_norm": 1.1117457151412964,
      "learning_rate": 3.1655335711968866e-05,
      "loss": 2.7182,
      "step": 566800
    },
    {
      "epoch": 183.76012965964344,
      "grad_norm": 1.3831983804702759,
      "learning_rate": 3.165209211806682e-05,
      "loss": 2.7489,
      "step": 566900
    },
    {
      "epoch": 183.79254457050243,
      "grad_norm": 1.1896382570266724,
      "learning_rate": 3.1648848524164776e-05,
      "loss": 2.7424,
      "step": 567000
    },
    {
      "epoch": 183.82495948136142,
      "grad_norm": 1.0582517385482788,
      "learning_rate": 3.1645604930262735e-05,
      "loss": 2.7137,
      "step": 567100
    },
    {
      "epoch": 183.8573743922204,
      "grad_norm": 1.0910331010818481,
      "learning_rate": 3.164239377229971e-05,
      "loss": 2.7652,
      "step": 567200
    },
    {
      "epoch": 183.88978930307943,
      "grad_norm": 1.0518053770065308,
      "learning_rate": 3.1639150178397665e-05,
      "loss": 2.7327,
      "step": 567300
    },
    {
      "epoch": 183.92220421393841,
      "grad_norm": 1.162092685699463,
      "learning_rate": 3.1635906584495623e-05,
      "loss": 2.7331,
      "step": 567400
    },
    {
      "epoch": 183.9546191247974,
      "grad_norm": 1.2303065061569214,
      "learning_rate": 3.163266299059358e-05,
      "loss": 2.7252,
      "step": 567500
    },
    {
      "epoch": 183.9870340356564,
      "grad_norm": 1.5329521894454956,
      "learning_rate": 3.1629419396691534e-05,
      "loss": 2.7186,
      "step": 567600
    },
    {
      "epoch": 184.0,
      "eval_bleu": 0.8012377775104981,
      "eval_loss": 3.995018720626831,
      "eval_runtime": 3.8352,
      "eval_samples_per_second": 128.286,
      "eval_steps_per_second": 2.086,
      "step": 567640
    },
    {
      "epoch": 184.0194489465154,
      "grad_norm": 1.2379125356674194,
      "learning_rate": 3.162617580278949e-05,
      "loss": 2.7313,
      "step": 567700
    },
    {
      "epoch": 184.0518638573744,
      "grad_norm": 1.2256840467453003,
      "learning_rate": 3.162293220888745e-05,
      "loss": 2.7122,
      "step": 567800
    },
    {
      "epoch": 184.0842787682334,
      "grad_norm": 1.293023943901062,
      "learning_rate": 3.1619688614985404e-05,
      "loss": 2.7215,
      "step": 567900
    },
    {
      "epoch": 184.11669367909238,
      "grad_norm": 1.1175509691238403,
      "learning_rate": 3.161644502108336e-05,
      "loss": 2.7194,
      "step": 568000
    },
    {
      "epoch": 184.14910858995137,
      "grad_norm": 1.2430347204208374,
      "learning_rate": 3.1613201427181314e-05,
      "loss": 2.7087,
      "step": 568100
    },
    {
      "epoch": 184.18152350081039,
      "grad_norm": 1.2052698135375977,
      "learning_rate": 3.160995783327927e-05,
      "loss": 2.7246,
      "step": 568200
    },
    {
      "epoch": 184.21393841166937,
      "grad_norm": 1.0975282192230225,
      "learning_rate": 3.160671423937723e-05,
      "loss": 2.7298,
      "step": 568300
    },
    {
      "epoch": 184.24635332252836,
      "grad_norm": 1.4143458604812622,
      "learning_rate": 3.1603470645475184e-05,
      "loss": 2.7556,
      "step": 568400
    },
    {
      "epoch": 184.27876823338735,
      "grad_norm": 1.332377314567566,
      "learning_rate": 3.160022705157314e-05,
      "loss": 2.7512,
      "step": 568500
    },
    {
      "epoch": 184.31118314424634,
      "grad_norm": 1.2320199012756348,
      "learning_rate": 3.159701589361012e-05,
      "loss": 2.74,
      "step": 568600
    },
    {
      "epoch": 184.34359805510536,
      "grad_norm": 1.2775522470474243,
      "learning_rate": 3.159377229970808e-05,
      "loss": 2.7426,
      "step": 568700
    },
    {
      "epoch": 184.37601296596435,
      "grad_norm": 1.1559759378433228,
      "learning_rate": 3.159052870580603e-05,
      "loss": 2.7226,
      "step": 568800
    },
    {
      "epoch": 184.40842787682334,
      "grad_norm": 1.190040946006775,
      "learning_rate": 3.158728511190399e-05,
      "loss": 2.7089,
      "step": 568900
    },
    {
      "epoch": 184.44084278768233,
      "grad_norm": 1.4375066757202148,
      "learning_rate": 3.158404151800195e-05,
      "loss": 2.7384,
      "step": 569000
    },
    {
      "epoch": 184.47325769854132,
      "grad_norm": 1.0737287998199463,
      "learning_rate": 3.15807979240999e-05,
      "loss": 2.7279,
      "step": 569100
    },
    {
      "epoch": 184.50567260940034,
      "grad_norm": 1.4458931684494019,
      "learning_rate": 3.157755433019786e-05,
      "loss": 2.7408,
      "step": 569200
    },
    {
      "epoch": 184.53808752025932,
      "grad_norm": 1.437193512916565,
      "learning_rate": 3.157431073629581e-05,
      "loss": 2.7296,
      "step": 569300
    },
    {
      "epoch": 184.5705024311183,
      "grad_norm": 1.2247815132141113,
      "learning_rate": 3.157106714239377e-05,
      "loss": 2.7161,
      "step": 569400
    },
    {
      "epoch": 184.6029173419773,
      "grad_norm": 1.2466880083084106,
      "learning_rate": 3.156782354849173e-05,
      "loss": 2.7561,
      "step": 569500
    },
    {
      "epoch": 184.6353322528363,
      "grad_norm": 1.0893261432647705,
      "learning_rate": 3.156457995458969e-05,
      "loss": 2.7394,
      "step": 569600
    },
    {
      "epoch": 184.6677471636953,
      "grad_norm": 1.2431551218032837,
      "learning_rate": 3.1561336360687646e-05,
      "loss": 2.7213,
      "step": 569700
    },
    {
      "epoch": 184.7001620745543,
      "grad_norm": 1.3076342344284058,
      "learning_rate": 3.1558092766785605e-05,
      "loss": 2.7516,
      "step": 569800
    },
    {
      "epoch": 184.7325769854133,
      "grad_norm": 1.3271139860153198,
      "learning_rate": 3.155484917288356e-05,
      "loss": 2.7144,
      "step": 569900
    },
    {
      "epoch": 184.76499189627228,
      "grad_norm": 1.1369833946228027,
      "learning_rate": 3.1551605578981516e-05,
      "loss": 2.737,
      "step": 570000
    },
    {
      "epoch": 184.79740680713127,
      "grad_norm": 1.2718249559402466,
      "learning_rate": 3.1548361985079474e-05,
      "loss": 2.7324,
      "step": 570100
    },
    {
      "epoch": 184.82982171799028,
      "grad_norm": 1.1861565113067627,
      "learning_rate": 3.1545118391177426e-05,
      "loss": 2.7164,
      "step": 570200
    },
    {
      "epoch": 184.86223662884927,
      "grad_norm": 1.1033159494400024,
      "learning_rate": 3.1541874797275385e-05,
      "loss": 2.7312,
      "step": 570300
    },
    {
      "epoch": 184.89465153970826,
      "grad_norm": 1.175022006034851,
      "learning_rate": 3.153863120337334e-05,
      "loss": 2.7452,
      "step": 570400
    },
    {
      "epoch": 184.92706645056725,
      "grad_norm": 1.1407922506332397,
      "learning_rate": 3.1535387609471296e-05,
      "loss": 2.7381,
      "step": 570500
    },
    {
      "epoch": 184.95948136142624,
      "grad_norm": 1.10029935836792,
      "learning_rate": 3.1532144015569255e-05,
      "loss": 2.7146,
      "step": 570600
    },
    {
      "epoch": 184.99189627228526,
      "grad_norm": 1.2381577491760254,
      "learning_rate": 3.152893285760623e-05,
      "loss": 2.7392,
      "step": 570700
    },
    {
      "epoch": 185.0,
      "eval_bleu": 0.9815388521099738,
      "eval_loss": 3.991222381591797,
      "eval_runtime": 4.033,
      "eval_samples_per_second": 121.993,
      "eval_steps_per_second": 1.984,
      "step": 570725
    },
    {
      "epoch": 185.02431118314425,
      "grad_norm": 1.1236833333969116,
      "learning_rate": 3.1525689263704184e-05,
      "loss": 2.7151,
      "step": 570800
    },
    {
      "epoch": 185.05672609400324,
      "grad_norm": 1.042257308959961,
      "learning_rate": 3.152244566980214e-05,
      "loss": 2.7182,
      "step": 570900
    },
    {
      "epoch": 185.08914100486223,
      "grad_norm": 1.3227468729019165,
      "learning_rate": 3.15192020759001e-05,
      "loss": 2.7266,
      "step": 571000
    },
    {
      "epoch": 185.12155591572125,
      "grad_norm": 1.1773988008499146,
      "learning_rate": 3.1515958481998054e-05,
      "loss": 2.7279,
      "step": 571100
    },
    {
      "epoch": 185.15397082658023,
      "grad_norm": 1.150628685951233,
      "learning_rate": 3.151271488809601e-05,
      "loss": 2.7239,
      "step": 571200
    },
    {
      "epoch": 185.18638573743922,
      "grad_norm": 1.104272484779358,
      "learning_rate": 3.150947129419397e-05,
      "loss": 2.7328,
      "step": 571300
    },
    {
      "epoch": 185.2188006482982,
      "grad_norm": 1.2456825971603394,
      "learning_rate": 3.150622770029192e-05,
      "loss": 2.7372,
      "step": 571400
    },
    {
      "epoch": 185.2512155591572,
      "grad_norm": 1.2605853080749512,
      "learning_rate": 3.150298410638988e-05,
      "loss": 2.7148,
      "step": 571500
    },
    {
      "epoch": 185.28363047001622,
      "grad_norm": 1.20473313331604,
      "learning_rate": 3.1499740512487834e-05,
      "loss": 2.7125,
      "step": 571600
    },
    {
      "epoch": 185.3160453808752,
      "grad_norm": 1.196596384048462,
      "learning_rate": 3.149649691858579e-05,
      "loss": 2.7183,
      "step": 571700
    },
    {
      "epoch": 185.3484602917342,
      "grad_norm": 1.183541178703308,
      "learning_rate": 3.149325332468375e-05,
      "loss": 2.7477,
      "step": 571800
    },
    {
      "epoch": 185.3808752025932,
      "grad_norm": 1.2161002159118652,
      "learning_rate": 3.1490009730781703e-05,
      "loss": 2.7358,
      "step": 571900
    },
    {
      "epoch": 185.41329011345218,
      "grad_norm": 1.2112189531326294,
      "learning_rate": 3.148676613687966e-05,
      "loss": 2.747,
      "step": 572000
    },
    {
      "epoch": 185.4457050243112,
      "grad_norm": 1.147985816001892,
      "learning_rate": 3.148352254297762e-05,
      "loss": 2.7311,
      "step": 572100
    },
    {
      "epoch": 185.47811993517018,
      "grad_norm": 1.050855278968811,
      "learning_rate": 3.148027894907557e-05,
      "loss": 2.7385,
      "step": 572200
    },
    {
      "epoch": 185.51053484602917,
      "grad_norm": 1.5377305746078491,
      "learning_rate": 3.147703535517353e-05,
      "loss": 2.7232,
      "step": 572300
    },
    {
      "epoch": 185.54294975688816,
      "grad_norm": 1.2596272230148315,
      "learning_rate": 3.147382419721051e-05,
      "loss": 2.73,
      "step": 572400
    },
    {
      "epoch": 185.57536466774715,
      "grad_norm": 1.1694958209991455,
      "learning_rate": 3.147058060330847e-05,
      "loss": 2.7328,
      "step": 572500
    },
    {
      "epoch": 185.60777957860617,
      "grad_norm": 1.2038943767547607,
      "learning_rate": 3.146733700940642e-05,
      "loss": 2.7404,
      "step": 572600
    },
    {
      "epoch": 185.64019448946516,
      "grad_norm": 1.1490910053253174,
      "learning_rate": 3.146409341550438e-05,
      "loss": 2.754,
      "step": 572700
    },
    {
      "epoch": 185.67260940032415,
      "grad_norm": 1.110521674156189,
      "learning_rate": 3.146084982160234e-05,
      "loss": 2.7048,
      "step": 572800
    },
    {
      "epoch": 185.70502431118314,
      "grad_norm": 1.5549297332763672,
      "learning_rate": 3.145760622770029e-05,
      "loss": 2.7161,
      "step": 572900
    },
    {
      "epoch": 185.73743922204213,
      "grad_norm": 1.022109031677246,
      "learning_rate": 3.145436263379825e-05,
      "loss": 2.7342,
      "step": 573000
    },
    {
      "epoch": 185.76985413290114,
      "grad_norm": 1.3931570053100586,
      "learning_rate": 3.145111903989621e-05,
      "loss": 2.7432,
      "step": 573100
    },
    {
      "epoch": 185.80226904376013,
      "grad_norm": 1.2649245262145996,
      "learning_rate": 3.1447875445994166e-05,
      "loss": 2.7194,
      "step": 573200
    },
    {
      "epoch": 185.83468395461912,
      "grad_norm": 1.0887489318847656,
      "learning_rate": 3.1444631852092125e-05,
      "loss": 2.7324,
      "step": 573300
    },
    {
      "epoch": 185.8670988654781,
      "grad_norm": 1.104629397392273,
      "learning_rate": 3.1441388258190076e-05,
      "loss": 2.7201,
      "step": 573400
    },
    {
      "epoch": 185.8995137763371,
      "grad_norm": 1.2361462116241455,
      "learning_rate": 3.1438144664288035e-05,
      "loss": 2.7488,
      "step": 573500
    },
    {
      "epoch": 185.93192868719612,
      "grad_norm": 1.2101285457611084,
      "learning_rate": 3.1434901070385994e-05,
      "loss": 2.7364,
      "step": 573600
    },
    {
      "epoch": 185.9643435980551,
      "grad_norm": 1.3775626420974731,
      "learning_rate": 3.1431657476483946e-05,
      "loss": 2.7324,
      "step": 573700
    },
    {
      "epoch": 185.9967585089141,
      "grad_norm": 1.2892621755599976,
      "learning_rate": 3.1428413882581905e-05,
      "loss": 2.7221,
      "step": 573800
    },
    {
      "epoch": 186.0,
      "eval_bleu": 0.9675561169725665,
      "eval_loss": 3.9928526878356934,
      "eval_runtime": 3.6924,
      "eval_samples_per_second": 133.246,
      "eval_steps_per_second": 2.167,
      "step": 573810
    },
    {
      "epoch": 186.0291734197731,
      "grad_norm": 1.3422085046768188,
      "learning_rate": 3.142517028867986e-05,
      "loss": 2.7301,
      "step": 573900
    },
    {
      "epoch": 186.06158833063208,
      "grad_norm": 1.4070614576339722,
      "learning_rate": 3.1421926694777815e-05,
      "loss": 2.7233,
      "step": 574000
    },
    {
      "epoch": 186.0940032414911,
      "grad_norm": 1.2028921842575073,
      "learning_rate": 3.1418683100875774e-05,
      "loss": 2.724,
      "step": 574100
    },
    {
      "epoch": 186.12641815235008,
      "grad_norm": 1.1631348133087158,
      "learning_rate": 3.1415439506973726e-05,
      "loss": 2.7457,
      "step": 574200
    },
    {
      "epoch": 186.15883306320907,
      "grad_norm": 1.117144227027893,
      "learning_rate": 3.1412195913071685e-05,
      "loss": 2.7252,
      "step": 574300
    },
    {
      "epoch": 186.19124797406806,
      "grad_norm": 1.3001484870910645,
      "learning_rate": 3.1408952319169644e-05,
      "loss": 2.7387,
      "step": 574400
    },
    {
      "epoch": 186.22366288492708,
      "grad_norm": 1.1717983484268188,
      "learning_rate": 3.1405708725267596e-05,
      "loss": 2.7138,
      "step": 574500
    },
    {
      "epoch": 186.25607779578607,
      "grad_norm": 1.3979816436767578,
      "learning_rate": 3.1402465131365554e-05,
      "loss": 2.7299,
      "step": 574600
    },
    {
      "epoch": 186.28849270664506,
      "grad_norm": 1.0924303531646729,
      "learning_rate": 3.139922153746351e-05,
      "loss": 2.7111,
      "step": 574700
    },
    {
      "epoch": 186.32090761750405,
      "grad_norm": 1.177008867263794,
      "learning_rate": 3.1395977943561465e-05,
      "loss": 2.7242,
      "step": 574800
    },
    {
      "epoch": 186.35332252836304,
      "grad_norm": 1.1544314622879028,
      "learning_rate": 3.1392734349659424e-05,
      "loss": 2.7025,
      "step": 574900
    },
    {
      "epoch": 186.38573743922205,
      "grad_norm": 1.3199985027313232,
      "learning_rate": 3.1389490755757376e-05,
      "loss": 2.7139,
      "step": 575000
    },
    {
      "epoch": 186.41815235008104,
      "grad_norm": 1.05278480052948,
      "learning_rate": 3.1386247161855334e-05,
      "loss": 2.7257,
      "step": 575100
    },
    {
      "epoch": 186.45056726094003,
      "grad_norm": 1.3106696605682373,
      "learning_rate": 3.138300356795329e-05,
      "loss": 2.7436,
      "step": 575200
    },
    {
      "epoch": 186.48298217179902,
      "grad_norm": 1.3695144653320312,
      "learning_rate": 3.1379759974051245e-05,
      "loss": 2.7089,
      "step": 575300
    },
    {
      "epoch": 186.515397082658,
      "grad_norm": 1.490317940711975,
      "learning_rate": 3.1376516380149204e-05,
      "loss": 2.737,
      "step": 575400
    },
    {
      "epoch": 186.54781199351703,
      "grad_norm": 1.2249683141708374,
      "learning_rate": 3.137327278624716e-05,
      "loss": 2.7324,
      "step": 575500
    },
    {
      "epoch": 186.58022690437602,
      "grad_norm": 1.2787549495697021,
      "learning_rate": 3.137002919234512e-05,
      "loss": 2.7006,
      "step": 575600
    },
    {
      "epoch": 186.612641815235,
      "grad_norm": 1.1801764965057373,
      "learning_rate": 3.136678559844308e-05,
      "loss": 2.7266,
      "step": 575700
    },
    {
      "epoch": 186.645056726094,
      "grad_norm": 1.3350129127502441,
      "learning_rate": 3.136354200454104e-05,
      "loss": 2.7203,
      "step": 575800
    },
    {
      "epoch": 186.677471636953,
      "grad_norm": 1.3588321208953857,
      "learning_rate": 3.136029841063899e-05,
      "loss": 2.7306,
      "step": 575900
    },
    {
      "epoch": 186.709886547812,
      "grad_norm": 1.183034062385559,
      "learning_rate": 3.135705481673695e-05,
      "loss": 2.7425,
      "step": 576000
    },
    {
      "epoch": 186.742301458671,
      "grad_norm": 1.1904780864715576,
      "learning_rate": 3.13538112228349e-05,
      "loss": 2.7454,
      "step": 576100
    },
    {
      "epoch": 186.77471636952998,
      "grad_norm": 1.2405149936676025,
      "learning_rate": 3.135056762893286e-05,
      "loss": 2.7224,
      "step": 576200
    },
    {
      "epoch": 186.80713128038897,
      "grad_norm": 1.2470382452011108,
      "learning_rate": 3.134732403503082e-05,
      "loss": 2.7225,
      "step": 576300
    },
    {
      "epoch": 186.83954619124796,
      "grad_norm": 1.141697883605957,
      "learning_rate": 3.134408044112877e-05,
      "loss": 2.7162,
      "step": 576400
    },
    {
      "epoch": 186.87196110210698,
      "grad_norm": 1.1681276559829712,
      "learning_rate": 3.134083684722673e-05,
      "loss": 2.7324,
      "step": 576500
    },
    {
      "epoch": 186.90437601296597,
      "grad_norm": 1.1118282079696655,
      "learning_rate": 3.133759325332469e-05,
      "loss": 2.7215,
      "step": 576600
    },
    {
      "epoch": 186.93679092382496,
      "grad_norm": 1.1715266704559326,
      "learning_rate": 3.133434965942264e-05,
      "loss": 2.736,
      "step": 576700
    },
    {
      "epoch": 186.96920583468395,
      "grad_norm": 1.2496824264526367,
      "learning_rate": 3.13311060655206e-05,
      "loss": 2.7392,
      "step": 576800
    },
    {
      "epoch": 187.0,
      "eval_bleu": 1.0375204913802776,
      "eval_loss": 3.995023250579834,
      "eval_runtime": 3.9017,
      "eval_samples_per_second": 126.099,
      "eval_steps_per_second": 2.05,
      "step": 576895
    },
    {
      "epoch": 187.00162074554294,
      "grad_norm": 1.2338519096374512,
      "learning_rate": 3.132786247161856e-05,
      "loss": 2.7313,
      "step": 576900
    },
    {
      "epoch": 187.03403565640195,
      "grad_norm": 1.4329206943511963,
      "learning_rate": 3.132461887771651e-05,
      "loss": 2.7205,
      "step": 577000
    },
    {
      "epoch": 187.06645056726094,
      "grad_norm": 1.3474828004837036,
      "learning_rate": 3.132137528381447e-05,
      "loss": 2.722,
      "step": 577100
    },
    {
      "epoch": 187.09886547811993,
      "grad_norm": 1.329558253288269,
      "learning_rate": 3.131813168991242e-05,
      "loss": 2.7185,
      "step": 577200
    },
    {
      "epoch": 187.13128038897892,
      "grad_norm": 1.1700743436813354,
      "learning_rate": 3.131488809601038e-05,
      "loss": 2.7035,
      "step": 577300
    },
    {
      "epoch": 187.1636952998379,
      "grad_norm": 1.3750100135803223,
      "learning_rate": 3.131164450210834e-05,
      "loss": 2.7174,
      "step": 577400
    },
    {
      "epoch": 187.19611021069693,
      "grad_norm": 1.0092802047729492,
      "learning_rate": 3.130840090820629e-05,
      "loss": 2.7174,
      "step": 577500
    },
    {
      "epoch": 187.22852512155592,
      "grad_norm": 1.0486804246902466,
      "learning_rate": 3.130515731430425e-05,
      "loss": 2.692,
      "step": 577600
    },
    {
      "epoch": 187.2609400324149,
      "grad_norm": 1.389314889907837,
      "learning_rate": 3.130191372040221e-05,
      "loss": 2.7264,
      "step": 577700
    },
    {
      "epoch": 187.2933549432739,
      "grad_norm": 1.2013025283813477,
      "learning_rate": 3.129867012650016e-05,
      "loss": 2.7199,
      "step": 577800
    },
    {
      "epoch": 187.32576985413291,
      "grad_norm": 1.0832704305648804,
      "learning_rate": 3.129542653259812e-05,
      "loss": 2.7232,
      "step": 577900
    },
    {
      "epoch": 187.3581847649919,
      "grad_norm": 1.31478750705719,
      "learning_rate": 3.129218293869608e-05,
      "loss": 2.7248,
      "step": 578000
    },
    {
      "epoch": 187.3905996758509,
      "grad_norm": 1.0341709852218628,
      "learning_rate": 3.1288939344794036e-05,
      "loss": 2.7254,
      "step": 578100
    },
    {
      "epoch": 187.42301458670988,
      "grad_norm": 1.1685870885849,
      "learning_rate": 3.1285695750891995e-05,
      "loss": 2.6888,
      "step": 578200
    },
    {
      "epoch": 187.45542949756887,
      "grad_norm": 1.092115044593811,
      "learning_rate": 3.1282452156989946e-05,
      "loss": 2.7288,
      "step": 578300
    },
    {
      "epoch": 187.4878444084279,
      "grad_norm": 1.1927123069763184,
      "learning_rate": 3.127924099902692e-05,
      "loss": 2.7171,
      "step": 578400
    },
    {
      "epoch": 187.52025931928688,
      "grad_norm": 1.1414037942886353,
      "learning_rate": 3.1275997405124876e-05,
      "loss": 2.7233,
      "step": 578500
    },
    {
      "epoch": 187.55267423014587,
      "grad_norm": 1.4083677530288696,
      "learning_rate": 3.1272753811222835e-05,
      "loss": 2.7138,
      "step": 578600
    },
    {
      "epoch": 187.58508914100486,
      "grad_norm": 1.2448508739471436,
      "learning_rate": 3.1269510217320794e-05,
      "loss": 2.7528,
      "step": 578700
    },
    {
      "epoch": 187.61750405186385,
      "grad_norm": 1.2573920488357544,
      "learning_rate": 3.126626662341875e-05,
      "loss": 2.7515,
      "step": 578800
    },
    {
      "epoch": 187.64991896272286,
      "grad_norm": 1.263382077217102,
      "learning_rate": 3.126302302951671e-05,
      "loss": 2.7633,
      "step": 578900
    },
    {
      "epoch": 187.68233387358185,
      "grad_norm": 1.2596139907836914,
      "learning_rate": 3.125977943561466e-05,
      "loss": 2.7272,
      "step": 579000
    },
    {
      "epoch": 187.71474878444084,
      "grad_norm": 1.0883831977844238,
      "learning_rate": 3.125653584171262e-05,
      "loss": 2.7328,
      "step": 579100
    },
    {
      "epoch": 187.74716369529983,
      "grad_norm": 1.1822313070297241,
      "learning_rate": 3.125329224781058e-05,
      "loss": 2.7142,
      "step": 579200
    },
    {
      "epoch": 187.77957860615882,
      "grad_norm": 1.3860316276550293,
      "learning_rate": 3.125004865390853e-05,
      "loss": 2.7368,
      "step": 579300
    },
    {
      "epoch": 187.81199351701784,
      "grad_norm": 1.176101565361023,
      "learning_rate": 3.124680506000649e-05,
      "loss": 2.7305,
      "step": 579400
    },
    {
      "epoch": 187.84440842787683,
      "grad_norm": 1.2566821575164795,
      "learning_rate": 3.124356146610444e-05,
      "loss": 2.7274,
      "step": 579500
    },
    {
      "epoch": 187.87682333873582,
      "grad_norm": 1.445021152496338,
      "learning_rate": 3.12403178722024e-05,
      "loss": 2.7363,
      "step": 579600
    },
    {
      "epoch": 187.9092382495948,
      "grad_norm": 1.1853868961334229,
      "learning_rate": 3.123707427830036e-05,
      "loss": 2.7258,
      "step": 579700
    },
    {
      "epoch": 187.9416531604538,
      "grad_norm": 1.6190612316131592,
      "learning_rate": 3.123383068439831e-05,
      "loss": 2.7442,
      "step": 579800
    },
    {
      "epoch": 187.97406807131281,
      "grad_norm": 1.1630253791809082,
      "learning_rate": 3.123058709049627e-05,
      "loss": 2.7375,
      "step": 579900
    },
    {
      "epoch": 188.0,
      "eval_bleu": 0.9310745567570439,
      "eval_loss": 3.9969727993011475,
      "eval_runtime": 4.0662,
      "eval_samples_per_second": 120.997,
      "eval_steps_per_second": 1.967,
      "step": 579980
    },
    {
      "epoch": 188.0064829821718,
      "grad_norm": 1.0401908159255981,
      "learning_rate": 3.122734349659423e-05,
      "loss": 2.7394,
      "step": 580000
    },
    {
      "epoch": 188.0388978930308,
      "grad_norm": 1.1269506216049194,
      "learning_rate": 3.122409990269218e-05,
      "loss": 2.7049,
      "step": 580100
    },
    {
      "epoch": 188.07131280388978,
      "grad_norm": 1.2114123106002808,
      "learning_rate": 3.122085630879014e-05,
      "loss": 2.7167,
      "step": 580200
    },
    {
      "epoch": 188.10372771474877,
      "grad_norm": 1.0889992713928223,
      "learning_rate": 3.121761271488809e-05,
      "loss": 2.717,
      "step": 580300
    },
    {
      "epoch": 188.1361426256078,
      "grad_norm": 1.3284661769866943,
      "learning_rate": 3.121440155692508e-05,
      "loss": 2.718,
      "step": 580400
    },
    {
      "epoch": 188.16855753646678,
      "grad_norm": 1.149793028831482,
      "learning_rate": 3.121115796302303e-05,
      "loss": 2.7324,
      "step": 580500
    },
    {
      "epoch": 188.20097244732577,
      "grad_norm": 1.1568503379821777,
      "learning_rate": 3.120791436912099e-05,
      "loss": 2.7277,
      "step": 580600
    },
    {
      "epoch": 188.23338735818476,
      "grad_norm": 1.0437580347061157,
      "learning_rate": 3.1204703211157966e-05,
      "loss": 2.737,
      "step": 580700
    },
    {
      "epoch": 188.26580226904375,
      "grad_norm": 1.3357875347137451,
      "learning_rate": 3.1201459617255925e-05,
      "loss": 2.7277,
      "step": 580800
    },
    {
      "epoch": 188.29821717990276,
      "grad_norm": 1.1378461122512817,
      "learning_rate": 3.119821602335388e-05,
      "loss": 2.7196,
      "step": 580900
    },
    {
      "epoch": 188.33063209076175,
      "grad_norm": 1.150238037109375,
      "learning_rate": 3.1194972429451835e-05,
      "loss": 2.7228,
      "step": 581000
    },
    {
      "epoch": 188.36304700162074,
      "grad_norm": 1.2844828367233276,
      "learning_rate": 3.119172883554979e-05,
      "loss": 2.7314,
      "step": 581100
    },
    {
      "epoch": 188.39546191247973,
      "grad_norm": 1.2411727905273438,
      "learning_rate": 3.1188485241647746e-05,
      "loss": 2.7351,
      "step": 581200
    },
    {
      "epoch": 188.42787682333875,
      "grad_norm": 1.1774682998657227,
      "learning_rate": 3.1185241647745705e-05,
      "loss": 2.7187,
      "step": 581300
    },
    {
      "epoch": 188.46029173419774,
      "grad_norm": 1.0975275039672852,
      "learning_rate": 3.118199805384366e-05,
      "loss": 2.7052,
      "step": 581400
    },
    {
      "epoch": 188.49270664505673,
      "grad_norm": 1.2359617948532104,
      "learning_rate": 3.1178754459941616e-05,
      "loss": 2.7128,
      "step": 581500
    },
    {
      "epoch": 188.52512155591572,
      "grad_norm": 1.1967766284942627,
      "learning_rate": 3.1175510866039574e-05,
      "loss": 2.7295,
      "step": 581600
    },
    {
      "epoch": 188.5575364667747,
      "grad_norm": 1.2702046632766724,
      "learning_rate": 3.1172267272137526e-05,
      "loss": 2.7085,
      "step": 581700
    },
    {
      "epoch": 188.58995137763372,
      "grad_norm": 1.1230411529541016,
      "learning_rate": 3.1169023678235485e-05,
      "loss": 2.7319,
      "step": 581800
    },
    {
      "epoch": 188.6223662884927,
      "grad_norm": 1.209801435470581,
      "learning_rate": 3.116578008433344e-05,
      "loss": 2.7144,
      "step": 581900
    },
    {
      "epoch": 188.6547811993517,
      "grad_norm": 1.2349427938461304,
      "learning_rate": 3.1162536490431396e-05,
      "loss": 2.7219,
      "step": 582000
    },
    {
      "epoch": 188.6871961102107,
      "grad_norm": 1.2821980714797974,
      "learning_rate": 3.1159292896529355e-05,
      "loss": 2.7261,
      "step": 582100
    },
    {
      "epoch": 188.71961102106968,
      "grad_norm": 1.1484105587005615,
      "learning_rate": 3.115604930262731e-05,
      "loss": 2.7249,
      "step": 582200
    },
    {
      "epoch": 188.7520259319287,
      "grad_norm": 1.1155792474746704,
      "learning_rate": 3.115280570872527e-05,
      "loss": 2.7279,
      "step": 582300
    },
    {
      "epoch": 188.7844408427877,
      "grad_norm": 1.1672718524932861,
      "learning_rate": 3.114956211482323e-05,
      "loss": 2.7254,
      "step": 582400
    },
    {
      "epoch": 188.81685575364668,
      "grad_norm": 1.1378536224365234,
      "learning_rate": 3.114631852092118e-05,
      "loss": 2.7166,
      "step": 582500
    },
    {
      "epoch": 188.84927066450567,
      "grad_norm": 1.1983637809753418,
      "learning_rate": 3.114307492701914e-05,
      "loss": 2.7177,
      "step": 582600
    },
    {
      "epoch": 188.88168557536466,
      "grad_norm": 1.1062747240066528,
      "learning_rate": 3.11398313331171e-05,
      "loss": 2.7335,
      "step": 582700
    },
    {
      "epoch": 188.91410048622367,
      "grad_norm": 1.2129456996917725,
      "learning_rate": 3.113658773921505e-05,
      "loss": 2.7408,
      "step": 582800
    },
    {
      "epoch": 188.94651539708266,
      "grad_norm": 1.2999347448349,
      "learning_rate": 3.113334414531301e-05,
      "loss": 2.7215,
      "step": 582900
    },
    {
      "epoch": 188.97893030794165,
      "grad_norm": 1.0195869207382202,
      "learning_rate": 3.113013298734999e-05,
      "loss": 2.7207,
      "step": 583000
    },
    {
      "epoch": 189.0,
      "eval_bleu": 0.9963363818363409,
      "eval_loss": 3.995985507965088,
      "eval_runtime": 4.4735,
      "eval_samples_per_second": 109.98,
      "eval_steps_per_second": 1.788,
      "step": 583065
    },
    {
      "epoch": 189.01134521880064,
      "grad_norm": 1.3609421253204346,
      "learning_rate": 3.112688939344795e-05,
      "loss": 2.7313,
      "step": 583100
    },
    {
      "epoch": 189.04376012965963,
      "grad_norm": 1.141424298286438,
      "learning_rate": 3.11236457995459e-05,
      "loss": 2.7228,
      "step": 583200
    },
    {
      "epoch": 189.07617504051865,
      "grad_norm": 0.9940455555915833,
      "learning_rate": 3.112040220564386e-05,
      "loss": 2.7378,
      "step": 583300
    },
    {
      "epoch": 189.10858995137764,
      "grad_norm": 1.1591874361038208,
      "learning_rate": 3.111715861174181e-05,
      "loss": 2.6964,
      "step": 583400
    },
    {
      "epoch": 189.14100486223663,
      "grad_norm": 1.149031162261963,
      "learning_rate": 3.111391501783977e-05,
      "loss": 2.7359,
      "step": 583500
    },
    {
      "epoch": 189.17341977309562,
      "grad_norm": 1.2573790550231934,
      "learning_rate": 3.111067142393773e-05,
      "loss": 2.7158,
      "step": 583600
    },
    {
      "epoch": 189.2058346839546,
      "grad_norm": 1.4086109399795532,
      "learning_rate": 3.110742783003568e-05,
      "loss": 2.7278,
      "step": 583700
    },
    {
      "epoch": 189.23824959481362,
      "grad_norm": 1.242989420890808,
      "learning_rate": 3.110418423613364e-05,
      "loss": 2.711,
      "step": 583800
    },
    {
      "epoch": 189.2706645056726,
      "grad_norm": 1.238128423690796,
      "learning_rate": 3.11009406422316e-05,
      "loss": 2.7155,
      "step": 583900
    },
    {
      "epoch": 189.3030794165316,
      "grad_norm": 1.1818172931671143,
      "learning_rate": 3.109769704832955e-05,
      "loss": 2.738,
      "step": 584000
    },
    {
      "epoch": 189.3354943273906,
      "grad_norm": 1.3228241205215454,
      "learning_rate": 3.109445345442751e-05,
      "loss": 2.7125,
      "step": 584100
    },
    {
      "epoch": 189.36790923824958,
      "grad_norm": 1.1869962215423584,
      "learning_rate": 3.1091209860525467e-05,
      "loss": 2.7188,
      "step": 584200
    },
    {
      "epoch": 189.4003241491086,
      "grad_norm": 1.2083191871643066,
      "learning_rate": 3.108796626662342e-05,
      "loss": 2.7116,
      "step": 584300
    },
    {
      "epoch": 189.4327390599676,
      "grad_norm": 1.325278878211975,
      "learning_rate": 3.108472267272138e-05,
      "loss": 2.7131,
      "step": 584400
    },
    {
      "epoch": 189.46515397082658,
      "grad_norm": 1.1761183738708496,
      "learning_rate": 3.108147907881933e-05,
      "loss": 2.7117,
      "step": 584500
    },
    {
      "epoch": 189.49756888168557,
      "grad_norm": 1.1790601015090942,
      "learning_rate": 3.107823548491729e-05,
      "loss": 2.7155,
      "step": 584600
    },
    {
      "epoch": 189.52998379254458,
      "grad_norm": 1.2773255109786987,
      "learning_rate": 3.107499189101525e-05,
      "loss": 2.7255,
      "step": 584700
    },
    {
      "epoch": 189.56239870340357,
      "grad_norm": 1.2021079063415527,
      "learning_rate": 3.10717482971132e-05,
      "loss": 2.7307,
      "step": 584800
    },
    {
      "epoch": 189.59481361426256,
      "grad_norm": 1.3334029912948608,
      "learning_rate": 3.106850470321116e-05,
      "loss": 2.7183,
      "step": 584900
    },
    {
      "epoch": 189.62722852512155,
      "grad_norm": 1.0980513095855713,
      "learning_rate": 3.1065261109309116e-05,
      "loss": 2.7026,
      "step": 585000
    },
    {
      "epoch": 189.65964343598054,
      "grad_norm": 1.1269546747207642,
      "learning_rate": 3.106201751540707e-05,
      "loss": 2.7259,
      "step": 585100
    },
    {
      "epoch": 189.69205834683956,
      "grad_norm": 1.500813603401184,
      "learning_rate": 3.105877392150503e-05,
      "loss": 2.7465,
      "step": 585200
    },
    {
      "epoch": 189.72447325769855,
      "grad_norm": 1.291752576828003,
      "learning_rate": 3.1055530327602986e-05,
      "loss": 2.7327,
      "step": 585300
    },
    {
      "epoch": 189.75688816855754,
      "grad_norm": 1.2771276235580444,
      "learning_rate": 3.1052286733700944e-05,
      "loss": 2.7142,
      "step": 585400
    },
    {
      "epoch": 189.78930307941653,
      "grad_norm": 1.1973087787628174,
      "learning_rate": 3.10490431397989e-05,
      "loss": 2.7443,
      "step": 585500
    },
    {
      "epoch": 189.82171799027552,
      "grad_norm": 1.1089985370635986,
      "learning_rate": 3.1045799545896855e-05,
      "loss": 2.7248,
      "step": 585600
    },
    {
      "epoch": 189.85413290113453,
      "grad_norm": 1.1863412857055664,
      "learning_rate": 3.1042555951994814e-05,
      "loss": 2.7049,
      "step": 585700
    },
    {
      "epoch": 189.88654781199352,
      "grad_norm": 1.2434773445129395,
      "learning_rate": 3.103931235809277e-05,
      "loss": 2.7162,
      "step": 585800
    },
    {
      "epoch": 189.9189627228525,
      "grad_norm": 1.1715744733810425,
      "learning_rate": 3.1036068764190725e-05,
      "loss": 2.7151,
      "step": 585900
    },
    {
      "epoch": 189.9513776337115,
      "grad_norm": 1.1978447437286377,
      "learning_rate": 3.103282517028868e-05,
      "loss": 2.7256,
      "step": 586000
    },
    {
      "epoch": 189.9837925445705,
      "grad_norm": 1.0041149854660034,
      "learning_rate": 3.102958157638664e-05,
      "loss": 2.7308,
      "step": 586100
    },
    {
      "epoch": 190.0,
      "eval_bleu": 1.0065845005468324,
      "eval_loss": 4.005578994750977,
      "eval_runtime": 3.9921,
      "eval_samples_per_second": 123.243,
      "eval_steps_per_second": 2.004,
      "step": 586150
    },
    {
      "epoch": 190.0162074554295,
      "grad_norm": 1.2401803731918335,
      "learning_rate": 3.1026337982484594e-05,
      "loss": 2.7312,
      "step": 586200
    },
    {
      "epoch": 190.0486223662885,
      "grad_norm": 1.2067010402679443,
      "learning_rate": 3.102309438858255e-05,
      "loss": 2.7104,
      "step": 586300
    },
    {
      "epoch": 190.0810372771475,
      "grad_norm": 1.13480806350708,
      "learning_rate": 3.1019850794680505e-05,
      "loss": 2.7221,
      "step": 586400
    },
    {
      "epoch": 190.11345218800648,
      "grad_norm": 1.2783061265945435,
      "learning_rate": 3.1016607200778463e-05,
      "loss": 2.7309,
      "step": 586500
    },
    {
      "epoch": 190.14586709886547,
      "grad_norm": 1.2550076246261597,
      "learning_rate": 3.101336360687642e-05,
      "loss": 2.7242,
      "step": 586600
    },
    {
      "epoch": 190.17828200972448,
      "grad_norm": 1.268349051475525,
      "learning_rate": 3.1010120012974374e-05,
      "loss": 2.7204,
      "step": 586700
    },
    {
      "epoch": 190.21069692058347,
      "grad_norm": 1.2236272096633911,
      "learning_rate": 3.100687641907233e-05,
      "loss": 2.7068,
      "step": 586800
    },
    {
      "epoch": 190.24311183144246,
      "grad_norm": 1.343281626701355,
      "learning_rate": 3.100363282517029e-05,
      "loss": 2.7082,
      "step": 586900
    },
    {
      "epoch": 190.27552674230145,
      "grad_norm": 1.3085540533065796,
      "learning_rate": 3.100042166720727e-05,
      "loss": 2.7267,
      "step": 587000
    },
    {
      "epoch": 190.30794165316044,
      "grad_norm": 1.2617082595825195,
      "learning_rate": 3.099717807330522e-05,
      "loss": 2.7375,
      "step": 587100
    },
    {
      "epoch": 190.34035656401946,
      "grad_norm": 1.1817554235458374,
      "learning_rate": 3.099393447940318e-05,
      "loss": 2.7298,
      "step": 587200
    },
    {
      "epoch": 190.37277147487845,
      "grad_norm": 1.057268500328064,
      "learning_rate": 3.099069088550114e-05,
      "loss": 2.728,
      "step": 587300
    },
    {
      "epoch": 190.40518638573744,
      "grad_norm": 1.10739004611969,
      "learning_rate": 3.098744729159909e-05,
      "loss": 2.721,
      "step": 587400
    },
    {
      "epoch": 190.43760129659643,
      "grad_norm": 1.129494547843933,
      "learning_rate": 3.098420369769705e-05,
      "loss": 2.7229,
      "step": 587500
    },
    {
      "epoch": 190.47001620745542,
      "grad_norm": 1.5216785669326782,
      "learning_rate": 3.098099253973403e-05,
      "loss": 2.7151,
      "step": 587600
    },
    {
      "epoch": 190.50243111831443,
      "grad_norm": 1.0903234481811523,
      "learning_rate": 3.0977748945831986e-05,
      "loss": 2.729,
      "step": 587700
    },
    {
      "epoch": 190.53484602917342,
      "grad_norm": 1.2777808904647827,
      "learning_rate": 3.097450535192994e-05,
      "loss": 2.7324,
      "step": 587800
    },
    {
      "epoch": 190.5672609400324,
      "grad_norm": 1.2090259790420532,
      "learning_rate": 3.09712617580279e-05,
      "loss": 2.6846,
      "step": 587900
    },
    {
      "epoch": 190.5996758508914,
      "grad_norm": 1.2310999631881714,
      "learning_rate": 3.096801816412585e-05,
      "loss": 2.7212,
      "step": 588000
    },
    {
      "epoch": 190.63209076175042,
      "grad_norm": 1.3251222372055054,
      "learning_rate": 3.096477457022381e-05,
      "loss": 2.7239,
      "step": 588100
    },
    {
      "epoch": 190.6645056726094,
      "grad_norm": 1.2201111316680908,
      "learning_rate": 3.0961530976321766e-05,
      "loss": 2.7285,
      "step": 588200
    },
    {
      "epoch": 190.6969205834684,
      "grad_norm": 1.3385837078094482,
      "learning_rate": 3.095828738241972e-05,
      "loss": 2.7466,
      "step": 588300
    },
    {
      "epoch": 190.7293354943274,
      "grad_norm": 1.1520699262619019,
      "learning_rate": 3.095504378851768e-05,
      "loss": 2.7107,
      "step": 588400
    },
    {
      "epoch": 190.76175040518638,
      "grad_norm": 1.3829549551010132,
      "learning_rate": 3.0951800194615636e-05,
      "loss": 2.7088,
      "step": 588500
    },
    {
      "epoch": 190.7941653160454,
      "grad_norm": 1.4085309505462646,
      "learning_rate": 3.094855660071359e-05,
      "loss": 2.7258,
      "step": 588600
    },
    {
      "epoch": 190.82658022690438,
      "grad_norm": 1.1874667406082153,
      "learning_rate": 3.0945313006811546e-05,
      "loss": 2.716,
      "step": 588700
    },
    {
      "epoch": 190.85899513776337,
      "grad_norm": 1.2298718690872192,
      "learning_rate": 3.0942069412909505e-05,
      "loss": 2.7212,
      "step": 588800
    },
    {
      "epoch": 190.89141004862236,
      "grad_norm": 1.2340137958526611,
      "learning_rate": 3.0938825819007464e-05,
      "loss": 2.7198,
      "step": 588900
    },
    {
      "epoch": 190.92382495948135,
      "grad_norm": 1.1660306453704834,
      "learning_rate": 3.093558222510542e-05,
      "loss": 2.717,
      "step": 589000
    },
    {
      "epoch": 190.95623987034037,
      "grad_norm": 1.2089515924453735,
      "learning_rate": 3.0932338631203375e-05,
      "loss": 2.7124,
      "step": 589100
    },
    {
      "epoch": 190.98865478119936,
      "grad_norm": 1.3021364212036133,
      "learning_rate": 3.0929095037301333e-05,
      "loss": 2.7072,
      "step": 589200
    },
    {
      "epoch": 191.0,
      "eval_bleu": 1.0712249251256916,
      "eval_loss": 4.003695487976074,
      "eval_runtime": 3.8491,
      "eval_samples_per_second": 127.822,
      "eval_steps_per_second": 2.078,
      "step": 589235
    },
    {
      "epoch": 191.02106969205835,
      "grad_norm": 1.4535558223724365,
      "learning_rate": 3.092585144339929e-05,
      "loss": 2.7128,
      "step": 589300
    },
    {
      "epoch": 191.05348460291734,
      "grad_norm": 1.0793975591659546,
      "learning_rate": 3.0922607849497244e-05,
      "loss": 2.7305,
      "step": 589400
    },
    {
      "epoch": 191.08589951377633,
      "grad_norm": 1.1118874549865723,
      "learning_rate": 3.09193642555952e-05,
      "loss": 2.7077,
      "step": 589500
    },
    {
      "epoch": 191.11831442463534,
      "grad_norm": 1.1245390176773071,
      "learning_rate": 3.091612066169316e-05,
      "loss": 2.7028,
      "step": 589600
    },
    {
      "epoch": 191.15072933549433,
      "grad_norm": 1.1717774868011475,
      "learning_rate": 3.0912877067791114e-05,
      "loss": 2.7052,
      "step": 589700
    },
    {
      "epoch": 191.18314424635332,
      "grad_norm": 1.219771146774292,
      "learning_rate": 3.090963347388907e-05,
      "loss": 2.6976,
      "step": 589800
    },
    {
      "epoch": 191.2155591572123,
      "grad_norm": 1.1686646938323975,
      "learning_rate": 3.0906389879987024e-05,
      "loss": 2.7128,
      "step": 589900
    },
    {
      "epoch": 191.2479740680713,
      "grad_norm": 1.2455079555511475,
      "learning_rate": 3.090314628608498e-05,
      "loss": 2.71,
      "step": 590000
    },
    {
      "epoch": 191.28038897893032,
      "grad_norm": 1.2409824132919312,
      "learning_rate": 3.089990269218294e-05,
      "loss": 2.7003,
      "step": 590100
    },
    {
      "epoch": 191.3128038897893,
      "grad_norm": 1.4155998229980469,
      "learning_rate": 3.0896659098280894e-05,
      "loss": 2.7116,
      "step": 590200
    },
    {
      "epoch": 191.3452188006483,
      "grad_norm": 1.3142790794372559,
      "learning_rate": 3.089341550437885e-05,
      "loss": 2.7027,
      "step": 590300
    },
    {
      "epoch": 191.3776337115073,
      "grad_norm": 1.09092116355896,
      "learning_rate": 3.089017191047681e-05,
      "loss": 2.7096,
      "step": 590400
    },
    {
      "epoch": 191.41004862236628,
      "grad_norm": 1.1816091537475586,
      "learning_rate": 3.088692831657476e-05,
      "loss": 2.7079,
      "step": 590500
    },
    {
      "epoch": 191.4424635332253,
      "grad_norm": 1.2052550315856934,
      "learning_rate": 3.088368472267272e-05,
      "loss": 2.7586,
      "step": 590600
    },
    {
      "epoch": 191.47487844408428,
      "grad_norm": 1.2411515712738037,
      "learning_rate": 3.088044112877068e-05,
      "loss": 2.7197,
      "step": 590700
    },
    {
      "epoch": 191.50729335494327,
      "grad_norm": 1.278054118156433,
      "learning_rate": 3.087719753486863e-05,
      "loss": 2.7103,
      "step": 590800
    },
    {
      "epoch": 191.53970826580226,
      "grad_norm": 1.2613532543182373,
      "learning_rate": 3.087395394096659e-05,
      "loss": 2.7184,
      "step": 590900
    },
    {
      "epoch": 191.57212317666125,
      "grad_norm": 1.149423599243164,
      "learning_rate": 3.087071034706454e-05,
      "loss": 2.7146,
      "step": 591000
    },
    {
      "epoch": 191.60453808752027,
      "grad_norm": 1.1703855991363525,
      "learning_rate": 3.08674667531625e-05,
      "loss": 2.7155,
      "step": 591100
    },
    {
      "epoch": 191.63695299837926,
      "grad_norm": 1.2916067838668823,
      "learning_rate": 3.086422315926046e-05,
      "loss": 2.7416,
      "step": 591200
    },
    {
      "epoch": 191.66936790923825,
      "grad_norm": 1.1744890213012695,
      "learning_rate": 3.086097956535842e-05,
      "loss": 2.7211,
      "step": 591300
    },
    {
      "epoch": 191.70178282009724,
      "grad_norm": 1.225582480430603,
      "learning_rate": 3.085773597145638e-05,
      "loss": 2.7195,
      "step": 591400
    },
    {
      "epoch": 191.73419773095625,
      "grad_norm": 1.142067551612854,
      "learning_rate": 3.085449237755434e-05,
      "loss": 2.717,
      "step": 591500
    },
    {
      "epoch": 191.76661264181524,
      "grad_norm": 1.1570020914077759,
      "learning_rate": 3.085128121959131e-05,
      "loss": 2.7399,
      "step": 591600
    },
    {
      "epoch": 191.79902755267423,
      "grad_norm": 1.1089446544647217,
      "learning_rate": 3.084803762568926e-05,
      "loss": 2.7146,
      "step": 591700
    },
    {
      "epoch": 191.83144246353322,
      "grad_norm": 1.3323661088943481,
      "learning_rate": 3.084479403178722e-05,
      "loss": 2.7232,
      "step": 591800
    },
    {
      "epoch": 191.8638573743922,
      "grad_norm": 1.1091941595077515,
      "learning_rate": 3.084155043788518e-05,
      "loss": 2.7355,
      "step": 591900
    },
    {
      "epoch": 191.89627228525123,
      "grad_norm": 1.094220757484436,
      "learning_rate": 3.0838306843983136e-05,
      "loss": 2.7128,
      "step": 592000
    },
    {
      "epoch": 191.92868719611022,
      "grad_norm": 1.2368927001953125,
      "learning_rate": 3.0835063250081095e-05,
      "loss": 2.7338,
      "step": 592100
    },
    {
      "epoch": 191.9611021069692,
      "grad_norm": 1.0004686117172241,
      "learning_rate": 3.083181965617905e-05,
      "loss": 2.7266,
      "step": 592200
    },
    {
      "epoch": 191.9935170178282,
      "grad_norm": 1.1896922588348389,
      "learning_rate": 3.0828576062277006e-05,
      "loss": 2.7495,
      "step": 592300
    },
    {
      "epoch": 192.0,
      "eval_bleu": 1.1133055155794784,
      "eval_loss": 4.001255512237549,
      "eval_runtime": 3.9011,
      "eval_samples_per_second": 126.118,
      "eval_steps_per_second": 2.051,
      "step": 592320
    },
    {
      "epoch": 192.02593192868719,
      "grad_norm": 0.9809382557868958,
      "learning_rate": 3.0825332468374964e-05,
      "loss": 2.6944,
      "step": 592400
    },
    {
      "epoch": 192.0583468395462,
      "grad_norm": 1.1906771659851074,
      "learning_rate": 3.0822088874472916e-05,
      "loss": 2.7155,
      "step": 592500
    },
    {
      "epoch": 192.0907617504052,
      "grad_norm": 1.24130117893219,
      "learning_rate": 3.0818845280570875e-05,
      "loss": 2.7291,
      "step": 592600
    },
    {
      "epoch": 192.12317666126418,
      "grad_norm": 1.1031659841537476,
      "learning_rate": 3.0815601686668834e-05,
      "loss": 2.7027,
      "step": 592700
    },
    {
      "epoch": 192.15559157212317,
      "grad_norm": 1.2025638818740845,
      "learning_rate": 3.0812358092766786e-05,
      "loss": 2.7138,
      "step": 592800
    },
    {
      "epoch": 192.18800648298216,
      "grad_norm": 1.175869345664978,
      "learning_rate": 3.0809114498864745e-05,
      "loss": 2.7323,
      "step": 592900
    },
    {
      "epoch": 192.22042139384118,
      "grad_norm": 1.2079503536224365,
      "learning_rate": 3.08058709049627e-05,
      "loss": 2.7012,
      "step": 593000
    },
    {
      "epoch": 192.25283630470017,
      "grad_norm": 1.2191580533981323,
      "learning_rate": 3.0802627311060655e-05,
      "loss": 2.7394,
      "step": 593100
    },
    {
      "epoch": 192.28525121555916,
      "grad_norm": 1.335256576538086,
      "learning_rate": 3.0799383717158614e-05,
      "loss": 2.7223,
      "step": 593200
    },
    {
      "epoch": 192.31766612641815,
      "grad_norm": 1.3981913328170776,
      "learning_rate": 3.0796140123256566e-05,
      "loss": 2.7212,
      "step": 593300
    },
    {
      "epoch": 192.35008103727714,
      "grad_norm": 1.183958649635315,
      "learning_rate": 3.0792896529354525e-05,
      "loss": 2.7127,
      "step": 593400
    },
    {
      "epoch": 192.38249594813615,
      "grad_norm": 1.1551082134246826,
      "learning_rate": 3.0789652935452484e-05,
      "loss": 2.7056,
      "step": 593500
    },
    {
      "epoch": 192.41491085899514,
      "grad_norm": 1.5386004447937012,
      "learning_rate": 3.078644177748946e-05,
      "loss": 2.7177,
      "step": 593600
    },
    {
      "epoch": 192.44732576985413,
      "grad_norm": 1.1706629991531372,
      "learning_rate": 3.078319818358741e-05,
      "loss": 2.7157,
      "step": 593700
    },
    {
      "epoch": 192.47974068071312,
      "grad_norm": 1.2105145454406738,
      "learning_rate": 3.07799870256244e-05,
      "loss": 2.7257,
      "step": 593800
    },
    {
      "epoch": 192.5121555915721,
      "grad_norm": 1.0980135202407837,
      "learning_rate": 3.077674343172235e-05,
      "loss": 2.6923,
      "step": 593900
    },
    {
      "epoch": 192.54457050243113,
      "grad_norm": 1.5453208684921265,
      "learning_rate": 3.077349983782031e-05,
      "loss": 2.718,
      "step": 594000
    },
    {
      "epoch": 192.57698541329012,
      "grad_norm": 1.241230845451355,
      "learning_rate": 3.077025624391826e-05,
      "loss": 2.7037,
      "step": 594100
    },
    {
      "epoch": 192.6094003241491,
      "grad_norm": 1.181331753730774,
      "learning_rate": 3.076701265001622e-05,
      "loss": 2.7034,
      "step": 594200
    },
    {
      "epoch": 192.6418152350081,
      "grad_norm": 1.3707911968231201,
      "learning_rate": 3.076376905611418e-05,
      "loss": 2.7111,
      "step": 594300
    },
    {
      "epoch": 192.67423014586709,
      "grad_norm": 1.3667298555374146,
      "learning_rate": 3.076052546221213e-05,
      "loss": 2.7259,
      "step": 594400
    },
    {
      "epoch": 192.7066450567261,
      "grad_norm": 1.279508352279663,
      "learning_rate": 3.075728186831009e-05,
      "loss": 2.7046,
      "step": 594500
    },
    {
      "epoch": 192.7390599675851,
      "grad_norm": 1.1759698390960693,
      "learning_rate": 3.075403827440805e-05,
      "loss": 2.7085,
      "step": 594600
    },
    {
      "epoch": 192.77147487844408,
      "grad_norm": 1.1436947584152222,
      "learning_rate": 3.0750794680506e-05,
      "loss": 2.7247,
      "step": 594700
    },
    {
      "epoch": 192.80388978930307,
      "grad_norm": 1.19465172290802,
      "learning_rate": 3.074755108660396e-05,
      "loss": 2.7144,
      "step": 594800
    },
    {
      "epoch": 192.8363047001621,
      "grad_norm": 1.2390021085739136,
      "learning_rate": 3.074430749270191e-05,
      "loss": 2.7259,
      "step": 594900
    },
    {
      "epoch": 192.86871961102108,
      "grad_norm": 1.1015571355819702,
      "learning_rate": 3.074106389879987e-05,
      "loss": 2.7171,
      "step": 595000
    },
    {
      "epoch": 192.90113452188007,
      "grad_norm": 1.2603518962860107,
      "learning_rate": 3.073782030489783e-05,
      "loss": 2.7344,
      "step": 595100
    },
    {
      "epoch": 192.93354943273906,
      "grad_norm": 1.2179337739944458,
      "learning_rate": 3.073457671099578e-05,
      "loss": 2.7241,
      "step": 595200
    },
    {
      "epoch": 192.96596434359805,
      "grad_norm": 1.1920922994613647,
      "learning_rate": 3.073133311709374e-05,
      "loss": 2.7332,
      "step": 595300
    },
    {
      "epoch": 192.99837925445706,
      "grad_norm": 1.2397748231887817,
      "learning_rate": 3.07280895231917e-05,
      "loss": 2.7299,
      "step": 595400
    },
    {
      "epoch": 193.0,
      "eval_bleu": 1.0917746409927176,
      "eval_loss": 4.005705833435059,
      "eval_runtime": 3.7893,
      "eval_samples_per_second": 129.838,
      "eval_steps_per_second": 2.111,
      "step": 595405
    },
    {
      "epoch": 193.03079416531605,
      "grad_norm": 1.1683616638183594,
      "learning_rate": 3.0724845929289656e-05,
      "loss": 2.7103,
      "step": 595500
    },
    {
      "epoch": 193.06320907617504,
      "grad_norm": 1.3965057134628296,
      "learning_rate": 3.0721602335387615e-05,
      "loss": 2.6945,
      "step": 595600
    },
    {
      "epoch": 193.09562398703403,
      "grad_norm": 1.2866246700286865,
      "learning_rate": 3.071835874148557e-05,
      "loss": 2.6882,
      "step": 595700
    },
    {
      "epoch": 193.12803889789302,
      "grad_norm": 1.0259106159210205,
      "learning_rate": 3.0715115147583525e-05,
      "loss": 2.7064,
      "step": 595800
    },
    {
      "epoch": 193.16045380875204,
      "grad_norm": 1.1986207962036133,
      "learning_rate": 3.0711871553681484e-05,
      "loss": 2.7167,
      "step": 595900
    },
    {
      "epoch": 193.19286871961103,
      "grad_norm": 1.1782457828521729,
      "learning_rate": 3.0708627959779436e-05,
      "loss": 2.6944,
      "step": 596000
    },
    {
      "epoch": 193.22528363047002,
      "grad_norm": 1.1748707294464111,
      "learning_rate": 3.0705384365877395e-05,
      "loss": 2.7083,
      "step": 596100
    },
    {
      "epoch": 193.257698541329,
      "grad_norm": 1.2023406028747559,
      "learning_rate": 3.0702140771975353e-05,
      "loss": 2.7221,
      "step": 596200
    },
    {
      "epoch": 193.290113452188,
      "grad_norm": 1.5621333122253418,
      "learning_rate": 3.0698897178073305e-05,
      "loss": 2.7108,
      "step": 596300
    },
    {
      "epoch": 193.322528363047,
      "grad_norm": 1.1921314001083374,
      "learning_rate": 3.0695653584171264e-05,
      "loss": 2.6979,
      "step": 596400
    },
    {
      "epoch": 193.354943273906,
      "grad_norm": 1.1023603677749634,
      "learning_rate": 3.069240999026922e-05,
      "loss": 2.7109,
      "step": 596500
    },
    {
      "epoch": 193.387358184765,
      "grad_norm": 1.422300100326538,
      "learning_rate": 3.0689166396367175e-05,
      "loss": 2.7249,
      "step": 596600
    },
    {
      "epoch": 193.41977309562398,
      "grad_norm": 1.3522740602493286,
      "learning_rate": 3.0685922802465134e-05,
      "loss": 2.7176,
      "step": 596700
    },
    {
      "epoch": 193.45218800648297,
      "grad_norm": 1.1875278949737549,
      "learning_rate": 3.068267920856309e-05,
      "loss": 2.7205,
      "step": 596800
    },
    {
      "epoch": 193.484602917342,
      "grad_norm": 1.0949455499649048,
      "learning_rate": 3.0679435614661044e-05,
      "loss": 2.714,
      "step": 596900
    },
    {
      "epoch": 193.51701782820098,
      "grad_norm": 1.1488476991653442,
      "learning_rate": 3.0676192020759e-05,
      "loss": 2.7274,
      "step": 597000
    },
    {
      "epoch": 193.54943273905997,
      "grad_norm": 1.2702962160110474,
      "learning_rate": 3.0672948426856955e-05,
      "loss": 2.7232,
      "step": 597100
    },
    {
      "epoch": 193.58184764991896,
      "grad_norm": 1.3262995481491089,
      "learning_rate": 3.0669704832954914e-05,
      "loss": 2.7082,
      "step": 597200
    },
    {
      "epoch": 193.61426256077795,
      "grad_norm": 1.132949709892273,
      "learning_rate": 3.066646123905287e-05,
      "loss": 2.7063,
      "step": 597300
    },
    {
      "epoch": 193.64667747163696,
      "grad_norm": 1.26923406124115,
      "learning_rate": 3.0663217645150825e-05,
      "loss": 2.7084,
      "step": 597400
    },
    {
      "epoch": 193.67909238249595,
      "grad_norm": 1.3800112009048462,
      "learning_rate": 3.065997405124878e-05,
      "loss": 2.7069,
      "step": 597500
    },
    {
      "epoch": 193.71150729335494,
      "grad_norm": 1.2124131917953491,
      "learning_rate": 3.065673045734674e-05,
      "loss": 2.721,
      "step": 597600
    },
    {
      "epoch": 193.74392220421393,
      "grad_norm": 1.1076793670654297,
      "learning_rate": 3.0653486863444694e-05,
      "loss": 2.7221,
      "step": 597700
    },
    {
      "epoch": 193.77633711507292,
      "grad_norm": 1.1523206233978271,
      "learning_rate": 3.065027570548167e-05,
      "loss": 2.7122,
      "step": 597800
    },
    {
      "epoch": 193.80875202593194,
      "grad_norm": 1.211272954940796,
      "learning_rate": 3.064703211157963e-05,
      "loss": 2.7081,
      "step": 597900
    },
    {
      "epoch": 193.84116693679093,
      "grad_norm": 1.0615403652191162,
      "learning_rate": 3.064378851767759e-05,
      "loss": 2.7344,
      "step": 598000
    },
    {
      "epoch": 193.87358184764992,
      "grad_norm": 1.1459758281707764,
      "learning_rate": 3.064054492377554e-05,
      "loss": 2.7221,
      "step": 598100
    },
    {
      "epoch": 193.9059967585089,
      "grad_norm": 1.219448447227478,
      "learning_rate": 3.06373013298735e-05,
      "loss": 2.7494,
      "step": 598200
    },
    {
      "epoch": 193.93841166936792,
      "grad_norm": 1.0337330102920532,
      "learning_rate": 3.063405773597146e-05,
      "loss": 2.7131,
      "step": 598300
    },
    {
      "epoch": 193.9708265802269,
      "grad_norm": 1.3263511657714844,
      "learning_rate": 3.063081414206941e-05,
      "loss": 2.7248,
      "step": 598400
    },
    {
      "epoch": 194.0,
      "eval_bleu": 1.0506582105117095,
      "eval_loss": 4.01139497756958,
      "eval_runtime": 3.7082,
      "eval_samples_per_second": 132.678,
      "eval_steps_per_second": 2.157,
      "step": 598490
    },
    {
      "epoch": 194.0032414910859,
      "grad_norm": 1.2939409017562866,
      "learning_rate": 3.062757054816737e-05,
      "loss": 2.7168,
      "step": 598500
    },
    {
      "epoch": 194.0356564019449,
      "grad_norm": 1.2657949924468994,
      "learning_rate": 3.062432695426533e-05,
      "loss": 2.7082,
      "step": 598600
    },
    {
      "epoch": 194.06807131280388,
      "grad_norm": 1.1807618141174316,
      "learning_rate": 3.062108336036329e-05,
      "loss": 2.723,
      "step": 598700
    },
    {
      "epoch": 194.1004862236629,
      "grad_norm": 1.5050201416015625,
      "learning_rate": 3.0617839766461246e-05,
      "loss": 2.6991,
      "step": 598800
    },
    {
      "epoch": 194.1329011345219,
      "grad_norm": 1.2049981355667114,
      "learning_rate": 3.06145961725592e-05,
      "loss": 2.6922,
      "step": 598900
    },
    {
      "epoch": 194.16531604538088,
      "grad_norm": 1.277747631072998,
      "learning_rate": 3.0611352578657156e-05,
      "loss": 2.7065,
      "step": 599000
    },
    {
      "epoch": 194.19773095623987,
      "grad_norm": 1.3810813426971436,
      "learning_rate": 3.0608108984755115e-05,
      "loss": 2.7003,
      "step": 599100
    },
    {
      "epoch": 194.23014586709886,
      "grad_norm": 1.1750835180282593,
      "learning_rate": 3.060486539085307e-05,
      "loss": 2.7195,
      "step": 599200
    },
    {
      "epoch": 194.26256077795787,
      "grad_norm": 1.3451117277145386,
      "learning_rate": 3.0601621796951026e-05,
      "loss": 2.6957,
      "step": 599300
    },
    {
      "epoch": 194.29497568881686,
      "grad_norm": 1.2307957410812378,
      "learning_rate": 3.059837820304898e-05,
      "loss": 2.7229,
      "step": 599400
    },
    {
      "epoch": 194.32739059967585,
      "grad_norm": 1.2990235090255737,
      "learning_rate": 3.0595134609146937e-05,
      "loss": 2.7004,
      "step": 599500
    },
    {
      "epoch": 194.35980551053484,
      "grad_norm": 1.1590383052825928,
      "learning_rate": 3.0591891015244895e-05,
      "loss": 2.7284,
      "step": 599600
    },
    {
      "epoch": 194.39222042139383,
      "grad_norm": 1.4485626220703125,
      "learning_rate": 3.058864742134285e-05,
      "loss": 2.7169,
      "step": 599700
    },
    {
      "epoch": 194.42463533225285,
      "grad_norm": 1.3111019134521484,
      "learning_rate": 3.0585436263379825e-05,
      "loss": 2.6968,
      "step": 599800
    },
    {
      "epoch": 194.45705024311184,
      "grad_norm": 1.2269930839538574,
      "learning_rate": 3.0582192669477784e-05,
      "loss": 2.7079,
      "step": 599900
    },
    {
      "epoch": 194.48946515397083,
      "grad_norm": 1.2209476232528687,
      "learning_rate": 3.057894907557574e-05,
      "loss": 2.7006,
      "step": 600000
    },
    {
      "epoch": 194.52188006482982,
      "grad_norm": 1.235108494758606,
      "learning_rate": 3.0575705481673694e-05,
      "loss": 2.7202,
      "step": 600100
    },
    {
      "epoch": 194.5542949756888,
      "grad_norm": 1.1335386037826538,
      "learning_rate": 3.057246188777165e-05,
      "loss": 2.7149,
      "step": 600200
    },
    {
      "epoch": 194.58670988654782,
      "grad_norm": 1.2279973030090332,
      "learning_rate": 3.056921829386961e-05,
      "loss": 2.7244,
      "step": 600300
    },
    {
      "epoch": 194.6191247974068,
      "grad_norm": 1.2269704341888428,
      "learning_rate": 3.0565974699967564e-05,
      "loss": 2.7352,
      "step": 600400
    },
    {
      "epoch": 194.6515397082658,
      "grad_norm": 1.178343415260315,
      "learning_rate": 3.056273110606552e-05,
      "loss": 2.6973,
      "step": 600500
    },
    {
      "epoch": 194.6839546191248,
      "grad_norm": 1.080075740814209,
      "learning_rate": 3.0559487512163475e-05,
      "loss": 2.7174,
      "step": 600600
    },
    {
      "epoch": 194.71636952998378,
      "grad_norm": 1.4025547504425049,
      "learning_rate": 3.055624391826143e-05,
      "loss": 2.711,
      "step": 600700
    },
    {
      "epoch": 194.7487844408428,
      "grad_norm": 1.3417142629623413,
      "learning_rate": 3.055300032435939e-05,
      "loss": 2.7249,
      "step": 600800
    },
    {
      "epoch": 194.7811993517018,
      "grad_norm": 1.3041102886199951,
      "learning_rate": 3.0549756730457344e-05,
      "loss": 2.7009,
      "step": 600900
    },
    {
      "epoch": 194.81361426256078,
      "grad_norm": 1.2480838298797607,
      "learning_rate": 3.05465131365553e-05,
      "loss": 2.7082,
      "step": 601000
    },
    {
      "epoch": 194.84602917341977,
      "grad_norm": 1.1539568901062012,
      "learning_rate": 3.054326954265326e-05,
      "loss": 2.73,
      "step": 601100
    },
    {
      "epoch": 194.87844408427875,
      "grad_norm": 1.162169098854065,
      "learning_rate": 3.0540025948751214e-05,
      "loss": 2.7206,
      "step": 601200
    },
    {
      "epoch": 194.91085899513777,
      "grad_norm": 1.2640708684921265,
      "learning_rate": 3.053678235484917e-05,
      "loss": 2.722,
      "step": 601300
    },
    {
      "epoch": 194.94327390599676,
      "grad_norm": 1.3336938619613647,
      "learning_rate": 3.053353876094713e-05,
      "loss": 2.7174,
      "step": 601400
    },
    {
      "epoch": 194.97568881685575,
      "grad_norm": 1.1813346147537231,
      "learning_rate": 3.053029516704509e-05,
      "loss": 2.7122,
      "step": 601500
    },
    {
      "epoch": 195.0,
      "eval_bleu": 1.018884453977455,
      "eval_loss": 4.009092807769775,
      "eval_runtime": 4.3146,
      "eval_samples_per_second": 114.032,
      "eval_steps_per_second": 1.854,
      "step": 601575
    },
    {
      "epoch": 195.00810372771474,
      "grad_norm": 1.0556704998016357,
      "learning_rate": 3.052705157314304e-05,
      "loss": 2.7102,
      "step": 601600
    },
    {
      "epoch": 195.04051863857376,
      "grad_norm": 1.1129107475280762,
      "learning_rate": 3.0523807979241e-05,
      "loss": 2.6793,
      "step": 601700
    },
    {
      "epoch": 195.07293354943275,
      "grad_norm": 1.1798850297927856,
      "learning_rate": 3.052059682127797e-05,
      "loss": 2.7039,
      "step": 601800
    },
    {
      "epoch": 195.10534846029174,
      "grad_norm": 1.1020327806472778,
      "learning_rate": 3.0517353227375934e-05,
      "loss": 2.7036,
      "step": 601900
    },
    {
      "epoch": 195.13776337115073,
      "grad_norm": 1.13910710811615,
      "learning_rate": 3.0514109633473892e-05,
      "loss": 2.7004,
      "step": 602000
    },
    {
      "epoch": 195.17017828200972,
      "grad_norm": 1.3407491445541382,
      "learning_rate": 3.0510866039571844e-05,
      "loss": 2.6981,
      "step": 602100
    },
    {
      "epoch": 195.20259319286873,
      "grad_norm": 1.1993821859359741,
      "learning_rate": 3.0507622445669803e-05,
      "loss": 2.7216,
      "step": 602200
    },
    {
      "epoch": 195.23500810372772,
      "grad_norm": 1.0951231718063354,
      "learning_rate": 3.0504378851767762e-05,
      "loss": 2.6966,
      "step": 602300
    },
    {
      "epoch": 195.2674230145867,
      "grad_norm": 1.150687336921692,
      "learning_rate": 3.0501135257865714e-05,
      "loss": 2.7141,
      "step": 602400
    },
    {
      "epoch": 195.2998379254457,
      "grad_norm": 1.177547812461853,
      "learning_rate": 3.0497891663963673e-05,
      "loss": 2.7175,
      "step": 602500
    },
    {
      "epoch": 195.3322528363047,
      "grad_norm": 1.2337379455566406,
      "learning_rate": 3.049464807006163e-05,
      "loss": 2.7141,
      "step": 602600
    },
    {
      "epoch": 195.3646677471637,
      "grad_norm": 1.4227631092071533,
      "learning_rate": 3.0491404476159587e-05,
      "loss": 2.6917,
      "step": 602700
    },
    {
      "epoch": 195.3970826580227,
      "grad_norm": 1.3311394453048706,
      "learning_rate": 3.048819331819656e-05,
      "loss": 2.7108,
      "step": 602800
    },
    {
      "epoch": 195.4294975688817,
      "grad_norm": 1.19175124168396,
      "learning_rate": 3.048494972429452e-05,
      "loss": 2.7275,
      "step": 602900
    },
    {
      "epoch": 195.46191247974068,
      "grad_norm": 1.2406929731369019,
      "learning_rate": 3.0481738566331498e-05,
      "loss": 2.7071,
      "step": 603000
    },
    {
      "epoch": 195.49432739059966,
      "grad_norm": 1.2002894878387451,
      "learning_rate": 3.0478494972429456e-05,
      "loss": 2.7129,
      "step": 603100
    },
    {
      "epoch": 195.52674230145868,
      "grad_norm": 1.2780234813690186,
      "learning_rate": 3.047525137852741e-05,
      "loss": 2.7233,
      "step": 603200
    },
    {
      "epoch": 195.55915721231767,
      "grad_norm": 1.1743106842041016,
      "learning_rate": 3.0472007784625367e-05,
      "loss": 2.7137,
      "step": 603300
    },
    {
      "epoch": 195.59157212317666,
      "grad_norm": 1.043790340423584,
      "learning_rate": 3.0468764190723326e-05,
      "loss": 2.7265,
      "step": 603400
    },
    {
      "epoch": 195.62398703403565,
      "grad_norm": 1.224818229675293,
      "learning_rate": 3.0465520596821278e-05,
      "loss": 2.699,
      "step": 603500
    },
    {
      "epoch": 195.65640194489464,
      "grad_norm": 1.472195029258728,
      "learning_rate": 3.0462277002919237e-05,
      "loss": 2.7037,
      "step": 603600
    },
    {
      "epoch": 195.68881685575366,
      "grad_norm": 1.2885751724243164,
      "learning_rate": 3.0459033409017192e-05,
      "loss": 2.6988,
      "step": 603700
    },
    {
      "epoch": 195.72123176661265,
      "grad_norm": 1.2279207706451416,
      "learning_rate": 3.0455789815115147e-05,
      "loss": 2.7285,
      "step": 603800
    },
    {
      "epoch": 195.75364667747164,
      "grad_norm": 1.1443860530853271,
      "learning_rate": 3.0452546221213106e-05,
      "loss": 2.7131,
      "step": 603900
    },
    {
      "epoch": 195.78606158833063,
      "grad_norm": 1.132583737373352,
      "learning_rate": 3.044930262731106e-05,
      "loss": 2.7082,
      "step": 604000
    },
    {
      "epoch": 195.81847649918961,
      "grad_norm": 1.0115755796432495,
      "learning_rate": 3.044605903340902e-05,
      "loss": 2.7174,
      "step": 604100
    },
    {
      "epoch": 195.85089141004863,
      "grad_norm": 1.2144639492034912,
      "learning_rate": 3.044281543950698e-05,
      "loss": 2.7107,
      "step": 604200
    },
    {
      "epoch": 195.88330632090762,
      "grad_norm": 1.2280325889587402,
      "learning_rate": 3.043957184560493e-05,
      "loss": 2.7055,
      "step": 604300
    },
    {
      "epoch": 195.9157212317666,
      "grad_norm": 1.2399297952651978,
      "learning_rate": 3.043632825170289e-05,
      "loss": 2.7036,
      "step": 604400
    },
    {
      "epoch": 195.9481361426256,
      "grad_norm": 1.1040912866592407,
      "learning_rate": 3.043308465780084e-05,
      "loss": 2.7113,
      "step": 604500
    },
    {
      "epoch": 195.9805510534846,
      "grad_norm": 1.3822773694992065,
      "learning_rate": 3.04298410638988e-05,
      "loss": 2.7391,
      "step": 604600
    },
    {
      "epoch": 196.0,
      "eval_bleu": 1.081110359264959,
      "eval_loss": 4.015615463256836,
      "eval_runtime": 4.4291,
      "eval_samples_per_second": 111.084,
      "eval_steps_per_second": 1.806,
      "step": 604660
    },
    {
      "epoch": 196.0129659643436,
      "grad_norm": 1.1563007831573486,
      "learning_rate": 3.042659746999676e-05,
      "loss": 2.7013,
      "step": 604700
    },
    {
      "epoch": 196.0453808752026,
      "grad_norm": 1.2017061710357666,
      "learning_rate": 3.042335387609471e-05,
      "loss": 2.7152,
      "step": 604800
    },
    {
      "epoch": 196.07779578606159,
      "grad_norm": 1.2903258800506592,
      "learning_rate": 3.042011028219267e-05,
      "loss": 2.7044,
      "step": 604900
    },
    {
      "epoch": 196.11021069692057,
      "grad_norm": 1.2145445346832275,
      "learning_rate": 3.041686668829063e-05,
      "loss": 2.7086,
      "step": 605000
    },
    {
      "epoch": 196.1426256077796,
      "grad_norm": 1.239189624786377,
      "learning_rate": 3.0413623094388584e-05,
      "loss": 2.6923,
      "step": 605100
    },
    {
      "epoch": 196.17504051863858,
      "grad_norm": 1.1683803796768188,
      "learning_rate": 3.0410379500486543e-05,
      "loss": 2.6828,
      "step": 605200
    },
    {
      "epoch": 196.20745542949757,
      "grad_norm": 1.3305550813674927,
      "learning_rate": 3.04071359065845e-05,
      "loss": 2.6941,
      "step": 605300
    },
    {
      "epoch": 196.23987034035656,
      "grad_norm": 1.344638705253601,
      "learning_rate": 3.0403892312682453e-05,
      "loss": 2.7,
      "step": 605400
    },
    {
      "epoch": 196.27228525121555,
      "grad_norm": 1.4097871780395508,
      "learning_rate": 3.0400648718780412e-05,
      "loss": 2.6945,
      "step": 605500
    },
    {
      "epoch": 196.30470016207457,
      "grad_norm": 1.3066434860229492,
      "learning_rate": 3.0397405124878364e-05,
      "loss": 2.728,
      "step": 605600
    },
    {
      "epoch": 196.33711507293356,
      "grad_norm": 1.0520638227462769,
      "learning_rate": 3.0394161530976323e-05,
      "loss": 2.7208,
      "step": 605700
    },
    {
      "epoch": 196.36952998379255,
      "grad_norm": 1.4485594034194946,
      "learning_rate": 3.039091793707428e-05,
      "loss": 2.7077,
      "step": 605800
    },
    {
      "epoch": 196.40194489465154,
      "grad_norm": 1.2777214050292969,
      "learning_rate": 3.0387674343172233e-05,
      "loss": 2.7021,
      "step": 605900
    },
    {
      "epoch": 196.43435980551052,
      "grad_norm": 1.2186318635940552,
      "learning_rate": 3.0384430749270192e-05,
      "loss": 2.7219,
      "step": 606000
    },
    {
      "epoch": 196.46677471636954,
      "grad_norm": 1.2956253290176392,
      "learning_rate": 3.038121959130717e-05,
      "loss": 2.694,
      "step": 606100
    },
    {
      "epoch": 196.49918962722853,
      "grad_norm": 1.1459376811981201,
      "learning_rate": 3.037797599740513e-05,
      "loss": 2.7172,
      "step": 606200
    },
    {
      "epoch": 196.53160453808752,
      "grad_norm": 1.068520188331604,
      "learning_rate": 3.037473240350308e-05,
      "loss": 2.7128,
      "step": 606300
    },
    {
      "epoch": 196.5640194489465,
      "grad_norm": 1.255752444267273,
      "learning_rate": 3.037148880960104e-05,
      "loss": 2.7035,
      "step": 606400
    },
    {
      "epoch": 196.5964343598055,
      "grad_norm": 1.1251438856124878,
      "learning_rate": 3.0368245215698998e-05,
      "loss": 2.722,
      "step": 606500
    },
    {
      "epoch": 196.62884927066452,
      "grad_norm": 1.2280302047729492,
      "learning_rate": 3.036500162179695e-05,
      "loss": 2.7186,
      "step": 606600
    },
    {
      "epoch": 196.6612641815235,
      "grad_norm": 1.1804161071777344,
      "learning_rate": 3.036175802789491e-05,
      "loss": 2.7148,
      "step": 606700
    },
    {
      "epoch": 196.6936790923825,
      "grad_norm": 1.1904679536819458,
      "learning_rate": 3.0358514433992864e-05,
      "loss": 2.7013,
      "step": 606800
    },
    {
      "epoch": 196.72609400324149,
      "grad_norm": 1.077048420906067,
      "learning_rate": 3.0355270840090823e-05,
      "loss": 2.7014,
      "step": 606900
    },
    {
      "epoch": 196.75850891410047,
      "grad_norm": 1.1881202459335327,
      "learning_rate": 3.035202724618878e-05,
      "loss": 2.7244,
      "step": 607000
    },
    {
      "epoch": 196.7909238249595,
      "grad_norm": 1.1304391622543335,
      "learning_rate": 3.0348783652286734e-05,
      "loss": 2.7081,
      "step": 607100
    },
    {
      "epoch": 196.82333873581848,
      "grad_norm": 1.2894245386123657,
      "learning_rate": 3.0345540058384692e-05,
      "loss": 2.7151,
      "step": 607200
    },
    {
      "epoch": 196.85575364667747,
      "grad_norm": 1.3198761940002441,
      "learning_rate": 3.034229646448265e-05,
      "loss": 2.7118,
      "step": 607300
    },
    {
      "epoch": 196.88816855753646,
      "grad_norm": 1.3042017221450806,
      "learning_rate": 3.0339052870580603e-05,
      "loss": 2.7195,
      "step": 607400
    },
    {
      "epoch": 196.92058346839545,
      "grad_norm": 1.4328324794769287,
      "learning_rate": 3.0335809276678562e-05,
      "loss": 2.7099,
      "step": 607500
    },
    {
      "epoch": 196.95299837925447,
      "grad_norm": 1.1986074447631836,
      "learning_rate": 3.033256568277652e-05,
      "loss": 2.6968,
      "step": 607600
    },
    {
      "epoch": 196.98541329011346,
      "grad_norm": 1.391926884651184,
      "learning_rate": 3.0329322088874473e-05,
      "loss": 2.7217,
      "step": 607700
    },
    {
      "epoch": 197.0,
      "eval_bleu": 1.1322184652682257,
      "eval_loss": 4.01526403427124,
      "eval_runtime": 3.7693,
      "eval_samples_per_second": 130.529,
      "eval_steps_per_second": 2.122,
      "step": 607745
    },
    {
      "epoch": 197.01782820097245,
      "grad_norm": 1.0795525312423706,
      "learning_rate": 3.032607849497243e-05,
      "loss": 2.717,
      "step": 607800
    },
    {
      "epoch": 197.05024311183143,
      "grad_norm": 1.1366480588912964,
      "learning_rate": 3.0322834901070383e-05,
      "loss": 2.69,
      "step": 607900
    },
    {
      "epoch": 197.08265802269042,
      "grad_norm": 1.1636048555374146,
      "learning_rate": 3.0319591307168342e-05,
      "loss": 2.6931,
      "step": 608000
    },
    {
      "epoch": 197.11507293354944,
      "grad_norm": 1.3083899021148682,
      "learning_rate": 3.03163477132663e-05,
      "loss": 2.6976,
      "step": 608100
    },
    {
      "epoch": 197.14748784440843,
      "grad_norm": 1.1667615175247192,
      "learning_rate": 3.0313104119364256e-05,
      "loss": 2.7226,
      "step": 608200
    },
    {
      "epoch": 197.17990275526742,
      "grad_norm": 1.319128394126892,
      "learning_rate": 3.0309860525462215e-05,
      "loss": 2.7098,
      "step": 608300
    },
    {
      "epoch": 197.2123176661264,
      "grad_norm": 1.0904618501663208,
      "learning_rate": 3.0306616931560174e-05,
      "loss": 2.7155,
      "step": 608400
    },
    {
      "epoch": 197.24473257698543,
      "grad_norm": 1.3092175722122192,
      "learning_rate": 3.0303373337658126e-05,
      "loss": 2.6854,
      "step": 608500
    },
    {
      "epoch": 197.27714748784442,
      "grad_norm": 1.184224009513855,
      "learning_rate": 3.0300129743756084e-05,
      "loss": 2.6845,
      "step": 608600
    },
    {
      "epoch": 197.3095623987034,
      "grad_norm": 1.2998223304748535,
      "learning_rate": 3.0296886149854043e-05,
      "loss": 2.6925,
      "step": 608700
    },
    {
      "epoch": 197.3419773095624,
      "grad_norm": 1.1177221536636353,
      "learning_rate": 3.0293642555951995e-05,
      "loss": 2.7074,
      "step": 608800
    },
    {
      "epoch": 197.37439222042138,
      "grad_norm": 1.3755598068237305,
      "learning_rate": 3.0290398962049954e-05,
      "loss": 2.6883,
      "step": 608900
    },
    {
      "epoch": 197.4068071312804,
      "grad_norm": 1.1556031703948975,
      "learning_rate": 3.0287155368147906e-05,
      "loss": 2.7183,
      "step": 609000
    },
    {
      "epoch": 197.4392220421394,
      "grad_norm": 1.0936959981918335,
      "learning_rate": 3.0283911774245864e-05,
      "loss": 2.7186,
      "step": 609100
    },
    {
      "epoch": 197.47163695299838,
      "grad_norm": 1.1809139251708984,
      "learning_rate": 3.0280668180343823e-05,
      "loss": 2.7163,
      "step": 609200
    },
    {
      "epoch": 197.50405186385737,
      "grad_norm": 1.3444074392318726,
      "learning_rate": 3.027742458644178e-05,
      "loss": 2.7055,
      "step": 609300
    },
    {
      "epoch": 197.53646677471636,
      "grad_norm": 1.4213718175888062,
      "learning_rate": 3.0274180992539737e-05,
      "loss": 2.714,
      "step": 609400
    },
    {
      "epoch": 197.56888168557538,
      "grad_norm": 1.0567066669464111,
      "learning_rate": 3.0270937398637693e-05,
      "loss": 2.7183,
      "step": 609500
    },
    {
      "epoch": 197.60129659643437,
      "grad_norm": 1.119396686553955,
      "learning_rate": 3.0267693804735648e-05,
      "loss": 2.7039,
      "step": 609600
    },
    {
      "epoch": 197.63371150729336,
      "grad_norm": 1.0061957836151123,
      "learning_rate": 3.0264450210833607e-05,
      "loss": 2.69,
      "step": 609700
    },
    {
      "epoch": 197.66612641815234,
      "grad_norm": 1.2509348392486572,
      "learning_rate": 3.026120661693156e-05,
      "loss": 2.7141,
      "step": 609800
    },
    {
      "epoch": 197.69854132901133,
      "grad_norm": 1.1651402711868286,
      "learning_rate": 3.0257963023029517e-05,
      "loss": 2.6904,
      "step": 609900
    },
    {
      "epoch": 197.73095623987035,
      "grad_norm": 1.1594691276550293,
      "learning_rate": 3.0254719429127476e-05,
      "loss": 2.6924,
      "step": 610000
    },
    {
      "epoch": 197.76337115072934,
      "grad_norm": 1.3189562559127808,
      "learning_rate": 3.0251475835225428e-05,
      "loss": 2.7142,
      "step": 610100
    },
    {
      "epoch": 197.79578606158833,
      "grad_norm": 1.149695634841919,
      "learning_rate": 3.0248232241323387e-05,
      "loss": 2.7106,
      "step": 610200
    },
    {
      "epoch": 197.82820097244732,
      "grad_norm": 1.1338541507720947,
      "learning_rate": 3.0244988647421346e-05,
      "loss": 2.7221,
      "step": 610300
    },
    {
      "epoch": 197.8606158833063,
      "grad_norm": 1.0337811708450317,
      "learning_rate": 3.0241745053519298e-05,
      "loss": 2.7183,
      "step": 610400
    },
    {
      "epoch": 197.89303079416533,
      "grad_norm": 1.3038904666900635,
      "learning_rate": 3.0238501459617256e-05,
      "loss": 2.724,
      "step": 610500
    },
    {
      "epoch": 197.92544570502432,
      "grad_norm": 1.2006603479385376,
      "learning_rate": 3.0235257865715215e-05,
      "loss": 2.7179,
      "step": 610600
    },
    {
      "epoch": 197.9578606158833,
      "grad_norm": 1.0706734657287598,
      "learning_rate": 3.023201427181317e-05,
      "loss": 2.72,
      "step": 610700
    },
    {
      "epoch": 197.9902755267423,
      "grad_norm": 1.2504692077636719,
      "learning_rate": 3.022877067791113e-05,
      "loss": 2.7339,
      "step": 610800
    },
    {
      "epoch": 198.0,
      "eval_bleu": 0.9840553124749668,
      "eval_loss": 4.009250164031982,
      "eval_runtime": 4.1739,
      "eval_samples_per_second": 117.875,
      "eval_steps_per_second": 1.917,
      "step": 610830
    },
    {
      "epoch": 198.02269043760128,
      "grad_norm": 1.4255620241165161,
      "learning_rate": 3.022552708400908e-05,
      "loss": 2.6985,
      "step": 610900
    },
    {
      "epoch": 198.0551053484603,
      "grad_norm": 1.1762036085128784,
      "learning_rate": 3.022228349010704e-05,
      "loss": 2.7027,
      "step": 611000
    },
    {
      "epoch": 198.0875202593193,
      "grad_norm": 1.2036484479904175,
      "learning_rate": 3.0219039896205e-05,
      "loss": 2.6984,
      "step": 611100
    },
    {
      "epoch": 198.11993517017828,
      "grad_norm": 1.1583815813064575,
      "learning_rate": 3.021579630230295e-05,
      "loss": 2.7035,
      "step": 611200
    },
    {
      "epoch": 198.15235008103727,
      "grad_norm": 1.1545624732971191,
      "learning_rate": 3.021255270840091e-05,
      "loss": 2.6995,
      "step": 611300
    },
    {
      "epoch": 198.18476499189626,
      "grad_norm": 1.4169347286224365,
      "learning_rate": 3.0209309114498868e-05,
      "loss": 2.7133,
      "step": 611400
    },
    {
      "epoch": 198.21717990275528,
      "grad_norm": 1.441290020942688,
      "learning_rate": 3.020606552059682e-05,
      "loss": 2.7208,
      "step": 611500
    },
    {
      "epoch": 198.24959481361427,
      "grad_norm": 1.105827808380127,
      "learning_rate": 3.020282192669478e-05,
      "loss": 2.7055,
      "step": 611600
    },
    {
      "epoch": 198.28200972447326,
      "grad_norm": 1.0908293724060059,
      "learning_rate": 3.0199578332792738e-05,
      "loss": 2.7104,
      "step": 611700
    },
    {
      "epoch": 198.31442463533224,
      "grad_norm": 1.281441330909729,
      "learning_rate": 3.0196334738890693e-05,
      "loss": 2.695,
      "step": 611800
    },
    {
      "epoch": 198.34683954619126,
      "grad_norm": 1.1597613096237183,
      "learning_rate": 3.019309114498865e-05,
      "loss": 2.7026,
      "step": 611900
    },
    {
      "epoch": 198.37925445705025,
      "grad_norm": 1.1374660730361938,
      "learning_rate": 3.0189847551086604e-05,
      "loss": 2.7141,
      "step": 612000
    },
    {
      "epoch": 198.41166936790924,
      "grad_norm": 1.0911422967910767,
      "learning_rate": 3.0186636393123578e-05,
      "loss": 2.6987,
      "step": 612100
    },
    {
      "epoch": 198.44408427876823,
      "grad_norm": 1.2842673063278198,
      "learning_rate": 3.0183392799221537e-05,
      "loss": 2.698,
      "step": 612200
    },
    {
      "epoch": 198.47649918962722,
      "grad_norm": 1.2183676958084106,
      "learning_rate": 3.0180149205319496e-05,
      "loss": 2.7061,
      "step": 612300
    },
    {
      "epoch": 198.50891410048624,
      "grad_norm": 1.251059889793396,
      "learning_rate": 3.017690561141745e-05,
      "loss": 2.7162,
      "step": 612400
    },
    {
      "epoch": 198.54132901134523,
      "grad_norm": 1.229390025138855,
      "learning_rate": 3.017366201751541e-05,
      "loss": 2.7413,
      "step": 612500
    },
    {
      "epoch": 198.57374392220422,
      "grad_norm": 1.2604951858520508,
      "learning_rate": 3.017041842361337e-05,
      "loss": 2.6902,
      "step": 612600
    },
    {
      "epoch": 198.6061588330632,
      "grad_norm": 1.1326454877853394,
      "learning_rate": 3.016717482971132e-05,
      "loss": 2.6764,
      "step": 612700
    },
    {
      "epoch": 198.6385737439222,
      "grad_norm": 1.1841567754745483,
      "learning_rate": 3.016393123580928e-05,
      "loss": 2.6988,
      "step": 612800
    },
    {
      "epoch": 198.6709886547812,
      "grad_norm": 1.2864325046539307,
      "learning_rate": 3.0160687641907238e-05,
      "loss": 2.7103,
      "step": 612900
    },
    {
      "epoch": 198.7034035656402,
      "grad_norm": 1.486519455909729,
      "learning_rate": 3.015744404800519e-05,
      "loss": 2.7068,
      "step": 613000
    },
    {
      "epoch": 198.7358184764992,
      "grad_norm": 1.2628037929534912,
      "learning_rate": 3.015420045410315e-05,
      "loss": 2.7076,
      "step": 613100
    },
    {
      "epoch": 198.76823338735818,
      "grad_norm": 1.2727681398391724,
      "learning_rate": 3.01509568602011e-05,
      "loss": 2.7072,
      "step": 613200
    },
    {
      "epoch": 198.80064829821717,
      "grad_norm": 1.2408007383346558,
      "learning_rate": 3.014771326629906e-05,
      "loss": 2.7144,
      "step": 613300
    },
    {
      "epoch": 198.8330632090762,
      "grad_norm": 1.2244421243667603,
      "learning_rate": 3.0144469672397018e-05,
      "loss": 2.7015,
      "step": 613400
    },
    {
      "epoch": 198.86547811993518,
      "grad_norm": 1.2747365236282349,
      "learning_rate": 3.0141226078494973e-05,
      "loss": 2.7141,
      "step": 613500
    },
    {
      "epoch": 198.89789303079417,
      "grad_norm": 1.3336461782455444,
      "learning_rate": 3.0137982484592932e-05,
      "loss": 2.7146,
      "step": 613600
    },
    {
      "epoch": 198.93030794165315,
      "grad_norm": 1.3714396953582764,
      "learning_rate": 3.0134738890690887e-05,
      "loss": 2.7051,
      "step": 613700
    },
    {
      "epoch": 198.96272285251214,
      "grad_norm": 1.156259536743164,
      "learning_rate": 3.0131495296788843e-05,
      "loss": 2.7014,
      "step": 613800
    },
    {
      "epoch": 198.99513776337116,
      "grad_norm": 1.1837306022644043,
      "learning_rate": 3.01282517028868e-05,
      "loss": 2.7104,
      "step": 613900
    },
    {
      "epoch": 199.0,
      "eval_bleu": 0.8971642389460668,
      "eval_loss": 4.012119293212891,
      "eval_runtime": 4.2683,
      "eval_samples_per_second": 115.27,
      "eval_steps_per_second": 1.874,
      "step": 613915
    },
    {
      "epoch": 199.02755267423015,
      "grad_norm": 1.3940056562423706,
      "learning_rate": 3.012500810898476e-05,
      "loss": 2.6719,
      "step": 614000
    },
    {
      "epoch": 199.05996758508914,
      "grad_norm": 1.3677453994750977,
      "learning_rate": 3.0121764515082712e-05,
      "loss": 2.7263,
      "step": 614100
    },
    {
      "epoch": 199.09238249594813,
      "grad_norm": 1.2913199663162231,
      "learning_rate": 3.011855335711969e-05,
      "loss": 2.6844,
      "step": 614200
    },
    {
      "epoch": 199.12479740680712,
      "grad_norm": 1.3366285562515259,
      "learning_rate": 3.011530976321765e-05,
      "loss": 2.6808,
      "step": 614300
    },
    {
      "epoch": 199.15721231766614,
      "grad_norm": 1.3110227584838867,
      "learning_rate": 3.0112066169315608e-05,
      "loss": 2.6939,
      "step": 614400
    },
    {
      "epoch": 199.18962722852513,
      "grad_norm": 1.1791067123413086,
      "learning_rate": 3.010882257541356e-05,
      "loss": 2.7041,
      "step": 614500
    },
    {
      "epoch": 199.22204213938411,
      "grad_norm": 1.1338846683502197,
      "learning_rate": 3.0105578981511518e-05,
      "loss": 2.6767,
      "step": 614600
    },
    {
      "epoch": 199.2544570502431,
      "grad_norm": 1.2777142524719238,
      "learning_rate": 3.010233538760947e-05,
      "loss": 2.712,
      "step": 614700
    },
    {
      "epoch": 199.2868719611021,
      "grad_norm": 1.2514903545379639,
      "learning_rate": 3.009909179370743e-05,
      "loss": 2.698,
      "step": 614800
    },
    {
      "epoch": 199.3192868719611,
      "grad_norm": 1.4692620038986206,
      "learning_rate": 3.0095848199805388e-05,
      "loss": 2.707,
      "step": 614900
    },
    {
      "epoch": 199.3517017828201,
      "grad_norm": 1.3070446252822876,
      "learning_rate": 3.009260460590334e-05,
      "loss": 2.6811,
      "step": 615000
    },
    {
      "epoch": 199.3841166936791,
      "grad_norm": 1.194165587425232,
      "learning_rate": 3.00893610120013e-05,
      "loss": 2.6867,
      "step": 615100
    },
    {
      "epoch": 199.41653160453808,
      "grad_norm": 1.2947437763214111,
      "learning_rate": 3.0086117418099257e-05,
      "loss": 2.6809,
      "step": 615200
    },
    {
      "epoch": 199.4489465153971,
      "grad_norm": 1.2393614053726196,
      "learning_rate": 3.0082873824197212e-05,
      "loss": 2.71,
      "step": 615300
    },
    {
      "epoch": 199.4813614262561,
      "grad_norm": 1.1528481245040894,
      "learning_rate": 3.0079630230295168e-05,
      "loss": 2.6986,
      "step": 615400
    },
    {
      "epoch": 199.51377633711508,
      "grad_norm": 1.2359129190444946,
      "learning_rate": 3.0076386636393123e-05,
      "loss": 2.7156,
      "step": 615500
    },
    {
      "epoch": 199.54619124797406,
      "grad_norm": 1.411956548690796,
      "learning_rate": 3.0073143042491082e-05,
      "loss": 2.7128,
      "step": 615600
    },
    {
      "epoch": 199.57860615883305,
      "grad_norm": 1.2878613471984863,
      "learning_rate": 3.006989944858904e-05,
      "loss": 2.7023,
      "step": 615700
    },
    {
      "epoch": 199.61102106969207,
      "grad_norm": 1.1704771518707275,
      "learning_rate": 3.0066655854686993e-05,
      "loss": 2.7109,
      "step": 615800
    },
    {
      "epoch": 199.64343598055106,
      "grad_norm": 1.343170404434204,
      "learning_rate": 3.006341226078495e-05,
      "loss": 2.728,
      "step": 615900
    },
    {
      "epoch": 199.67585089141005,
      "grad_norm": 1.2164462804794312,
      "learning_rate": 3.006016866688291e-05,
      "loss": 2.696,
      "step": 616000
    },
    {
      "epoch": 199.70826580226904,
      "grad_norm": 1.137230396270752,
      "learning_rate": 3.0056925072980862e-05,
      "loss": 2.7109,
      "step": 616100
    },
    {
      "epoch": 199.74068071312803,
      "grad_norm": 1.1302390098571777,
      "learning_rate": 3.005371391501784e-05,
      "loss": 2.6978,
      "step": 616200
    },
    {
      "epoch": 199.77309562398705,
      "grad_norm": 1.2328293323516846,
      "learning_rate": 3.00504703211158e-05,
      "loss": 2.7017,
      "step": 616300
    },
    {
      "epoch": 199.80551053484604,
      "grad_norm": 1.0897010564804077,
      "learning_rate": 3.0047226727213757e-05,
      "loss": 2.7199,
      "step": 616400
    },
    {
      "epoch": 199.83792544570503,
      "grad_norm": 1.1875265836715698,
      "learning_rate": 3.004398313331171e-05,
      "loss": 2.7192,
      "step": 616500
    },
    {
      "epoch": 199.87034035656401,
      "grad_norm": 1.2466816902160645,
      "learning_rate": 3.0040739539409668e-05,
      "loss": 2.7165,
      "step": 616600
    },
    {
      "epoch": 199.902755267423,
      "grad_norm": 1.2690292596817017,
      "learning_rate": 3.0037495945507627e-05,
      "loss": 2.7238,
      "step": 616700
    },
    {
      "epoch": 199.93517017828202,
      "grad_norm": 1.322210669517517,
      "learning_rate": 3.003425235160558e-05,
      "loss": 2.7121,
      "step": 616800
    },
    {
      "epoch": 199.967585089141,
      "grad_norm": 1.4413642883300781,
      "learning_rate": 3.0031008757703538e-05,
      "loss": 2.7199,
      "step": 616900
    },
    {
      "epoch": 200.0,
      "grad_norm": 1.1974689960479736,
      "learning_rate": 3.002776516380149e-05,
      "loss": 2.7076,
      "step": 617000
    },
    {
      "epoch": 200.0,
      "eval_bleu": 0.9848218826267832,
      "eval_loss": 4.023063659667969,
      "eval_runtime": 3.7294,
      "eval_samples_per_second": 131.924,
      "eval_steps_per_second": 2.145,
      "step": 617000
    },
    {
      "epoch": 200.032414910859,
      "grad_norm": 1.0763572454452515,
      "learning_rate": 3.0024521569899448e-05,
      "loss": 2.6943,
      "step": 617100
    },
    {
      "epoch": 200.06482982171798,
      "grad_norm": 1.1853854656219482,
      "learning_rate": 3.0021277975997407e-05,
      "loss": 2.6995,
      "step": 617200
    },
    {
      "epoch": 200.097244732577,
      "grad_norm": 1.321498155593872,
      "learning_rate": 3.0018034382095362e-05,
      "loss": 2.6945,
      "step": 617300
    },
    {
      "epoch": 200.12965964343599,
      "grad_norm": 1.3548736572265625,
      "learning_rate": 3.001479078819332e-05,
      "loss": 2.7077,
      "step": 617400
    },
    {
      "epoch": 200.16207455429497,
      "grad_norm": 1.4213573932647705,
      "learning_rate": 3.001154719429128e-05,
      "loss": 2.7105,
      "step": 617500
    },
    {
      "epoch": 200.19448946515396,
      "grad_norm": 1.6188350915908813,
      "learning_rate": 3.0008303600389232e-05,
      "loss": 2.7046,
      "step": 617600
    },
    {
      "epoch": 200.22690437601295,
      "grad_norm": 1.2026289701461792,
      "learning_rate": 3.000506000648719e-05,
      "loss": 2.6867,
      "step": 617700
    },
    {
      "epoch": 200.25931928687197,
      "grad_norm": 1.0612746477127075,
      "learning_rate": 3.0001816412585143e-05,
      "loss": 2.7119,
      "step": 617800
    },
    {
      "epoch": 200.29173419773096,
      "grad_norm": 1.2690726518630981,
      "learning_rate": 2.99985728186831e-05,
      "loss": 2.6967,
      "step": 617900
    },
    {
      "epoch": 200.32414910858995,
      "grad_norm": 1.2484148740768433,
      "learning_rate": 2.999532922478106e-05,
      "loss": 2.6858,
      "step": 618000
    },
    {
      "epoch": 200.35656401944894,
      "grad_norm": 1.3121613264083862,
      "learning_rate": 2.9992085630879012e-05,
      "loss": 2.7038,
      "step": 618100
    },
    {
      "epoch": 200.38897893030793,
      "grad_norm": 1.058026671409607,
      "learning_rate": 2.998887447291599e-05,
      "loss": 2.6974,
      "step": 618200
    },
    {
      "epoch": 200.42139384116695,
      "grad_norm": 1.5688396692276,
      "learning_rate": 2.998563087901395e-05,
      "loss": 2.7032,
      "step": 618300
    },
    {
      "epoch": 200.45380875202594,
      "grad_norm": 1.4503737688064575,
      "learning_rate": 2.9982387285111907e-05,
      "loss": 2.6937,
      "step": 618400
    },
    {
      "epoch": 200.48622366288492,
      "grad_norm": 1.3128761053085327,
      "learning_rate": 2.997914369120986e-05,
      "loss": 2.7179,
      "step": 618500
    },
    {
      "epoch": 200.5186385737439,
      "grad_norm": 1.4032213687896729,
      "learning_rate": 2.9975900097307818e-05,
      "loss": 2.6931,
      "step": 618600
    },
    {
      "epoch": 200.5510534846029,
      "grad_norm": 1.2369643449783325,
      "learning_rate": 2.9972656503405777e-05,
      "loss": 2.7093,
      "step": 618700
    },
    {
      "epoch": 200.58346839546192,
      "grad_norm": 1.1387221813201904,
      "learning_rate": 2.996941290950373e-05,
      "loss": 2.7192,
      "step": 618800
    },
    {
      "epoch": 200.6158833063209,
      "grad_norm": 1.3927631378173828,
      "learning_rate": 2.9966169315601687e-05,
      "loss": 2.6873,
      "step": 618900
    },
    {
      "epoch": 200.6482982171799,
      "grad_norm": 1.227242350578308,
      "learning_rate": 2.9962925721699646e-05,
      "loss": 2.6994,
      "step": 619000
    },
    {
      "epoch": 200.6807131280389,
      "grad_norm": 1.1018450260162354,
      "learning_rate": 2.99596821277976e-05,
      "loss": 2.723,
      "step": 619100
    },
    {
      "epoch": 200.7131280388979,
      "grad_norm": 1.156613826751709,
      "learning_rate": 2.995643853389556e-05,
      "loss": 2.6882,
      "step": 619200
    },
    {
      "epoch": 200.7455429497569,
      "grad_norm": 1.219713568687439,
      "learning_rate": 2.9953194939993512e-05,
      "loss": 2.7234,
      "step": 619300
    },
    {
      "epoch": 200.77795786061589,
      "grad_norm": 1.1692335605621338,
      "learning_rate": 2.994995134609147e-05,
      "loss": 2.7025,
      "step": 619400
    },
    {
      "epoch": 200.81037277147487,
      "grad_norm": 1.1952139139175415,
      "learning_rate": 2.994670775218943e-05,
      "loss": 2.7029,
      "step": 619500
    },
    {
      "epoch": 200.84278768233386,
      "grad_norm": 1.6090563535690308,
      "learning_rate": 2.994346415828738e-05,
      "loss": 2.6936,
      "step": 619600
    },
    {
      "epoch": 200.87520259319288,
      "grad_norm": 1.2521394491195679,
      "learning_rate": 2.994022056438534e-05,
      "loss": 2.722,
      "step": 619700
    },
    {
      "epoch": 200.90761750405187,
      "grad_norm": 1.2499690055847168,
      "learning_rate": 2.99369769704833e-05,
      "loss": 2.7129,
      "step": 619800
    },
    {
      "epoch": 200.94003241491086,
      "grad_norm": 1.210996150970459,
      "learning_rate": 2.993373337658125e-05,
      "loss": 2.6991,
      "step": 619900
    },
    {
      "epoch": 200.97244732576985,
      "grad_norm": 1.1302642822265625,
      "learning_rate": 2.993048978267921e-05,
      "loss": 2.7176,
      "step": 620000
    },
    {
      "epoch": 201.0,
      "eval_bleu": 1.037132392861645,
      "eval_loss": 4.021056175231934,
      "eval_runtime": 4.2637,
      "eval_samples_per_second": 115.393,
      "eval_steps_per_second": 1.876,
      "step": 620085
    },
    {
      "epoch": 201.00486223662884,
      "grad_norm": 1.4723591804504395,
      "learning_rate": 2.9927246188777165e-05,
      "loss": 2.69,
      "step": 620100
    },
    {
      "epoch": 201.03727714748786,
      "grad_norm": 1.2492467164993286,
      "learning_rate": 2.9924035030814146e-05,
      "loss": 2.7091,
      "step": 620200
    },
    {
      "epoch": 201.06969205834685,
      "grad_norm": 1.2123501300811768,
      "learning_rate": 2.99207914369121e-05,
      "loss": 2.7028,
      "step": 620300
    },
    {
      "epoch": 201.10210696920583,
      "grad_norm": 1.0793300867080688,
      "learning_rate": 2.9917547843010057e-05,
      "loss": 2.6951,
      "step": 620400
    },
    {
      "epoch": 201.13452188006482,
      "grad_norm": 1.1721901893615723,
      "learning_rate": 2.991430424910801e-05,
      "loss": 2.6959,
      "step": 620500
    },
    {
      "epoch": 201.1669367909238,
      "grad_norm": 1.076263427734375,
      "learning_rate": 2.9911060655205968e-05,
      "loss": 2.6831,
      "step": 620600
    },
    {
      "epoch": 201.19935170178283,
      "grad_norm": 1.204010248184204,
      "learning_rate": 2.9907817061303927e-05,
      "loss": 2.7189,
      "step": 620700
    },
    {
      "epoch": 201.23176661264182,
      "grad_norm": 1.1736730337142944,
      "learning_rate": 2.9904573467401882e-05,
      "loss": 2.6953,
      "step": 620800
    },
    {
      "epoch": 201.2641815235008,
      "grad_norm": 1.1239724159240723,
      "learning_rate": 2.990132987349984e-05,
      "loss": 2.7116,
      "step": 620900
    },
    {
      "epoch": 201.2965964343598,
      "grad_norm": 1.1033949851989746,
      "learning_rate": 2.98980862795978e-05,
      "loss": 2.7016,
      "step": 621000
    },
    {
      "epoch": 201.3290113452188,
      "grad_norm": 1.2335641384124756,
      "learning_rate": 2.989484268569575e-05,
      "loss": 2.7003,
      "step": 621100
    },
    {
      "epoch": 201.3614262560778,
      "grad_norm": 1.1346110105514526,
      "learning_rate": 2.989159909179371e-05,
      "loss": 2.6761,
      "step": 621200
    },
    {
      "epoch": 201.3938411669368,
      "grad_norm": 1.218352198600769,
      "learning_rate": 2.988835549789167e-05,
      "loss": 2.6912,
      "step": 621300
    },
    {
      "epoch": 201.42625607779578,
      "grad_norm": 1.1985353231430054,
      "learning_rate": 2.988511190398962e-05,
      "loss": 2.6868,
      "step": 621400
    },
    {
      "epoch": 201.45867098865477,
      "grad_norm": 1.284837007522583,
      "learning_rate": 2.988186831008758e-05,
      "loss": 2.7214,
      "step": 621500
    },
    {
      "epoch": 201.49108589951376,
      "grad_norm": 0.9899381399154663,
      "learning_rate": 2.987862471618553e-05,
      "loss": 2.6888,
      "step": 621600
    },
    {
      "epoch": 201.52350081037278,
      "grad_norm": 1.2230790853500366,
      "learning_rate": 2.987538112228349e-05,
      "loss": 2.6887,
      "step": 621700
    },
    {
      "epoch": 201.55591572123177,
      "grad_norm": 1.315303921699524,
      "learning_rate": 2.987213752838145e-05,
      "loss": 2.7021,
      "step": 621800
    },
    {
      "epoch": 201.58833063209076,
      "grad_norm": 1.3238520622253418,
      "learning_rate": 2.9868893934479404e-05,
      "loss": 2.704,
      "step": 621900
    },
    {
      "epoch": 201.62074554294975,
      "grad_norm": 1.1486855745315552,
      "learning_rate": 2.986565034057736e-05,
      "loss": 2.7289,
      "step": 622000
    },
    {
      "epoch": 201.65316045380874,
      "grad_norm": 1.2210701704025269,
      "learning_rate": 2.986240674667532e-05,
      "loss": 2.6897,
      "step": 622100
    },
    {
      "epoch": 201.68557536466776,
      "grad_norm": 1.148681640625,
      "learning_rate": 2.9859195588712296e-05,
      "loss": 2.6927,
      "step": 622200
    },
    {
      "epoch": 201.71799027552674,
      "grad_norm": 1.1809141635894775,
      "learning_rate": 2.9855951994810248e-05,
      "loss": 2.6969,
      "step": 622300
    },
    {
      "epoch": 201.75040518638573,
      "grad_norm": 1.3388311862945557,
      "learning_rate": 2.9852708400908207e-05,
      "loss": 2.7179,
      "step": 622400
    },
    {
      "epoch": 201.78282009724472,
      "grad_norm": 1.2372082471847534,
      "learning_rate": 2.9849464807006166e-05,
      "loss": 2.6952,
      "step": 622500
    },
    {
      "epoch": 201.81523500810374,
      "grad_norm": 1.3859663009643555,
      "learning_rate": 2.984622121310412e-05,
      "loss": 2.7016,
      "step": 622600
    },
    {
      "epoch": 201.84764991896273,
      "grad_norm": 1.1556587219238281,
      "learning_rate": 2.984297761920208e-05,
      "loss": 2.6983,
      "step": 622700
    },
    {
      "epoch": 201.88006482982172,
      "grad_norm": 1.2110096216201782,
      "learning_rate": 2.9839734025300032e-05,
      "loss": 2.7048,
      "step": 622800
    },
    {
      "epoch": 201.9124797406807,
      "grad_norm": 1.1662248373031616,
      "learning_rate": 2.983649043139799e-05,
      "loss": 2.6927,
      "step": 622900
    },
    {
      "epoch": 201.9448946515397,
      "grad_norm": 1.0751991271972656,
      "learning_rate": 2.983324683749595e-05,
      "loss": 2.7165,
      "step": 623000
    },
    {
      "epoch": 201.97730956239872,
      "grad_norm": 1.233134388923645,
      "learning_rate": 2.98300032435939e-05,
      "loss": 2.698,
      "step": 623100
    },
    {
      "epoch": 202.0,
      "eval_bleu": 1.0102114837480862,
      "eval_loss": 4.0279154777526855,
      "eval_runtime": 3.931,
      "eval_samples_per_second": 125.16,
      "eval_steps_per_second": 2.035,
      "step": 623170
    },
    {
      "epoch": 202.0097244732577,
      "grad_norm": 1.249611496925354,
      "learning_rate": 2.982675964969186e-05,
      "loss": 2.6964,
      "step": 623200
    },
    {
      "epoch": 202.0421393841167,
      "grad_norm": 1.2486188411712646,
      "learning_rate": 2.982351605578982e-05,
      "loss": 2.6811,
      "step": 623300
    },
    {
      "epoch": 202.07455429497568,
      "grad_norm": 1.2258802652359009,
      "learning_rate": 2.982027246188777e-05,
      "loss": 2.7009,
      "step": 623400
    },
    {
      "epoch": 202.10696920583467,
      "grad_norm": 1.1680700778961182,
      "learning_rate": 2.981702886798573e-05,
      "loss": 2.6884,
      "step": 623500
    },
    {
      "epoch": 202.1393841166937,
      "grad_norm": 1.2474780082702637,
      "learning_rate": 2.9813785274083688e-05,
      "loss": 2.7018,
      "step": 623600
    },
    {
      "epoch": 202.17179902755268,
      "grad_norm": 1.229470133781433,
      "learning_rate": 2.981054168018164e-05,
      "loss": 2.7083,
      "step": 623700
    },
    {
      "epoch": 202.20421393841167,
      "grad_norm": 1.1442006826400757,
      "learning_rate": 2.98072980862796e-05,
      "loss": 2.6855,
      "step": 623800
    },
    {
      "epoch": 202.23662884927066,
      "grad_norm": 1.186414361000061,
      "learning_rate": 2.9804054492377554e-05,
      "loss": 2.7051,
      "step": 623900
    },
    {
      "epoch": 202.26904376012965,
      "grad_norm": 1.1820425987243652,
      "learning_rate": 2.9800810898475513e-05,
      "loss": 2.7036,
      "step": 624000
    },
    {
      "epoch": 202.30145867098867,
      "grad_norm": 1.0887041091918945,
      "learning_rate": 2.9797567304573472e-05,
      "loss": 2.7083,
      "step": 624100
    },
    {
      "epoch": 202.33387358184766,
      "grad_norm": 1.2520586252212524,
      "learning_rate": 2.9794323710671424e-05,
      "loss": 2.7122,
      "step": 624200
    },
    {
      "epoch": 202.36628849270664,
      "grad_norm": 1.1627790927886963,
      "learning_rate": 2.97911125527084e-05,
      "loss": 2.7102,
      "step": 624300
    },
    {
      "epoch": 202.39870340356563,
      "grad_norm": 1.2527257204055786,
      "learning_rate": 2.978786895880636e-05,
      "loss": 2.6889,
      "step": 624400
    },
    {
      "epoch": 202.43111831442462,
      "grad_norm": 1.15194833278656,
      "learning_rate": 2.9784625364904316e-05,
      "loss": 2.7041,
      "step": 624500
    },
    {
      "epoch": 202.46353322528364,
      "grad_norm": 1.2034718990325928,
      "learning_rate": 2.978138177100227e-05,
      "loss": 2.7079,
      "step": 624600
    },
    {
      "epoch": 202.49594813614263,
      "grad_norm": 1.2460265159606934,
      "learning_rate": 2.977813817710023e-05,
      "loss": 2.6961,
      "step": 624700
    },
    {
      "epoch": 202.52836304700162,
      "grad_norm": 1.2013647556304932,
      "learning_rate": 2.977489458319819e-05,
      "loss": 2.6859,
      "step": 624800
    },
    {
      "epoch": 202.5607779578606,
      "grad_norm": 1.3657634258270264,
      "learning_rate": 2.977165098929614e-05,
      "loss": 2.7002,
      "step": 624900
    },
    {
      "epoch": 202.5931928687196,
      "grad_norm": 1.160030484199524,
      "learning_rate": 2.97684073953941e-05,
      "loss": 2.6954,
      "step": 625000
    },
    {
      "epoch": 202.62560777957862,
      "grad_norm": 1.3109521865844727,
      "learning_rate": 2.976516380149205e-05,
      "loss": 2.6854,
      "step": 625100
    },
    {
      "epoch": 202.6580226904376,
      "grad_norm": 1.2300807237625122,
      "learning_rate": 2.976192020759001e-05,
      "loss": 2.6898,
      "step": 625200
    },
    {
      "epoch": 202.6904376012966,
      "grad_norm": 1.4414016008377075,
      "learning_rate": 2.975867661368797e-05,
      "loss": 2.6866,
      "step": 625300
    },
    {
      "epoch": 202.72285251215558,
      "grad_norm": 1.1090202331542969,
      "learning_rate": 2.975543301978592e-05,
      "loss": 2.6891,
      "step": 625400
    },
    {
      "epoch": 202.75526742301457,
      "grad_norm": 1.249236822128296,
      "learning_rate": 2.975218942588388e-05,
      "loss": 2.699,
      "step": 625500
    },
    {
      "epoch": 202.7876823338736,
      "grad_norm": 1.2174171209335327,
      "learning_rate": 2.9748945831981838e-05,
      "loss": 2.7004,
      "step": 625600
    },
    {
      "epoch": 202.82009724473258,
      "grad_norm": 1.1633656024932861,
      "learning_rate": 2.9745702238079793e-05,
      "loss": 2.7029,
      "step": 625700
    },
    {
      "epoch": 202.85251215559157,
      "grad_norm": 1.088680624961853,
      "learning_rate": 2.9742458644177752e-05,
      "loss": 2.6971,
      "step": 625800
    },
    {
      "epoch": 202.88492706645056,
      "grad_norm": 1.2236008644104004,
      "learning_rate": 2.973921505027571e-05,
      "loss": 2.7151,
      "step": 625900
    },
    {
      "epoch": 202.91734197730958,
      "grad_norm": 1.2566475868225098,
      "learning_rate": 2.9735971456373663e-05,
      "loss": 2.6931,
      "step": 626000
    },
    {
      "epoch": 202.94975688816857,
      "grad_norm": 1.2377009391784668,
      "learning_rate": 2.973272786247162e-05,
      "loss": 2.701,
      "step": 626100
    },
    {
      "epoch": 202.98217179902755,
      "grad_norm": 1.1193277835845947,
      "learning_rate": 2.9729484268569574e-05,
      "loss": 2.7242,
      "step": 626200
    },
    {
      "epoch": 203.0,
      "eval_bleu": 0.8169418692993277,
      "eval_loss": 4.029264450073242,
      "eval_runtime": 3.7939,
      "eval_samples_per_second": 129.681,
      "eval_steps_per_second": 2.109,
      "step": 626255
    },
    {
      "epoch": 203.01458670988654,
      "grad_norm": 1.241962194442749,
      "learning_rate": 2.9726273110606555e-05,
      "loss": 2.7014,
      "step": 626300
    },
    {
      "epoch": 203.04700162074553,
      "grad_norm": 1.4036633968353271,
      "learning_rate": 2.972302951670451e-05,
      "loss": 2.6993,
      "step": 626400
    },
    {
      "epoch": 203.07941653160455,
      "grad_norm": 1.1772524118423462,
      "learning_rate": 2.971978592280247e-05,
      "loss": 2.709,
      "step": 626500
    },
    {
      "epoch": 203.11183144246354,
      "grad_norm": 1.1907198429107666,
      "learning_rate": 2.971654232890042e-05,
      "loss": 2.6854,
      "step": 626600
    },
    {
      "epoch": 203.14424635332253,
      "grad_norm": 1.381970763206482,
      "learning_rate": 2.971329873499838e-05,
      "loss": 2.6959,
      "step": 626700
    },
    {
      "epoch": 203.17666126418152,
      "grad_norm": 1.3485891819000244,
      "learning_rate": 2.9710055141096338e-05,
      "loss": 2.7064,
      "step": 626800
    },
    {
      "epoch": 203.2090761750405,
      "grad_norm": 1.1666314601898193,
      "learning_rate": 2.9706843983133316e-05,
      "loss": 2.6852,
      "step": 626900
    },
    {
      "epoch": 203.24149108589953,
      "grad_norm": 1.190011739730835,
      "learning_rate": 2.9703600389231268e-05,
      "loss": 2.7025,
      "step": 627000
    },
    {
      "epoch": 203.27390599675851,
      "grad_norm": 1.6774386167526245,
      "learning_rate": 2.9700356795329227e-05,
      "loss": 2.6841,
      "step": 627100
    },
    {
      "epoch": 203.3063209076175,
      "grad_norm": 1.1794772148132324,
      "learning_rate": 2.9697113201427186e-05,
      "loss": 2.6917,
      "step": 627200
    },
    {
      "epoch": 203.3387358184765,
      "grad_norm": 1.1938236951828003,
      "learning_rate": 2.9693869607525138e-05,
      "loss": 2.6944,
      "step": 627300
    },
    {
      "epoch": 203.37115072933548,
      "grad_norm": 1.2244616746902466,
      "learning_rate": 2.9690626013623096e-05,
      "loss": 2.6938,
      "step": 627400
    },
    {
      "epoch": 203.4035656401945,
      "grad_norm": 1.2367900609970093,
      "learning_rate": 2.9687382419721055e-05,
      "loss": 2.7072,
      "step": 627500
    },
    {
      "epoch": 203.4359805510535,
      "grad_norm": 1.1950678825378418,
      "learning_rate": 2.9684138825819007e-05,
      "loss": 2.7106,
      "step": 627600
    },
    {
      "epoch": 203.46839546191248,
      "grad_norm": 1.108817219734192,
      "learning_rate": 2.9680895231916966e-05,
      "loss": 2.6687,
      "step": 627700
    },
    {
      "epoch": 203.50081037277147,
      "grad_norm": 1.113351583480835,
      "learning_rate": 2.967765163801492e-05,
      "loss": 2.7067,
      "step": 627800
    },
    {
      "epoch": 203.53322528363046,
      "grad_norm": 1.4919066429138184,
      "learning_rate": 2.9674408044112876e-05,
      "loss": 2.7038,
      "step": 627900
    },
    {
      "epoch": 203.56564019448948,
      "grad_norm": 1.3183610439300537,
      "learning_rate": 2.9671164450210835e-05,
      "loss": 2.7126,
      "step": 628000
    },
    {
      "epoch": 203.59805510534846,
      "grad_norm": 1.1980161666870117,
      "learning_rate": 2.966792085630879e-05,
      "loss": 2.6849,
      "step": 628100
    },
    {
      "epoch": 203.63047001620745,
      "grad_norm": 1.2602739334106445,
      "learning_rate": 2.966467726240675e-05,
      "loss": 2.7024,
      "step": 628200
    },
    {
      "epoch": 203.66288492706644,
      "grad_norm": 1.3790596723556519,
      "learning_rate": 2.9661433668504708e-05,
      "loss": 2.6959,
      "step": 628300
    },
    {
      "epoch": 203.69529983792543,
      "grad_norm": 1.2586528062820435,
      "learning_rate": 2.965819007460266e-05,
      "loss": 2.6905,
      "step": 628400
    },
    {
      "epoch": 203.72771474878445,
      "grad_norm": 1.145858645439148,
      "learning_rate": 2.965494648070062e-05,
      "loss": 2.6883,
      "step": 628500
    },
    {
      "epoch": 203.76012965964344,
      "grad_norm": 1.1196550130844116,
      "learning_rate": 2.9651702886798577e-05,
      "loss": 2.7021,
      "step": 628600
    },
    {
      "epoch": 203.79254457050243,
      "grad_norm": 1.1178644895553589,
      "learning_rate": 2.964845929289653e-05,
      "loss": 2.6796,
      "step": 628700
    },
    {
      "epoch": 203.82495948136142,
      "grad_norm": 1.3556171655654907,
      "learning_rate": 2.9645215698994488e-05,
      "loss": 2.6838,
      "step": 628800
    },
    {
      "epoch": 203.8573743922204,
      "grad_norm": 1.2492332458496094,
      "learning_rate": 2.964197210509244e-05,
      "loss": 2.7264,
      "step": 628900
    },
    {
      "epoch": 203.88978930307943,
      "grad_norm": 1.4907143115997314,
      "learning_rate": 2.96387285111904e-05,
      "loss": 2.7194,
      "step": 629000
    },
    {
      "epoch": 203.92220421393841,
      "grad_norm": 1.1092288494110107,
      "learning_rate": 2.9635484917288358e-05,
      "loss": 2.6914,
      "step": 629100
    },
    {
      "epoch": 203.9546191247974,
      "grad_norm": 1.1472978591918945,
      "learning_rate": 2.9632241323386313e-05,
      "loss": 2.6989,
      "step": 629200
    },
    {
      "epoch": 203.9870340356564,
      "grad_norm": 1.1323968172073364,
      "learning_rate": 2.9628997729484272e-05,
      "loss": 2.7082,
      "step": 629300
    },
    {
      "epoch": 204.0,
      "eval_bleu": 0.9583476773854895,
      "eval_loss": 4.023859977722168,
      "eval_runtime": 3.8107,
      "eval_samples_per_second": 129.11,
      "eval_steps_per_second": 2.099,
      "step": 629340
    },
    {
      "epoch": 204.0194489465154,
      "grad_norm": 1.402826189994812,
      "learning_rate": 2.962575413558223e-05,
      "loss": 2.6728,
      "step": 629400
    },
    {
      "epoch": 204.0518638573744,
      "grad_norm": 1.0747255086898804,
      "learning_rate": 2.9622510541680182e-05,
      "loss": 2.6891,
      "step": 629500
    },
    {
      "epoch": 204.0842787682334,
      "grad_norm": 1.1647847890853882,
      "learning_rate": 2.961926694777814e-05,
      "loss": 2.691,
      "step": 629600
    },
    {
      "epoch": 204.11669367909238,
      "grad_norm": 1.309675693511963,
      "learning_rate": 2.9616023353876093e-05,
      "loss": 2.6928,
      "step": 629700
    },
    {
      "epoch": 204.14910858995137,
      "grad_norm": 1.1065325736999512,
      "learning_rate": 2.9612779759974052e-05,
      "loss": 2.6864,
      "step": 629800
    },
    {
      "epoch": 204.18152350081039,
      "grad_norm": 1.341607928276062,
      "learning_rate": 2.960953616607201e-05,
      "loss": 2.7013,
      "step": 629900
    },
    {
      "epoch": 204.21393841166937,
      "grad_norm": 1.2754724025726318,
      "learning_rate": 2.9606292572169963e-05,
      "loss": 2.6896,
      "step": 630000
    },
    {
      "epoch": 204.24635332252836,
      "grad_norm": 1.5235421657562256,
      "learning_rate": 2.960304897826792e-05,
      "loss": 2.7062,
      "step": 630100
    },
    {
      "epoch": 204.27876823338735,
      "grad_norm": 1.0502866506576538,
      "learning_rate": 2.959980538436588e-05,
      "loss": 2.6949,
      "step": 630200
    },
    {
      "epoch": 204.31118314424634,
      "grad_norm": 1.1202296018600464,
      "learning_rate": 2.9596561790463832e-05,
      "loss": 2.6859,
      "step": 630300
    },
    {
      "epoch": 204.34359805510536,
      "grad_norm": 1.142431616783142,
      "learning_rate": 2.959331819656179e-05,
      "loss": 2.6942,
      "step": 630400
    },
    {
      "epoch": 204.37601296596435,
      "grad_norm": 1.3420701026916504,
      "learning_rate": 2.959007460265975e-05,
      "loss": 2.6782,
      "step": 630500
    },
    {
      "epoch": 204.40842787682334,
      "grad_norm": 1.2041529417037964,
      "learning_rate": 2.9586831008757705e-05,
      "loss": 2.7037,
      "step": 630600
    },
    {
      "epoch": 204.44084278768233,
      "grad_norm": 1.0986592769622803,
      "learning_rate": 2.9583587414855664e-05,
      "loss": 2.7,
      "step": 630700
    },
    {
      "epoch": 204.47325769854132,
      "grad_norm": 1.2595771551132202,
      "learning_rate": 2.9580343820953616e-05,
      "loss": 2.7083,
      "step": 630800
    },
    {
      "epoch": 204.50567260940034,
      "grad_norm": 1.1317975521087646,
      "learning_rate": 2.9577132662990597e-05,
      "loss": 2.7058,
      "step": 630900
    },
    {
      "epoch": 204.53808752025932,
      "grad_norm": 1.2125107049942017,
      "learning_rate": 2.9573889069088552e-05,
      "loss": 2.6953,
      "step": 631000
    },
    {
      "epoch": 204.5705024311183,
      "grad_norm": 1.2544419765472412,
      "learning_rate": 2.9570645475186507e-05,
      "loss": 2.6912,
      "step": 631100
    },
    {
      "epoch": 204.6029173419773,
      "grad_norm": 1.489112377166748,
      "learning_rate": 2.9567401881284463e-05,
      "loss": 2.6626,
      "step": 631200
    },
    {
      "epoch": 204.6353322528363,
      "grad_norm": 1.199568748474121,
      "learning_rate": 2.956415828738242e-05,
      "loss": 2.6998,
      "step": 631300
    },
    {
      "epoch": 204.6677471636953,
      "grad_norm": 1.173560619354248,
      "learning_rate": 2.956091469348038e-05,
      "loss": 2.7047,
      "step": 631400
    },
    {
      "epoch": 204.7001620745543,
      "grad_norm": 1.3654417991638184,
      "learning_rate": 2.9557671099578332e-05,
      "loss": 2.6826,
      "step": 631500
    },
    {
      "epoch": 204.7325769854133,
      "grad_norm": 1.4086090326309204,
      "learning_rate": 2.955442750567629e-05,
      "loss": 2.6847,
      "step": 631600
    },
    {
      "epoch": 204.76499189627228,
      "grad_norm": 1.2575620412826538,
      "learning_rate": 2.955118391177425e-05,
      "loss": 2.7008,
      "step": 631700
    },
    {
      "epoch": 204.79740680713127,
      "grad_norm": 1.1164888143539429,
      "learning_rate": 2.9547940317872202e-05,
      "loss": 2.7002,
      "step": 631800
    },
    {
      "epoch": 204.82982171799028,
      "grad_norm": 1.3069859743118286,
      "learning_rate": 2.954469672397016e-05,
      "loss": 2.7041,
      "step": 631900
    },
    {
      "epoch": 204.86223662884927,
      "grad_norm": 1.2290995121002197,
      "learning_rate": 2.954145313006812e-05,
      "loss": 2.694,
      "step": 632000
    },
    {
      "epoch": 204.89465153970826,
      "grad_norm": 1.37373948097229,
      "learning_rate": 2.953820953616607e-05,
      "loss": 2.6962,
      "step": 632100
    },
    {
      "epoch": 204.92706645056725,
      "grad_norm": 1.1515440940856934,
      "learning_rate": 2.953496594226403e-05,
      "loss": 2.7069,
      "step": 632200
    },
    {
      "epoch": 204.95948136142624,
      "grad_norm": 1.2061543464660645,
      "learning_rate": 2.9531722348361985e-05,
      "loss": 2.6963,
      "step": 632300
    },
    {
      "epoch": 204.99189627228526,
      "grad_norm": 1.0710676908493042,
      "learning_rate": 2.9528478754459944e-05,
      "loss": 2.7127,
      "step": 632400
    },
    {
      "epoch": 205.0,
      "eval_bleu": 1.106421485354993,
      "eval_loss": 4.0245819091796875,
      "eval_runtime": 4.429,
      "eval_samples_per_second": 111.087,
      "eval_steps_per_second": 1.806,
      "step": 632425
    },
    {
      "epoch": 205.02431118314425,
      "grad_norm": 1.1344667673110962,
      "learning_rate": 2.9525235160557903e-05,
      "loss": 2.6972,
      "step": 632500
    },
    {
      "epoch": 205.05672609400324,
      "grad_norm": 1.371339201927185,
      "learning_rate": 2.9521991566655855e-05,
      "loss": 2.6906,
      "step": 632600
    },
    {
      "epoch": 205.08914100486223,
      "grad_norm": 1.0981379747390747,
      "learning_rate": 2.9518747972753814e-05,
      "loss": 2.6823,
      "step": 632700
    },
    {
      "epoch": 205.12155591572125,
      "grad_norm": 1.218930721282959,
      "learning_rate": 2.9515504378851772e-05,
      "loss": 2.7023,
      "step": 632800
    },
    {
      "epoch": 205.15397082658023,
      "grad_norm": 1.177703857421875,
      "learning_rate": 2.9512293220888747e-05,
      "loss": 2.6998,
      "step": 632900
    },
    {
      "epoch": 205.18638573743922,
      "grad_norm": 1.219401240348816,
      "learning_rate": 2.9509049626986702e-05,
      "loss": 2.6929,
      "step": 633000
    },
    {
      "epoch": 205.2188006482982,
      "grad_norm": 1.0749963521957397,
      "learning_rate": 2.950580603308466e-05,
      "loss": 2.6901,
      "step": 633100
    },
    {
      "epoch": 205.2512155591572,
      "grad_norm": 1.234944224357605,
      "learning_rate": 2.950256243918262e-05,
      "loss": 2.7038,
      "step": 633200
    },
    {
      "epoch": 205.28363047001622,
      "grad_norm": 1.4081374406814575,
      "learning_rate": 2.949931884528057e-05,
      "loss": 2.6855,
      "step": 633300
    },
    {
      "epoch": 205.3160453808752,
      "grad_norm": 1.4178639650344849,
      "learning_rate": 2.949607525137853e-05,
      "loss": 2.6882,
      "step": 633400
    },
    {
      "epoch": 205.3484602917342,
      "grad_norm": 1.2142133712768555,
      "learning_rate": 2.9492831657476482e-05,
      "loss": 2.7033,
      "step": 633500
    },
    {
      "epoch": 205.3808752025932,
      "grad_norm": 1.4035704135894775,
      "learning_rate": 2.948958806357444e-05,
      "loss": 2.7008,
      "step": 633600
    },
    {
      "epoch": 205.41329011345218,
      "grad_norm": 1.1581096649169922,
      "learning_rate": 2.94863444696724e-05,
      "loss": 2.6908,
      "step": 633700
    },
    {
      "epoch": 205.4457050243112,
      "grad_norm": 1.1744602918624878,
      "learning_rate": 2.948310087577035e-05,
      "loss": 2.7109,
      "step": 633800
    },
    {
      "epoch": 205.47811993517018,
      "grad_norm": 1.309217929840088,
      "learning_rate": 2.947985728186831e-05,
      "loss": 2.6972,
      "step": 633900
    },
    {
      "epoch": 205.51053484602917,
      "grad_norm": 1.311964511871338,
      "learning_rate": 2.947661368796627e-05,
      "loss": 2.7027,
      "step": 634000
    },
    {
      "epoch": 205.54294975688816,
      "grad_norm": 1.010765790939331,
      "learning_rate": 2.9473370094064224e-05,
      "loss": 2.68,
      "step": 634100
    },
    {
      "epoch": 205.57536466774715,
      "grad_norm": 1.1988706588745117,
      "learning_rate": 2.9470126500162183e-05,
      "loss": 2.6743,
      "step": 634200
    },
    {
      "epoch": 205.60777957860617,
      "grad_norm": 1.2477432489395142,
      "learning_rate": 2.9466915342199158e-05,
      "loss": 2.7155,
      "step": 634300
    },
    {
      "epoch": 205.64019448946516,
      "grad_norm": 1.1130822896957397,
      "learning_rate": 2.9463671748297116e-05,
      "loss": 2.6875,
      "step": 634400
    },
    {
      "epoch": 205.67260940032415,
      "grad_norm": 1.2391867637634277,
      "learning_rate": 2.946042815439507e-05,
      "loss": 2.7013,
      "step": 634500
    },
    {
      "epoch": 205.70502431118314,
      "grad_norm": 1.2346864938735962,
      "learning_rate": 2.9457184560493027e-05,
      "loss": 2.6774,
      "step": 634600
    },
    {
      "epoch": 205.73743922204213,
      "grad_norm": 1.5131903886795044,
      "learning_rate": 2.9453940966590982e-05,
      "loss": 2.6904,
      "step": 634700
    },
    {
      "epoch": 205.76985413290114,
      "grad_norm": 1.2928545475006104,
      "learning_rate": 2.945069737268894e-05,
      "loss": 2.7111,
      "step": 634800
    },
    {
      "epoch": 205.80226904376013,
      "grad_norm": 1.0852699279785156,
      "learning_rate": 2.94474537787869e-05,
      "loss": 2.6827,
      "step": 634900
    },
    {
      "epoch": 205.83468395461912,
      "grad_norm": 1.1211235523223877,
      "learning_rate": 2.9444210184884852e-05,
      "loss": 2.7019,
      "step": 635000
    },
    {
      "epoch": 205.8670988654781,
      "grad_norm": 1.1396254301071167,
      "learning_rate": 2.944096659098281e-05,
      "loss": 2.7008,
      "step": 635100
    },
    {
      "epoch": 205.8995137763371,
      "grad_norm": 1.2351957559585571,
      "learning_rate": 2.943772299708077e-05,
      "loss": 2.681,
      "step": 635200
    },
    {
      "epoch": 205.93192868719612,
      "grad_norm": 1.261837124824524,
      "learning_rate": 2.943447940317872e-05,
      "loss": 2.6975,
      "step": 635300
    },
    {
      "epoch": 205.9643435980551,
      "grad_norm": 1.0943905115127563,
      "learning_rate": 2.943123580927668e-05,
      "loss": 2.6891,
      "step": 635400
    },
    {
      "epoch": 205.9967585089141,
      "grad_norm": 1.146937608718872,
      "learning_rate": 2.942799221537464e-05,
      "loss": 2.6826,
      "step": 635500
    },
    {
      "epoch": 206.0,
      "eval_bleu": 1.2352459483111322,
      "eval_loss": 4.031408309936523,
      "eval_runtime": 3.9852,
      "eval_samples_per_second": 123.458,
      "eval_steps_per_second": 2.007,
      "step": 635510
    },
    {
      "epoch": 206.0291734197731,
      "grad_norm": 1.1982307434082031,
      "learning_rate": 2.942474862147259e-05,
      "loss": 2.684,
      "step": 635600
    },
    {
      "epoch": 206.06158833063208,
      "grad_norm": 1.3150686025619507,
      "learning_rate": 2.942150502757055e-05,
      "loss": 2.7038,
      "step": 635700
    },
    {
      "epoch": 206.0940032414911,
      "grad_norm": 1.248144507408142,
      "learning_rate": 2.9418261433668505e-05,
      "loss": 2.678,
      "step": 635800
    },
    {
      "epoch": 206.12641815235008,
      "grad_norm": 1.116672396659851,
      "learning_rate": 2.9415017839766464e-05,
      "loss": 2.6923,
      "step": 635900
    },
    {
      "epoch": 206.15883306320907,
      "grad_norm": 1.1396632194519043,
      "learning_rate": 2.9411774245864422e-05,
      "loss": 2.6704,
      "step": 636000
    },
    {
      "epoch": 206.19124797406806,
      "grad_norm": 1.2235746383666992,
      "learning_rate": 2.9408530651962374e-05,
      "loss": 2.7103,
      "step": 636100
    },
    {
      "epoch": 206.22366288492708,
      "grad_norm": 1.0589268207550049,
      "learning_rate": 2.9405287058060333e-05,
      "loss": 2.6898,
      "step": 636200
    },
    {
      "epoch": 206.25607779578607,
      "grad_norm": 1.2550663948059082,
      "learning_rate": 2.9402043464158292e-05,
      "loss": 2.6711,
      "step": 636300
    },
    {
      "epoch": 206.28849270664506,
      "grad_norm": 1.2909141778945923,
      "learning_rate": 2.9398799870256244e-05,
      "loss": 2.674,
      "step": 636400
    },
    {
      "epoch": 206.32090761750405,
      "grad_norm": 1.1489770412445068,
      "learning_rate": 2.9395556276354203e-05,
      "loss": 2.6873,
      "step": 636500
    },
    {
      "epoch": 206.35332252836304,
      "grad_norm": 1.1404097080230713,
      "learning_rate": 2.939231268245216e-05,
      "loss": 2.696,
      "step": 636600
    },
    {
      "epoch": 206.38573743922205,
      "grad_norm": 1.1538523435592651,
      "learning_rate": 2.9389069088550113e-05,
      "loss": 2.6783,
      "step": 636700
    },
    {
      "epoch": 206.41815235008104,
      "grad_norm": 1.1639434099197388,
      "learning_rate": 2.9385825494648072e-05,
      "loss": 2.7092,
      "step": 636800
    },
    {
      "epoch": 206.45056726094003,
      "grad_norm": 1.164228081703186,
      "learning_rate": 2.9382581900746027e-05,
      "loss": 2.6992,
      "step": 636900
    },
    {
      "epoch": 206.48298217179902,
      "grad_norm": 1.1386094093322754,
      "learning_rate": 2.9379338306843983e-05,
      "loss": 2.701,
      "step": 637000
    },
    {
      "epoch": 206.515397082658,
      "grad_norm": 1.289670705795288,
      "learning_rate": 2.937609471294194e-05,
      "loss": 2.6814,
      "step": 637100
    },
    {
      "epoch": 206.54781199351703,
      "grad_norm": 1.1367040872573853,
      "learning_rate": 2.9372851119039897e-05,
      "loss": 2.6968,
      "step": 637200
    },
    {
      "epoch": 206.58022690437602,
      "grad_norm": 1.3790034055709839,
      "learning_rate": 2.936963996107687e-05,
      "loss": 2.6901,
      "step": 637300
    },
    {
      "epoch": 206.612641815235,
      "grad_norm": 1.247444748878479,
      "learning_rate": 2.936639636717483e-05,
      "loss": 2.682,
      "step": 637400
    },
    {
      "epoch": 206.645056726094,
      "grad_norm": 1.2557405233383179,
      "learning_rate": 2.936315277327279e-05,
      "loss": 2.7056,
      "step": 637500
    },
    {
      "epoch": 206.677471636953,
      "grad_norm": 1.2697771787643433,
      "learning_rate": 2.9359909179370744e-05,
      "loss": 2.6907,
      "step": 637600
    },
    {
      "epoch": 206.709886547812,
      "grad_norm": 1.2457114458084106,
      "learning_rate": 2.9356665585468703e-05,
      "loss": 2.7027,
      "step": 637700
    },
    {
      "epoch": 206.742301458671,
      "grad_norm": 1.2459497451782227,
      "learning_rate": 2.9353421991566658e-05,
      "loss": 2.7006,
      "step": 637800
    },
    {
      "epoch": 206.77471636952998,
      "grad_norm": 1.221893310546875,
      "learning_rate": 2.9350178397664613e-05,
      "loss": 2.6774,
      "step": 637900
    },
    {
      "epoch": 206.80713128038897,
      "grad_norm": 1.2251219749450684,
      "learning_rate": 2.9346934803762572e-05,
      "loss": 2.7104,
      "step": 638000
    },
    {
      "epoch": 206.83954619124796,
      "grad_norm": 1.3519659042358398,
      "learning_rate": 2.9343691209860524e-05,
      "loss": 2.6795,
      "step": 638100
    },
    {
      "epoch": 206.87196110210698,
      "grad_norm": 1.2687559127807617,
      "learning_rate": 2.9340447615958483e-05,
      "loss": 2.7112,
      "step": 638200
    },
    {
      "epoch": 206.90437601296597,
      "grad_norm": 1.3102471828460693,
      "learning_rate": 2.933720402205644e-05,
      "loss": 2.6991,
      "step": 638300
    },
    {
      "epoch": 206.93679092382496,
      "grad_norm": 1.5348345041275024,
      "learning_rate": 2.9333960428154394e-05,
      "loss": 2.6875,
      "step": 638400
    },
    {
      "epoch": 206.96920583468395,
      "grad_norm": 1.241515040397644,
      "learning_rate": 2.9330716834252352e-05,
      "loss": 2.7065,
      "step": 638500
    },
    {
      "epoch": 207.0,
      "eval_bleu": 1.1263352151643078,
      "eval_loss": 4.029929161071777,
      "eval_runtime": 3.8511,
      "eval_samples_per_second": 127.757,
      "eval_steps_per_second": 2.077,
      "step": 638595
    },
    {
      "epoch": 207.00162074554294,
      "grad_norm": 1.1260954141616821,
      "learning_rate": 2.932747324035031e-05,
      "loss": 2.711,
      "step": 638600
    },
    {
      "epoch": 207.03403565640195,
      "grad_norm": 1.4109861850738525,
      "learning_rate": 2.9324229646448263e-05,
      "loss": 2.6778,
      "step": 638700
    },
    {
      "epoch": 207.06645056726094,
      "grad_norm": 1.359645128250122,
      "learning_rate": 2.9320986052546222e-05,
      "loss": 2.68,
      "step": 638800
    },
    {
      "epoch": 207.09886547811993,
      "grad_norm": 1.5246779918670654,
      "learning_rate": 2.931774245864418e-05,
      "loss": 2.6715,
      "step": 638900
    },
    {
      "epoch": 207.13128038897892,
      "grad_norm": 1.3080986738204956,
      "learning_rate": 2.9314498864742136e-05,
      "loss": 2.6929,
      "step": 639000
    },
    {
      "epoch": 207.1636952998379,
      "grad_norm": 1.2061017751693726,
      "learning_rate": 2.9311255270840095e-05,
      "loss": 2.6808,
      "step": 639100
    },
    {
      "epoch": 207.19611021069693,
      "grad_norm": 1.302847146987915,
      "learning_rate": 2.9308011676938047e-05,
      "loss": 2.6858,
      "step": 639200
    },
    {
      "epoch": 207.22852512155592,
      "grad_norm": 1.3027526140213013,
      "learning_rate": 2.9304768083036005e-05,
      "loss": 2.7273,
      "step": 639300
    },
    {
      "epoch": 207.2609400324149,
      "grad_norm": 1.3525947332382202,
      "learning_rate": 2.9301524489133964e-05,
      "loss": 2.691,
      "step": 639400
    },
    {
      "epoch": 207.2933549432739,
      "grad_norm": 1.067476511001587,
      "learning_rate": 2.9298280895231916e-05,
      "loss": 2.6974,
      "step": 639500
    },
    {
      "epoch": 207.32576985413291,
      "grad_norm": 1.4393746852874756,
      "learning_rate": 2.9295037301329875e-05,
      "loss": 2.6751,
      "step": 639600
    },
    {
      "epoch": 207.3581847649919,
      "grad_norm": 1.2802014350891113,
      "learning_rate": 2.9291793707427834e-05,
      "loss": 2.6956,
      "step": 639700
    },
    {
      "epoch": 207.3905996758509,
      "grad_norm": 1.2790406942367554,
      "learning_rate": 2.9288550113525786e-05,
      "loss": 2.6833,
      "step": 639800
    },
    {
      "epoch": 207.42301458670988,
      "grad_norm": 1.239383339881897,
      "learning_rate": 2.9285306519623744e-05,
      "loss": 2.6996,
      "step": 639900
    },
    {
      "epoch": 207.45542949756887,
      "grad_norm": 1.1650768518447876,
      "learning_rate": 2.92820629257217e-05,
      "loss": 2.7104,
      "step": 640000
    },
    {
      "epoch": 207.4878444084279,
      "grad_norm": 1.5402491092681885,
      "learning_rate": 2.927881933181966e-05,
      "loss": 2.7022,
      "step": 640100
    },
    {
      "epoch": 207.52025931928688,
      "grad_norm": 1.2031127214431763,
      "learning_rate": 2.9275575737917614e-05,
      "loss": 2.7154,
      "step": 640200
    },
    {
      "epoch": 207.55267423014587,
      "grad_norm": 1.102184534072876,
      "learning_rate": 2.927236457995459e-05,
      "loss": 2.6832,
      "step": 640300
    },
    {
      "epoch": 207.58508914100486,
      "grad_norm": 1.1757845878601074,
      "learning_rate": 2.9269120986052544e-05,
      "loss": 2.6845,
      "step": 640400
    },
    {
      "epoch": 207.61750405186385,
      "grad_norm": 1.313434362411499,
      "learning_rate": 2.9265877392150502e-05,
      "loss": 2.7003,
      "step": 640500
    },
    {
      "epoch": 207.64991896272286,
      "grad_norm": 1.1599137783050537,
      "learning_rate": 2.926263379824846e-05,
      "loss": 2.686,
      "step": 640600
    },
    {
      "epoch": 207.68233387358185,
      "grad_norm": 1.3991823196411133,
      "learning_rate": 2.9259390204346416e-05,
      "loss": 2.6886,
      "step": 640700
    },
    {
      "epoch": 207.71474878444084,
      "grad_norm": 1.2627981901168823,
      "learning_rate": 2.9256146610444375e-05,
      "loss": 2.6849,
      "step": 640800
    },
    {
      "epoch": 207.74716369529983,
      "grad_norm": 1.3242132663726807,
      "learning_rate": 2.9252903016542334e-05,
      "loss": 2.6786,
      "step": 640900
    },
    {
      "epoch": 207.77957860615882,
      "grad_norm": 1.2652169466018677,
      "learning_rate": 2.9249659422640286e-05,
      "loss": 2.7115,
      "step": 641000
    },
    {
      "epoch": 207.81199351701784,
      "grad_norm": 1.1362015008926392,
      "learning_rate": 2.9246415828738245e-05,
      "loss": 2.6788,
      "step": 641100
    },
    {
      "epoch": 207.84440842787683,
      "grad_norm": 1.1670286655426025,
      "learning_rate": 2.9243172234836203e-05,
      "loss": 2.6817,
      "step": 641200
    },
    {
      "epoch": 207.87682333873582,
      "grad_norm": 1.1766276359558105,
      "learning_rate": 2.9239928640934155e-05,
      "loss": 2.6892,
      "step": 641300
    },
    {
      "epoch": 207.9092382495948,
      "grad_norm": 1.282164454460144,
      "learning_rate": 2.9236685047032114e-05,
      "loss": 2.6922,
      "step": 641400
    },
    {
      "epoch": 207.9416531604538,
      "grad_norm": 1.224039077758789,
      "learning_rate": 2.9233441453130066e-05,
      "loss": 2.6761,
      "step": 641500
    },
    {
      "epoch": 207.97406807131281,
      "grad_norm": 1.3005818128585815,
      "learning_rate": 2.9230197859228025e-05,
      "loss": 2.6945,
      "step": 641600
    },
    {
      "epoch": 208.0,
      "eval_bleu": 1.1600353396384018,
      "eval_loss": 4.030982971191406,
      "eval_runtime": 3.8676,
      "eval_samples_per_second": 127.211,
      "eval_steps_per_second": 2.068,
      "step": 641680
    },
    {
      "epoch": 208.0064829821718,
      "grad_norm": 1.3617068529129028,
      "learning_rate": 2.9226954265325983e-05,
      "loss": 2.6888,
      "step": 641700
    },
    {
      "epoch": 208.0388978930308,
      "grad_norm": 1.2575359344482422,
      "learning_rate": 2.922371067142394e-05,
      "loss": 2.6886,
      "step": 641800
    },
    {
      "epoch": 208.07131280388978,
      "grad_norm": 1.120195984840393,
      "learning_rate": 2.9220467077521894e-05,
      "loss": 2.6769,
      "step": 641900
    },
    {
      "epoch": 208.10372771474877,
      "grad_norm": 1.2868634462356567,
      "learning_rate": 2.9217223483619853e-05,
      "loss": 2.6847,
      "step": 642000
    },
    {
      "epoch": 208.1361426256078,
      "grad_norm": 1.065274953842163,
      "learning_rate": 2.9213979889717808e-05,
      "loss": 2.6773,
      "step": 642100
    },
    {
      "epoch": 208.16855753646678,
      "grad_norm": 1.2846413850784302,
      "learning_rate": 2.9210736295815767e-05,
      "loss": 2.7115,
      "step": 642200
    },
    {
      "epoch": 208.20097244732577,
      "grad_norm": 1.0917108058929443,
      "learning_rate": 2.9207492701913726e-05,
      "loss": 2.6908,
      "step": 642300
    },
    {
      "epoch": 208.23338735818476,
      "grad_norm": 1.4382914304733276,
      "learning_rate": 2.9204249108011678e-05,
      "loss": 2.676,
      "step": 642400
    },
    {
      "epoch": 208.26580226904375,
      "grad_norm": 1.172713041305542,
      "learning_rate": 2.9201005514109636e-05,
      "loss": 2.694,
      "step": 642500
    },
    {
      "epoch": 208.29821717990276,
      "grad_norm": 1.2438123226165771,
      "learning_rate": 2.919776192020759e-05,
      "loss": 2.6851,
      "step": 642600
    },
    {
      "epoch": 208.33063209076175,
      "grad_norm": 1.286476492881775,
      "learning_rate": 2.9194518326305547e-05,
      "loss": 2.6758,
      "step": 642700
    },
    {
      "epoch": 208.36304700162074,
      "grad_norm": 1.2831292152404785,
      "learning_rate": 2.9191274732403506e-05,
      "loss": 2.6884,
      "step": 642800
    },
    {
      "epoch": 208.39546191247973,
      "grad_norm": 1.4341564178466797,
      "learning_rate": 2.9188031138501458e-05,
      "loss": 2.6952,
      "step": 642900
    },
    {
      "epoch": 208.42787682333875,
      "grad_norm": 1.262731671333313,
      "learning_rate": 2.9184787544599417e-05,
      "loss": 2.6848,
      "step": 643000
    },
    {
      "epoch": 208.46029173419774,
      "grad_norm": 1.289196252822876,
      "learning_rate": 2.9181543950697375e-05,
      "loss": 2.685,
      "step": 643100
    },
    {
      "epoch": 208.49270664505673,
      "grad_norm": 1.1716687679290771,
      "learning_rate": 2.917830035679533e-05,
      "loss": 2.6867,
      "step": 643200
    },
    {
      "epoch": 208.52512155591572,
      "grad_norm": 1.4046342372894287,
      "learning_rate": 2.917505676289329e-05,
      "loss": 2.6903,
      "step": 643300
    },
    {
      "epoch": 208.5575364667747,
      "grad_norm": 1.1666733026504517,
      "learning_rate": 2.917181316899124e-05,
      "loss": 2.7042,
      "step": 643400
    },
    {
      "epoch": 208.58995137763372,
      "grad_norm": 1.4284799098968506,
      "learning_rate": 2.91685695750892e-05,
      "loss": 2.6782,
      "step": 643500
    },
    {
      "epoch": 208.6223662884927,
      "grad_norm": 1.3660714626312256,
      "learning_rate": 2.916532598118716e-05,
      "loss": 2.6858,
      "step": 643600
    },
    {
      "epoch": 208.6547811993517,
      "grad_norm": 1.1715710163116455,
      "learning_rate": 2.916208238728511e-05,
      "loss": 2.6824,
      "step": 643700
    },
    {
      "epoch": 208.6871961102107,
      "grad_norm": 1.210687518119812,
      "learning_rate": 2.915883879338307e-05,
      "loss": 2.6842,
      "step": 643800
    },
    {
      "epoch": 208.71961102106968,
      "grad_norm": 1.2216521501541138,
      "learning_rate": 2.915559519948103e-05,
      "loss": 2.6858,
      "step": 643900
    },
    {
      "epoch": 208.7520259319287,
      "grad_norm": 1.3467038869857788,
      "learning_rate": 2.915235160557898e-05,
      "loss": 2.7059,
      "step": 644000
    },
    {
      "epoch": 208.7844408427877,
      "grad_norm": 1.219467043876648,
      "learning_rate": 2.914910801167694e-05,
      "loss": 2.682,
      "step": 644100
    },
    {
      "epoch": 208.81685575364668,
      "grad_norm": 1.5041056871414185,
      "learning_rate": 2.9145864417774898e-05,
      "loss": 2.6825,
      "step": 644200
    },
    {
      "epoch": 208.84927066450567,
      "grad_norm": 1.1794366836547852,
      "learning_rate": 2.9142653259811876e-05,
      "loss": 2.7023,
      "step": 644300
    },
    {
      "epoch": 208.88168557536466,
      "grad_norm": 1.1113790273666382,
      "learning_rate": 2.9139409665909828e-05,
      "loss": 2.7,
      "step": 644400
    },
    {
      "epoch": 208.91410048622367,
      "grad_norm": 1.1774652004241943,
      "learning_rate": 2.9136166072007786e-05,
      "loss": 2.6988,
      "step": 644500
    },
    {
      "epoch": 208.94651539708266,
      "grad_norm": 1.4415068626403809,
      "learning_rate": 2.9132922478105745e-05,
      "loss": 2.6932,
      "step": 644600
    },
    {
      "epoch": 208.97893030794165,
      "grad_norm": 1.132120132446289,
      "learning_rate": 2.9129678884203697e-05,
      "loss": 2.6842,
      "step": 644700
    },
    {
      "epoch": 209.0,
      "eval_bleu": 0.9347565518734674,
      "eval_loss": 4.035119533538818,
      "eval_runtime": 3.9214,
      "eval_samples_per_second": 125.464,
      "eval_steps_per_second": 2.04,
      "step": 644765
    },
    {
      "epoch": 209.01134521880064,
      "grad_norm": 1.1458089351654053,
      "learning_rate": 2.9126435290301656e-05,
      "loss": 2.688,
      "step": 644800
    },
    {
      "epoch": 209.04376012965963,
      "grad_norm": 1.2825647592544556,
      "learning_rate": 2.912319169639961e-05,
      "loss": 2.6779,
      "step": 644900
    },
    {
      "epoch": 209.07617504051865,
      "grad_norm": 1.299936056137085,
      "learning_rate": 2.911994810249757e-05,
      "loss": 2.6775,
      "step": 645000
    },
    {
      "epoch": 209.10858995137764,
      "grad_norm": 1.3958286046981812,
      "learning_rate": 2.911670450859553e-05,
      "loss": 2.6687,
      "step": 645100
    },
    {
      "epoch": 209.14100486223663,
      "grad_norm": 1.2617695331573486,
      "learning_rate": 2.911346091469348e-05,
      "loss": 2.6878,
      "step": 645200
    },
    {
      "epoch": 209.17341977309562,
      "grad_norm": 1.4162096977233887,
      "learning_rate": 2.911021732079144e-05,
      "loss": 2.6761,
      "step": 645300
    },
    {
      "epoch": 209.2058346839546,
      "grad_norm": 1.1761081218719482,
      "learning_rate": 2.9106973726889398e-05,
      "loss": 2.6766,
      "step": 645400
    },
    {
      "epoch": 209.23824959481362,
      "grad_norm": 1.5949760675430298,
      "learning_rate": 2.910373013298735e-05,
      "loss": 2.6871,
      "step": 645500
    },
    {
      "epoch": 209.2706645056726,
      "grad_norm": 1.1593446731567383,
      "learning_rate": 2.910048653908531e-05,
      "loss": 2.6833,
      "step": 645600
    },
    {
      "epoch": 209.3030794165316,
      "grad_norm": 1.078765630722046,
      "learning_rate": 2.909724294518326e-05,
      "loss": 2.6732,
      "step": 645700
    },
    {
      "epoch": 209.3354943273906,
      "grad_norm": 1.300870656967163,
      "learning_rate": 2.909399935128122e-05,
      "loss": 2.689,
      "step": 645800
    },
    {
      "epoch": 209.36790923824958,
      "grad_norm": 1.1864073276519775,
      "learning_rate": 2.9090755757379178e-05,
      "loss": 2.6777,
      "step": 645900
    },
    {
      "epoch": 209.4003241491086,
      "grad_norm": 1.1120970249176025,
      "learning_rate": 2.9087512163477134e-05,
      "loss": 2.6759,
      "step": 646000
    },
    {
      "epoch": 209.4327390599676,
      "grad_norm": 1.409530758857727,
      "learning_rate": 2.908426856957509e-05,
      "loss": 2.6908,
      "step": 646100
    },
    {
      "epoch": 209.46515397082658,
      "grad_norm": 1.2841476202011108,
      "learning_rate": 2.9081024975673048e-05,
      "loss": 2.6665,
      "step": 646200
    },
    {
      "epoch": 209.49756888168557,
      "grad_norm": 1.2967535257339478,
      "learning_rate": 2.9077813817710025e-05,
      "loss": 2.696,
      "step": 646300
    },
    {
      "epoch": 209.52998379254458,
      "grad_norm": 1.2755228281021118,
      "learning_rate": 2.9074570223807977e-05,
      "loss": 2.7016,
      "step": 646400
    },
    {
      "epoch": 209.56239870340357,
      "grad_norm": 1.3398727178573608,
      "learning_rate": 2.9071326629905936e-05,
      "loss": 2.6805,
      "step": 646500
    },
    {
      "epoch": 209.59481361426256,
      "grad_norm": 1.074207067489624,
      "learning_rate": 2.9068083036003895e-05,
      "loss": 2.7043,
      "step": 646600
    },
    {
      "epoch": 209.62722852512155,
      "grad_norm": 1.2143852710723877,
      "learning_rate": 2.906483944210185e-05,
      "loss": 2.6859,
      "step": 646700
    },
    {
      "epoch": 209.65964343598054,
      "grad_norm": 1.1848706007003784,
      "learning_rate": 2.9061628284138825e-05,
      "loss": 2.685,
      "step": 646800
    },
    {
      "epoch": 209.69205834683956,
      "grad_norm": 1.2182270288467407,
      "learning_rate": 2.9058384690236783e-05,
      "loss": 2.7029,
      "step": 646900
    },
    {
      "epoch": 209.72447325769855,
      "grad_norm": 1.3473191261291504,
      "learning_rate": 2.9055141096334742e-05,
      "loss": 2.695,
      "step": 647000
    },
    {
      "epoch": 209.75688816855754,
      "grad_norm": 1.0754352807998657,
      "learning_rate": 2.9051897502432694e-05,
      "loss": 2.7073,
      "step": 647100
    },
    {
      "epoch": 209.78930307941653,
      "grad_norm": 1.216836929321289,
      "learning_rate": 2.9048653908530653e-05,
      "loss": 2.6786,
      "step": 647200
    },
    {
      "epoch": 209.82171799027552,
      "grad_norm": 1.3642454147338867,
      "learning_rate": 2.9045410314628608e-05,
      "loss": 2.6979,
      "step": 647300
    },
    {
      "epoch": 209.85413290113453,
      "grad_norm": 1.2104774713516235,
      "learning_rate": 2.9042166720726567e-05,
      "loss": 2.6921,
      "step": 647400
    },
    {
      "epoch": 209.88654781199352,
      "grad_norm": 1.2461707592010498,
      "learning_rate": 2.9038923126824526e-05,
      "loss": 2.67,
      "step": 647500
    },
    {
      "epoch": 209.9189627228525,
      "grad_norm": 1.2107661962509155,
      "learning_rate": 2.9035679532922478e-05,
      "loss": 2.6898,
      "step": 647600
    },
    {
      "epoch": 209.9513776337115,
      "grad_norm": 1.0768485069274902,
      "learning_rate": 2.9032435939020436e-05,
      "loss": 2.6913,
      "step": 647700
    },
    {
      "epoch": 209.9837925445705,
      "grad_norm": 1.2397420406341553,
      "learning_rate": 2.9029192345118395e-05,
      "loss": 2.7165,
      "step": 647800
    },
    {
      "epoch": 210.0,
      "eval_bleu": 1.0219798308118992,
      "eval_loss": 4.034186363220215,
      "eval_runtime": 3.9231,
      "eval_samples_per_second": 125.41,
      "eval_steps_per_second": 2.039,
      "step": 647850
    },
    {
      "epoch": 210.0162074554295,
      "grad_norm": 1.2425942420959473,
      "learning_rate": 2.9025948751216347e-05,
      "loss": 2.7075,
      "step": 647900
    },
    {
      "epoch": 210.0486223662885,
      "grad_norm": 1.2588714361190796,
      "learning_rate": 2.9022705157314306e-05,
      "loss": 2.6869,
      "step": 648000
    },
    {
      "epoch": 210.0810372771475,
      "grad_norm": 1.3619905710220337,
      "learning_rate": 2.9019461563412265e-05,
      "loss": 2.6766,
      "step": 648100
    },
    {
      "epoch": 210.11345218800648,
      "grad_norm": 1.379606008529663,
      "learning_rate": 2.9016217969510217e-05,
      "loss": 2.6703,
      "step": 648200
    },
    {
      "epoch": 210.14586709886547,
      "grad_norm": 1.168068766593933,
      "learning_rate": 2.9012974375608175e-05,
      "loss": 2.6722,
      "step": 648300
    },
    {
      "epoch": 210.17828200972448,
      "grad_norm": 1.145612120628357,
      "learning_rate": 2.900973078170613e-05,
      "loss": 2.6737,
      "step": 648400
    },
    {
      "epoch": 210.21069692058347,
      "grad_norm": 1.1936033964157104,
      "learning_rate": 2.900648718780409e-05,
      "loss": 2.6679,
      "step": 648500
    },
    {
      "epoch": 210.24311183144246,
      "grad_norm": 1.2740117311477661,
      "learning_rate": 2.9003243593902045e-05,
      "loss": 2.68,
      "step": 648600
    },
    {
      "epoch": 210.27552674230145,
      "grad_norm": 1.3191801309585571,
      "learning_rate": 2.9e-05,
      "loss": 2.6669,
      "step": 648700
    },
    {
      "epoch": 210.30794165316044,
      "grad_norm": 1.406281590461731,
      "learning_rate": 2.899675640609796e-05,
      "loss": 2.6989,
      "step": 648800
    },
    {
      "epoch": 210.34035656401946,
      "grad_norm": 1.262739896774292,
      "learning_rate": 2.8993512812195918e-05,
      "loss": 2.6919,
      "step": 648900
    },
    {
      "epoch": 210.37277147487845,
      "grad_norm": 1.2319700717926025,
      "learning_rate": 2.899026921829387e-05,
      "loss": 2.7013,
      "step": 649000
    },
    {
      "epoch": 210.40518638573744,
      "grad_norm": 1.1814861297607422,
      "learning_rate": 2.898702562439183e-05,
      "loss": 2.684,
      "step": 649100
    },
    {
      "epoch": 210.43760129659643,
      "grad_norm": 1.2699326276779175,
      "learning_rate": 2.8983782030489787e-05,
      "loss": 2.6731,
      "step": 649200
    },
    {
      "epoch": 210.47001620745542,
      "grad_norm": 1.1994872093200684,
      "learning_rate": 2.898053843658774e-05,
      "loss": 2.6862,
      "step": 649300
    },
    {
      "epoch": 210.50243111831443,
      "grad_norm": 1.25969398021698,
      "learning_rate": 2.8977294842685698e-05,
      "loss": 2.679,
      "step": 649400
    },
    {
      "epoch": 210.53484602917342,
      "grad_norm": 1.1908605098724365,
      "learning_rate": 2.897405124878365e-05,
      "loss": 2.6768,
      "step": 649500
    },
    {
      "epoch": 210.5672609400324,
      "grad_norm": 1.3214644193649292,
      "learning_rate": 2.897080765488161e-05,
      "loss": 2.7011,
      "step": 649600
    },
    {
      "epoch": 210.5996758508914,
      "grad_norm": 1.3846068382263184,
      "learning_rate": 2.8967564060979567e-05,
      "loss": 2.6961,
      "step": 649700
    },
    {
      "epoch": 210.63209076175042,
      "grad_norm": 1.2638459205627441,
      "learning_rate": 2.8964320467077523e-05,
      "loss": 2.7004,
      "step": 649800
    },
    {
      "epoch": 210.6645056726094,
      "grad_norm": 1.344581961631775,
      "learning_rate": 2.896107687317548e-05,
      "loss": 2.6961,
      "step": 649900
    },
    {
      "epoch": 210.6969205834684,
      "grad_norm": 1.1167887449264526,
      "learning_rate": 2.895783327927344e-05,
      "loss": 2.6796,
      "step": 650000
    },
    {
      "epoch": 210.7293354943274,
      "grad_norm": 1.1840205192565918,
      "learning_rate": 2.8954589685371392e-05,
      "loss": 2.6664,
      "step": 650100
    },
    {
      "epoch": 210.76175040518638,
      "grad_norm": 1.194032073020935,
      "learning_rate": 2.895134609146935e-05,
      "loss": 2.6818,
      "step": 650200
    },
    {
      "epoch": 210.7941653160454,
      "grad_norm": 1.0467686653137207,
      "learning_rate": 2.8948102497567303e-05,
      "loss": 2.6779,
      "step": 650300
    },
    {
      "epoch": 210.82658022690438,
      "grad_norm": 1.3813507556915283,
      "learning_rate": 2.894485890366526e-05,
      "loss": 2.7001,
      "step": 650400
    },
    {
      "epoch": 210.85899513776337,
      "grad_norm": 1.2296816110610962,
      "learning_rate": 2.894161530976322e-05,
      "loss": 2.6954,
      "step": 650500
    },
    {
      "epoch": 210.89141004862236,
      "grad_norm": 1.3472602367401123,
      "learning_rate": 2.8938371715861172e-05,
      "loss": 2.6858,
      "step": 650600
    },
    {
      "epoch": 210.92382495948135,
      "grad_norm": 1.2155163288116455,
      "learning_rate": 2.893512812195913e-05,
      "loss": 2.6919,
      "step": 650700
    },
    {
      "epoch": 210.95623987034037,
      "grad_norm": 1.0993969440460205,
      "learning_rate": 2.893191696399611e-05,
      "loss": 2.6735,
      "step": 650800
    },
    {
      "epoch": 210.98865478119936,
      "grad_norm": 1.2125065326690674,
      "learning_rate": 2.8928673370094068e-05,
      "loss": 2.7055,
      "step": 650900
    },
    {
      "epoch": 211.0,
      "eval_bleu": 1.0763209404378768,
      "eval_loss": 4.030750274658203,
      "eval_runtime": 3.6742,
      "eval_samples_per_second": 133.905,
      "eval_steps_per_second": 2.177,
      "step": 650935
    },
    {
      "epoch": 211.02106969205835,
      "grad_norm": 1.253189206123352,
      "learning_rate": 2.892542977619202e-05,
      "loss": 2.6906,
      "step": 651000
    },
    {
      "epoch": 211.05348460291734,
      "grad_norm": 1.1534249782562256,
      "learning_rate": 2.8922186182289978e-05,
      "loss": 2.6558,
      "step": 651100
    },
    {
      "epoch": 211.08589951377633,
      "grad_norm": 1.3788305521011353,
      "learning_rate": 2.8918942588387937e-05,
      "loss": 2.7144,
      "step": 651200
    },
    {
      "epoch": 211.11831442463534,
      "grad_norm": 1.1602323055267334,
      "learning_rate": 2.891569899448589e-05,
      "loss": 2.6807,
      "step": 651300
    },
    {
      "epoch": 211.15072933549433,
      "grad_norm": 1.2560688257217407,
      "learning_rate": 2.8912455400583848e-05,
      "loss": 2.6847,
      "step": 651400
    },
    {
      "epoch": 211.18314424635332,
      "grad_norm": 1.2826546430587769,
      "learning_rate": 2.8909211806681806e-05,
      "loss": 2.6755,
      "step": 651500
    },
    {
      "epoch": 211.2155591572123,
      "grad_norm": 1.2746423482894897,
      "learning_rate": 2.8906000648718784e-05,
      "loss": 2.6804,
      "step": 651600
    },
    {
      "epoch": 211.2479740680713,
      "grad_norm": 1.1656590700149536,
      "learning_rate": 2.8902757054816736e-05,
      "loss": 2.6867,
      "step": 651700
    },
    {
      "epoch": 211.28038897893032,
      "grad_norm": 1.2992959022521973,
      "learning_rate": 2.8899513460914695e-05,
      "loss": 2.6885,
      "step": 651800
    },
    {
      "epoch": 211.3128038897893,
      "grad_norm": 1.1702256202697754,
      "learning_rate": 2.8896269867012654e-05,
      "loss": 2.6789,
      "step": 651900
    },
    {
      "epoch": 211.3452188006483,
      "grad_norm": 1.2223560810089111,
      "learning_rate": 2.8893026273110606e-05,
      "loss": 2.6734,
      "step": 652000
    },
    {
      "epoch": 211.3776337115073,
      "grad_norm": 1.2029907703399658,
      "learning_rate": 2.8889782679208564e-05,
      "loss": 2.6737,
      "step": 652100
    },
    {
      "epoch": 211.41004862236628,
      "grad_norm": 1.248357892036438,
      "learning_rate": 2.888653908530652e-05,
      "loss": 2.6987,
      "step": 652200
    },
    {
      "epoch": 211.4424635332253,
      "grad_norm": 1.1914113759994507,
      "learning_rate": 2.888329549140448e-05,
      "loss": 2.6741,
      "step": 652300
    },
    {
      "epoch": 211.47487844408428,
      "grad_norm": 1.273126244544983,
      "learning_rate": 2.8880051897502437e-05,
      "loss": 2.674,
      "step": 652400
    },
    {
      "epoch": 211.50729335494327,
      "grad_norm": 1.1153360605239868,
      "learning_rate": 2.887680830360039e-05,
      "loss": 2.6656,
      "step": 652500
    },
    {
      "epoch": 211.53970826580226,
      "grad_norm": 1.155781626701355,
      "learning_rate": 2.8873564709698348e-05,
      "loss": 2.7096,
      "step": 652600
    },
    {
      "epoch": 211.57212317666125,
      "grad_norm": 1.271610140800476,
      "learning_rate": 2.8870321115796307e-05,
      "loss": 2.6812,
      "step": 652700
    },
    {
      "epoch": 211.60453808752027,
      "grad_norm": 1.371217131614685,
      "learning_rate": 2.886707752189426e-05,
      "loss": 2.6786,
      "step": 652800
    },
    {
      "epoch": 211.63695299837926,
      "grad_norm": 1.099146842956543,
      "learning_rate": 2.8863833927992217e-05,
      "loss": 2.6922,
      "step": 652900
    },
    {
      "epoch": 211.66936790923825,
      "grad_norm": 1.2294890880584717,
      "learning_rate": 2.886059033409017e-05,
      "loss": 2.6696,
      "step": 653000
    },
    {
      "epoch": 211.70178282009724,
      "grad_norm": 1.2036021947860718,
      "learning_rate": 2.8857346740188128e-05,
      "loss": 2.6811,
      "step": 653100
    },
    {
      "epoch": 211.73419773095625,
      "grad_norm": 1.426344871520996,
      "learning_rate": 2.8854103146286087e-05,
      "loss": 2.691,
      "step": 653200
    },
    {
      "epoch": 211.76661264181524,
      "grad_norm": 1.4865323305130005,
      "learning_rate": 2.8850859552384042e-05,
      "loss": 2.6858,
      "step": 653300
    },
    {
      "epoch": 211.79902755267423,
      "grad_norm": 1.3669471740722656,
      "learning_rate": 2.8847615958482e-05,
      "loss": 2.6744,
      "step": 653400
    },
    {
      "epoch": 211.83144246353322,
      "grad_norm": 1.2340266704559326,
      "learning_rate": 2.8844372364579956e-05,
      "loss": 2.6875,
      "step": 653500
    },
    {
      "epoch": 211.8638573743922,
      "grad_norm": 1.2044371366500854,
      "learning_rate": 2.884112877067791e-05,
      "loss": 2.6823,
      "step": 653600
    },
    {
      "epoch": 211.89627228525123,
      "grad_norm": 1.082375407218933,
      "learning_rate": 2.883788517677587e-05,
      "loss": 2.686,
      "step": 653700
    },
    {
      "epoch": 211.92868719611022,
      "grad_norm": 1.216558575630188,
      "learning_rate": 2.883464158287383e-05,
      "loss": 2.6864,
      "step": 653800
    },
    {
      "epoch": 211.9611021069692,
      "grad_norm": 1.2799787521362305,
      "learning_rate": 2.883139798897178e-05,
      "loss": 2.6985,
      "step": 653900
    },
    {
      "epoch": 211.9935170178282,
      "grad_norm": 1.158552885055542,
      "learning_rate": 2.882815439506974e-05,
      "loss": 2.6744,
      "step": 654000
    },
    {
      "epoch": 212.0,
      "eval_bleu": 1.1033191635125077,
      "eval_loss": 4.043333053588867,
      "eval_runtime": 3.9264,
      "eval_samples_per_second": 125.307,
      "eval_steps_per_second": 2.038,
      "step": 654020
    },
    {
      "epoch": 212.02593192868719,
      "grad_norm": 1.2246705293655396,
      "learning_rate": 2.8824910801167692e-05,
      "loss": 2.6849,
      "step": 654100
    },
    {
      "epoch": 212.0583468395462,
      "grad_norm": 1.3768736124038696,
      "learning_rate": 2.882166720726565e-05,
      "loss": 2.6806,
      "step": 654200
    },
    {
      "epoch": 212.0907617504052,
      "grad_norm": 1.3440120220184326,
      "learning_rate": 2.881842361336361e-05,
      "loss": 2.6521,
      "step": 654300
    },
    {
      "epoch": 212.12317666126418,
      "grad_norm": 1.1305309534072876,
      "learning_rate": 2.881518001946156e-05,
      "loss": 2.6776,
      "step": 654400
    },
    {
      "epoch": 212.15559157212317,
      "grad_norm": 1.2467856407165527,
      "learning_rate": 2.881193642555952e-05,
      "loss": 2.6518,
      "step": 654500
    },
    {
      "epoch": 212.18800648298216,
      "grad_norm": 1.5030845403671265,
      "learning_rate": 2.880869283165748e-05,
      "loss": 2.6784,
      "step": 654600
    },
    {
      "epoch": 212.22042139384118,
      "grad_norm": 1.138381838798523,
      "learning_rate": 2.8805449237755434e-05,
      "loss": 2.6871,
      "step": 654700
    },
    {
      "epoch": 212.25283630470017,
      "grad_norm": 1.1779980659484863,
      "learning_rate": 2.8802205643853393e-05,
      "loss": 2.6981,
      "step": 654800
    },
    {
      "epoch": 212.28525121555916,
      "grad_norm": 1.1473270654678345,
      "learning_rate": 2.8798994485890367e-05,
      "loss": 2.6883,
      "step": 654900
    },
    {
      "epoch": 212.31766612641815,
      "grad_norm": 1.2654002904891968,
      "learning_rate": 2.8795750891988326e-05,
      "loss": 2.7027,
      "step": 655000
    },
    {
      "epoch": 212.35008103727714,
      "grad_norm": 1.5162479877471924,
      "learning_rate": 2.879250729808628e-05,
      "loss": 2.6639,
      "step": 655100
    },
    {
      "epoch": 212.38249594813615,
      "grad_norm": 1.2772568464279175,
      "learning_rate": 2.8789263704184237e-05,
      "loss": 2.6919,
      "step": 655200
    },
    {
      "epoch": 212.41491085899514,
      "grad_norm": 1.4318561553955078,
      "learning_rate": 2.8786020110282192e-05,
      "loss": 2.695,
      "step": 655300
    },
    {
      "epoch": 212.44732576985413,
      "grad_norm": 1.1727209091186523,
      "learning_rate": 2.878277651638015e-05,
      "loss": 2.6966,
      "step": 655400
    },
    {
      "epoch": 212.47974068071312,
      "grad_norm": 1.0817291736602783,
      "learning_rate": 2.877953292247811e-05,
      "loss": 2.6637,
      "step": 655500
    },
    {
      "epoch": 212.5121555915721,
      "grad_norm": 1.2437890768051147,
      "learning_rate": 2.877628932857606e-05,
      "loss": 2.6859,
      "step": 655600
    },
    {
      "epoch": 212.54457050243113,
      "grad_norm": 1.1296699047088623,
      "learning_rate": 2.877304573467402e-05,
      "loss": 2.677,
      "step": 655700
    },
    {
      "epoch": 212.57698541329012,
      "grad_norm": 1.1390633583068848,
      "learning_rate": 2.876980214077198e-05,
      "loss": 2.6836,
      "step": 655800
    },
    {
      "epoch": 212.6094003241491,
      "grad_norm": 1.1357955932617188,
      "learning_rate": 2.876655854686993e-05,
      "loss": 2.68,
      "step": 655900
    },
    {
      "epoch": 212.6418152350081,
      "grad_norm": 1.263686180114746,
      "learning_rate": 2.876331495296789e-05,
      "loss": 2.6778,
      "step": 656000
    },
    {
      "epoch": 212.67423014586709,
      "grad_norm": 1.125335693359375,
      "learning_rate": 2.876007135906585e-05,
      "loss": 2.6725,
      "step": 656100
    },
    {
      "epoch": 212.7066450567261,
      "grad_norm": 1.2251741886138916,
      "learning_rate": 2.87568277651638e-05,
      "loss": 2.6741,
      "step": 656200
    },
    {
      "epoch": 212.7390599675851,
      "grad_norm": 1.2254623174667358,
      "learning_rate": 2.875358417126176e-05,
      "loss": 2.7035,
      "step": 656300
    },
    {
      "epoch": 212.77147487844408,
      "grad_norm": 1.1859683990478516,
      "learning_rate": 2.8750340577359715e-05,
      "loss": 2.698,
      "step": 656400
    },
    {
      "epoch": 212.80388978930307,
      "grad_norm": 1.2915236949920654,
      "learning_rate": 2.8747096983457673e-05,
      "loss": 2.6859,
      "step": 656500
    },
    {
      "epoch": 212.8363047001621,
      "grad_norm": 1.5691417455673218,
      "learning_rate": 2.8743853389555632e-05,
      "loss": 2.6788,
      "step": 656600
    },
    {
      "epoch": 212.86871961102108,
      "grad_norm": 1.2952709197998047,
      "learning_rate": 2.8740609795653584e-05,
      "loss": 2.6909,
      "step": 656700
    },
    {
      "epoch": 212.90113452188007,
      "grad_norm": 1.2250213623046875,
      "learning_rate": 2.8737366201751543e-05,
      "loss": 2.687,
      "step": 656800
    },
    {
      "epoch": 212.93354943273906,
      "grad_norm": 1.37212336063385,
      "learning_rate": 2.87341226078495e-05,
      "loss": 2.7061,
      "step": 656900
    },
    {
      "epoch": 212.96596434359805,
      "grad_norm": 1.2648468017578125,
      "learning_rate": 2.8730879013947453e-05,
      "loss": 2.6734,
      "step": 657000
    },
    {
      "epoch": 212.99837925445706,
      "grad_norm": 1.406244158744812,
      "learning_rate": 2.8727635420045412e-05,
      "loss": 2.6886,
      "step": 657100
    },
    {
      "epoch": 213.0,
      "eval_bleu": 0.9310385606709095,
      "eval_loss": 4.044422626495361,
      "eval_runtime": 4.0342,
      "eval_samples_per_second": 121.956,
      "eval_steps_per_second": 1.983,
      "step": 657105
    },
    {
      "epoch": 213.03079416531605,
      "grad_norm": 1.1404672861099243,
      "learning_rate": 2.872439182614337e-05,
      "loss": 2.6646,
      "step": 657200
    },
    {
      "epoch": 213.06320907617504,
      "grad_norm": 1.3703126907348633,
      "learning_rate": 2.8721148232241323e-05,
      "loss": 2.661,
      "step": 657300
    },
    {
      "epoch": 213.09562398703403,
      "grad_norm": 1.2000242471694946,
      "learning_rate": 2.871790463833928e-05,
      "loss": 2.672,
      "step": 657400
    },
    {
      "epoch": 213.12803889789302,
      "grad_norm": 1.184252381324768,
      "learning_rate": 2.8714661044437237e-05,
      "loss": 2.6781,
      "step": 657500
    },
    {
      "epoch": 213.16045380875204,
      "grad_norm": 1.1019383668899536,
      "learning_rate": 2.8711417450535196e-05,
      "loss": 2.6757,
      "step": 657600
    },
    {
      "epoch": 213.19286871961103,
      "grad_norm": 1.264582633972168,
      "learning_rate": 2.870817385663315e-05,
      "loss": 2.6758,
      "step": 657700
    },
    {
      "epoch": 213.22528363047002,
      "grad_norm": 1.271348476409912,
      "learning_rate": 2.8704930262731106e-05,
      "loss": 2.6865,
      "step": 657800
    },
    {
      "epoch": 213.257698541329,
      "grad_norm": 1.2628244161605835,
      "learning_rate": 2.8701686668829065e-05,
      "loss": 2.674,
      "step": 657900
    },
    {
      "epoch": 213.290113452188,
      "grad_norm": 1.0930655002593994,
      "learning_rate": 2.8698443074927024e-05,
      "loss": 2.6951,
      "step": 658000
    },
    {
      "epoch": 213.322528363047,
      "grad_norm": 1.271788477897644,
      "learning_rate": 2.8695199481024976e-05,
      "loss": 2.6653,
      "step": 658100
    },
    {
      "epoch": 213.354943273906,
      "grad_norm": 1.2634458541870117,
      "learning_rate": 2.8691955887122935e-05,
      "loss": 2.6955,
      "step": 658200
    },
    {
      "epoch": 213.387358184765,
      "grad_norm": 1.3279632329940796,
      "learning_rate": 2.8688712293220887e-05,
      "loss": 2.6607,
      "step": 658300
    },
    {
      "epoch": 213.41977309562398,
      "grad_norm": 1.0870438814163208,
      "learning_rate": 2.8685468699318845e-05,
      "loss": 2.6729,
      "step": 658400
    },
    {
      "epoch": 213.45218800648297,
      "grad_norm": 1.389941930770874,
      "learning_rate": 2.8682225105416804e-05,
      "loss": 2.6518,
      "step": 658500
    },
    {
      "epoch": 213.484602917342,
      "grad_norm": 1.3130433559417725,
      "learning_rate": 2.8678981511514756e-05,
      "loss": 2.7027,
      "step": 658600
    },
    {
      "epoch": 213.51701782820098,
      "grad_norm": 1.2381173372268677,
      "learning_rate": 2.8675737917612715e-05,
      "loss": 2.6694,
      "step": 658700
    },
    {
      "epoch": 213.54943273905997,
      "grad_norm": 1.251741647720337,
      "learning_rate": 2.8672494323710674e-05,
      "loss": 2.6778,
      "step": 658800
    },
    {
      "epoch": 213.58184764991896,
      "grad_norm": 1.0961329936981201,
      "learning_rate": 2.866928316574765e-05,
      "loss": 2.6712,
      "step": 658900
    },
    {
      "epoch": 213.61426256077795,
      "grad_norm": 1.212446928024292,
      "learning_rate": 2.8666039571845603e-05,
      "loss": 2.6579,
      "step": 659000
    },
    {
      "epoch": 213.64667747163696,
      "grad_norm": 1.1846959590911865,
      "learning_rate": 2.8662795977943562e-05,
      "loss": 2.6805,
      "step": 659100
    },
    {
      "epoch": 213.67909238249595,
      "grad_norm": 1.3948335647583008,
      "learning_rate": 2.865955238404152e-05,
      "loss": 2.6931,
      "step": 659200
    },
    {
      "epoch": 213.71150729335494,
      "grad_norm": 1.1015515327453613,
      "learning_rate": 2.8656308790139476e-05,
      "loss": 2.6897,
      "step": 659300
    },
    {
      "epoch": 213.74392220421393,
      "grad_norm": 1.3352458477020264,
      "learning_rate": 2.865306519623743e-05,
      "loss": 2.6938,
      "step": 659400
    },
    {
      "epoch": 213.77633711507292,
      "grad_norm": 1.221447229385376,
      "learning_rate": 2.864982160233539e-05,
      "loss": 2.6802,
      "step": 659500
    },
    {
      "epoch": 213.80875202593194,
      "grad_norm": 1.1327102184295654,
      "learning_rate": 2.8646578008433346e-05,
      "loss": 2.6665,
      "step": 659600
    },
    {
      "epoch": 213.84116693679093,
      "grad_norm": 1.2475978136062622,
      "learning_rate": 2.8643334414531304e-05,
      "loss": 2.7045,
      "step": 659700
    },
    {
      "epoch": 213.87358184764992,
      "grad_norm": 1.1604095697402954,
      "learning_rate": 2.8640090820629256e-05,
      "loss": 2.6814,
      "step": 659800
    },
    {
      "epoch": 213.9059967585089,
      "grad_norm": 1.1758686304092407,
      "learning_rate": 2.8636847226727215e-05,
      "loss": 2.7124,
      "step": 659900
    },
    {
      "epoch": 213.93841166936792,
      "grad_norm": 1.5708001852035522,
      "learning_rate": 2.8633603632825174e-05,
      "loss": 2.6991,
      "step": 660000
    },
    {
      "epoch": 213.9708265802269,
      "grad_norm": 1.2908998727798462,
      "learning_rate": 2.8630360038923126e-05,
      "loss": 2.6915,
      "step": 660100
    },
    {
      "epoch": 214.0,
      "eval_bleu": 1.0514888852477702,
      "eval_loss": 4.039166450500488,
      "eval_runtime": 3.73,
      "eval_samples_per_second": 131.905,
      "eval_steps_per_second": 2.145,
      "step": 660190
    },
    {
      "epoch": 214.0032414910859,
      "grad_norm": 1.4787557125091553,
      "learning_rate": 2.8627116445021084e-05,
      "loss": 2.6906,
      "step": 660200
    },
    {
      "epoch": 214.0356564019449,
      "grad_norm": 1.2677812576293945,
      "learning_rate": 2.8623872851119043e-05,
      "loss": 2.6794,
      "step": 660300
    },
    {
      "epoch": 214.06807131280388,
      "grad_norm": 1.0786486864089966,
      "learning_rate": 2.8620629257216995e-05,
      "loss": 2.6601,
      "step": 660400
    },
    {
      "epoch": 214.1004862236629,
      "grad_norm": 1.1516529321670532,
      "learning_rate": 2.8617385663314954e-05,
      "loss": 2.6728,
      "step": 660500
    },
    {
      "epoch": 214.1329011345219,
      "grad_norm": 1.2997093200683594,
      "learning_rate": 2.8614142069412913e-05,
      "loss": 2.6661,
      "step": 660600
    },
    {
      "epoch": 214.16531604538088,
      "grad_norm": 1.2944732904434204,
      "learning_rate": 2.8610898475510868e-05,
      "loss": 2.6758,
      "step": 660700
    },
    {
      "epoch": 214.19773095623987,
      "grad_norm": 1.2338920831680298,
      "learning_rate": 2.8607654881608827e-05,
      "loss": 2.6739,
      "step": 660800
    },
    {
      "epoch": 214.23014586709886,
      "grad_norm": 1.12132728099823,
      "learning_rate": 2.86044437236458e-05,
      "loss": 2.6862,
      "step": 660900
    },
    {
      "epoch": 214.26256077795787,
      "grad_norm": 1.2627456188201904,
      "learning_rate": 2.8601200129743753e-05,
      "loss": 2.6554,
      "step": 661000
    },
    {
      "epoch": 214.29497568881686,
      "grad_norm": 1.2250545024871826,
      "learning_rate": 2.8597988971780738e-05,
      "loss": 2.6999,
      "step": 661100
    },
    {
      "epoch": 214.32739059967585,
      "grad_norm": 1.269757628440857,
      "learning_rate": 2.859474537787869e-05,
      "loss": 2.6863,
      "step": 661200
    },
    {
      "epoch": 214.35980551053484,
      "grad_norm": 1.3697049617767334,
      "learning_rate": 2.859150178397665e-05,
      "loss": 2.6804,
      "step": 661300
    },
    {
      "epoch": 214.39222042139383,
      "grad_norm": 1.2827519178390503,
      "learning_rate": 2.85882581900746e-05,
      "loss": 2.6603,
      "step": 661400
    },
    {
      "epoch": 214.42463533225285,
      "grad_norm": 1.6381646394729614,
      "learning_rate": 2.858501459617256e-05,
      "loss": 2.6671,
      "step": 661500
    },
    {
      "epoch": 214.45705024311184,
      "grad_norm": 1.32229483127594,
      "learning_rate": 2.8581771002270518e-05,
      "loss": 2.6664,
      "step": 661600
    },
    {
      "epoch": 214.48946515397083,
      "grad_norm": 1.377359390258789,
      "learning_rate": 2.8578527408368473e-05,
      "loss": 2.6725,
      "step": 661700
    },
    {
      "epoch": 214.52188006482982,
      "grad_norm": 1.3349065780639648,
      "learning_rate": 2.8575283814466432e-05,
      "loss": 2.6727,
      "step": 661800
    },
    {
      "epoch": 214.5542949756888,
      "grad_norm": 1.3091922998428345,
      "learning_rate": 2.8572040220564387e-05,
      "loss": 2.6677,
      "step": 661900
    },
    {
      "epoch": 214.58670988654782,
      "grad_norm": 1.259887456893921,
      "learning_rate": 2.8568796626662343e-05,
      "loss": 2.6829,
      "step": 662000
    },
    {
      "epoch": 214.6191247974068,
      "grad_norm": 0.9892401099205017,
      "learning_rate": 2.85655530327603e-05,
      "loss": 2.6745,
      "step": 662100
    },
    {
      "epoch": 214.6515397082658,
      "grad_norm": 1.2118346691131592,
      "learning_rate": 2.856230943885826e-05,
      "loss": 2.7134,
      "step": 662200
    },
    {
      "epoch": 214.6839546191248,
      "grad_norm": 1.4893490076065063,
      "learning_rate": 2.8559065844956212e-05,
      "loss": 2.6917,
      "step": 662300
    },
    {
      "epoch": 214.71636952998378,
      "grad_norm": 1.1298028230667114,
      "learning_rate": 2.855582225105417e-05,
      "loss": 2.6714,
      "step": 662400
    },
    {
      "epoch": 214.7487844408428,
      "grad_norm": 1.165505290031433,
      "learning_rate": 2.8552578657152123e-05,
      "loss": 2.6771,
      "step": 662500
    },
    {
      "epoch": 214.7811993517018,
      "grad_norm": 1.3263667821884155,
      "learning_rate": 2.854933506325008e-05,
      "loss": 2.673,
      "step": 662600
    },
    {
      "epoch": 214.81361426256078,
      "grad_norm": 1.3399803638458252,
      "learning_rate": 2.854609146934804e-05,
      "loss": 2.6755,
      "step": 662700
    },
    {
      "epoch": 214.84602917341977,
      "grad_norm": 1.139519214630127,
      "learning_rate": 2.8542847875445992e-05,
      "loss": 2.6845,
      "step": 662800
    },
    {
      "epoch": 214.87844408427875,
      "grad_norm": 1.1831709146499634,
      "learning_rate": 2.853960428154395e-05,
      "loss": 2.6887,
      "step": 662900
    },
    {
      "epoch": 214.91085899513777,
      "grad_norm": 1.1111887693405151,
      "learning_rate": 2.853636068764191e-05,
      "loss": 2.6952,
      "step": 663000
    },
    {
      "epoch": 214.94327390599676,
      "grad_norm": 1.2420021295547485,
      "learning_rate": 2.8533117093739865e-05,
      "loss": 2.6896,
      "step": 663100
    },
    {
      "epoch": 214.97568881685575,
      "grad_norm": 1.3565598726272583,
      "learning_rate": 2.8529873499837824e-05,
      "loss": 2.6841,
      "step": 663200
    },
    {
      "epoch": 215.0,
      "eval_bleu": 1.0088374755096485,
      "eval_loss": 4.045235633850098,
      "eval_runtime": 4.3024,
      "eval_samples_per_second": 114.355,
      "eval_steps_per_second": 1.859,
      "step": 663275
    },
    {
      "epoch": 215.00810372771474,
      "grad_norm": 1.1322897672653198,
      "learning_rate": 2.8526629905935776e-05,
      "loss": 2.6864,
      "step": 663300
    },
    {
      "epoch": 215.04051863857376,
      "grad_norm": 1.3553447723388672,
      "learning_rate": 2.8523386312033735e-05,
      "loss": 2.6883,
      "step": 663400
    },
    {
      "epoch": 215.07293354943275,
      "grad_norm": 1.2293190956115723,
      "learning_rate": 2.852017515407071e-05,
      "loss": 2.6866,
      "step": 663500
    },
    {
      "epoch": 215.10534846029174,
      "grad_norm": 1.2024734020233154,
      "learning_rate": 2.8516931560168668e-05,
      "loss": 2.6621,
      "step": 663600
    },
    {
      "epoch": 215.13776337115073,
      "grad_norm": 1.1954560279846191,
      "learning_rate": 2.8513687966266623e-05,
      "loss": 2.6638,
      "step": 663700
    },
    {
      "epoch": 215.17017828200972,
      "grad_norm": 1.2823796272277832,
      "learning_rate": 2.8510444372364582e-05,
      "loss": 2.6878,
      "step": 663800
    },
    {
      "epoch": 215.20259319286873,
      "grad_norm": 1.3378552198410034,
      "learning_rate": 2.850720077846254e-05,
      "loss": 2.6648,
      "step": 663900
    },
    {
      "epoch": 215.23500810372772,
      "grad_norm": 1.0908560752868652,
      "learning_rate": 2.8503957184560493e-05,
      "loss": 2.6993,
      "step": 664000
    },
    {
      "epoch": 215.2674230145867,
      "grad_norm": 1.1528754234313965,
      "learning_rate": 2.850071359065845e-05,
      "loss": 2.6697,
      "step": 664100
    },
    {
      "epoch": 215.2998379254457,
      "grad_norm": 1.1252769231796265,
      "learning_rate": 2.849746999675641e-05,
      "loss": 2.6673,
      "step": 664200
    },
    {
      "epoch": 215.3322528363047,
      "grad_norm": 1.2195656299591064,
      "learning_rate": 2.8494226402854362e-05,
      "loss": 2.6584,
      "step": 664300
    },
    {
      "epoch": 215.3646677471637,
      "grad_norm": 1.358872413635254,
      "learning_rate": 2.849098280895232e-05,
      "loss": 2.6704,
      "step": 664400
    },
    {
      "epoch": 215.3970826580227,
      "grad_norm": 1.1587412357330322,
      "learning_rate": 2.848773921505028e-05,
      "loss": 2.6787,
      "step": 664500
    },
    {
      "epoch": 215.4294975688817,
      "grad_norm": 1.2182852029800415,
      "learning_rate": 2.848449562114823e-05,
      "loss": 2.665,
      "step": 664600
    },
    {
      "epoch": 215.46191247974068,
      "grad_norm": 1.2067984342575073,
      "learning_rate": 2.848125202724619e-05,
      "loss": 2.6773,
      "step": 664700
    },
    {
      "epoch": 215.49432739059966,
      "grad_norm": 1.2631813287734985,
      "learning_rate": 2.8478008433344146e-05,
      "loss": 2.6729,
      "step": 664800
    },
    {
      "epoch": 215.52674230145868,
      "grad_norm": 1.1608489751815796,
      "learning_rate": 2.8474764839442104e-05,
      "loss": 2.6877,
      "step": 664900
    },
    {
      "epoch": 215.55915721231767,
      "grad_norm": 1.1671946048736572,
      "learning_rate": 2.8471521245540063e-05,
      "loss": 2.6862,
      "step": 665000
    },
    {
      "epoch": 215.59157212317666,
      "grad_norm": 1.1465071439743042,
      "learning_rate": 2.8468277651638015e-05,
      "loss": 2.6665,
      "step": 665100
    },
    {
      "epoch": 215.62398703403565,
      "grad_norm": 1.3664848804473877,
      "learning_rate": 2.8465034057735974e-05,
      "loss": 2.6672,
      "step": 665200
    },
    {
      "epoch": 215.65640194489464,
      "grad_norm": 1.1374911069869995,
      "learning_rate": 2.8461790463833933e-05,
      "loss": 2.6819,
      "step": 665300
    },
    {
      "epoch": 215.68881685575366,
      "grad_norm": 1.2252206802368164,
      "learning_rate": 2.8458546869931884e-05,
      "loss": 2.6865,
      "step": 665400
    },
    {
      "epoch": 215.72123176661265,
      "grad_norm": 1.2156739234924316,
      "learning_rate": 2.8455303276029843e-05,
      "loss": 2.6683,
      "step": 665500
    },
    {
      "epoch": 215.75364667747164,
      "grad_norm": 1.4541699886322021,
      "learning_rate": 2.8452059682127795e-05,
      "loss": 2.6628,
      "step": 665600
    },
    {
      "epoch": 215.78606158833063,
      "grad_norm": 1.2597702741622925,
      "learning_rate": 2.8448816088225754e-05,
      "loss": 2.6652,
      "step": 665700
    },
    {
      "epoch": 215.81847649918961,
      "grad_norm": 1.1182917356491089,
      "learning_rate": 2.8445572494323713e-05,
      "loss": 2.6786,
      "step": 665800
    },
    {
      "epoch": 215.85089141004863,
      "grad_norm": 1.1828163862228394,
      "learning_rate": 2.8442328900421668e-05,
      "loss": 2.6822,
      "step": 665900
    },
    {
      "epoch": 215.88330632090762,
      "grad_norm": 1.319173812866211,
      "learning_rate": 2.8439085306519623e-05,
      "loss": 2.667,
      "step": 666000
    },
    {
      "epoch": 215.9157212317666,
      "grad_norm": 1.2775248289108276,
      "learning_rate": 2.8435841712617582e-05,
      "loss": 2.7044,
      "step": 666100
    },
    {
      "epoch": 215.9481361426256,
      "grad_norm": 1.2340383529663086,
      "learning_rate": 2.8432598118715537e-05,
      "loss": 2.6771,
      "step": 666200
    },
    {
      "epoch": 215.9805510534846,
      "grad_norm": 1.475482702255249,
      "learning_rate": 2.8429354524813496e-05,
      "loss": 2.6905,
      "step": 666300
    },
    {
      "epoch": 216.0,
      "eval_bleu": 0.9066675178871111,
      "eval_loss": 4.040376663208008,
      "eval_runtime": 4.2967,
      "eval_samples_per_second": 114.506,
      "eval_steps_per_second": 1.862,
      "step": 666360
    },
    {
      "epoch": 216.0129659643436,
      "grad_norm": 1.1646957397460938,
      "learning_rate": 2.8426110930911455e-05,
      "loss": 2.6816,
      "step": 666400
    },
    {
      "epoch": 216.0453808752026,
      "grad_norm": 1.377425193786621,
      "learning_rate": 2.8422867337009407e-05,
      "loss": 2.674,
      "step": 666500
    },
    {
      "epoch": 216.07779578606159,
      "grad_norm": 1.154440999031067,
      "learning_rate": 2.8419623743107366e-05,
      "loss": 2.6782,
      "step": 666600
    },
    {
      "epoch": 216.11021069692057,
      "grad_norm": 1.2494933605194092,
      "learning_rate": 2.8416380149205318e-05,
      "loss": 2.6579,
      "step": 666700
    },
    {
      "epoch": 216.1426256077796,
      "grad_norm": 1.151202917098999,
      "learning_rate": 2.8413136555303276e-05,
      "loss": 2.6788,
      "step": 666800
    },
    {
      "epoch": 216.17504051863858,
      "grad_norm": 1.2085586786270142,
      "learning_rate": 2.8409892961401235e-05,
      "loss": 2.6827,
      "step": 666900
    },
    {
      "epoch": 216.20745542949757,
      "grad_norm": 1.130200743675232,
      "learning_rate": 2.8406649367499187e-05,
      "loss": 2.6803,
      "step": 667000
    },
    {
      "epoch": 216.23987034035656,
      "grad_norm": 1.286332607269287,
      "learning_rate": 2.8403405773597146e-05,
      "loss": 2.6875,
      "step": 667100
    },
    {
      "epoch": 216.27228525121555,
      "grad_norm": 1.4110968112945557,
      "learning_rate": 2.8400162179695105e-05,
      "loss": 2.6684,
      "step": 667200
    },
    {
      "epoch": 216.30470016207457,
      "grad_norm": 1.2073817253112793,
      "learning_rate": 2.839691858579306e-05,
      "loss": 2.6799,
      "step": 667300
    },
    {
      "epoch": 216.33711507293356,
      "grad_norm": 1.1290680170059204,
      "learning_rate": 2.839367499189102e-05,
      "loss": 2.6789,
      "step": 667400
    },
    {
      "epoch": 216.36952998379255,
      "grad_norm": 1.3117891550064087,
      "learning_rate": 2.8390463833927993e-05,
      "loss": 2.6757,
      "step": 667500
    },
    {
      "epoch": 216.40194489465154,
      "grad_norm": 1.4545272588729858,
      "learning_rate": 2.8387220240025952e-05,
      "loss": 2.6553,
      "step": 667600
    },
    {
      "epoch": 216.43435980551052,
      "grad_norm": 1.21290123462677,
      "learning_rate": 2.8383976646123904e-05,
      "loss": 2.6803,
      "step": 667700
    },
    {
      "epoch": 216.46677471636954,
      "grad_norm": 1.2281187772750854,
      "learning_rate": 2.8380733052221863e-05,
      "loss": 2.6765,
      "step": 667800
    },
    {
      "epoch": 216.49918962722853,
      "grad_norm": 1.3533557653427124,
      "learning_rate": 2.8377489458319818e-05,
      "loss": 2.6697,
      "step": 667900
    },
    {
      "epoch": 216.53160453808752,
      "grad_norm": 1.2118544578552246,
      "learning_rate": 2.8374245864417777e-05,
      "loss": 2.6728,
      "step": 668000
    },
    {
      "epoch": 216.5640194489465,
      "grad_norm": 1.2713676691055298,
      "learning_rate": 2.837103470645475e-05,
      "loss": 2.6612,
      "step": 668100
    },
    {
      "epoch": 216.5964343598055,
      "grad_norm": 1.381736159324646,
      "learning_rate": 2.836779111255271e-05,
      "loss": 2.6734,
      "step": 668200
    },
    {
      "epoch": 216.62884927066452,
      "grad_norm": 1.2091038227081299,
      "learning_rate": 2.8364547518650665e-05,
      "loss": 2.6688,
      "step": 668300
    },
    {
      "epoch": 216.6612641815235,
      "grad_norm": 1.1669139862060547,
      "learning_rate": 2.8361303924748624e-05,
      "loss": 2.661,
      "step": 668400
    },
    {
      "epoch": 216.6936790923825,
      "grad_norm": 1.3407469987869263,
      "learning_rate": 2.835806033084658e-05,
      "loss": 2.6828,
      "step": 668500
    },
    {
      "epoch": 216.72609400324149,
      "grad_norm": 1.2682271003723145,
      "learning_rate": 2.8354816736944535e-05,
      "loss": 2.6626,
      "step": 668600
    },
    {
      "epoch": 216.75850891410047,
      "grad_norm": 1.1965394020080566,
      "learning_rate": 2.8351573143042493e-05,
      "loss": 2.6794,
      "step": 668700
    },
    {
      "epoch": 216.7909238249595,
      "grad_norm": 1.2186847925186157,
      "learning_rate": 2.8348329549140452e-05,
      "loss": 2.6734,
      "step": 668800
    },
    {
      "epoch": 216.82333873581848,
      "grad_norm": 1.259474515914917,
      "learning_rate": 2.8345085955238404e-05,
      "loss": 2.6823,
      "step": 668900
    },
    {
      "epoch": 216.85575364667747,
      "grad_norm": 1.420589804649353,
      "learning_rate": 2.8341842361336363e-05,
      "loss": 2.6813,
      "step": 669000
    },
    {
      "epoch": 216.88816855753646,
      "grad_norm": 1.2397449016571045,
      "learning_rate": 2.833859876743432e-05,
      "loss": 2.6958,
      "step": 669100
    },
    {
      "epoch": 216.92058346839545,
      "grad_norm": 1.4522508382797241,
      "learning_rate": 2.8335355173532274e-05,
      "loss": 2.6688,
      "step": 669200
    },
    {
      "epoch": 216.95299837925447,
      "grad_norm": 1.3356927633285522,
      "learning_rate": 2.8332111579630232e-05,
      "loss": 2.6864,
      "step": 669300
    },
    {
      "epoch": 216.98541329011346,
      "grad_norm": 1.182837724685669,
      "learning_rate": 2.8328867985728184e-05,
      "loss": 2.6816,
      "step": 669400
    },
    {
      "epoch": 217.0,
      "eval_bleu": 1.1649012469212072,
      "eval_loss": 4.048314571380615,
      "eval_runtime": 3.9865,
      "eval_samples_per_second": 123.415,
      "eval_steps_per_second": 2.007,
      "step": 669445
    },
    {
      "epoch": 217.01782820097245,
      "grad_norm": 1.2063300609588623,
      "learning_rate": 2.8325624391826143e-05,
      "loss": 2.6781,
      "step": 669500
    },
    {
      "epoch": 217.05024311183143,
      "grad_norm": 1.31082022190094,
      "learning_rate": 2.8322380797924102e-05,
      "loss": 2.6671,
      "step": 669600
    },
    {
      "epoch": 217.08265802269042,
      "grad_norm": 1.1730376482009888,
      "learning_rate": 2.8319137204022057e-05,
      "loss": 2.6663,
      "step": 669700
    },
    {
      "epoch": 217.11507293354944,
      "grad_norm": 1.3058518171310425,
      "learning_rate": 2.8315893610120016e-05,
      "loss": 2.6503,
      "step": 669800
    },
    {
      "epoch": 217.14748784440843,
      "grad_norm": 1.3781675100326538,
      "learning_rate": 2.8312650016217975e-05,
      "loss": 2.6754,
      "step": 669900
    },
    {
      "epoch": 217.17990275526742,
      "grad_norm": 1.3001890182495117,
      "learning_rate": 2.8309406422315927e-05,
      "loss": 2.679,
      "step": 670000
    },
    {
      "epoch": 217.2123176661264,
      "grad_norm": 1.2778093814849854,
      "learning_rate": 2.8306162828413885e-05,
      "loss": 2.6655,
      "step": 670100
    },
    {
      "epoch": 217.24473257698543,
      "grad_norm": 1.2158842086791992,
      "learning_rate": 2.8302919234511844e-05,
      "loss": 2.6757,
      "step": 670200
    },
    {
      "epoch": 217.27714748784442,
      "grad_norm": 1.1900203227996826,
      "learning_rate": 2.8299675640609796e-05,
      "loss": 2.6758,
      "step": 670300
    },
    {
      "epoch": 217.3095623987034,
      "grad_norm": 1.2162766456604004,
      "learning_rate": 2.8296432046707755e-05,
      "loss": 2.6654,
      "step": 670400
    },
    {
      "epoch": 217.3419773095624,
      "grad_norm": 1.060732364654541,
      "learning_rate": 2.8293188452805707e-05,
      "loss": 2.6696,
      "step": 670500
    },
    {
      "epoch": 217.37439222042138,
      "grad_norm": 1.4311115741729736,
      "learning_rate": 2.8289944858903665e-05,
      "loss": 2.6672,
      "step": 670600
    },
    {
      "epoch": 217.4068071312804,
      "grad_norm": 1.2378673553466797,
      "learning_rate": 2.8286701265001624e-05,
      "loss": 2.6758,
      "step": 670700
    },
    {
      "epoch": 217.4392220421394,
      "grad_norm": 1.2436530590057373,
      "learning_rate": 2.828345767109958e-05,
      "loss": 2.6835,
      "step": 670800
    },
    {
      "epoch": 217.47163695299838,
      "grad_norm": 1.1219428777694702,
      "learning_rate": 2.8280214077197538e-05,
      "loss": 2.6738,
      "step": 670900
    },
    {
      "epoch": 217.50405186385737,
      "grad_norm": 1.304871916770935,
      "learning_rate": 2.8276970483295494e-05,
      "loss": 2.658,
      "step": 671000
    },
    {
      "epoch": 217.53646677471636,
      "grad_norm": 1.2045488357543945,
      "learning_rate": 2.827375932533247e-05,
      "loss": 2.6874,
      "step": 671100
    },
    {
      "epoch": 217.56888168557538,
      "grad_norm": 1.2394483089447021,
      "learning_rate": 2.8270515731430423e-05,
      "loss": 2.6856,
      "step": 671200
    },
    {
      "epoch": 217.60129659643437,
      "grad_norm": 1.098041296005249,
      "learning_rate": 2.8267272137528382e-05,
      "loss": 2.6728,
      "step": 671300
    },
    {
      "epoch": 217.63371150729336,
      "grad_norm": 1.0816079378128052,
      "learning_rate": 2.826402854362634e-05,
      "loss": 2.6743,
      "step": 671400
    },
    {
      "epoch": 217.66612641815234,
      "grad_norm": 1.4886267185211182,
      "learning_rate": 2.8260784949724296e-05,
      "loss": 2.6767,
      "step": 671500
    },
    {
      "epoch": 217.69854132901133,
      "grad_norm": 1.5017181634902954,
      "learning_rate": 2.8257541355822255e-05,
      "loss": 2.666,
      "step": 671600
    },
    {
      "epoch": 217.73095623987035,
      "grad_norm": 1.1560331583023071,
      "learning_rate": 2.8254297761920207e-05,
      "loss": 2.6575,
      "step": 671700
    },
    {
      "epoch": 217.76337115072934,
      "grad_norm": 1.288465142250061,
      "learning_rate": 2.8251054168018166e-05,
      "loss": 2.6827,
      "step": 671800
    },
    {
      "epoch": 217.79578606158833,
      "grad_norm": 1.244530200958252,
      "learning_rate": 2.8247810574116124e-05,
      "loss": 2.6845,
      "step": 671900
    },
    {
      "epoch": 217.82820097244732,
      "grad_norm": 1.2935428619384766,
      "learning_rate": 2.8244566980214076e-05,
      "loss": 2.6704,
      "step": 672000
    },
    {
      "epoch": 217.8606158833063,
      "grad_norm": 1.2065794467926025,
      "learning_rate": 2.8241323386312035e-05,
      "loss": 2.6688,
      "step": 672100
    },
    {
      "epoch": 217.89303079416533,
      "grad_norm": 1.2027440071105957,
      "learning_rate": 2.8238079792409994e-05,
      "loss": 2.6752,
      "step": 672200
    },
    {
      "epoch": 217.92544570502432,
      "grad_norm": 1.269881010055542,
      "learning_rate": 2.8234836198507946e-05,
      "loss": 2.6707,
      "step": 672300
    },
    {
      "epoch": 217.9578606158833,
      "grad_norm": 1.5008084774017334,
      "learning_rate": 2.8231592604605905e-05,
      "loss": 2.6839,
      "step": 672400
    },
    {
      "epoch": 217.9902755267423,
      "grad_norm": 1.4153461456298828,
      "learning_rate": 2.8228349010703863e-05,
      "loss": 2.6935,
      "step": 672500
    },
    {
      "epoch": 218.0,
      "eval_bleu": 1.0734751002543126,
      "eval_loss": 4.044694900512695,
      "eval_runtime": 4.0413,
      "eval_samples_per_second": 121.743,
      "eval_steps_per_second": 1.98,
      "step": 672530
    },
    {
      "epoch": 218.02269043760128,
      "grad_norm": 1.506082534790039,
      "learning_rate": 2.822513785274084e-05,
      "loss": 2.67,
      "step": 672600
    },
    {
      "epoch": 218.0551053484603,
      "grad_norm": 1.1255145072937012,
      "learning_rate": 2.8221894258838793e-05,
      "loss": 2.6702,
      "step": 672700
    },
    {
      "epoch": 218.0875202593193,
      "grad_norm": 1.243125319480896,
      "learning_rate": 2.8218650664936752e-05,
      "loss": 2.673,
      "step": 672800
    },
    {
      "epoch": 218.11993517017828,
      "grad_norm": 1.2367351055145264,
      "learning_rate": 2.8215407071034704e-05,
      "loss": 2.6652,
      "step": 672900
    },
    {
      "epoch": 218.15235008103727,
      "grad_norm": 1.3078597784042358,
      "learning_rate": 2.8212163477132663e-05,
      "loss": 2.6712,
      "step": 673000
    },
    {
      "epoch": 218.18476499189626,
      "grad_norm": 1.4244437217712402,
      "learning_rate": 2.820891988323062e-05,
      "loss": 2.6583,
      "step": 673100
    },
    {
      "epoch": 218.21717990275528,
      "grad_norm": 1.1661673784255981,
      "learning_rate": 2.8205676289328577e-05,
      "loss": 2.6847,
      "step": 673200
    },
    {
      "epoch": 218.24959481361427,
      "grad_norm": 1.2265534400939941,
      "learning_rate": 2.8202432695426535e-05,
      "loss": 2.6634,
      "step": 673300
    },
    {
      "epoch": 218.28200972447326,
      "grad_norm": 1.2779972553253174,
      "learning_rate": 2.8199189101524494e-05,
      "loss": 2.6546,
      "step": 673400
    },
    {
      "epoch": 218.31442463533224,
      "grad_norm": 1.2368412017822266,
      "learning_rate": 2.8195945507622446e-05,
      "loss": 2.6697,
      "step": 673500
    },
    {
      "epoch": 218.34683954619126,
      "grad_norm": 1.2454630136489868,
      "learning_rate": 2.8192701913720405e-05,
      "loss": 2.6739,
      "step": 673600
    },
    {
      "epoch": 218.37925445705025,
      "grad_norm": 1.1704360246658325,
      "learning_rate": 2.8189458319818364e-05,
      "loss": 2.6576,
      "step": 673700
    },
    {
      "epoch": 218.41166936790924,
      "grad_norm": 1.1457428932189941,
      "learning_rate": 2.8186214725916316e-05,
      "loss": 2.6797,
      "step": 673800
    },
    {
      "epoch": 218.44408427876823,
      "grad_norm": 1.222366213798523,
      "learning_rate": 2.8182971132014274e-05,
      "loss": 2.6581,
      "step": 673900
    },
    {
      "epoch": 218.47649918962722,
      "grad_norm": 1.1898921728134155,
      "learning_rate": 2.8179727538112226e-05,
      "loss": 2.6755,
      "step": 674000
    },
    {
      "epoch": 218.50891410048624,
      "grad_norm": 1.2089333534240723,
      "learning_rate": 2.8176483944210185e-05,
      "loss": 2.678,
      "step": 674100
    },
    {
      "epoch": 218.54132901134523,
      "grad_norm": 1.741952896118164,
      "learning_rate": 2.8173240350308144e-05,
      "loss": 2.6731,
      "step": 674200
    },
    {
      "epoch": 218.57374392220422,
      "grad_norm": 1.2929039001464844,
      "learning_rate": 2.8169996756406096e-05,
      "loss": 2.6766,
      "step": 674300
    },
    {
      "epoch": 218.6061588330632,
      "grad_norm": 1.0859569311141968,
      "learning_rate": 2.8166753162504054e-05,
      "loss": 2.6867,
      "step": 674400
    },
    {
      "epoch": 218.6385737439222,
      "grad_norm": 1.1727550029754639,
      "learning_rate": 2.8163509568602013e-05,
      "loss": 2.6754,
      "step": 674500
    },
    {
      "epoch": 218.6709886547812,
      "grad_norm": 1.2455302476882935,
      "learning_rate": 2.816026597469997e-05,
      "loss": 2.6794,
      "step": 674600
    },
    {
      "epoch": 218.7034035656402,
      "grad_norm": 1.3856337070465088,
      "learning_rate": 2.8157022380797927e-05,
      "loss": 2.6646,
      "step": 674700
    },
    {
      "epoch": 218.7358184764992,
      "grad_norm": 1.247036337852478,
      "learning_rate": 2.8153778786895886e-05,
      "loss": 2.6809,
      "step": 674800
    },
    {
      "epoch": 218.76823338735818,
      "grad_norm": 1.304435133934021,
      "learning_rate": 2.8150535192993838e-05,
      "loss": 2.6646,
      "step": 674900
    },
    {
      "epoch": 218.80064829821717,
      "grad_norm": 1.1859546899795532,
      "learning_rate": 2.8147291599091797e-05,
      "loss": 2.6775,
      "step": 675000
    },
    {
      "epoch": 218.8330632090762,
      "grad_norm": 1.2116878032684326,
      "learning_rate": 2.814404800518975e-05,
      "loss": 2.6659,
      "step": 675100
    },
    {
      "epoch": 218.86547811993518,
      "grad_norm": 1.5548601150512695,
      "learning_rate": 2.8140804411287707e-05,
      "loss": 2.6707,
      "step": 675200
    },
    {
      "epoch": 218.89789303079417,
      "grad_norm": 1.5016100406646729,
      "learning_rate": 2.8137560817385666e-05,
      "loss": 2.6998,
      "step": 675300
    },
    {
      "epoch": 218.93030794165315,
      "grad_norm": 1.4397491216659546,
      "learning_rate": 2.8134317223483618e-05,
      "loss": 2.663,
      "step": 675400
    },
    {
      "epoch": 218.96272285251214,
      "grad_norm": 1.3192739486694336,
      "learning_rate": 2.8131073629581577e-05,
      "loss": 2.6769,
      "step": 675500
    },
    {
      "epoch": 218.99513776337116,
      "grad_norm": 1.3332709074020386,
      "learning_rate": 2.8127830035679536e-05,
      "loss": 2.6781,
      "step": 675600
    },
    {
      "epoch": 219.0,
      "eval_bleu": 0.9740155407875708,
      "eval_loss": 4.050934791564941,
      "eval_runtime": 3.837,
      "eval_samples_per_second": 128.225,
      "eval_steps_per_second": 2.085,
      "step": 675615
    },
    {
      "epoch": 219.02755267423015,
      "grad_norm": 1.328160047531128,
      "learning_rate": 2.812458644177749e-05,
      "loss": 2.6559,
      "step": 675700
    },
    {
      "epoch": 219.05996758508914,
      "grad_norm": 1.1566543579101562,
      "learning_rate": 2.812134284787545e-05,
      "loss": 2.6683,
      "step": 675800
    },
    {
      "epoch": 219.09238249594813,
      "grad_norm": 1.1862685680389404,
      "learning_rate": 2.8118099253973402e-05,
      "loss": 2.658,
      "step": 675900
    },
    {
      "epoch": 219.12479740680712,
      "grad_norm": 1.1683071851730347,
      "learning_rate": 2.811485566007136e-05,
      "loss": 2.6785,
      "step": 676000
    },
    {
      "epoch": 219.15721231766614,
      "grad_norm": 1.196692943572998,
      "learning_rate": 2.811161206616932e-05,
      "loss": 2.6562,
      "step": 676100
    },
    {
      "epoch": 219.18962722852513,
      "grad_norm": 1.1285738945007324,
      "learning_rate": 2.810836847226727e-05,
      "loss": 2.6801,
      "step": 676200
    },
    {
      "epoch": 219.22204213938411,
      "grad_norm": 1.2712225914001465,
      "learning_rate": 2.810512487836523e-05,
      "loss": 2.6651,
      "step": 676300
    },
    {
      "epoch": 219.2544570502431,
      "grad_norm": 1.3109869956970215,
      "learning_rate": 2.810188128446319e-05,
      "loss": 2.67,
      "step": 676400
    },
    {
      "epoch": 219.2868719611021,
      "grad_norm": 1.2132501602172852,
      "learning_rate": 2.809863769056114e-05,
      "loss": 2.653,
      "step": 676500
    },
    {
      "epoch": 219.3192868719611,
      "grad_norm": 1.3037028312683105,
      "learning_rate": 2.80953940966591e-05,
      "loss": 2.6731,
      "step": 676600
    },
    {
      "epoch": 219.3517017828201,
      "grad_norm": 1.2346830368041992,
      "learning_rate": 2.8092150502757058e-05,
      "loss": 2.6611,
      "step": 676700
    },
    {
      "epoch": 219.3841166936791,
      "grad_norm": 1.4435895681381226,
      "learning_rate": 2.808890690885501e-05,
      "loss": 2.6603,
      "step": 676800
    },
    {
      "epoch": 219.41653160453808,
      "grad_norm": 1.2910797595977783,
      "learning_rate": 2.808566331495297e-05,
      "loss": 2.6739,
      "step": 676900
    },
    {
      "epoch": 219.4489465153971,
      "grad_norm": 1.3505730628967285,
      "learning_rate": 2.8082419721050924e-05,
      "loss": 2.6761,
      "step": 677000
    },
    {
      "epoch": 219.4813614262561,
      "grad_norm": 1.2208524942398071,
      "learning_rate": 2.8079208563087905e-05,
      "loss": 2.6755,
      "step": 677100
    },
    {
      "epoch": 219.51377633711508,
      "grad_norm": 1.2574315071105957,
      "learning_rate": 2.8075964969185857e-05,
      "loss": 2.6815,
      "step": 677200
    },
    {
      "epoch": 219.54619124797406,
      "grad_norm": 1.2224810123443604,
      "learning_rate": 2.8072721375283816e-05,
      "loss": 2.6778,
      "step": 677300
    },
    {
      "epoch": 219.57860615883305,
      "grad_norm": 1.3125840425491333,
      "learning_rate": 2.806947778138177e-05,
      "loss": 2.6729,
      "step": 677400
    },
    {
      "epoch": 219.61102106969207,
      "grad_norm": 1.5299887657165527,
      "learning_rate": 2.806623418747973e-05,
      "loss": 2.6776,
      "step": 677500
    },
    {
      "epoch": 219.64343598055106,
      "grad_norm": 1.1300569772720337,
      "learning_rate": 2.8062990593577686e-05,
      "loss": 2.6733,
      "step": 677600
    },
    {
      "epoch": 219.67585089141005,
      "grad_norm": 1.2294875383377075,
      "learning_rate": 2.805974699967564e-05,
      "loss": 2.6797,
      "step": 677700
    },
    {
      "epoch": 219.70826580226904,
      "grad_norm": 1.255763292312622,
      "learning_rate": 2.80565034057736e-05,
      "loss": 2.681,
      "step": 677800
    },
    {
      "epoch": 219.74068071312803,
      "grad_norm": 1.300268530845642,
      "learning_rate": 2.805325981187156e-05,
      "loss": 2.6762,
      "step": 677900
    },
    {
      "epoch": 219.77309562398705,
      "grad_norm": 1.1925188302993774,
      "learning_rate": 2.805001621796951e-05,
      "loss": 2.675,
      "step": 678000
    },
    {
      "epoch": 219.80551053484604,
      "grad_norm": 1.2947161197662354,
      "learning_rate": 2.804677262406747e-05,
      "loss": 2.6791,
      "step": 678100
    },
    {
      "epoch": 219.83792544570503,
      "grad_norm": 1.2389676570892334,
      "learning_rate": 2.804352903016542e-05,
      "loss": 2.6724,
      "step": 678200
    },
    {
      "epoch": 219.87034035656401,
      "grad_norm": 1.3128169775009155,
      "learning_rate": 2.8040317872202406e-05,
      "loss": 2.6597,
      "step": 678300
    },
    {
      "epoch": 219.902755267423,
      "grad_norm": 1.152900218963623,
      "learning_rate": 2.8037074278300358e-05,
      "loss": 2.6713,
      "step": 678400
    },
    {
      "epoch": 219.93517017828202,
      "grad_norm": 1.3585056066513062,
      "learning_rate": 2.8033830684398316e-05,
      "loss": 2.6748,
      "step": 678500
    },
    {
      "epoch": 219.967585089141,
      "grad_norm": 1.279649019241333,
      "learning_rate": 2.8030587090496268e-05,
      "loss": 2.665,
      "step": 678600
    },
    {
      "epoch": 220.0,
      "grad_norm": 1.340921401977539,
      "learning_rate": 2.8027343496594227e-05,
      "loss": 2.6691,
      "step": 678700
    },
    {
      "epoch": 220.0,
      "eval_bleu": 0.8660301754533986,
      "eval_loss": 4.050987720489502,
      "eval_runtime": 3.9937,
      "eval_samples_per_second": 123.194,
      "eval_steps_per_second": 2.003,
      "step": 678700
    },
    {
      "epoch": 220.032414910859,
      "grad_norm": 1.096775770187378,
      "learning_rate": 2.8024099902692186e-05,
      "loss": 2.68,
      "step": 678800
    },
    {
      "epoch": 220.06482982171798,
      "grad_norm": 1.1965306997299194,
      "learning_rate": 2.8020856308790138e-05,
      "loss": 2.6545,
      "step": 678900
    },
    {
      "epoch": 220.097244732577,
      "grad_norm": 1.2579879760742188,
      "learning_rate": 2.8017612714888096e-05,
      "loss": 2.6732,
      "step": 679000
    },
    {
      "epoch": 220.12965964343599,
      "grad_norm": 1.30719792842865,
      "learning_rate": 2.8014369120986055e-05,
      "loss": 2.6717,
      "step": 679100
    },
    {
      "epoch": 220.16207455429497,
      "grad_norm": 1.286331057548523,
      "learning_rate": 2.801112552708401e-05,
      "loss": 2.6686,
      "step": 679200
    },
    {
      "epoch": 220.19448946515396,
      "grad_norm": 1.459381341934204,
      "learning_rate": 2.8007881933181966e-05,
      "loss": 2.659,
      "step": 679300
    },
    {
      "epoch": 220.22690437601295,
      "grad_norm": 1.143733024597168,
      "learning_rate": 2.8004638339279925e-05,
      "loss": 2.6552,
      "step": 679400
    },
    {
      "epoch": 220.25931928687197,
      "grad_norm": 1.1948732137680054,
      "learning_rate": 2.800139474537788e-05,
      "loss": 2.665,
      "step": 679500
    },
    {
      "epoch": 220.29173419773096,
      "grad_norm": 1.2239586114883423,
      "learning_rate": 2.799815115147584e-05,
      "loss": 2.6556,
      "step": 679600
    },
    {
      "epoch": 220.32414910858995,
      "grad_norm": 1.3564685583114624,
      "learning_rate": 2.799490755757379e-05,
      "loss": 2.6735,
      "step": 679700
    },
    {
      "epoch": 220.35656401944894,
      "grad_norm": 1.0574243068695068,
      "learning_rate": 2.799166396367175e-05,
      "loss": 2.6683,
      "step": 679800
    },
    {
      "epoch": 220.38897893030793,
      "grad_norm": 1.3808982372283936,
      "learning_rate": 2.7988420369769708e-05,
      "loss": 2.6623,
      "step": 679900
    },
    {
      "epoch": 220.42139384116695,
      "grad_norm": 1.2156424522399902,
      "learning_rate": 2.798517677586766e-05,
      "loss": 2.6561,
      "step": 680000
    },
    {
      "epoch": 220.45380875202594,
      "grad_norm": 1.178391456604004,
      "learning_rate": 2.798193318196562e-05,
      "loss": 2.659,
      "step": 680100
    },
    {
      "epoch": 220.48622366288492,
      "grad_norm": 1.0713979005813599,
      "learning_rate": 2.7978689588063578e-05,
      "loss": 2.654,
      "step": 680200
    },
    {
      "epoch": 220.5186385737439,
      "grad_norm": 1.2839710712432861,
      "learning_rate": 2.797544599416153e-05,
      "loss": 2.6703,
      "step": 680300
    },
    {
      "epoch": 220.5510534846029,
      "grad_norm": 1.2337216138839722,
      "learning_rate": 2.797220240025949e-05,
      "loss": 2.6623,
      "step": 680400
    },
    {
      "epoch": 220.58346839546192,
      "grad_norm": 1.574710726737976,
      "learning_rate": 2.7968958806357447e-05,
      "loss": 2.6514,
      "step": 680500
    },
    {
      "epoch": 220.6158833063209,
      "grad_norm": 1.3652647733688354,
      "learning_rate": 2.7965715212455402e-05,
      "loss": 2.667,
      "step": 680600
    },
    {
      "epoch": 220.6482982171799,
      "grad_norm": 1.1897488832473755,
      "learning_rate": 2.796247161855336e-05,
      "loss": 2.6787,
      "step": 680700
    },
    {
      "epoch": 220.6807131280389,
      "grad_norm": 1.3718276023864746,
      "learning_rate": 2.7959228024651313e-05,
      "loss": 2.6932,
      "step": 680800
    },
    {
      "epoch": 220.7131280388979,
      "grad_norm": 1.2067021131515503,
      "learning_rate": 2.7955984430749272e-05,
      "loss": 2.6804,
      "step": 680900
    },
    {
      "epoch": 220.7455429497569,
      "grad_norm": 1.1748603582382202,
      "learning_rate": 2.795274083684723e-05,
      "loss": 2.6827,
      "step": 681000
    },
    {
      "epoch": 220.77795786061589,
      "grad_norm": 1.2137655019760132,
      "learning_rate": 2.7949497242945183e-05,
      "loss": 2.6763,
      "step": 681100
    },
    {
      "epoch": 220.81037277147487,
      "grad_norm": 1.2003282308578491,
      "learning_rate": 2.794625364904314e-05,
      "loss": 2.6761,
      "step": 681200
    },
    {
      "epoch": 220.84278768233386,
      "grad_norm": 1.2297039031982422,
      "learning_rate": 2.79430100551411e-05,
      "loss": 2.6928,
      "step": 681300
    },
    {
      "epoch": 220.87520259319288,
      "grad_norm": 1.311574101448059,
      "learning_rate": 2.7939766461239052e-05,
      "loss": 2.6435,
      "step": 681400
    },
    {
      "epoch": 220.90761750405187,
      "grad_norm": 1.2000924348831177,
      "learning_rate": 2.793652286733701e-05,
      "loss": 2.6853,
      "step": 681500
    },
    {
      "epoch": 220.94003241491086,
      "grad_norm": 1.420296549797058,
      "learning_rate": 2.7933279273434966e-05,
      "loss": 2.6646,
      "step": 681600
    },
    {
      "epoch": 220.97244732576985,
      "grad_norm": 1.2146832942962646,
      "learning_rate": 2.793003567953292e-05,
      "loss": 2.6714,
      "step": 681700
    },
    {
      "epoch": 221.0,
      "eval_bleu": 1.0364922050128638,
      "eval_loss": 4.047263145446777,
      "eval_runtime": 3.8791,
      "eval_samples_per_second": 126.833,
      "eval_steps_per_second": 2.062,
      "step": 681785
    },
    {
      "epoch": 221.00486223662884,
      "grad_norm": 1.383097767829895,
      "learning_rate": 2.792679208563088e-05,
      "loss": 2.68,
      "step": 681800
    },
    {
      "epoch": 221.03727714748786,
      "grad_norm": 1.3444831371307373,
      "learning_rate": 2.7923548491728836e-05,
      "loss": 2.6551,
      "step": 681900
    },
    {
      "epoch": 221.06969205834685,
      "grad_norm": 1.223251461982727,
      "learning_rate": 2.7920304897826794e-05,
      "loss": 2.6616,
      "step": 682000
    },
    {
      "epoch": 221.10210696920583,
      "grad_norm": 1.3852468729019165,
      "learning_rate": 2.7917061303924753e-05,
      "loss": 2.671,
      "step": 682100
    },
    {
      "epoch": 221.13452188006482,
      "grad_norm": 1.4679087400436401,
      "learning_rate": 2.7913817710022705e-05,
      "loss": 2.6624,
      "step": 682200
    },
    {
      "epoch": 221.1669367909238,
      "grad_norm": 1.2590298652648926,
      "learning_rate": 2.7910574116120664e-05,
      "loss": 2.6707,
      "step": 682300
    },
    {
      "epoch": 221.19935170178283,
      "grad_norm": 1.2077149152755737,
      "learning_rate": 2.7907330522218623e-05,
      "loss": 2.6551,
      "step": 682400
    },
    {
      "epoch": 221.23176661264182,
      "grad_norm": 1.3860211372375488,
      "learning_rate": 2.7904086928316575e-05,
      "loss": 2.6607,
      "step": 682500
    },
    {
      "epoch": 221.2641815235008,
      "grad_norm": 1.1357290744781494,
      "learning_rate": 2.7900843334414533e-05,
      "loss": 2.6634,
      "step": 682600
    },
    {
      "epoch": 221.2965964343598,
      "grad_norm": 1.2083059549331665,
      "learning_rate": 2.7897599740512485e-05,
      "loss": 2.6736,
      "step": 682700
    },
    {
      "epoch": 221.3290113452188,
      "grad_norm": 1.2928136587142944,
      "learning_rate": 2.7894356146610444e-05,
      "loss": 2.6668,
      "step": 682800
    },
    {
      "epoch": 221.3614262560778,
      "grad_norm": 1.4354612827301025,
      "learning_rate": 2.7891112552708403e-05,
      "loss": 2.6797,
      "step": 682900
    },
    {
      "epoch": 221.3938411669368,
      "grad_norm": 1.3324941396713257,
      "learning_rate": 2.7887868958806358e-05,
      "loss": 2.655,
      "step": 683000
    },
    {
      "epoch": 221.42625607779578,
      "grad_norm": 1.293453335762024,
      "learning_rate": 2.7884625364904317e-05,
      "loss": 2.6759,
      "step": 683100
    },
    {
      "epoch": 221.45867098865477,
      "grad_norm": 1.333236575126648,
      "learning_rate": 2.7881381771002276e-05,
      "loss": 2.6947,
      "step": 683200
    },
    {
      "epoch": 221.49108589951376,
      "grad_norm": 1.265039086341858,
      "learning_rate": 2.7878138177100228e-05,
      "loss": 2.6672,
      "step": 683300
    },
    {
      "epoch": 221.52350081037278,
      "grad_norm": 1.2722370624542236,
      "learning_rate": 2.7874894583198186e-05,
      "loss": 2.6606,
      "step": 683400
    },
    {
      "epoch": 221.55591572123177,
      "grad_norm": 1.1665107011795044,
      "learning_rate": 2.7871650989296145e-05,
      "loss": 2.6779,
      "step": 683500
    },
    {
      "epoch": 221.58833063209076,
      "grad_norm": 1.3102242946624756,
      "learning_rate": 2.7868407395394097e-05,
      "loss": 2.663,
      "step": 683600
    },
    {
      "epoch": 221.62074554294975,
      "grad_norm": 1.0969970226287842,
      "learning_rate": 2.7865163801492056e-05,
      "loss": 2.6673,
      "step": 683700
    },
    {
      "epoch": 221.65316045380874,
      "grad_norm": 1.3040969371795654,
      "learning_rate": 2.7861920207590008e-05,
      "loss": 2.6582,
      "step": 683800
    },
    {
      "epoch": 221.68557536466776,
      "grad_norm": 1.2340887784957886,
      "learning_rate": 2.7858676613687966e-05,
      "loss": 2.6585,
      "step": 683900
    },
    {
      "epoch": 221.71799027552674,
      "grad_norm": 1.431254506111145,
      "learning_rate": 2.7855433019785925e-05,
      "loss": 2.6555,
      "step": 684000
    },
    {
      "epoch": 221.75040518638573,
      "grad_norm": 1.3647100925445557,
      "learning_rate": 2.785218942588388e-05,
      "loss": 2.6757,
      "step": 684100
    },
    {
      "epoch": 221.78282009724472,
      "grad_norm": 1.1259065866470337,
      "learning_rate": 2.7848945831981836e-05,
      "loss": 2.6471,
      "step": 684200
    },
    {
      "epoch": 221.81523500810374,
      "grad_norm": 1.2352948188781738,
      "learning_rate": 2.7845734674018814e-05,
      "loss": 2.6726,
      "step": 684300
    },
    {
      "epoch": 221.84764991896273,
      "grad_norm": 1.0744569301605225,
      "learning_rate": 2.7842491080116772e-05,
      "loss": 2.6694,
      "step": 684400
    },
    {
      "epoch": 221.88006482982172,
      "grad_norm": 1.3267263174057007,
      "learning_rate": 2.7839247486214724e-05,
      "loss": 2.6726,
      "step": 684500
    },
    {
      "epoch": 221.9124797406807,
      "grad_norm": 1.253415822982788,
      "learning_rate": 2.7836003892312683e-05,
      "loss": 2.6622,
      "step": 684600
    },
    {
      "epoch": 221.9448946515397,
      "grad_norm": 1.1190650463104248,
      "learning_rate": 2.7832760298410642e-05,
      "loss": 2.6611,
      "step": 684700
    },
    {
      "epoch": 221.97730956239872,
      "grad_norm": 1.1023017168045044,
      "learning_rate": 2.7829516704508597e-05,
      "loss": 2.6975,
      "step": 684800
    },
    {
      "epoch": 222.0,
      "eval_bleu": 0.9442853557446155,
      "eval_loss": 4.058575630187988,
      "eval_runtime": 4.1108,
      "eval_samples_per_second": 119.684,
      "eval_steps_per_second": 1.946,
      "step": 684870
    },
    {
      "epoch": 222.0097244732577,
      "grad_norm": 1.2459383010864258,
      "learning_rate": 2.7826273110606556e-05,
      "loss": 2.6487,
      "step": 684900
    },
    {
      "epoch": 222.0421393841167,
      "grad_norm": 1.0840078592300415,
      "learning_rate": 2.7823029516704508e-05,
      "loss": 2.6577,
      "step": 685000
    },
    {
      "epoch": 222.07455429497568,
      "grad_norm": 1.2838432788848877,
      "learning_rate": 2.781981835874149e-05,
      "loss": 2.6527,
      "step": 685100
    },
    {
      "epoch": 222.10696920583467,
      "grad_norm": 1.3175833225250244,
      "learning_rate": 2.781657476483944e-05,
      "loss": 2.636,
      "step": 685200
    },
    {
      "epoch": 222.1393841166937,
      "grad_norm": 1.3379850387573242,
      "learning_rate": 2.78133311709374e-05,
      "loss": 2.6629,
      "step": 685300
    },
    {
      "epoch": 222.17179902755268,
      "grad_norm": 1.2738237380981445,
      "learning_rate": 2.7810087577035355e-05,
      "loss": 2.6722,
      "step": 685400
    },
    {
      "epoch": 222.20421393841167,
      "grad_norm": 1.2604167461395264,
      "learning_rate": 2.7806843983133314e-05,
      "loss": 2.6767,
      "step": 685500
    },
    {
      "epoch": 222.23662884927066,
      "grad_norm": 1.1297197341918945,
      "learning_rate": 2.7803600389231273e-05,
      "loss": 2.6597,
      "step": 685600
    },
    {
      "epoch": 222.26904376012965,
      "grad_norm": 1.4287461042404175,
      "learning_rate": 2.7800356795329225e-05,
      "loss": 2.6767,
      "step": 685700
    },
    {
      "epoch": 222.30145867098867,
      "grad_norm": 1.4022783041000366,
      "learning_rate": 2.7797113201427183e-05,
      "loss": 2.6558,
      "step": 685800
    },
    {
      "epoch": 222.33387358184766,
      "grad_norm": 1.2275512218475342,
      "learning_rate": 2.7793869607525142e-05,
      "loss": 2.6767,
      "step": 685900
    },
    {
      "epoch": 222.36628849270664,
      "grad_norm": 1.3606337308883667,
      "learning_rate": 2.7790626013623094e-05,
      "loss": 2.673,
      "step": 686000
    },
    {
      "epoch": 222.39870340356563,
      "grad_norm": 1.2095099687576294,
      "learning_rate": 2.7787382419721053e-05,
      "loss": 2.6509,
      "step": 686100
    },
    {
      "epoch": 222.43111831442462,
      "grad_norm": 1.0742161273956299,
      "learning_rate": 2.7784138825819005e-05,
      "loss": 2.6664,
      "step": 686200
    },
    {
      "epoch": 222.46353322528364,
      "grad_norm": 1.266964316368103,
      "learning_rate": 2.7780895231916964e-05,
      "loss": 2.6717,
      "step": 686300
    },
    {
      "epoch": 222.49594813614263,
      "grad_norm": 1.3819626569747925,
      "learning_rate": 2.7777651638014922e-05,
      "loss": 2.6927,
      "step": 686400
    },
    {
      "epoch": 222.52836304700162,
      "grad_norm": 1.216394305229187,
      "learning_rate": 2.7774408044112878e-05,
      "loss": 2.6627,
      "step": 686500
    },
    {
      "epoch": 222.5607779578606,
      "grad_norm": 1.1605650186538696,
      "learning_rate": 2.7771164450210836e-05,
      "loss": 2.68,
      "step": 686600
    },
    {
      "epoch": 222.5931928687196,
      "grad_norm": 1.2425874471664429,
      "learning_rate": 2.7767920856308792e-05,
      "loss": 2.6683,
      "step": 686700
    },
    {
      "epoch": 222.62560777957862,
      "grad_norm": 1.327833652496338,
      "learning_rate": 2.7764677262406747e-05,
      "loss": 2.6909,
      "step": 686800
    },
    {
      "epoch": 222.6580226904376,
      "grad_norm": 1.4522244930267334,
      "learning_rate": 2.7761433668504706e-05,
      "loss": 2.6498,
      "step": 686900
    },
    {
      "epoch": 222.6904376012966,
      "grad_norm": 1.598743200302124,
      "learning_rate": 2.7758190074602665e-05,
      "loss": 2.6542,
      "step": 687000
    },
    {
      "epoch": 222.72285251215558,
      "grad_norm": 1.3564307689666748,
      "learning_rate": 2.7754946480700617e-05,
      "loss": 2.6708,
      "step": 687100
    },
    {
      "epoch": 222.75526742301457,
      "grad_norm": 1.0861122608184814,
      "learning_rate": 2.7751702886798575e-05,
      "loss": 2.6808,
      "step": 687200
    },
    {
      "epoch": 222.7876823338736,
      "grad_norm": 1.2076795101165771,
      "learning_rate": 2.7748459292896527e-05,
      "loss": 2.6822,
      "step": 687300
    },
    {
      "epoch": 222.82009724473258,
      "grad_norm": 1.3851819038391113,
      "learning_rate": 2.7745215698994486e-05,
      "loss": 2.6681,
      "step": 687400
    },
    {
      "epoch": 222.85251215559157,
      "grad_norm": 1.3858885765075684,
      "learning_rate": 2.7741972105092445e-05,
      "loss": 2.6464,
      "step": 687500
    },
    {
      "epoch": 222.88492706645056,
      "grad_norm": 1.248852252960205,
      "learning_rate": 2.7738728511190397e-05,
      "loss": 2.6588,
      "step": 687600
    },
    {
      "epoch": 222.91734197730958,
      "grad_norm": 1.2928255796432495,
      "learning_rate": 2.7735484917288355e-05,
      "loss": 2.6657,
      "step": 687700
    },
    {
      "epoch": 222.94975688816857,
      "grad_norm": 1.3623881340026855,
      "learning_rate": 2.7732241323386314e-05,
      "loss": 2.6603,
      "step": 687800
    },
    {
      "epoch": 222.98217179902755,
      "grad_norm": 1.3444010019302368,
      "learning_rate": 2.772899772948427e-05,
      "loss": 2.6837,
      "step": 687900
    },
    {
      "epoch": 223.0,
      "eval_bleu": 0.9424050098263557,
      "eval_loss": 4.052264213562012,
      "eval_runtime": 3.8852,
      "eval_samples_per_second": 126.635,
      "eval_steps_per_second": 2.059,
      "step": 687955
    },
    {
      "epoch": 223.01458670988654,
      "grad_norm": 1.2024612426757812,
      "learning_rate": 2.772575413558223e-05,
      "loss": 2.6548,
      "step": 688000
    },
    {
      "epoch": 223.04700162074553,
      "grad_norm": 1.5214091539382935,
      "learning_rate": 2.7722510541680187e-05,
      "loss": 2.669,
      "step": 688100
    },
    {
      "epoch": 223.07941653160455,
      "grad_norm": 1.392193078994751,
      "learning_rate": 2.771926694777814e-05,
      "loss": 2.6576,
      "step": 688200
    },
    {
      "epoch": 223.11183144246354,
      "grad_norm": 1.1940946578979492,
      "learning_rate": 2.7716023353876098e-05,
      "loss": 2.6576,
      "step": 688300
    },
    {
      "epoch": 223.14424635332253,
      "grad_norm": 1.2223440408706665,
      "learning_rate": 2.771277975997405e-05,
      "loss": 2.6782,
      "step": 688400
    },
    {
      "epoch": 223.17666126418152,
      "grad_norm": 1.2267780303955078,
      "learning_rate": 2.770953616607201e-05,
      "loss": 2.63,
      "step": 688500
    },
    {
      "epoch": 223.2090761750405,
      "grad_norm": 1.5859174728393555,
      "learning_rate": 2.7706292572169967e-05,
      "loss": 2.6825,
      "step": 688600
    },
    {
      "epoch": 223.24149108589953,
      "grad_norm": 1.2254170179367065,
      "learning_rate": 2.770304897826792e-05,
      "loss": 2.6895,
      "step": 688700
    },
    {
      "epoch": 223.27390599675851,
      "grad_norm": 1.2045446634292603,
      "learning_rate": 2.7699805384365878e-05,
      "loss": 2.6697,
      "step": 688800
    },
    {
      "epoch": 223.3063209076175,
      "grad_norm": 1.3185126781463623,
      "learning_rate": 2.7696561790463837e-05,
      "loss": 2.6703,
      "step": 688900
    },
    {
      "epoch": 223.3387358184765,
      "grad_norm": 1.191498041152954,
      "learning_rate": 2.7693318196561792e-05,
      "loss": 2.6596,
      "step": 689000
    },
    {
      "epoch": 223.37115072933548,
      "grad_norm": 1.283889889717102,
      "learning_rate": 2.7690107038598766e-05,
      "loss": 2.6624,
      "step": 689100
    },
    {
      "epoch": 223.4035656401945,
      "grad_norm": 1.243609070777893,
      "learning_rate": 2.7686863444696725e-05,
      "loss": 2.6471,
      "step": 689200
    },
    {
      "epoch": 223.4359805510535,
      "grad_norm": 1.2392973899841309,
      "learning_rate": 2.7683619850794684e-05,
      "loss": 2.6747,
      "step": 689300
    },
    {
      "epoch": 223.46839546191248,
      "grad_norm": 1.2689794301986694,
      "learning_rate": 2.7680376256892636e-05,
      "loss": 2.6661,
      "step": 689400
    },
    {
      "epoch": 223.50081037277147,
      "grad_norm": 1.2339152097702026,
      "learning_rate": 2.7677132662990595e-05,
      "loss": 2.655,
      "step": 689500
    },
    {
      "epoch": 223.53322528363046,
      "grad_norm": 1.421256184577942,
      "learning_rate": 2.767388906908855e-05,
      "loss": 2.6659,
      "step": 689600
    },
    {
      "epoch": 223.56564019448948,
      "grad_norm": 1.2020858526229858,
      "learning_rate": 2.767064547518651e-05,
      "loss": 2.671,
      "step": 689700
    },
    {
      "epoch": 223.59805510534846,
      "grad_norm": 1.2312241792678833,
      "learning_rate": 2.7667401881284467e-05,
      "loss": 2.6629,
      "step": 689800
    },
    {
      "epoch": 223.63047001620745,
      "grad_norm": 1.3403289318084717,
      "learning_rate": 2.766415828738242e-05,
      "loss": 2.6528,
      "step": 689900
    },
    {
      "epoch": 223.66288492706644,
      "grad_norm": 1.1951326131820679,
      "learning_rate": 2.7660914693480378e-05,
      "loss": 2.6901,
      "step": 690000
    },
    {
      "epoch": 223.69529983792543,
      "grad_norm": 1.255020022392273,
      "learning_rate": 2.7657671099578337e-05,
      "loss": 2.6745,
      "step": 690100
    },
    {
      "epoch": 223.72771474878445,
      "grad_norm": 1.2017359733581543,
      "learning_rate": 2.765442750567629e-05,
      "loss": 2.6568,
      "step": 690200
    },
    {
      "epoch": 223.76012965964344,
      "grad_norm": 1.3454056978225708,
      "learning_rate": 2.7651183911774248e-05,
      "loss": 2.6755,
      "step": 690300
    },
    {
      "epoch": 223.79254457050243,
      "grad_norm": 1.3051466941833496,
      "learning_rate": 2.7647940317872206e-05,
      "loss": 2.6582,
      "step": 690400
    },
    {
      "epoch": 223.82495948136142,
      "grad_norm": 1.3565559387207031,
      "learning_rate": 2.764469672397016e-05,
      "loss": 2.6431,
      "step": 690500
    },
    {
      "epoch": 223.8573743922204,
      "grad_norm": 1.3594352006912231,
      "learning_rate": 2.7641453130068117e-05,
      "loss": 2.6432,
      "step": 690600
    },
    {
      "epoch": 223.88978930307943,
      "grad_norm": 1.2398945093154907,
      "learning_rate": 2.7638209536166072e-05,
      "loss": 2.6831,
      "step": 690700
    },
    {
      "epoch": 223.92220421393841,
      "grad_norm": 1.2131375074386597,
      "learning_rate": 2.7634965942264028e-05,
      "loss": 2.6488,
      "step": 690800
    },
    {
      "epoch": 223.9546191247974,
      "grad_norm": 1.2822837829589844,
      "learning_rate": 2.7631722348361987e-05,
      "loss": 2.6831,
      "step": 690900
    },
    {
      "epoch": 223.9870340356564,
      "grad_norm": 1.2859644889831543,
      "learning_rate": 2.7628478754459942e-05,
      "loss": 2.6678,
      "step": 691000
    },
    {
      "epoch": 224.0,
      "eval_bleu": 1.0101838601392734,
      "eval_loss": 4.060714244842529,
      "eval_runtime": 4.2946,
      "eval_samples_per_second": 114.563,
      "eval_steps_per_second": 1.863,
      "step": 691040
    },
    {
      "epoch": 224.0194489465154,
      "grad_norm": Infinity,
      "learning_rate": 2.76252351605579e-05,
      "loss": 2.6515,
      "step": 691100
    },
    {
      "epoch": 224.0518638573744,
      "grad_norm": 1.5118545293807983,
      "learning_rate": 2.7622024002594875e-05,
      "loss": 2.6587,
      "step": 691200
    },
    {
      "epoch": 224.0842787682334,
      "grad_norm": 1.3832932710647583,
      "learning_rate": 2.7618780408692834e-05,
      "loss": 2.6615,
      "step": 691300
    },
    {
      "epoch": 224.11669367909238,
      "grad_norm": 1.266413927078247,
      "learning_rate": 2.761553681479079e-05,
      "loss": 2.651,
      "step": 691400
    },
    {
      "epoch": 224.14910858995137,
      "grad_norm": 1.3747926950454712,
      "learning_rate": 2.7612293220888748e-05,
      "loss": 2.6516,
      "step": 691500
    },
    {
      "epoch": 224.18152350081039,
      "grad_norm": 1.2851736545562744,
      "learning_rate": 2.7609049626986703e-05,
      "loss": 2.6356,
      "step": 691600
    },
    {
      "epoch": 224.21393841166937,
      "grad_norm": 1.1206109523773193,
      "learning_rate": 2.760580603308466e-05,
      "loss": 2.6706,
      "step": 691700
    },
    {
      "epoch": 224.24635332252836,
      "grad_norm": 1.338396668434143,
      "learning_rate": 2.7602562439182617e-05,
      "loss": 2.668,
      "step": 691800
    },
    {
      "epoch": 224.27876823338735,
      "grad_norm": 1.171968936920166,
      "learning_rate": 2.759931884528057e-05,
      "loss": 2.6787,
      "step": 691900
    },
    {
      "epoch": 224.31118314424634,
      "grad_norm": 1.172008752822876,
      "learning_rate": 2.7596075251378528e-05,
      "loss": 2.6474,
      "step": 692000
    },
    {
      "epoch": 224.34359805510536,
      "grad_norm": 1.2383450269699097,
      "learning_rate": 2.7592831657476487e-05,
      "loss": 2.6482,
      "step": 692100
    },
    {
      "epoch": 224.37601296596435,
      "grad_norm": 1.3052067756652832,
      "learning_rate": 2.758958806357444e-05,
      "loss": 2.6618,
      "step": 692200
    },
    {
      "epoch": 224.40842787682334,
      "grad_norm": 1.2745378017425537,
      "learning_rate": 2.7586344469672398e-05,
      "loss": 2.6622,
      "step": 692300
    },
    {
      "epoch": 224.44084278768233,
      "grad_norm": 1.2555643320083618,
      "learning_rate": 2.7583100875770356e-05,
      "loss": 2.681,
      "step": 692400
    },
    {
      "epoch": 224.47325769854132,
      "grad_norm": 1.2429499626159668,
      "learning_rate": 2.7579857281868308e-05,
      "loss": 2.666,
      "step": 692500
    },
    {
      "epoch": 224.50567260940034,
      "grad_norm": 1.2504065036773682,
      "learning_rate": 2.7576613687966267e-05,
      "loss": 2.6762,
      "step": 692600
    },
    {
      "epoch": 224.53808752025932,
      "grad_norm": 1.2409968376159668,
      "learning_rate": 2.7573370094064226e-05,
      "loss": 2.6436,
      "step": 692700
    },
    {
      "epoch": 224.5705024311183,
      "grad_norm": 1.2723848819732666,
      "learning_rate": 2.757012650016218e-05,
      "loss": 2.6549,
      "step": 692800
    },
    {
      "epoch": 224.6029173419773,
      "grad_norm": 1.2200684547424316,
      "learning_rate": 2.756688290626014e-05,
      "loss": 2.6802,
      "step": 692900
    },
    {
      "epoch": 224.6353322528363,
      "grad_norm": 1.4995230436325073,
      "learning_rate": 2.7563639312358092e-05,
      "loss": 2.6592,
      "step": 693000
    },
    {
      "epoch": 224.6677471636953,
      "grad_norm": 1.1987181901931763,
      "learning_rate": 2.756039571845605e-05,
      "loss": 2.6517,
      "step": 693100
    },
    {
      "epoch": 224.7001620745543,
      "grad_norm": 1.3727846145629883,
      "learning_rate": 2.755718456049303e-05,
      "loss": 2.6708,
      "step": 693200
    },
    {
      "epoch": 224.7325769854133,
      "grad_norm": 1.1736544370651245,
      "learning_rate": 2.7553940966590984e-05,
      "loss": 2.6714,
      "step": 693300
    },
    {
      "epoch": 224.76499189627228,
      "grad_norm": 1.328493595123291,
      "learning_rate": 2.755069737268894e-05,
      "loss": 2.653,
      "step": 693400
    },
    {
      "epoch": 224.79740680713127,
      "grad_norm": 1.25771963596344,
      "learning_rate": 2.7547453778786898e-05,
      "loss": 2.6848,
      "step": 693500
    },
    {
      "epoch": 224.82982171799028,
      "grad_norm": 1.1823348999023438,
      "learning_rate": 2.7544210184884857e-05,
      "loss": 2.6776,
      "step": 693600
    },
    {
      "epoch": 224.86223662884927,
      "grad_norm": 1.4752967357635498,
      "learning_rate": 2.754096659098281e-05,
      "loss": 2.651,
      "step": 693700
    },
    {
      "epoch": 224.89465153970826,
      "grad_norm": 1.4336878061294556,
      "learning_rate": 2.7537722997080767e-05,
      "loss": 2.654,
      "step": 693800
    },
    {
      "epoch": 224.92706645056725,
      "grad_norm": 1.101402997970581,
      "learning_rate": 2.7534479403178726e-05,
      "loss": 2.6639,
      "step": 693900
    },
    {
      "epoch": 224.95948136142624,
      "grad_norm": 1.328392505645752,
      "learning_rate": 2.7531235809276678e-05,
      "loss": 2.6623,
      "step": 694000
    },
    {
      "epoch": 224.99189627228526,
      "grad_norm": 1.3673450946807861,
      "learning_rate": 2.7527992215374637e-05,
      "loss": 2.692,
      "step": 694100
    },
    {
      "epoch": 225.0,
      "eval_bleu": 0.9877172596142997,
      "eval_loss": 4.064535140991211,
      "eval_runtime": 4.2594,
      "eval_samples_per_second": 115.51,
      "eval_steps_per_second": 1.878,
      "step": 694125
    },
    {
      "epoch": 225.02431118314425,
      "grad_norm": 1.2300989627838135,
      "learning_rate": 2.752474862147259e-05,
      "loss": 2.6778,
      "step": 694200
    },
    {
      "epoch": 225.05672609400324,
      "grad_norm": 1.4004267454147339,
      "learning_rate": 2.7521505027570547e-05,
      "loss": 2.6507,
      "step": 694300
    },
    {
      "epoch": 225.08914100486223,
      "grad_norm": 1.2105803489685059,
      "learning_rate": 2.7518261433668506e-05,
      "loss": 2.6382,
      "step": 694400
    },
    {
      "epoch": 225.12155591572125,
      "grad_norm": 1.2988032102584839,
      "learning_rate": 2.751501783976646e-05,
      "loss": 2.6546,
      "step": 694500
    },
    {
      "epoch": 225.15397082658023,
      "grad_norm": 1.1929123401641846,
      "learning_rate": 2.751177424586442e-05,
      "loss": 2.6647,
      "step": 694600
    },
    {
      "epoch": 225.18638573743922,
      "grad_norm": 1.2849256992340088,
      "learning_rate": 2.750853065196238e-05,
      "loss": 2.6482,
      "step": 694700
    },
    {
      "epoch": 225.2188006482982,
      "grad_norm": 1.1865609884262085,
      "learning_rate": 2.750528705806033e-05,
      "loss": 2.6823,
      "step": 694800
    },
    {
      "epoch": 225.2512155591572,
      "grad_norm": 1.1796441078186035,
      "learning_rate": 2.750204346415829e-05,
      "loss": 2.665,
      "step": 694900
    },
    {
      "epoch": 225.28363047001622,
      "grad_norm": 1.2784799337387085,
      "learning_rate": 2.749879987025625e-05,
      "loss": 2.6512,
      "step": 695000
    },
    {
      "epoch": 225.3160453808752,
      "grad_norm": 1.1098347902297974,
      "learning_rate": 2.74955562763542e-05,
      "loss": 2.6803,
      "step": 695100
    },
    {
      "epoch": 225.3484602917342,
      "grad_norm": 1.1227056980133057,
      "learning_rate": 2.7492345118391178e-05,
      "loss": 2.6814,
      "step": 695200
    },
    {
      "epoch": 225.3808752025932,
      "grad_norm": 1.2231100797653198,
      "learning_rate": 2.7489101524489137e-05,
      "loss": 2.6608,
      "step": 695300
    },
    {
      "epoch": 225.41329011345218,
      "grad_norm": 1.1760302782058716,
      "learning_rate": 2.7485857930587096e-05,
      "loss": 2.6654,
      "step": 695400
    },
    {
      "epoch": 225.4457050243112,
      "grad_norm": 1.4094606637954712,
      "learning_rate": 2.7482614336685048e-05,
      "loss": 2.6417,
      "step": 695500
    },
    {
      "epoch": 225.47811993517018,
      "grad_norm": 1.2488611936569214,
      "learning_rate": 2.7479370742783006e-05,
      "loss": 2.6693,
      "step": 695600
    },
    {
      "epoch": 225.51053484602917,
      "grad_norm": 1.2318190336227417,
      "learning_rate": 2.747612714888096e-05,
      "loss": 2.6518,
      "step": 695700
    },
    {
      "epoch": 225.54294975688816,
      "grad_norm": 1.2640173435211182,
      "learning_rate": 2.7472883554978917e-05,
      "loss": 2.6486,
      "step": 695800
    },
    {
      "epoch": 225.57536466774715,
      "grad_norm": 1.260486364364624,
      "learning_rate": 2.7469639961076876e-05,
      "loss": 2.6513,
      "step": 695900
    },
    {
      "epoch": 225.60777957860617,
      "grad_norm": 1.2284517288208008,
      "learning_rate": 2.7466396367174828e-05,
      "loss": 2.6837,
      "step": 696000
    },
    {
      "epoch": 225.64019448946516,
      "grad_norm": 1.3208608627319336,
      "learning_rate": 2.7463152773272787e-05,
      "loss": 2.6429,
      "step": 696100
    },
    {
      "epoch": 225.67260940032415,
      "grad_norm": 1.3520355224609375,
      "learning_rate": 2.7459909179370745e-05,
      "loss": 2.6659,
      "step": 696200
    },
    {
      "epoch": 225.70502431118314,
      "grad_norm": 1.2446826696395874,
      "learning_rate": 2.74566655854687e-05,
      "loss": 2.6518,
      "step": 696300
    },
    {
      "epoch": 225.73743922204213,
      "grad_norm": 1.2504452466964722,
      "learning_rate": 2.745342199156666e-05,
      "loss": 2.6681,
      "step": 696400
    },
    {
      "epoch": 225.76985413290114,
      "grad_norm": 1.3568265438079834,
      "learning_rate": 2.745017839766461e-05,
      "loss": 2.6668,
      "step": 696500
    },
    {
      "epoch": 225.80226904376013,
      "grad_norm": 1.2791575193405151,
      "learning_rate": 2.744693480376257e-05,
      "loss": 2.6697,
      "step": 696600
    },
    {
      "epoch": 225.83468395461912,
      "grad_norm": 1.2698687314987183,
      "learning_rate": 2.744369120986053e-05,
      "loss": 2.6626,
      "step": 696700
    },
    {
      "epoch": 225.8670988654781,
      "grad_norm": 1.3420262336730957,
      "learning_rate": 2.744044761595848e-05,
      "loss": 2.6566,
      "step": 696800
    },
    {
      "epoch": 225.8995137763371,
      "grad_norm": 1.2937644720077515,
      "learning_rate": 2.743720402205644e-05,
      "loss": 2.6647,
      "step": 696900
    },
    {
      "epoch": 225.93192868719612,
      "grad_norm": 1.1062732934951782,
      "learning_rate": 2.7433960428154398e-05,
      "loss": 2.6334,
      "step": 697000
    },
    {
      "epoch": 225.9643435980551,
      "grad_norm": 1.2478439807891846,
      "learning_rate": 2.743071683425235e-05,
      "loss": 2.67,
      "step": 697100
    },
    {
      "epoch": 225.9967585089141,
      "grad_norm": 1.1641638278961182,
      "learning_rate": 2.7427505676289328e-05,
      "loss": 2.6804,
      "step": 697200
    },
    {
      "epoch": 226.0,
      "eval_bleu": 0.9641206284933683,
      "eval_loss": 4.046017646789551,
      "eval_runtime": 3.8978,
      "eval_samples_per_second": 126.226,
      "eval_steps_per_second": 2.052,
      "step": 697210
    },
    {
      "epoch": 226.0291734197731,
      "grad_norm": 1.44881272315979,
      "learning_rate": 2.7424262082387287e-05,
      "loss": 2.648,
      "step": 697300
    },
    {
      "epoch": 226.06158833063208,
      "grad_norm": 1.201947808265686,
      "learning_rate": 2.7421018488485246e-05,
      "loss": 2.6775,
      "step": 697400
    },
    {
      "epoch": 226.0940032414911,
      "grad_norm": 1.202575445175171,
      "learning_rate": 2.7417774894583198e-05,
      "loss": 2.6664,
      "step": 697500
    },
    {
      "epoch": 226.12641815235008,
      "grad_norm": 1.2166600227355957,
      "learning_rate": 2.7414531300681156e-05,
      "loss": 2.6437,
      "step": 697600
    },
    {
      "epoch": 226.15883306320907,
      "grad_norm": 1.3934754133224487,
      "learning_rate": 2.7411320142718134e-05,
      "loss": 2.6718,
      "step": 697700
    },
    {
      "epoch": 226.19124797406806,
      "grad_norm": 1.465412974357605,
      "learning_rate": 2.7408076548816093e-05,
      "loss": 2.6676,
      "step": 697800
    },
    {
      "epoch": 226.22366288492708,
      "grad_norm": 1.2546566724777222,
      "learning_rate": 2.7404832954914045e-05,
      "loss": 2.6619,
      "step": 697900
    },
    {
      "epoch": 226.25607779578607,
      "grad_norm": 1.2135785818099976,
      "learning_rate": 2.7401589361012004e-05,
      "loss": 2.6596,
      "step": 698000
    },
    {
      "epoch": 226.28849270664506,
      "grad_norm": 1.446702003479004,
      "learning_rate": 2.7398345767109955e-05,
      "loss": 2.6573,
      "step": 698100
    },
    {
      "epoch": 226.32090761750405,
      "grad_norm": 1.19196355342865,
      "learning_rate": 2.7395102173207914e-05,
      "loss": 2.6694,
      "step": 698200
    },
    {
      "epoch": 226.35332252836304,
      "grad_norm": 1.2789809703826904,
      "learning_rate": 2.7391858579305873e-05,
      "loss": 2.6594,
      "step": 698300
    },
    {
      "epoch": 226.38573743922205,
      "grad_norm": 1.2129511833190918,
      "learning_rate": 2.7388614985403825e-05,
      "loss": 2.6644,
      "step": 698400
    },
    {
      "epoch": 226.41815235008104,
      "grad_norm": 1.1429930925369263,
      "learning_rate": 2.7385371391501784e-05,
      "loss": 2.6546,
      "step": 698500
    },
    {
      "epoch": 226.45056726094003,
      "grad_norm": 1.1951420307159424,
      "learning_rate": 2.7382127797599742e-05,
      "loss": 2.6506,
      "step": 698600
    },
    {
      "epoch": 226.48298217179902,
      "grad_norm": 1.1550483703613281,
      "learning_rate": 2.7378884203697698e-05,
      "loss": 2.6744,
      "step": 698700
    },
    {
      "epoch": 226.515397082658,
      "grad_norm": 1.275910496711731,
      "learning_rate": 2.7375640609795657e-05,
      "loss": 2.6351,
      "step": 698800
    },
    {
      "epoch": 226.54781199351703,
      "grad_norm": 1.1043012142181396,
      "learning_rate": 2.7372397015893615e-05,
      "loss": 2.6504,
      "step": 698900
    },
    {
      "epoch": 226.58022690437602,
      "grad_norm": 1.3721486330032349,
      "learning_rate": 2.7369153421991567e-05,
      "loss": 2.6738,
      "step": 699000
    },
    {
      "epoch": 226.612641815235,
      "grad_norm": 1.3401201963424683,
      "learning_rate": 2.7365909828089526e-05,
      "loss": 2.6561,
      "step": 699100
    },
    {
      "epoch": 226.645056726094,
      "grad_norm": 1.3126351833343506,
      "learning_rate": 2.7362666234187478e-05,
      "loss": 2.6659,
      "step": 699200
    },
    {
      "epoch": 226.677471636953,
      "grad_norm": 1.266931414604187,
      "learning_rate": 2.7359422640285437e-05,
      "loss": 2.6519,
      "step": 699300
    },
    {
      "epoch": 226.709886547812,
      "grad_norm": 1.5103075504302979,
      "learning_rate": 2.7356179046383395e-05,
      "loss": 2.6647,
      "step": 699400
    },
    {
      "epoch": 226.742301458671,
      "grad_norm": 1.2530027627944946,
      "learning_rate": 2.7352935452481347e-05,
      "loss": 2.6721,
      "step": 699500
    },
    {
      "epoch": 226.77471636952998,
      "grad_norm": 1.287428379058838,
      "learning_rate": 2.7349691858579306e-05,
      "loss": 2.6526,
      "step": 699600
    },
    {
      "epoch": 226.80713128038897,
      "grad_norm": 1.310928463935852,
      "learning_rate": 2.7346448264677265e-05,
      "loss": 2.653,
      "step": 699700
    },
    {
      "epoch": 226.83954619124796,
      "grad_norm": 1.438746690750122,
      "learning_rate": 2.734320467077522e-05,
      "loss": 2.6534,
      "step": 699800
    },
    {
      "epoch": 226.87196110210698,
      "grad_norm": 1.220906376838684,
      "learning_rate": 2.733996107687318e-05,
      "loss": 2.6497,
      "step": 699900
    },
    {
      "epoch": 226.90437601296597,
      "grad_norm": 1.1473429203033447,
      "learning_rate": 2.7336717482971134e-05,
      "loss": 2.6661,
      "step": 700000
    },
    {
      "epoch": 226.93679092382496,
      "grad_norm": 1.2387566566467285,
      "learning_rate": 2.733347388906909e-05,
      "loss": 2.6592,
      "step": 700100
    },
    {
      "epoch": 226.96920583468395,
      "grad_norm": 1.560476541519165,
      "learning_rate": 2.7330262731106064e-05,
      "loss": 2.6605,
      "step": 700200
    },
    {
      "epoch": 227.0,
      "eval_bleu": 0.9918528095781114,
      "eval_loss": 4.0605854988098145,
      "eval_runtime": 4.0675,
      "eval_samples_per_second": 120.959,
      "eval_steps_per_second": 1.967,
      "step": 700295
    },
    {
      "epoch": 227.00162074554294,
      "grad_norm": 1.6247868537902832,
      "learning_rate": 2.7327019137204023e-05,
      "loss": 2.6548,
      "step": 700300
    },
    {
      "epoch": 227.03403565640195,
      "grad_norm": 1.2293304204940796,
      "learning_rate": 2.732377554330198e-05,
      "loss": 2.6479,
      "step": 700400
    },
    {
      "epoch": 227.06645056726094,
      "grad_norm": 1.3019018173217773,
      "learning_rate": 2.7320531949399937e-05,
      "loss": 2.6441,
      "step": 700500
    },
    {
      "epoch": 227.09886547811993,
      "grad_norm": 1.2164497375488281,
      "learning_rate": 2.7317288355497896e-05,
      "loss": 2.6503,
      "step": 700600
    },
    {
      "epoch": 227.13128038897892,
      "grad_norm": 1.1471641063690186,
      "learning_rate": 2.7314044761595848e-05,
      "loss": 2.6481,
      "step": 700700
    },
    {
      "epoch": 227.1636952998379,
      "grad_norm": 1.201943039894104,
      "learning_rate": 2.7310801167693806e-05,
      "loss": 2.6525,
      "step": 700800
    },
    {
      "epoch": 227.19611021069693,
      "grad_norm": 1.193585753440857,
      "learning_rate": 2.7307557573791765e-05,
      "loss": 2.659,
      "step": 700900
    },
    {
      "epoch": 227.22852512155592,
      "grad_norm": 1.4420945644378662,
      "learning_rate": 2.7304313979889717e-05,
      "loss": 2.6508,
      "step": 701000
    },
    {
      "epoch": 227.2609400324149,
      "grad_norm": 1.3718518018722534,
      "learning_rate": 2.7301070385987676e-05,
      "loss": 2.6687,
      "step": 701100
    },
    {
      "epoch": 227.2933549432739,
      "grad_norm": 1.2105573415756226,
      "learning_rate": 2.7297826792085635e-05,
      "loss": 2.6436,
      "step": 701200
    },
    {
      "epoch": 227.32576985413291,
      "grad_norm": 1.1239463090896606,
      "learning_rate": 2.7294583198183587e-05,
      "loss": 2.6568,
      "step": 701300
    },
    {
      "epoch": 227.3581847649919,
      "grad_norm": 1.287890076637268,
      "learning_rate": 2.7291339604281545e-05,
      "loss": 2.6298,
      "step": 701400
    },
    {
      "epoch": 227.3905996758509,
      "grad_norm": 1.1051543951034546,
      "learning_rate": 2.72880960103795e-05,
      "loss": 2.6651,
      "step": 701500
    },
    {
      "epoch": 227.42301458670988,
      "grad_norm": 1.404408574104309,
      "learning_rate": 2.728485241647746e-05,
      "loss": 2.6681,
      "step": 701600
    },
    {
      "epoch": 227.45542949756887,
      "grad_norm": 1.2411249876022339,
      "learning_rate": 2.7281608822575415e-05,
      "loss": 2.6738,
      "step": 701700
    },
    {
      "epoch": 227.4878444084279,
      "grad_norm": 1.3321781158447266,
      "learning_rate": 2.727836522867337e-05,
      "loss": 2.6607,
      "step": 701800
    },
    {
      "epoch": 227.52025931928688,
      "grad_norm": 1.4380041360855103,
      "learning_rate": 2.727512163477133e-05,
      "loss": 2.6605,
      "step": 701900
    },
    {
      "epoch": 227.55267423014587,
      "grad_norm": 1.1324772834777832,
      "learning_rate": 2.7271878040869288e-05,
      "loss": 2.6578,
      "step": 702000
    },
    {
      "epoch": 227.58508914100486,
      "grad_norm": 1.428140640258789,
      "learning_rate": 2.726863444696724e-05,
      "loss": 2.6526,
      "step": 702100
    },
    {
      "epoch": 227.61750405186385,
      "grad_norm": 1.1142443418502808,
      "learning_rate": 2.7265390853065198e-05,
      "loss": 2.6466,
      "step": 702200
    },
    {
      "epoch": 227.64991896272286,
      "grad_norm": 1.404462456703186,
      "learning_rate": 2.7262147259163157e-05,
      "loss": 2.6859,
      "step": 702300
    },
    {
      "epoch": 227.68233387358185,
      "grad_norm": 1.1374753713607788,
      "learning_rate": 2.725890366526111e-05,
      "loss": 2.6547,
      "step": 702400
    },
    {
      "epoch": 227.71474878444084,
      "grad_norm": 1.2079910039901733,
      "learning_rate": 2.7255660071359068e-05,
      "loss": 2.6743,
      "step": 702500
    },
    {
      "epoch": 227.74716369529983,
      "grad_norm": 1.3486881256103516,
      "learning_rate": 2.725241647745702e-05,
      "loss": 2.6438,
      "step": 702600
    },
    {
      "epoch": 227.77957860615882,
      "grad_norm": 1.3399971723556519,
      "learning_rate": 2.724917288355498e-05,
      "loss": 2.6455,
      "step": 702700
    },
    {
      "epoch": 227.81199351701784,
      "grad_norm": 1.2338099479675293,
      "learning_rate": 2.7245929289652937e-05,
      "loss": 2.6579,
      "step": 702800
    },
    {
      "epoch": 227.84440842787683,
      "grad_norm": 1.1814801692962646,
      "learning_rate": 2.7242685695750893e-05,
      "loss": 2.6615,
      "step": 702900
    },
    {
      "epoch": 227.87682333873582,
      "grad_norm": 1.2106372117996216,
      "learning_rate": 2.723944210184885e-05,
      "loss": 2.6642,
      "step": 703000
    },
    {
      "epoch": 227.9092382495948,
      "grad_norm": 1.5179476737976074,
      "learning_rate": 2.723619850794681e-05,
      "loss": 2.6655,
      "step": 703100
    },
    {
      "epoch": 227.9416531604538,
      "grad_norm": 1.3565163612365723,
      "learning_rate": 2.7232954914044762e-05,
      "loss": 2.6863,
      "step": 703200
    },
    {
      "epoch": 227.97406807131281,
      "grad_norm": 1.1426985263824463,
      "learning_rate": 2.722971132014272e-05,
      "loss": 2.6491,
      "step": 703300
    },
    {
      "epoch": 228.0,
      "eval_bleu": 1.0971712867279804,
      "eval_loss": 4.063529968261719,
      "eval_runtime": 4.3301,
      "eval_samples_per_second": 113.624,
      "eval_steps_per_second": 1.848,
      "step": 703380
    },
    {
      "epoch": 228.0064829821718,
      "grad_norm": 1.2008413076400757,
      "learning_rate": 2.722646772624068e-05,
      "loss": 2.655,
      "step": 703400
    },
    {
      "epoch": 228.0388978930308,
      "grad_norm": 1.4087072610855103,
      "learning_rate": 2.722322413233863e-05,
      "loss": 2.6468,
      "step": 703500
    },
    {
      "epoch": 228.07131280388978,
      "grad_norm": 1.1698826551437378,
      "learning_rate": 2.721998053843659e-05,
      "loss": 2.6435,
      "step": 703600
    },
    {
      "epoch": 228.10372771474877,
      "grad_norm": 1.3543211221694946,
      "learning_rate": 2.7216736944534542e-05,
      "loss": 2.6477,
      "step": 703700
    },
    {
      "epoch": 228.1361426256078,
      "grad_norm": 1.3200584650039673,
      "learning_rate": 2.72134933506325e-05,
      "loss": 2.6406,
      "step": 703800
    },
    {
      "epoch": 228.16855753646678,
      "grad_norm": 1.21198570728302,
      "learning_rate": 2.721024975673046e-05,
      "loss": 2.6544,
      "step": 703900
    },
    {
      "epoch": 228.20097244732577,
      "grad_norm": 1.2389090061187744,
      "learning_rate": 2.7207006162828415e-05,
      "loss": 2.648,
      "step": 704000
    },
    {
      "epoch": 228.23338735818476,
      "grad_norm": 1.2217340469360352,
      "learning_rate": 2.720379500486539e-05,
      "loss": 2.6399,
      "step": 704100
    },
    {
      "epoch": 228.26580226904375,
      "grad_norm": 1.201720952987671,
      "learning_rate": 2.7200551410963348e-05,
      "loss": 2.6379,
      "step": 704200
    },
    {
      "epoch": 228.29821717990276,
      "grad_norm": 1.1572673320770264,
      "learning_rate": 2.7197307817061307e-05,
      "loss": 2.657,
      "step": 704300
    },
    {
      "epoch": 228.33063209076175,
      "grad_norm": 1.1831706762313843,
      "learning_rate": 2.719406422315926e-05,
      "loss": 2.6749,
      "step": 704400
    },
    {
      "epoch": 228.36304700162074,
      "grad_norm": 1.1718921661376953,
      "learning_rate": 2.7190820629257218e-05,
      "loss": 2.6481,
      "step": 704500
    },
    {
      "epoch": 228.39546191247973,
      "grad_norm": 1.2671772241592407,
      "learning_rate": 2.7187577035355176e-05,
      "loss": 2.6635,
      "step": 704600
    },
    {
      "epoch": 228.42787682333875,
      "grad_norm": 1.2687485218048096,
      "learning_rate": 2.7184333441453132e-05,
      "loss": 2.6537,
      "step": 704700
    },
    {
      "epoch": 228.46029173419774,
      "grad_norm": 1.338216781616211,
      "learning_rate": 2.718108984755109e-05,
      "loss": 2.6584,
      "step": 704800
    },
    {
      "epoch": 228.49270664505673,
      "grad_norm": 1.2537245750427246,
      "learning_rate": 2.7177846253649042e-05,
      "loss": 2.65,
      "step": 704900
    },
    {
      "epoch": 228.52512155591572,
      "grad_norm": 1.4676247835159302,
      "learning_rate": 2.7174602659747e-05,
      "loss": 2.6638,
      "step": 705000
    },
    {
      "epoch": 228.5575364667747,
      "grad_norm": 1.330759882926941,
      "learning_rate": 2.717135906584496e-05,
      "loss": 2.6575,
      "step": 705100
    },
    {
      "epoch": 228.58995137763372,
      "grad_norm": 1.1544368267059326,
      "learning_rate": 2.7168115471942912e-05,
      "loss": 2.6469,
      "step": 705200
    },
    {
      "epoch": 228.6223662884927,
      "grad_norm": 1.2478671073913574,
      "learning_rate": 2.716487187804087e-05,
      "loss": 2.6489,
      "step": 705300
    },
    {
      "epoch": 228.6547811993517,
      "grad_norm": 1.2466226816177368,
      "learning_rate": 2.716162828413883e-05,
      "loss": 2.6627,
      "step": 705400
    },
    {
      "epoch": 228.6871961102107,
      "grad_norm": 1.1858528852462769,
      "learning_rate": 2.715838469023678e-05,
      "loss": 2.6851,
      "step": 705500
    },
    {
      "epoch": 228.71961102106968,
      "grad_norm": 1.3699955940246582,
      "learning_rate": 2.715514109633474e-05,
      "loss": 2.6628,
      "step": 705600
    },
    {
      "epoch": 228.7520259319287,
      "grad_norm": 1.2988629341125488,
      "learning_rate": 2.71518975024327e-05,
      "loss": 2.6718,
      "step": 705700
    },
    {
      "epoch": 228.7844408427877,
      "grad_norm": 1.2623752355575562,
      "learning_rate": 2.714865390853065e-05,
      "loss": 2.6795,
      "step": 705800
    },
    {
      "epoch": 228.81685575364668,
      "grad_norm": 1.0781803131103516,
      "learning_rate": 2.714541031462861e-05,
      "loss": 2.663,
      "step": 705900
    },
    {
      "epoch": 228.84927066450567,
      "grad_norm": 1.2266346216201782,
      "learning_rate": 2.7142166720726565e-05,
      "loss": 2.6509,
      "step": 706000
    },
    {
      "epoch": 228.88168557536466,
      "grad_norm": 1.0810010433197021,
      "learning_rate": 2.7138923126824524e-05,
      "loss": 2.6495,
      "step": 706100
    },
    {
      "epoch": 228.91410048622367,
      "grad_norm": 1.1845184564590454,
      "learning_rate": 2.7135679532922482e-05,
      "loss": 2.6646,
      "step": 706200
    },
    {
      "epoch": 228.94651539708266,
      "grad_norm": 1.3571913242340088,
      "learning_rate": 2.7132435939020434e-05,
      "loss": 2.6702,
      "step": 706300
    },
    {
      "epoch": 228.97893030794165,
      "grad_norm": 1.4665852785110474,
      "learning_rate": 2.7129192345118393e-05,
      "loss": 2.6615,
      "step": 706400
    },
    {
      "epoch": 229.0,
      "eval_bleu": 1.0201793487375244,
      "eval_loss": 4.061805725097656,
      "eval_runtime": 4.1538,
      "eval_samples_per_second": 118.444,
      "eval_steps_per_second": 1.926,
      "step": 706465
    },
    {
      "epoch": 229.01134521880064,
      "grad_norm": 1.2242027521133423,
      "learning_rate": 2.7125948751216352e-05,
      "loss": 2.6558,
      "step": 706500
    },
    {
      "epoch": 229.04376012965963,
      "grad_norm": 1.2025529146194458,
      "learning_rate": 2.7122705157314304e-05,
      "loss": 2.6445,
      "step": 706600
    },
    {
      "epoch": 229.07617504051865,
      "grad_norm": 1.592969536781311,
      "learning_rate": 2.7119461563412263e-05,
      "loss": 2.6585,
      "step": 706700
    },
    {
      "epoch": 229.10858995137764,
      "grad_norm": 1.0803216695785522,
      "learning_rate": 2.7116217969510214e-05,
      "loss": 2.6447,
      "step": 706800
    },
    {
      "epoch": 229.14100486223663,
      "grad_norm": 1.3063372373580933,
      "learning_rate": 2.7112974375608173e-05,
      "loss": 2.682,
      "step": 706900
    },
    {
      "epoch": 229.17341977309562,
      "grad_norm": 1.4658797979354858,
      "learning_rate": 2.7109730781706132e-05,
      "loss": 2.638,
      "step": 707000
    },
    {
      "epoch": 229.2058346839546,
      "grad_norm": 1.3265588283538818,
      "learning_rate": 2.7106487187804087e-05,
      "loss": 2.6636,
      "step": 707100
    },
    {
      "epoch": 229.23824959481362,
      "grad_norm": 1.1603471040725708,
      "learning_rate": 2.7103243593902046e-05,
      "loss": 2.6738,
      "step": 707200
    },
    {
      "epoch": 229.2706645056726,
      "grad_norm": 1.3702595233917236,
      "learning_rate": 2.7100000000000005e-05,
      "loss": 2.6524,
      "step": 707300
    },
    {
      "epoch": 229.3030794165316,
      "grad_norm": 1.1456148624420166,
      "learning_rate": 2.7096756406097957e-05,
      "loss": 2.6594,
      "step": 707400
    },
    {
      "epoch": 229.3354943273906,
      "grad_norm": 1.3752329349517822,
      "learning_rate": 2.7093512812195916e-05,
      "loss": 2.6573,
      "step": 707500
    },
    {
      "epoch": 229.36790923824958,
      "grad_norm": 1.6149617433547974,
      "learning_rate": 2.7090269218293874e-05,
      "loss": 2.6711,
      "step": 707600
    },
    {
      "epoch": 229.4003241491086,
      "grad_norm": 1.2259984016418457,
      "learning_rate": 2.7087025624391826e-05,
      "loss": 2.6503,
      "step": 707700
    },
    {
      "epoch": 229.4327390599676,
      "grad_norm": 1.3096177577972412,
      "learning_rate": 2.7083782030489785e-05,
      "loss": 2.6537,
      "step": 707800
    },
    {
      "epoch": 229.46515397082658,
      "grad_norm": 1.3321431875228882,
      "learning_rate": 2.7080538436587737e-05,
      "loss": 2.6532,
      "step": 707900
    },
    {
      "epoch": 229.49756888168557,
      "grad_norm": 1.1511322259902954,
      "learning_rate": 2.7077294842685696e-05,
      "loss": 2.648,
      "step": 708000
    },
    {
      "epoch": 229.52998379254458,
      "grad_norm": 1.3068245649337769,
      "learning_rate": 2.7074083684722673e-05,
      "loss": 2.6429,
      "step": 708100
    },
    {
      "epoch": 229.56239870340357,
      "grad_norm": 1.201391339302063,
      "learning_rate": 2.7070840090820632e-05,
      "loss": 2.6763,
      "step": 708200
    },
    {
      "epoch": 229.59481361426256,
      "grad_norm": 1.3660167455673218,
      "learning_rate": 2.7067596496918584e-05,
      "loss": 2.6395,
      "step": 708300
    },
    {
      "epoch": 229.62722852512155,
      "grad_norm": 1.1079901456832886,
      "learning_rate": 2.7064352903016543e-05,
      "loss": 2.6419,
      "step": 708400
    },
    {
      "epoch": 229.65964343598054,
      "grad_norm": 1.3120312690734863,
      "learning_rate": 2.70611093091145e-05,
      "loss": 2.6475,
      "step": 708500
    },
    {
      "epoch": 229.69205834683956,
      "grad_norm": 1.3570330142974854,
      "learning_rate": 2.7057865715212454e-05,
      "loss": 2.6529,
      "step": 708600
    },
    {
      "epoch": 229.72447325769855,
      "grad_norm": 1.1186549663543701,
      "learning_rate": 2.7054622121310412e-05,
      "loss": 2.6646,
      "step": 708700
    },
    {
      "epoch": 229.75688816855754,
      "grad_norm": 1.2476743459701538,
      "learning_rate": 2.705137852740837e-05,
      "loss": 2.643,
      "step": 708800
    },
    {
      "epoch": 229.78930307941653,
      "grad_norm": 1.350830078125,
      "learning_rate": 2.7048134933506326e-05,
      "loss": 2.6358,
      "step": 708900
    },
    {
      "epoch": 229.82171799027552,
      "grad_norm": 1.0859137773513794,
      "learning_rate": 2.7044891339604285e-05,
      "loss": 2.6571,
      "step": 709000
    },
    {
      "epoch": 229.85413290113453,
      "grad_norm": 1.2660012245178223,
      "learning_rate": 2.704164774570224e-05,
      "loss": 2.6593,
      "step": 709100
    },
    {
      "epoch": 229.88654781199352,
      "grad_norm": 1.1745352745056152,
      "learning_rate": 2.7038404151800196e-05,
      "loss": 2.6428,
      "step": 709200
    },
    {
      "epoch": 229.9189627228525,
      "grad_norm": 1.1420222520828247,
      "learning_rate": 2.7035160557898155e-05,
      "loss": 2.6484,
      "step": 709300
    },
    {
      "epoch": 229.9513776337115,
      "grad_norm": 1.4004052877426147,
      "learning_rate": 2.7031916963996107e-05,
      "loss": 2.671,
      "step": 709400
    },
    {
      "epoch": 229.9837925445705,
      "grad_norm": 1.1655433177947998,
      "learning_rate": 2.7028673370094065e-05,
      "loss": 2.6724,
      "step": 709500
    },
    {
      "epoch": 230.0,
      "eval_bleu": 1.1644955160373314,
      "eval_loss": 4.059418201446533,
      "eval_runtime": 4.1509,
      "eval_samples_per_second": 118.53,
      "eval_steps_per_second": 1.927,
      "step": 709550
    },
    {
      "epoch": 230.0162074554295,
      "grad_norm": 1.106701135635376,
      "learning_rate": 2.7025429776192024e-05,
      "loss": 2.6398,
      "step": 709600
    },
    {
      "epoch": 230.0486223662885,
      "grad_norm": 1.2410449981689453,
      "learning_rate": 2.7022186182289976e-05,
      "loss": 2.6527,
      "step": 709700
    },
    {
      "epoch": 230.0810372771475,
      "grad_norm": 1.2157833576202393,
      "learning_rate": 2.7018942588387935e-05,
      "loss": 2.6664,
      "step": 709800
    },
    {
      "epoch": 230.11345218800648,
      "grad_norm": 1.3081817626953125,
      "learning_rate": 2.7015698994485894e-05,
      "loss": 2.6564,
      "step": 709900
    },
    {
      "epoch": 230.14586709886547,
      "grad_norm": 1.1455726623535156,
      "learning_rate": 2.7012455400583846e-05,
      "loss": 2.6502,
      "step": 710000
    },
    {
      "epoch": 230.17828200972448,
      "grad_norm": 1.1074825525283813,
      "learning_rate": 2.7009244242620823e-05,
      "loss": 2.6469,
      "step": 710100
    },
    {
      "epoch": 230.21069692058347,
      "grad_norm": 1.2780734300613403,
      "learning_rate": 2.7006000648718782e-05,
      "loss": 2.6544,
      "step": 710200
    },
    {
      "epoch": 230.24311183144246,
      "grad_norm": 1.1953668594360352,
      "learning_rate": 2.700275705481674e-05,
      "loss": 2.6533,
      "step": 710300
    },
    {
      "epoch": 230.27552674230145,
      "grad_norm": 1.2369356155395508,
      "learning_rate": 2.6999513460914693e-05,
      "loss": 2.6376,
      "step": 710400
    },
    {
      "epoch": 230.30794165316044,
      "grad_norm": 1.2915675640106201,
      "learning_rate": 2.699626986701265e-05,
      "loss": 2.6622,
      "step": 710500
    },
    {
      "epoch": 230.34035656401946,
      "grad_norm": 1.4209719896316528,
      "learning_rate": 2.6993026273110607e-05,
      "loss": 2.6404,
      "step": 710600
    },
    {
      "epoch": 230.37277147487845,
      "grad_norm": 1.320054292678833,
      "learning_rate": 2.6989782679208566e-05,
      "loss": 2.6448,
      "step": 710700
    },
    {
      "epoch": 230.40518638573744,
      "grad_norm": 1.2796250581741333,
      "learning_rate": 2.698653908530652e-05,
      "loss": 2.6484,
      "step": 710800
    },
    {
      "epoch": 230.43760129659643,
      "grad_norm": 1.3341290950775146,
      "learning_rate": 2.6983295491404476e-05,
      "loss": 2.6734,
      "step": 710900
    },
    {
      "epoch": 230.47001620745542,
      "grad_norm": 1.1009801626205444,
      "learning_rate": 2.6980051897502435e-05,
      "loss": 2.6498,
      "step": 711000
    },
    {
      "epoch": 230.50243111831443,
      "grad_norm": 1.1341361999511719,
      "learning_rate": 2.6976808303600394e-05,
      "loss": 2.6645,
      "step": 711100
    },
    {
      "epoch": 230.53484602917342,
      "grad_norm": 1.3172060251235962,
      "learning_rate": 2.6973564709698346e-05,
      "loss": 2.6591,
      "step": 711200
    },
    {
      "epoch": 230.5672609400324,
      "grad_norm": 1.607986330986023,
      "learning_rate": 2.6970321115796305e-05,
      "loss": 2.6197,
      "step": 711300
    },
    {
      "epoch": 230.5996758508914,
      "grad_norm": 1.2833199501037598,
      "learning_rate": 2.6967077521894263e-05,
      "loss": 2.6597,
      "step": 711400
    },
    {
      "epoch": 230.63209076175042,
      "grad_norm": 1.27361261844635,
      "learning_rate": 2.6963833927992215e-05,
      "loss": 2.6304,
      "step": 711500
    },
    {
      "epoch": 230.6645056726094,
      "grad_norm": 1.2032872438430786,
      "learning_rate": 2.6960590334090174e-05,
      "loss": 2.6459,
      "step": 711600
    },
    {
      "epoch": 230.6969205834684,
      "grad_norm": 1.3878161907196045,
      "learning_rate": 2.6957346740188126e-05,
      "loss": 2.6367,
      "step": 711700
    },
    {
      "epoch": 230.7293354943274,
      "grad_norm": 1.2657561302185059,
      "learning_rate": 2.6954103146286085e-05,
      "loss": 2.6435,
      "step": 711800
    },
    {
      "epoch": 230.76175040518638,
      "grad_norm": 1.374825358390808,
      "learning_rate": 2.6950859552384043e-05,
      "loss": 2.6575,
      "step": 711900
    },
    {
      "epoch": 230.7941653160454,
      "grad_norm": 1.6057490110397339,
      "learning_rate": 2.6947615958482e-05,
      "loss": 2.6773,
      "step": 712000
    },
    {
      "epoch": 230.82658022690438,
      "grad_norm": 1.0685076713562012,
      "learning_rate": 2.6944404800518973e-05,
      "loss": 2.6589,
      "step": 712100
    },
    {
      "epoch": 230.85899513776337,
      "grad_norm": 1.1595970392227173,
      "learning_rate": 2.6941161206616932e-05,
      "loss": 2.6912,
      "step": 712200
    },
    {
      "epoch": 230.89141004862236,
      "grad_norm": 1.1711750030517578,
      "learning_rate": 2.693791761271489e-05,
      "loss": 2.6608,
      "step": 712300
    },
    {
      "epoch": 230.92382495948135,
      "grad_norm": 1.1748080253601074,
      "learning_rate": 2.6934674018812843e-05,
      "loss": 2.654,
      "step": 712400
    },
    {
      "epoch": 230.95623987034037,
      "grad_norm": 1.2693142890930176,
      "learning_rate": 2.69314304249108e-05,
      "loss": 2.6709,
      "step": 712500
    },
    {
      "epoch": 230.98865478119936,
      "grad_norm": 1.5390925407409668,
      "learning_rate": 2.692818683100876e-05,
      "loss": 2.6585,
      "step": 712600
    },
    {
      "epoch": 231.0,
      "eval_bleu": 1.0948967022887177,
      "eval_loss": 4.061936378479004,
      "eval_runtime": 4.0225,
      "eval_samples_per_second": 122.313,
      "eval_steps_per_second": 1.989,
      "step": 712635
    },
    {
      "epoch": 231.02106969205835,
      "grad_norm": 1.3428890705108643,
      "learning_rate": 2.6924943237106716e-05,
      "loss": 2.6884,
      "step": 712700
    },
    {
      "epoch": 231.05348460291734,
      "grad_norm": 1.4265609979629517,
      "learning_rate": 2.6921699643204674e-05,
      "loss": 2.6458,
      "step": 712800
    },
    {
      "epoch": 231.08589951377633,
      "grad_norm": 1.253711223602295,
      "learning_rate": 2.6918456049302626e-05,
      "loss": 2.6241,
      "step": 712900
    },
    {
      "epoch": 231.11831442463534,
      "grad_norm": 1.253944754600525,
      "learning_rate": 2.6915212455400585e-05,
      "loss": 2.6612,
      "step": 713000
    },
    {
      "epoch": 231.15072933549433,
      "grad_norm": 1.3609185218811035,
      "learning_rate": 2.6911968861498544e-05,
      "loss": 2.6641,
      "step": 713100
    },
    {
      "epoch": 231.18314424635332,
      "grad_norm": 1.4599162340164185,
      "learning_rate": 2.6908725267596496e-05,
      "loss": 2.6393,
      "step": 713200
    },
    {
      "epoch": 231.2155591572123,
      "grad_norm": 1.1640079021453857,
      "learning_rate": 2.6905481673694454e-05,
      "loss": 2.6381,
      "step": 713300
    },
    {
      "epoch": 231.2479740680713,
      "grad_norm": 1.5221928358078003,
      "learning_rate": 2.6902238079792413e-05,
      "loss": 2.6444,
      "step": 713400
    },
    {
      "epoch": 231.28038897893032,
      "grad_norm": 1.3038122653961182,
      "learning_rate": 2.6898994485890365e-05,
      "loss": 2.6496,
      "step": 713500
    },
    {
      "epoch": 231.3128038897893,
      "grad_norm": 1.2011030912399292,
      "learning_rate": 2.6895750891988324e-05,
      "loss": 2.6494,
      "step": 713600
    },
    {
      "epoch": 231.3452188006483,
      "grad_norm": 1.1582047939300537,
      "learning_rate": 2.6892507298086283e-05,
      "loss": 2.6458,
      "step": 713700
    },
    {
      "epoch": 231.3776337115073,
      "grad_norm": 1.3661510944366455,
      "learning_rate": 2.6889263704184238e-05,
      "loss": 2.6477,
      "step": 713800
    },
    {
      "epoch": 231.41004862236628,
      "grad_norm": 1.3622097969055176,
      "learning_rate": 2.6886020110282197e-05,
      "loss": 2.666,
      "step": 713900
    },
    {
      "epoch": 231.4424635332253,
      "grad_norm": 1.2208051681518555,
      "learning_rate": 2.688277651638015e-05,
      "loss": 2.6511,
      "step": 714000
    },
    {
      "epoch": 231.47487844408428,
      "grad_norm": 1.4358309507369995,
      "learning_rate": 2.6879565358417123e-05,
      "loss": 2.6585,
      "step": 714100
    },
    {
      "epoch": 231.50729335494327,
      "grad_norm": 1.1620761156082153,
      "learning_rate": 2.6876321764515082e-05,
      "loss": 2.6526,
      "step": 714200
    },
    {
      "epoch": 231.53970826580226,
      "grad_norm": 1.3873101472854614,
      "learning_rate": 2.687307817061304e-05,
      "loss": 2.6495,
      "step": 714300
    },
    {
      "epoch": 231.57212317666125,
      "grad_norm": 1.3085001707077026,
      "learning_rate": 2.6869834576710996e-05,
      "loss": 2.6554,
      "step": 714400
    },
    {
      "epoch": 231.60453808752027,
      "grad_norm": 1.3236939907073975,
      "learning_rate": 2.6866590982808955e-05,
      "loss": 2.6421,
      "step": 714500
    },
    {
      "epoch": 231.63695299837926,
      "grad_norm": 1.6392707824707031,
      "learning_rate": 2.6863347388906913e-05,
      "loss": 2.6405,
      "step": 714600
    },
    {
      "epoch": 231.66936790923825,
      "grad_norm": 1.0957815647125244,
      "learning_rate": 2.6860103795004865e-05,
      "loss": 2.6698,
      "step": 714700
    },
    {
      "epoch": 231.70178282009724,
      "grad_norm": 1.1622745990753174,
      "learning_rate": 2.6856860201102824e-05,
      "loss": 2.6668,
      "step": 714800
    },
    {
      "epoch": 231.73419773095625,
      "grad_norm": 1.1761271953582764,
      "learning_rate": 2.6853616607200783e-05,
      "loss": 2.6435,
      "step": 714900
    },
    {
      "epoch": 231.76661264181524,
      "grad_norm": 1.178031325340271,
      "learning_rate": 2.6850373013298735e-05,
      "loss": 2.6513,
      "step": 715000
    },
    {
      "epoch": 231.79902755267423,
      "grad_norm": 1.2785978317260742,
      "learning_rate": 2.6847129419396694e-05,
      "loss": 2.6644,
      "step": 715100
    },
    {
      "epoch": 231.83144246353322,
      "grad_norm": 1.2748382091522217,
      "learning_rate": 2.6843885825494646e-05,
      "loss": 2.6294,
      "step": 715200
    },
    {
      "epoch": 231.8638573743922,
      "grad_norm": 1.2398065328598022,
      "learning_rate": 2.6840642231592604e-05,
      "loss": 2.6583,
      "step": 715300
    },
    {
      "epoch": 231.89627228525123,
      "grad_norm": 1.1969521045684814,
      "learning_rate": 2.6837398637690563e-05,
      "loss": 2.658,
      "step": 715400
    },
    {
      "epoch": 231.92868719611022,
      "grad_norm": 1.400325059890747,
      "learning_rate": 2.683415504378852e-05,
      "loss": 2.6761,
      "step": 715500
    },
    {
      "epoch": 231.9611021069692,
      "grad_norm": 1.3116697072982788,
      "learning_rate": 2.6830911449886477e-05,
      "loss": 2.6363,
      "step": 715600
    },
    {
      "epoch": 231.9935170178282,
      "grad_norm": 1.3351430892944336,
      "learning_rate": 2.6827667855984432e-05,
      "loss": 2.648,
      "step": 715700
    },
    {
      "epoch": 232.0,
      "eval_bleu": 1.1212577014854908,
      "eval_loss": 4.069778919219971,
      "eval_runtime": 4.0045,
      "eval_samples_per_second": 122.861,
      "eval_steps_per_second": 1.998,
      "step": 715720
    },
    {
      "epoch": 232.02593192868719,
      "grad_norm": 1.2696290016174316,
      "learning_rate": 2.6824424262082388e-05,
      "loss": 2.6536,
      "step": 715800
    },
    {
      "epoch": 232.0583468395462,
      "grad_norm": 1.043109655380249,
      "learning_rate": 2.6821180668180347e-05,
      "loss": 2.6494,
      "step": 715900
    },
    {
      "epoch": 232.0907617504052,
      "grad_norm": 1.0792189836502075,
      "learning_rate": 2.6817937074278305e-05,
      "loss": 2.6456,
      "step": 716000
    },
    {
      "epoch": 232.12317666126418,
      "grad_norm": 1.2476671934127808,
      "learning_rate": 2.6814693480376257e-05,
      "loss": 2.6394,
      "step": 716100
    },
    {
      "epoch": 232.15559157212317,
      "grad_norm": 1.4264570474624634,
      "learning_rate": 2.6811449886474216e-05,
      "loss": 2.6671,
      "step": 716200
    },
    {
      "epoch": 232.18800648298216,
      "grad_norm": 1.21830153465271,
      "learning_rate": 2.6808206292572168e-05,
      "loss": 2.6393,
      "step": 716300
    },
    {
      "epoch": 232.22042139384118,
      "grad_norm": 1.2126675844192505,
      "learning_rate": 2.6804962698670127e-05,
      "loss": 2.6386,
      "step": 716400
    },
    {
      "epoch": 232.25283630470017,
      "grad_norm": 1.37553870677948,
      "learning_rate": 2.6801719104768085e-05,
      "loss": 2.6636,
      "step": 716500
    },
    {
      "epoch": 232.28525121555916,
      "grad_norm": 1.2321629524230957,
      "learning_rate": 2.6798475510866037e-05,
      "loss": 2.6213,
      "step": 716600
    },
    {
      "epoch": 232.31766612641815,
      "grad_norm": 1.2799400091171265,
      "learning_rate": 2.6795231916963996e-05,
      "loss": 2.6626,
      "step": 716700
    },
    {
      "epoch": 232.35008103727714,
      "grad_norm": 1.2060781717300415,
      "learning_rate": 2.6791988323061955e-05,
      "loss": 2.6363,
      "step": 716800
    },
    {
      "epoch": 232.38249594813615,
      "grad_norm": 1.4739577770233154,
      "learning_rate": 2.678874472915991e-05,
      "loss": 2.6487,
      "step": 716900
    },
    {
      "epoch": 232.41491085899514,
      "grad_norm": 1.2045814990997314,
      "learning_rate": 2.678550113525787e-05,
      "loss": 2.6525,
      "step": 717000
    },
    {
      "epoch": 232.44732576985413,
      "grad_norm": 1.2012423276901245,
      "learning_rate": 2.678225754135582e-05,
      "loss": 2.6611,
      "step": 717100
    },
    {
      "epoch": 232.47974068071312,
      "grad_norm": 1.2950373888015747,
      "learning_rate": 2.6779046383392802e-05,
      "loss": 2.6424,
      "step": 717200
    },
    {
      "epoch": 232.5121555915721,
      "grad_norm": 1.367498755455017,
      "learning_rate": 2.6775802789490758e-05,
      "loss": 2.6587,
      "step": 717300
    },
    {
      "epoch": 232.54457050243113,
      "grad_norm": 1.3107633590698242,
      "learning_rate": 2.6772559195588713e-05,
      "loss": 2.6398,
      "step": 717400
    },
    {
      "epoch": 232.57698541329012,
      "grad_norm": 1.160146713256836,
      "learning_rate": 2.6769315601686668e-05,
      "loss": 2.6578,
      "step": 717500
    },
    {
      "epoch": 232.6094003241491,
      "grad_norm": 1.1879962682724,
      "learning_rate": 2.6766072007784627e-05,
      "loss": 2.6578,
      "step": 717600
    },
    {
      "epoch": 232.6418152350081,
      "grad_norm": 1.3029893636703491,
      "learning_rate": 2.6762828413882586e-05,
      "loss": 2.6183,
      "step": 717700
    },
    {
      "epoch": 232.67423014586709,
      "grad_norm": 1.3105965852737427,
      "learning_rate": 2.6759584819980538e-05,
      "loss": 2.6552,
      "step": 717800
    },
    {
      "epoch": 232.7066450567261,
      "grad_norm": 1.198707103729248,
      "learning_rate": 2.6756341226078496e-05,
      "loss": 2.6656,
      "step": 717900
    },
    {
      "epoch": 232.7390599675851,
      "grad_norm": 1.3091107606887817,
      "learning_rate": 2.6753097632176455e-05,
      "loss": 2.6451,
      "step": 718000
    },
    {
      "epoch": 232.77147487844408,
      "grad_norm": 1.4904919862747192,
      "learning_rate": 2.6749854038274407e-05,
      "loss": 2.6565,
      "step": 718100
    },
    {
      "epoch": 232.80388978930307,
      "grad_norm": 1.3257979154586792,
      "learning_rate": 2.6746610444372366e-05,
      "loss": 2.6692,
      "step": 718200
    },
    {
      "epoch": 232.8363047001621,
      "grad_norm": 1.267590045928955,
      "learning_rate": 2.6743366850470325e-05,
      "loss": 2.6534,
      "step": 718300
    },
    {
      "epoch": 232.86871961102108,
      "grad_norm": 1.4780446290969849,
      "learning_rate": 2.6740123256568277e-05,
      "loss": 2.6547,
      "step": 718400
    },
    {
      "epoch": 232.90113452188007,
      "grad_norm": 1.3022533655166626,
      "learning_rate": 2.6736879662666235e-05,
      "loss": 2.6481,
      "step": 718500
    },
    {
      "epoch": 232.93354943273906,
      "grad_norm": 1.427403211593628,
      "learning_rate": 2.673363606876419e-05,
      "loss": 2.6468,
      "step": 718600
    },
    {
      "epoch": 232.96596434359805,
      "grad_norm": 1.2532117366790771,
      "learning_rate": 2.673039247486215e-05,
      "loss": 2.6616,
      "step": 718700
    },
    {
      "epoch": 232.99837925445706,
      "grad_norm": 1.2366122007369995,
      "learning_rate": 2.6727148880960108e-05,
      "loss": 2.648,
      "step": 718800
    },
    {
      "epoch": 233.0,
      "eval_bleu": 1.0137046017922928,
      "eval_loss": 4.0652008056640625,
      "eval_runtime": 4.0563,
      "eval_samples_per_second": 121.292,
      "eval_steps_per_second": 1.972,
      "step": 718805
    },
    {
      "epoch": 233.03079416531605,
      "grad_norm": 1.4355361461639404,
      "learning_rate": 2.672390528705806e-05,
      "loss": 2.6499,
      "step": 718900
    },
    {
      "epoch": 233.06320907617504,
      "grad_norm": 1.285114049911499,
      "learning_rate": 2.672066169315602e-05,
      "loss": 2.6637,
      "step": 719000
    },
    {
      "epoch": 233.09562398703403,
      "grad_norm": 1.273475170135498,
      "learning_rate": 2.6717418099253978e-05,
      "loss": 2.6636,
      "step": 719100
    },
    {
      "epoch": 233.12803889789302,
      "grad_norm": 1.4520480632781982,
      "learning_rate": 2.671417450535193e-05,
      "loss": 2.6521,
      "step": 719200
    },
    {
      "epoch": 233.16045380875204,
      "grad_norm": 1.3809422254562378,
      "learning_rate": 2.671093091144989e-05,
      "loss": 2.6551,
      "step": 719300
    },
    {
      "epoch": 233.19286871961103,
      "grad_norm": 1.141861081123352,
      "learning_rate": 2.6707687317547847e-05,
      "loss": 2.6574,
      "step": 719400
    },
    {
      "epoch": 233.22528363047002,
      "grad_norm": 1.257896900177002,
      "learning_rate": 2.67044437236458e-05,
      "loss": 2.6328,
      "step": 719500
    },
    {
      "epoch": 233.257698541329,
      "grad_norm": 1.224474310874939,
      "learning_rate": 2.6701200129743758e-05,
      "loss": 2.6253,
      "step": 719600
    },
    {
      "epoch": 233.290113452188,
      "grad_norm": 1.234387755393982,
      "learning_rate": 2.6697956535841713e-05,
      "loss": 2.6403,
      "step": 719700
    },
    {
      "epoch": 233.322528363047,
      "grad_norm": 1.2337239980697632,
      "learning_rate": 2.669471294193967e-05,
      "loss": 2.6597,
      "step": 719800
    },
    {
      "epoch": 233.354943273906,
      "grad_norm": 1.48115873336792,
      "learning_rate": 2.6691501783976646e-05,
      "loss": 2.6338,
      "step": 719900
    },
    {
      "epoch": 233.387358184765,
      "grad_norm": 1.1636066436767578,
      "learning_rate": 2.6688258190074605e-05,
      "loss": 2.6367,
      "step": 720000
    },
    {
      "epoch": 233.41977309562398,
      "grad_norm": 1.3905147314071655,
      "learning_rate": 2.6685047032111583e-05,
      "loss": 2.6323,
      "step": 720100
    },
    {
      "epoch": 233.45218800648297,
      "grad_norm": 1.2826858758926392,
      "learning_rate": 2.6681803438209535e-05,
      "loss": 2.6462,
      "step": 720200
    },
    {
      "epoch": 233.484602917342,
      "grad_norm": 1.3634917736053467,
      "learning_rate": 2.6678559844307494e-05,
      "loss": 2.6636,
      "step": 720300
    },
    {
      "epoch": 233.51701782820098,
      "grad_norm": 1.1887133121490479,
      "learning_rate": 2.6675316250405452e-05,
      "loss": 2.6485,
      "step": 720400
    },
    {
      "epoch": 233.54943273905997,
      "grad_norm": 1.0536043643951416,
      "learning_rate": 2.6672072656503404e-05,
      "loss": 2.6633,
      "step": 720500
    },
    {
      "epoch": 233.58184764991896,
      "grad_norm": 1.1848443746566772,
      "learning_rate": 2.6668829062601363e-05,
      "loss": 2.6423,
      "step": 720600
    },
    {
      "epoch": 233.61426256077795,
      "grad_norm": 1.1726025342941284,
      "learning_rate": 2.6665585468699322e-05,
      "loss": 2.6489,
      "step": 720700
    },
    {
      "epoch": 233.64667747163696,
      "grad_norm": 1.2847055196762085,
      "learning_rate": 2.6662341874797274e-05,
      "loss": 2.6613,
      "step": 720800
    },
    {
      "epoch": 233.67909238249595,
      "grad_norm": 1.3455690145492554,
      "learning_rate": 2.6659098280895232e-05,
      "loss": 2.66,
      "step": 720900
    },
    {
      "epoch": 233.71150729335494,
      "grad_norm": 1.343429446220398,
      "learning_rate": 2.665585468699319e-05,
      "loss": 2.634,
      "step": 721000
    },
    {
      "epoch": 233.74392220421393,
      "grad_norm": 1.0648846626281738,
      "learning_rate": 2.6652611093091147e-05,
      "loss": 2.6656,
      "step": 721100
    },
    {
      "epoch": 233.77633711507292,
      "grad_norm": 1.343449354171753,
      "learning_rate": 2.6649367499189105e-05,
      "loss": 2.6594,
      "step": 721200
    },
    {
      "epoch": 233.80875202593194,
      "grad_norm": 1.2087510824203491,
      "learning_rate": 2.6646123905287057e-05,
      "loss": 2.64,
      "step": 721300
    },
    {
      "epoch": 233.84116693679093,
      "grad_norm": 1.4747847318649292,
      "learning_rate": 2.6642880311385016e-05,
      "loss": 2.6581,
      "step": 721400
    },
    {
      "epoch": 233.87358184764992,
      "grad_norm": 1.2593914270401,
      "learning_rate": 2.6639636717482975e-05,
      "loss": 2.6379,
      "step": 721500
    },
    {
      "epoch": 233.9059967585089,
      "grad_norm": 1.2597090005874634,
      "learning_rate": 2.6636393123580927e-05,
      "loss": 2.6569,
      "step": 721600
    },
    {
      "epoch": 233.93841166936792,
      "grad_norm": 1.291986346244812,
      "learning_rate": 2.6633149529678885e-05,
      "loss": 2.6507,
      "step": 721700
    },
    {
      "epoch": 233.9708265802269,
      "grad_norm": 1.350884199142456,
      "learning_rate": 2.6629905935776844e-05,
      "loss": 2.6593,
      "step": 721800
    },
    {
      "epoch": 234.0,
      "eval_bleu": 0.9642430612435015,
      "eval_loss": 4.068702220916748,
      "eval_runtime": 4.329,
      "eval_samples_per_second": 113.653,
      "eval_steps_per_second": 1.848,
      "step": 721890
    },
    {
      "epoch": 234.0032414910859,
      "grad_norm": 1.3231672048568726,
      "learning_rate": 2.6626662341874796e-05,
      "loss": 2.642,
      "step": 721900
    },
    {
      "epoch": 234.0356564019449,
      "grad_norm": 1.270370364189148,
      "learning_rate": 2.6623418747972755e-05,
      "loss": 2.6591,
      "step": 722000
    },
    {
      "epoch": 234.06807131280388,
      "grad_norm": 1.318857192993164,
      "learning_rate": 2.662017515407071e-05,
      "loss": 2.6399,
      "step": 722100
    },
    {
      "epoch": 234.1004862236629,
      "grad_norm": 1.1560691595077515,
      "learning_rate": 2.661693156016867e-05,
      "loss": 2.6321,
      "step": 722200
    },
    {
      "epoch": 234.1329011345219,
      "grad_norm": 1.0796414613723755,
      "learning_rate": 2.6613687966266628e-05,
      "loss": 2.6381,
      "step": 722300
    },
    {
      "epoch": 234.16531604538088,
      "grad_norm": 1.4034357070922852,
      "learning_rate": 2.6610476808303602e-05,
      "loss": 2.6495,
      "step": 722400
    },
    {
      "epoch": 234.19773095623987,
      "grad_norm": 1.24163019657135,
      "learning_rate": 2.6607233214401554e-05,
      "loss": 2.6361,
      "step": 722500
    },
    {
      "epoch": 234.23014586709886,
      "grad_norm": 1.0707725286483765,
      "learning_rate": 2.6603989620499513e-05,
      "loss": 2.6493,
      "step": 722600
    },
    {
      "epoch": 234.26256077795787,
      "grad_norm": 1.3767199516296387,
      "learning_rate": 2.660074602659747e-05,
      "loss": 2.6479,
      "step": 722700
    },
    {
      "epoch": 234.29497568881686,
      "grad_norm": 1.364606261253357,
      "learning_rate": 2.6597502432695427e-05,
      "loss": 2.6405,
      "step": 722800
    },
    {
      "epoch": 234.32739059967585,
      "grad_norm": 1.2727081775665283,
      "learning_rate": 2.6594258838793386e-05,
      "loss": 2.6333,
      "step": 722900
    },
    {
      "epoch": 234.35980551053484,
      "grad_norm": 1.3117460012435913,
      "learning_rate": 2.6591015244891344e-05,
      "loss": 2.6452,
      "step": 723000
    },
    {
      "epoch": 234.39222042139383,
      "grad_norm": 1.225111961364746,
      "learning_rate": 2.6587771650989296e-05,
      "loss": 2.6613,
      "step": 723100
    },
    {
      "epoch": 234.42463533225285,
      "grad_norm": 1.4056766033172607,
      "learning_rate": 2.6584528057087255e-05,
      "loss": 2.6366,
      "step": 723200
    },
    {
      "epoch": 234.45705024311184,
      "grad_norm": 1.1755993366241455,
      "learning_rate": 2.6581284463185214e-05,
      "loss": 2.6386,
      "step": 723300
    },
    {
      "epoch": 234.48946515397083,
      "grad_norm": 1.3241370916366577,
      "learning_rate": 2.6578040869283166e-05,
      "loss": 2.6418,
      "step": 723400
    },
    {
      "epoch": 234.52188006482982,
      "grad_norm": 1.3020426034927368,
      "learning_rate": 2.6574797275381125e-05,
      "loss": 2.6512,
      "step": 723500
    },
    {
      "epoch": 234.5542949756888,
      "grad_norm": 1.4390318393707275,
      "learning_rate": 2.6571553681479077e-05,
      "loss": 2.6597,
      "step": 723600
    },
    {
      "epoch": 234.58670988654782,
      "grad_norm": 1.2108389139175415,
      "learning_rate": 2.6568310087577035e-05,
      "loss": 2.6326,
      "step": 723700
    },
    {
      "epoch": 234.6191247974068,
      "grad_norm": 1.2434015274047852,
      "learning_rate": 2.6565066493674994e-05,
      "loss": 2.6245,
      "step": 723800
    },
    {
      "epoch": 234.6515397082658,
      "grad_norm": 1.3602290153503418,
      "learning_rate": 2.656182289977295e-05,
      "loss": 2.6491,
      "step": 723900
    },
    {
      "epoch": 234.6839546191248,
      "grad_norm": 1.3957958221435547,
      "learning_rate": 2.6558579305870905e-05,
      "loss": 2.6587,
      "step": 724000
    },
    {
      "epoch": 234.71636952998378,
      "grad_norm": 1.1381500959396362,
      "learning_rate": 2.6555335711968864e-05,
      "loss": 2.6473,
      "step": 724100
    },
    {
      "epoch": 234.7487844408428,
      "grad_norm": 1.2711704969406128,
      "learning_rate": 2.655209211806682e-05,
      "loss": 2.6659,
      "step": 724200
    },
    {
      "epoch": 234.7811993517018,
      "grad_norm": 1.6399738788604736,
      "learning_rate": 2.6548848524164778e-05,
      "loss": 2.6546,
      "step": 724300
    },
    {
      "epoch": 234.81361426256078,
      "grad_norm": 1.5127424001693726,
      "learning_rate": 2.654560493026273e-05,
      "loss": 2.667,
      "step": 724400
    },
    {
      "epoch": 234.84602917341977,
      "grad_norm": 1.2325330972671509,
      "learning_rate": 2.654236133636069e-05,
      "loss": 2.6483,
      "step": 724500
    },
    {
      "epoch": 234.87844408427875,
      "grad_norm": 1.19629967212677,
      "learning_rate": 2.6539117742458647e-05,
      "loss": 2.6477,
      "step": 724600
    },
    {
      "epoch": 234.91085899513777,
      "grad_norm": 1.1673611402511597,
      "learning_rate": 2.65358741485566e-05,
      "loss": 2.6377,
      "step": 724700
    },
    {
      "epoch": 234.94327390599676,
      "grad_norm": 1.1467833518981934,
      "learning_rate": 2.6532630554654558e-05,
      "loss": 2.6568,
      "step": 724800
    },
    {
      "epoch": 234.97568881685575,
      "grad_norm": 1.2622864246368408,
      "learning_rate": 2.6529386960752517e-05,
      "loss": 2.6469,
      "step": 724900
    },
    {
      "epoch": 235.0,
      "eval_bleu": 0.9932538828934385,
      "eval_loss": 4.0697479248046875,
      "eval_runtime": 4.7973,
      "eval_samples_per_second": 102.558,
      "eval_steps_per_second": 1.668,
      "step": 724975
    },
    {
      "epoch": 235.00810372771474,
      "grad_norm": 1.2208207845687866,
      "learning_rate": 2.652614336685047e-05,
      "loss": 2.6562,
      "step": 725000
    },
    {
      "epoch": 235.04051863857376,
      "grad_norm": 1.1903212070465088,
      "learning_rate": 2.6522899772948427e-05,
      "loss": 2.6515,
      "step": 725100
    },
    {
      "epoch": 235.07293354943275,
      "grad_norm": 1.3900294303894043,
      "learning_rate": 2.6519656179046386e-05,
      "loss": 2.6466,
      "step": 725200
    },
    {
      "epoch": 235.10534846029174,
      "grad_norm": 1.2573033571243286,
      "learning_rate": 2.651641258514434e-05,
      "loss": 2.6538,
      "step": 725300
    },
    {
      "epoch": 235.13776337115073,
      "grad_norm": 1.1758370399475098,
      "learning_rate": 2.65131689912423e-05,
      "loss": 2.6441,
      "step": 725400
    },
    {
      "epoch": 235.17017828200972,
      "grad_norm": 1.139699935913086,
      "learning_rate": 2.6509925397340252e-05,
      "loss": 2.6473,
      "step": 725500
    },
    {
      "epoch": 235.20259319286873,
      "grad_norm": 1.1247007846832275,
      "learning_rate": 2.650668180343821e-05,
      "loss": 2.6561,
      "step": 725600
    },
    {
      "epoch": 235.23500810372772,
      "grad_norm": 1.1500028371810913,
      "learning_rate": 2.650343820953617e-05,
      "loss": 2.6334,
      "step": 725700
    },
    {
      "epoch": 235.2674230145867,
      "grad_norm": 1.4508206844329834,
      "learning_rate": 2.650019461563412e-05,
      "loss": 2.6338,
      "step": 725800
    },
    {
      "epoch": 235.2998379254457,
      "grad_norm": 1.1512517929077148,
      "learning_rate": 2.649695102173208e-05,
      "loss": 2.6358,
      "step": 725900
    },
    {
      "epoch": 235.3322528363047,
      "grad_norm": 1.3659688234329224,
      "learning_rate": 2.649370742783004e-05,
      "loss": 2.6453,
      "step": 726000
    },
    {
      "epoch": 235.3646677471637,
      "grad_norm": 1.3074913024902344,
      "learning_rate": 2.649046383392799e-05,
      "loss": 2.654,
      "step": 726100
    },
    {
      "epoch": 235.3970826580227,
      "grad_norm": 1.2284142971038818,
      "learning_rate": 2.648722024002595e-05,
      "loss": 2.6534,
      "step": 726200
    },
    {
      "epoch": 235.4294975688817,
      "grad_norm": 1.2839434146881104,
      "learning_rate": 2.648397664612391e-05,
      "loss": 2.6359,
      "step": 726300
    },
    {
      "epoch": 235.46191247974068,
      "grad_norm": 1.0958967208862305,
      "learning_rate": 2.6480733052221864e-05,
      "loss": 2.6418,
      "step": 726400
    },
    {
      "epoch": 235.49432739059966,
      "grad_norm": 1.353231430053711,
      "learning_rate": 2.647748945831982e-05,
      "loss": 2.6559,
      "step": 726500
    },
    {
      "epoch": 235.52674230145868,
      "grad_norm": 1.0858803987503052,
      "learning_rate": 2.6474245864417775e-05,
      "loss": 2.6537,
      "step": 726600
    },
    {
      "epoch": 235.55915721231767,
      "grad_norm": 1.37734055519104,
      "learning_rate": 2.6471002270515733e-05,
      "loss": 2.6576,
      "step": 726700
    },
    {
      "epoch": 235.59157212317666,
      "grad_norm": 1.2590982913970947,
      "learning_rate": 2.6467758676613692e-05,
      "loss": 2.6623,
      "step": 726800
    },
    {
      "epoch": 235.62398703403565,
      "grad_norm": 1.1486637592315674,
      "learning_rate": 2.6464515082711644e-05,
      "loss": 2.6298,
      "step": 726900
    },
    {
      "epoch": 235.65640194489464,
      "grad_norm": 1.5240864753723145,
      "learning_rate": 2.6461271488809603e-05,
      "loss": 2.6197,
      "step": 727000
    },
    {
      "epoch": 235.68881685575366,
      "grad_norm": 1.3419691324234009,
      "learning_rate": 2.645802789490756e-05,
      "loss": 2.654,
      "step": 727100
    },
    {
      "epoch": 235.72123176661265,
      "grad_norm": 1.2970325946807861,
      "learning_rate": 2.6454784301005513e-05,
      "loss": 2.6421,
      "step": 727200
    },
    {
      "epoch": 235.75364667747164,
      "grad_norm": 1.4808484315872192,
      "learning_rate": 2.6451540707103472e-05,
      "loss": 2.6282,
      "step": 727300
    },
    {
      "epoch": 235.78606158833063,
      "grad_norm": 1.3862801790237427,
      "learning_rate": 2.644829711320143e-05,
      "loss": 2.6556,
      "step": 727400
    },
    {
      "epoch": 235.81847649918961,
      "grad_norm": 1.1726816892623901,
      "learning_rate": 2.6445053519299383e-05,
      "loss": 2.6452,
      "step": 727500
    },
    {
      "epoch": 235.85089141004863,
      "grad_norm": 1.2945265769958496,
      "learning_rate": 2.644180992539734e-05,
      "loss": 2.6343,
      "step": 727600
    },
    {
      "epoch": 235.88330632090762,
      "grad_norm": 1.2218559980392456,
      "learning_rate": 2.6438566331495297e-05,
      "loss": 2.6493,
      "step": 727700
    },
    {
      "epoch": 235.9157212317666,
      "grad_norm": 1.154530644416809,
      "learning_rate": 2.6435322737593256e-05,
      "loss": 2.6578,
      "step": 727800
    },
    {
      "epoch": 235.9481361426256,
      "grad_norm": 1.0990865230560303,
      "learning_rate": 2.6432079143691214e-05,
      "loss": 2.6445,
      "step": 727900
    },
    {
      "epoch": 235.9805510534846,
      "grad_norm": 1.3699424266815186,
      "learning_rate": 2.6428835549789166e-05,
      "loss": 2.6549,
      "step": 728000
    },
    {
      "epoch": 236.0,
      "eval_bleu": 0.9352966693950513,
      "eval_loss": 4.068984031677246,
      "eval_runtime": 3.7643,
      "eval_samples_per_second": 130.702,
      "eval_steps_per_second": 2.125,
      "step": 728060
    },
    {
      "epoch": 236.0129659643436,
      "grad_norm": 1.4434566497802734,
      "learning_rate": 2.6425591955887125e-05,
      "loss": 2.6488,
      "step": 728100
    },
    {
      "epoch": 236.0453808752026,
      "grad_norm": 1.3306303024291992,
      "learning_rate": 2.6422348361985084e-05,
      "loss": 2.655,
      "step": 728200
    },
    {
      "epoch": 236.07779578606159,
      "grad_norm": 1.259376883506775,
      "learning_rate": 2.6419104768083036e-05,
      "loss": 2.6352,
      "step": 728300
    },
    {
      "epoch": 236.11021069692057,
      "grad_norm": 1.2405118942260742,
      "learning_rate": 2.6415893610120014e-05,
      "loss": 2.6277,
      "step": 728400
    },
    {
      "epoch": 236.1426256077796,
      "grad_norm": 1.3885796070098877,
      "learning_rate": 2.6412650016217972e-05,
      "loss": 2.6544,
      "step": 728500
    },
    {
      "epoch": 236.17504051863858,
      "grad_norm": 1.1357959508895874,
      "learning_rate": 2.640940642231593e-05,
      "loss": 2.6739,
      "step": 728600
    },
    {
      "epoch": 236.20745542949757,
      "grad_norm": 1.439030408859253,
      "learning_rate": 2.6406162828413883e-05,
      "loss": 2.6584,
      "step": 728700
    },
    {
      "epoch": 236.23987034035656,
      "grad_norm": 1.2274503707885742,
      "learning_rate": 2.6402919234511842e-05,
      "loss": 2.638,
      "step": 728800
    },
    {
      "epoch": 236.27228525121555,
      "grad_norm": 1.1027451753616333,
      "learning_rate": 2.6399675640609794e-05,
      "loss": 2.6351,
      "step": 728900
    },
    {
      "epoch": 236.30470016207457,
      "grad_norm": 1.4684277772903442,
      "learning_rate": 2.6396432046707753e-05,
      "loss": 2.6497,
      "step": 729000
    },
    {
      "epoch": 236.33711507293356,
      "grad_norm": 1.2617194652557373,
      "learning_rate": 2.639318845280571e-05,
      "loss": 2.6241,
      "step": 729100
    },
    {
      "epoch": 236.36952998379255,
      "grad_norm": 1.191591501235962,
      "learning_rate": 2.6389944858903663e-05,
      "loss": 2.6513,
      "step": 729200
    },
    {
      "epoch": 236.40194489465154,
      "grad_norm": 1.2905923128128052,
      "learning_rate": 2.6386701265001622e-05,
      "loss": 2.6486,
      "step": 729300
    },
    {
      "epoch": 236.43435980551052,
      "grad_norm": 1.1594922542572021,
      "learning_rate": 2.638345767109958e-05,
      "loss": 2.6339,
      "step": 729400
    },
    {
      "epoch": 236.46677471636954,
      "grad_norm": 1.2985018491744995,
      "learning_rate": 2.6380214077197536e-05,
      "loss": 2.6556,
      "step": 729500
    },
    {
      "epoch": 236.49918962722853,
      "grad_norm": 1.0804365873336792,
      "learning_rate": 2.6376970483295495e-05,
      "loss": 2.6401,
      "step": 729600
    },
    {
      "epoch": 236.53160453808752,
      "grad_norm": 1.1725856065750122,
      "learning_rate": 2.6373726889393454e-05,
      "loss": 2.639,
      "step": 729700
    },
    {
      "epoch": 236.5640194489465,
      "grad_norm": 1.1364599466323853,
      "learning_rate": 2.6370483295491406e-05,
      "loss": 2.648,
      "step": 729800
    },
    {
      "epoch": 236.5964343598055,
      "grad_norm": 1.497300148010254,
      "learning_rate": 2.6367239701589364e-05,
      "loss": 2.622,
      "step": 729900
    },
    {
      "epoch": 236.62884927066452,
      "grad_norm": 1.2343462705612183,
      "learning_rate": 2.6363996107687316e-05,
      "loss": 2.6477,
      "step": 730000
    },
    {
      "epoch": 236.6612641815235,
      "grad_norm": 1.3875797986984253,
      "learning_rate": 2.6360752513785275e-05,
      "loss": 2.6328,
      "step": 730100
    },
    {
      "epoch": 236.6936790923825,
      "grad_norm": 1.1373776197433472,
      "learning_rate": 2.6357508919883234e-05,
      "loss": 2.6429,
      "step": 730200
    },
    {
      "epoch": 236.72609400324149,
      "grad_norm": 1.3286758661270142,
      "learning_rate": 2.6354265325981186e-05,
      "loss": 2.6602,
      "step": 730300
    },
    {
      "epoch": 236.75850891410047,
      "grad_norm": 1.1790413856506348,
      "learning_rate": 2.6351054168018164e-05,
      "loss": 2.6207,
      "step": 730400
    },
    {
      "epoch": 236.7909238249595,
      "grad_norm": 1.1601029634475708,
      "learning_rate": 2.6347810574116122e-05,
      "loss": 2.662,
      "step": 730500
    },
    {
      "epoch": 236.82333873581848,
      "grad_norm": 1.2428377866744995,
      "learning_rate": 2.634456698021408e-05,
      "loss": 2.6481,
      "step": 730600
    },
    {
      "epoch": 236.85575364667747,
      "grad_norm": 1.4592641592025757,
      "learning_rate": 2.6341355822251055e-05,
      "loss": 2.6272,
      "step": 730700
    },
    {
      "epoch": 236.88816855753646,
      "grad_norm": 1.402180552482605,
      "learning_rate": 2.633811222834901e-05,
      "loss": 2.6387,
      "step": 730800
    },
    {
      "epoch": 236.92058346839545,
      "grad_norm": 1.2830475568771362,
      "learning_rate": 2.633486863444697e-05,
      "loss": 2.6414,
      "step": 730900
    },
    {
      "epoch": 236.95299837925447,
      "grad_norm": 1.470447301864624,
      "learning_rate": 2.6331625040544928e-05,
      "loss": 2.6528,
      "step": 731000
    },
    {
      "epoch": 236.98541329011346,
      "grad_norm": 1.1559221744537354,
      "learning_rate": 2.632838144664288e-05,
      "loss": 2.6584,
      "step": 731100
    },
    {
      "epoch": 237.0,
      "eval_bleu": 0.8443826991138237,
      "eval_loss": 4.074158191680908,
      "eval_runtime": 4.4775,
      "eval_samples_per_second": 109.882,
      "eval_steps_per_second": 1.787,
      "step": 731145
    },
    {
      "epoch": 237.01782820097245,
      "grad_norm": 1.219728946685791,
      "learning_rate": 2.632513785274084e-05,
      "loss": 2.641,
      "step": 731200
    },
    {
      "epoch": 237.05024311183143,
      "grad_norm": 1.2031619548797607,
      "learning_rate": 2.6321894258838798e-05,
      "loss": 2.6445,
      "step": 731300
    },
    {
      "epoch": 237.08265802269042,
      "grad_norm": 1.0784151554107666,
      "learning_rate": 2.631865066493675e-05,
      "loss": 2.6405,
      "step": 731400
    },
    {
      "epoch": 237.11507293354944,
      "grad_norm": 1.1467069387435913,
      "learning_rate": 2.631540707103471e-05,
      "loss": 2.6478,
      "step": 731500
    },
    {
      "epoch": 237.14748784440843,
      "grad_norm": 1.0939383506774902,
      "learning_rate": 2.631216347713266e-05,
      "loss": 2.6418,
      "step": 731600
    },
    {
      "epoch": 237.17990275526742,
      "grad_norm": 1.3183023929595947,
      "learning_rate": 2.630891988323062e-05,
      "loss": 2.6574,
      "step": 731700
    },
    {
      "epoch": 237.2123176661264,
      "grad_norm": 1.4113495349884033,
      "learning_rate": 2.6305676289328578e-05,
      "loss": 2.6689,
      "step": 731800
    },
    {
      "epoch": 237.24473257698543,
      "grad_norm": 1.1220449209213257,
      "learning_rate": 2.6302432695426533e-05,
      "loss": 2.6512,
      "step": 731900
    },
    {
      "epoch": 237.27714748784442,
      "grad_norm": 1.1178975105285645,
      "learning_rate": 2.6299189101524492e-05,
      "loss": 2.6395,
      "step": 732000
    },
    {
      "epoch": 237.3095623987034,
      "grad_norm": 1.2707598209381104,
      "learning_rate": 2.629594550762245e-05,
      "loss": 2.6278,
      "step": 732100
    },
    {
      "epoch": 237.3419773095624,
      "grad_norm": 1.3859875202178955,
      "learning_rate": 2.6292701913720403e-05,
      "loss": 2.6487,
      "step": 732200
    },
    {
      "epoch": 237.37439222042138,
      "grad_norm": 1.2113287448883057,
      "learning_rate": 2.628945831981836e-05,
      "loss": 2.6351,
      "step": 732300
    },
    {
      "epoch": 237.4068071312804,
      "grad_norm": 1.5712910890579224,
      "learning_rate": 2.6286214725916313e-05,
      "loss": 2.6483,
      "step": 732400
    },
    {
      "epoch": 237.4392220421394,
      "grad_norm": 1.4607051610946655,
      "learning_rate": 2.6282971132014272e-05,
      "loss": 2.6127,
      "step": 732500
    },
    {
      "epoch": 237.47163695299838,
      "grad_norm": 1.0743330717086792,
      "learning_rate": 2.627972753811223e-05,
      "loss": 2.616,
      "step": 732600
    },
    {
      "epoch": 237.50405186385737,
      "grad_norm": 1.1794228553771973,
      "learning_rate": 2.6276483944210183e-05,
      "loss": 2.6131,
      "step": 732700
    },
    {
      "epoch": 237.53646677471636,
      "grad_norm": 1.340993881225586,
      "learning_rate": 2.627324035030814e-05,
      "loss": 2.6508,
      "step": 732800
    },
    {
      "epoch": 237.56888168557538,
      "grad_norm": 1.4368090629577637,
      "learning_rate": 2.62699967564061e-05,
      "loss": 2.6549,
      "step": 732900
    },
    {
      "epoch": 237.60129659643437,
      "grad_norm": 1.3374814987182617,
      "learning_rate": 2.6266753162504056e-05,
      "loss": 2.6534,
      "step": 733000
    },
    {
      "epoch": 237.63371150729336,
      "grad_norm": 1.172997236251831,
      "learning_rate": 2.626350956860201e-05,
      "loss": 2.6297,
      "step": 733100
    },
    {
      "epoch": 237.66612641815234,
      "grad_norm": 1.4994443655014038,
      "learning_rate": 2.626026597469997e-05,
      "loss": 2.6491,
      "step": 733200
    },
    {
      "epoch": 237.69854132901133,
      "grad_norm": 1.1768690347671509,
      "learning_rate": 2.6257022380797925e-05,
      "loss": 2.6369,
      "step": 733300
    },
    {
      "epoch": 237.73095623987035,
      "grad_norm": 1.1691653728485107,
      "learning_rate": 2.6253778786895884e-05,
      "loss": 2.6692,
      "step": 733400
    },
    {
      "epoch": 237.76337115072934,
      "grad_norm": 1.4145008325576782,
      "learning_rate": 2.6250535192993836e-05,
      "loss": 2.6367,
      "step": 733500
    },
    {
      "epoch": 237.79578606158833,
      "grad_norm": 1.170240879058838,
      "learning_rate": 2.6247291599091795e-05,
      "loss": 2.6341,
      "step": 733600
    },
    {
      "epoch": 237.82820097244732,
      "grad_norm": 1.2817020416259766,
      "learning_rate": 2.6244048005189753e-05,
      "loss": 2.6531,
      "step": 733700
    },
    {
      "epoch": 237.8606158833063,
      "grad_norm": 1.1815766096115112,
      "learning_rate": 2.6240804411287705e-05,
      "loss": 2.6383,
      "step": 733800
    },
    {
      "epoch": 237.89303079416533,
      "grad_norm": 1.1571614742279053,
      "learning_rate": 2.6237560817385664e-05,
      "loss": 2.6368,
      "step": 733900
    },
    {
      "epoch": 237.92544570502432,
      "grad_norm": 1.4698941707611084,
      "learning_rate": 2.6234317223483623e-05,
      "loss": 2.6606,
      "step": 734000
    },
    {
      "epoch": 237.9578606158833,
      "grad_norm": 1.175139307975769,
      "learning_rate": 2.6231073629581575e-05,
      "loss": 2.6568,
      "step": 734100
    },
    {
      "epoch": 237.9902755267423,
      "grad_norm": 1.1573803424835205,
      "learning_rate": 2.6227830035679534e-05,
      "loss": 2.656,
      "step": 734200
    },
    {
      "epoch": 238.0,
      "eval_bleu": 0.808337117664316,
      "eval_loss": 4.078001976013184,
      "eval_runtime": 3.9312,
      "eval_samples_per_second": 125.152,
      "eval_steps_per_second": 2.035,
      "step": 734230
    },
    {
      "epoch": 238.02269043760128,
      "grad_norm": 1.3163275718688965,
      "learning_rate": 2.6224586441777492e-05,
      "loss": 2.6408,
      "step": 734300
    },
    {
      "epoch": 238.0551053484603,
      "grad_norm": 1.2154653072357178,
      "learning_rate": 2.622137528381447e-05,
      "loss": 2.6311,
      "step": 734400
    },
    {
      "epoch": 238.0875202593193,
      "grad_norm": 1.191727876663208,
      "learning_rate": 2.6218131689912422e-05,
      "loss": 2.6339,
      "step": 734500
    },
    {
      "epoch": 238.11993517017828,
      "grad_norm": 1.3686294555664062,
      "learning_rate": 2.621488809601038e-05,
      "loss": 2.6684,
      "step": 734600
    },
    {
      "epoch": 238.15235008103727,
      "grad_norm": 1.232094407081604,
      "learning_rate": 2.6211644502108336e-05,
      "loss": 2.6321,
      "step": 734700
    },
    {
      "epoch": 238.18476499189626,
      "grad_norm": 1.1319937705993652,
      "learning_rate": 2.620840090820629e-05,
      "loss": 2.6448,
      "step": 734800
    },
    {
      "epoch": 238.21717990275528,
      "grad_norm": 1.202237606048584,
      "learning_rate": 2.620515731430425e-05,
      "loss": 2.6297,
      "step": 734900
    },
    {
      "epoch": 238.24959481361427,
      "grad_norm": 1.2938224077224731,
      "learning_rate": 2.6201913720402206e-05,
      "loss": 2.619,
      "step": 735000
    },
    {
      "epoch": 238.28200972447326,
      "grad_norm": 1.5575957298278809,
      "learning_rate": 2.6198670126500164e-05,
      "loss": 2.6228,
      "step": 735100
    },
    {
      "epoch": 238.31442463533224,
      "grad_norm": 1.1892355680465698,
      "learning_rate": 2.6195426532598123e-05,
      "loss": 2.6724,
      "step": 735200
    },
    {
      "epoch": 238.34683954619126,
      "grad_norm": 1.4833298921585083,
      "learning_rate": 2.6192182938696075e-05,
      "loss": 2.634,
      "step": 735300
    },
    {
      "epoch": 238.37925445705025,
      "grad_norm": 1.0669670104980469,
      "learning_rate": 2.6188939344794034e-05,
      "loss": 2.6322,
      "step": 735400
    },
    {
      "epoch": 238.41166936790924,
      "grad_norm": 1.2462934255599976,
      "learning_rate": 2.6185695750891993e-05,
      "loss": 2.6406,
      "step": 735500
    },
    {
      "epoch": 238.44408427876823,
      "grad_norm": 1.3667606115341187,
      "learning_rate": 2.6182452156989944e-05,
      "loss": 2.6445,
      "step": 735600
    },
    {
      "epoch": 238.47649918962722,
      "grad_norm": 1.2163760662078857,
      "learning_rate": 2.6179208563087903e-05,
      "loss": 2.6559,
      "step": 735700
    },
    {
      "epoch": 238.50891410048624,
      "grad_norm": 1.301159143447876,
      "learning_rate": 2.6175964969185855e-05,
      "loss": 2.6481,
      "step": 735800
    },
    {
      "epoch": 238.54132901134523,
      "grad_norm": 1.1696592569351196,
      "learning_rate": 2.6172721375283814e-05,
      "loss": 2.6203,
      "step": 735900
    },
    {
      "epoch": 238.57374392220422,
      "grad_norm": 1.2898228168487549,
      "learning_rate": 2.6169477781381773e-05,
      "loss": 2.6441,
      "step": 736000
    },
    {
      "epoch": 238.6061588330632,
      "grad_norm": 1.197745442390442,
      "learning_rate": 2.6166234187479728e-05,
      "loss": 2.6405,
      "step": 736100
    },
    {
      "epoch": 238.6385737439222,
      "grad_norm": 1.4310694932937622,
      "learning_rate": 2.6162990593577687e-05,
      "loss": 2.6459,
      "step": 736200
    },
    {
      "epoch": 238.6709886547812,
      "grad_norm": 1.3298747539520264,
      "learning_rate": 2.6159746999675646e-05,
      "loss": 2.6437,
      "step": 736300
    },
    {
      "epoch": 238.7034035656402,
      "grad_norm": 1.2053354978561401,
      "learning_rate": 2.6156503405773597e-05,
      "loss": 2.6526,
      "step": 736400
    },
    {
      "epoch": 238.7358184764992,
      "grad_norm": 1.1399930715560913,
      "learning_rate": 2.6153259811871556e-05,
      "loss": 2.6511,
      "step": 736500
    },
    {
      "epoch": 238.76823338735818,
      "grad_norm": 1.4102637767791748,
      "learning_rate": 2.6150016217969515e-05,
      "loss": 2.6597,
      "step": 736600
    },
    {
      "epoch": 238.80064829821717,
      "grad_norm": 1.3293901681900024,
      "learning_rate": 2.6146772624067467e-05,
      "loss": 2.6293,
      "step": 736700
    },
    {
      "epoch": 238.8330632090762,
      "grad_norm": 1.354131817817688,
      "learning_rate": 2.6143529030165426e-05,
      "loss": 2.6424,
      "step": 736800
    },
    {
      "epoch": 238.86547811993518,
      "grad_norm": 1.2702893018722534,
      "learning_rate": 2.6140285436263378e-05,
      "loss": 2.6509,
      "step": 736900
    },
    {
      "epoch": 238.89789303079417,
      "grad_norm": 1.35001540184021,
      "learning_rate": 2.6137041842361336e-05,
      "loss": 2.64,
      "step": 737000
    },
    {
      "epoch": 238.93030794165315,
      "grad_norm": 1.3269151449203491,
      "learning_rate": 2.6133798248459295e-05,
      "loss": 2.6494,
      "step": 737100
    },
    {
      "epoch": 238.96272285251214,
      "grad_norm": 1.207816481590271,
      "learning_rate": 2.613055465455725e-05,
      "loss": 2.6307,
      "step": 737200
    },
    {
      "epoch": 238.99513776337116,
      "grad_norm": 1.1152387857437134,
      "learning_rate": 2.6127311060655206e-05,
      "loss": 2.6661,
      "step": 737300
    },
    {
      "epoch": 239.0,
      "eval_bleu": 1.136193541423576,
      "eval_loss": 4.074302673339844,
      "eval_runtime": 4.131,
      "eval_samples_per_second": 119.1,
      "eval_steps_per_second": 1.937,
      "step": 737315
    },
    {
      "epoch": 239.02755267423015,
      "grad_norm": 1.3772166967391968,
      "learning_rate": 2.6124067466753165e-05,
      "loss": 2.644,
      "step": 737400
    },
    {
      "epoch": 239.05996758508914,
      "grad_norm": 1.3688093423843384,
      "learning_rate": 2.612082387285112e-05,
      "loss": 2.6499,
      "step": 737500
    },
    {
      "epoch": 239.09238249594813,
      "grad_norm": 1.2275117635726929,
      "learning_rate": 2.611758027894908e-05,
      "loss": 2.6165,
      "step": 737600
    },
    {
      "epoch": 239.12479740680712,
      "grad_norm": 1.3701057434082031,
      "learning_rate": 2.6114336685047037e-05,
      "loss": 2.6279,
      "step": 737700
    },
    {
      "epoch": 239.15721231766614,
      "grad_norm": 1.3380727767944336,
      "learning_rate": 2.611109309114499e-05,
      "loss": 2.6167,
      "step": 737800
    },
    {
      "epoch": 239.18962722852513,
      "grad_norm": 1.2639693021774292,
      "learning_rate": 2.6107849497242948e-05,
      "loss": 2.6174,
      "step": 737900
    },
    {
      "epoch": 239.22204213938411,
      "grad_norm": 1.1049590110778809,
      "learning_rate": 2.61046059033409e-05,
      "loss": 2.6372,
      "step": 738000
    },
    {
      "epoch": 239.2544570502431,
      "grad_norm": 1.2292972803115845,
      "learning_rate": 2.610136230943886e-05,
      "loss": 2.6299,
      "step": 738100
    },
    {
      "epoch": 239.2868719611021,
      "grad_norm": 1.2856894731521606,
      "learning_rate": 2.6098118715536818e-05,
      "loss": 2.6505,
      "step": 738200
    },
    {
      "epoch": 239.3192868719611,
      "grad_norm": 1.188132405281067,
      "learning_rate": 2.609487512163477e-05,
      "loss": 2.6431,
      "step": 738300
    },
    {
      "epoch": 239.3517017828201,
      "grad_norm": 1.1453577280044556,
      "learning_rate": 2.6091663963671747e-05,
      "loss": 2.6496,
      "step": 738400
    },
    {
      "epoch": 239.3841166936791,
      "grad_norm": 1.295283317565918,
      "learning_rate": 2.6088420369769706e-05,
      "loss": 2.6473,
      "step": 738500
    },
    {
      "epoch": 239.41653160453808,
      "grad_norm": 1.282166600227356,
      "learning_rate": 2.6085176775867665e-05,
      "loss": 2.6296,
      "step": 738600
    },
    {
      "epoch": 239.4489465153971,
      "grad_norm": 1.2327133417129517,
      "learning_rate": 2.6081933181965617e-05,
      "loss": 2.6518,
      "step": 738700
    },
    {
      "epoch": 239.4813614262561,
      "grad_norm": 1.3222012519836426,
      "learning_rate": 2.6078689588063576e-05,
      "loss": 2.6365,
      "step": 738800
    },
    {
      "epoch": 239.51377633711508,
      "grad_norm": 1.2383815050125122,
      "learning_rate": 2.6075445994161534e-05,
      "loss": 2.6377,
      "step": 738900
    },
    {
      "epoch": 239.54619124797406,
      "grad_norm": 1.3412379026412964,
      "learning_rate": 2.6072202400259486e-05,
      "loss": 2.6404,
      "step": 739000
    },
    {
      "epoch": 239.57860615883305,
      "grad_norm": 1.341820240020752,
      "learning_rate": 2.6068958806357445e-05,
      "loss": 2.6468,
      "step": 739100
    },
    {
      "epoch": 239.61102106969207,
      "grad_norm": 1.2515408992767334,
      "learning_rate": 2.60657152124554e-05,
      "loss": 2.6456,
      "step": 739200
    },
    {
      "epoch": 239.64343598055106,
      "grad_norm": 1.205506682395935,
      "learning_rate": 2.606247161855336e-05,
      "loss": 2.6259,
      "step": 739300
    },
    {
      "epoch": 239.67585089141005,
      "grad_norm": 1.3393735885620117,
      "learning_rate": 2.6059228024651318e-05,
      "loss": 2.6357,
      "step": 739400
    },
    {
      "epoch": 239.70826580226904,
      "grad_norm": 1.3154945373535156,
      "learning_rate": 2.605598443074927e-05,
      "loss": 2.6329,
      "step": 739500
    },
    {
      "epoch": 239.74068071312803,
      "grad_norm": 1.235552191734314,
      "learning_rate": 2.605274083684723e-05,
      "loss": 2.6682,
      "step": 739600
    },
    {
      "epoch": 239.77309562398705,
      "grad_norm": 1.7127283811569214,
      "learning_rate": 2.6049497242945187e-05,
      "loss": 2.6492,
      "step": 739700
    },
    {
      "epoch": 239.80551053484604,
      "grad_norm": 1.4697116613388062,
      "learning_rate": 2.604625364904314e-05,
      "loss": 2.6333,
      "step": 739800
    },
    {
      "epoch": 239.83792544570503,
      "grad_norm": 1.2349892854690552,
      "learning_rate": 2.6043010055141098e-05,
      "loss": 2.6255,
      "step": 739900
    },
    {
      "epoch": 239.87034035656401,
      "grad_norm": 1.335774302482605,
      "learning_rate": 2.6039766461239057e-05,
      "loss": 2.6514,
      "step": 740000
    },
    {
      "epoch": 239.902755267423,
      "grad_norm": 1.279551386833191,
      "learning_rate": 2.603652286733701e-05,
      "loss": 2.6569,
      "step": 740100
    },
    {
      "epoch": 239.93517017828202,
      "grad_norm": 1.1685866117477417,
      "learning_rate": 2.6033279273434967e-05,
      "loss": 2.6328,
      "step": 740200
    },
    {
      "epoch": 239.967585089141,
      "grad_norm": 1.243342399597168,
      "learning_rate": 2.6030068115471945e-05,
      "loss": 2.6363,
      "step": 740300
    },
    {
      "epoch": 240.0,
      "grad_norm": 1.2066437005996704,
      "learning_rate": 2.6026824521569897e-05,
      "loss": 2.666,
      "step": 740400
    },
    {
      "epoch": 240.0,
      "eval_bleu": 1.0066965940170916,
      "eval_loss": 4.0725998878479,
      "eval_runtime": 3.8493,
      "eval_samples_per_second": 127.814,
      "eval_steps_per_second": 2.078,
      "step": 740400
    },
    {
      "epoch": 240.032414910859,
      "grad_norm": 1.217432975769043,
      "learning_rate": 2.6023580927667856e-05,
      "loss": 2.6278,
      "step": 740500
    },
    {
      "epoch": 240.06482982171798,
      "grad_norm": 1.2970125675201416,
      "learning_rate": 2.6020337333765815e-05,
      "loss": 2.6363,
      "step": 740600
    },
    {
      "epoch": 240.097244732577,
      "grad_norm": 1.2644033432006836,
      "learning_rate": 2.6017093739863767e-05,
      "loss": 2.6251,
      "step": 740700
    },
    {
      "epoch": 240.12965964343599,
      "grad_norm": 1.439972996711731,
      "learning_rate": 2.6013850145961725e-05,
      "loss": 2.6399,
      "step": 740800
    },
    {
      "epoch": 240.16207455429497,
      "grad_norm": 1.1805944442749023,
      "learning_rate": 2.6010606552059684e-05,
      "loss": 2.6463,
      "step": 740900
    },
    {
      "epoch": 240.19448946515396,
      "grad_norm": 1.3246246576309204,
      "learning_rate": 2.600736295815764e-05,
      "loss": 2.6291,
      "step": 741000
    },
    {
      "epoch": 240.22690437601295,
      "grad_norm": 1.2559375762939453,
      "learning_rate": 2.6004119364255598e-05,
      "loss": 2.6372,
      "step": 741100
    },
    {
      "epoch": 240.25931928687197,
      "grad_norm": 1.2716503143310547,
      "learning_rate": 2.6000875770353557e-05,
      "loss": 2.6476,
      "step": 741200
    },
    {
      "epoch": 240.29173419773096,
      "grad_norm": 1.1432294845581055,
      "learning_rate": 2.599763217645151e-05,
      "loss": 2.6303,
      "step": 741300
    },
    {
      "epoch": 240.32414910858995,
      "grad_norm": 1.233666181564331,
      "learning_rate": 2.5994388582549468e-05,
      "loss": 2.6307,
      "step": 741400
    },
    {
      "epoch": 240.35656401944894,
      "grad_norm": 1.2157114744186401,
      "learning_rate": 2.599114498864742e-05,
      "loss": 2.6373,
      "step": 741500
    },
    {
      "epoch": 240.38897893030793,
      "grad_norm": 1.5555721521377563,
      "learning_rate": 2.598790139474538e-05,
      "loss": 2.6482,
      "step": 741600
    },
    {
      "epoch": 240.42139384116695,
      "grad_norm": 1.1163758039474487,
      "learning_rate": 2.5984657800843337e-05,
      "loss": 2.6309,
      "step": 741700
    },
    {
      "epoch": 240.45380875202594,
      "grad_norm": 1.2485816478729248,
      "learning_rate": 2.598141420694129e-05,
      "loss": 2.6654,
      "step": 741800
    },
    {
      "epoch": 240.48622366288492,
      "grad_norm": 1.2488343715667725,
      "learning_rate": 2.5978170613039248e-05,
      "loss": 2.6412,
      "step": 741900
    },
    {
      "epoch": 240.5186385737439,
      "grad_norm": 1.213565707206726,
      "learning_rate": 2.5974927019137207e-05,
      "loss": 2.6488,
      "step": 742000
    },
    {
      "epoch": 240.5510534846029,
      "grad_norm": 1.4438328742980957,
      "learning_rate": 2.5971683425235162e-05,
      "loss": 2.6419,
      "step": 742100
    },
    {
      "epoch": 240.58346839546192,
      "grad_norm": 1.4060746431350708,
      "learning_rate": 2.5968439831333117e-05,
      "loss": 2.6431,
      "step": 742200
    },
    {
      "epoch": 240.6158833063209,
      "grad_norm": 1.1948150396347046,
      "learning_rate": 2.5965196237431076e-05,
      "loss": 2.6264,
      "step": 742300
    },
    {
      "epoch": 240.6482982171799,
      "grad_norm": 1.3064439296722412,
      "learning_rate": 2.596195264352903e-05,
      "loss": 2.6433,
      "step": 742400
    },
    {
      "epoch": 240.6807131280389,
      "grad_norm": 1.382271647453308,
      "learning_rate": 2.5958741485566006e-05,
      "loss": 2.6693,
      "step": 742500
    },
    {
      "epoch": 240.7131280388979,
      "grad_norm": 1.372106671333313,
      "learning_rate": 2.5955497891663965e-05,
      "loss": 2.6147,
      "step": 742600
    },
    {
      "epoch": 240.7455429497569,
      "grad_norm": 1.4165854454040527,
      "learning_rate": 2.595225429776192e-05,
      "loss": 2.6273,
      "step": 742700
    },
    {
      "epoch": 240.77795786061589,
      "grad_norm": 1.1771944761276245,
      "learning_rate": 2.594901070385988e-05,
      "loss": 2.6456,
      "step": 742800
    },
    {
      "epoch": 240.81037277147487,
      "grad_norm": 1.2825239896774292,
      "learning_rate": 2.5945767109957837e-05,
      "loss": 2.6549,
      "step": 742900
    },
    {
      "epoch": 240.84278768233386,
      "grad_norm": 1.1118991374969482,
      "learning_rate": 2.594252351605579e-05,
      "loss": 2.6578,
      "step": 743000
    },
    {
      "epoch": 240.87520259319288,
      "grad_norm": 1.1986414194107056,
      "learning_rate": 2.5939279922153748e-05,
      "loss": 2.6204,
      "step": 743100
    },
    {
      "epoch": 240.90761750405187,
      "grad_norm": 1.2749242782592773,
      "learning_rate": 2.5936036328251707e-05,
      "loss": 2.6247,
      "step": 743200
    },
    {
      "epoch": 240.94003241491086,
      "grad_norm": 1.4693547487258911,
      "learning_rate": 2.593279273434966e-05,
      "loss": 2.64,
      "step": 743300
    },
    {
      "epoch": 240.97244732576985,
      "grad_norm": 1.234912633895874,
      "learning_rate": 2.5929549140447618e-05,
      "loss": 2.6352,
      "step": 743400
    },
    {
      "epoch": 241.0,
      "eval_bleu": 1.0024739676173084,
      "eval_loss": 4.080326080322266,
      "eval_runtime": 4.2806,
      "eval_samples_per_second": 114.938,
      "eval_steps_per_second": 1.869,
      "step": 743485
    },
    {
      "epoch": 241.00486223662884,
      "grad_norm": 1.5501152276992798,
      "learning_rate": 2.5926305546545576e-05,
      "loss": 2.6514,
      "step": 743500
    },
    {
      "epoch": 241.03727714748786,
      "grad_norm": 1.243598222732544,
      "learning_rate": 2.5923061952643528e-05,
      "loss": 2.6241,
      "step": 743600
    },
    {
      "epoch": 241.06969205834685,
      "grad_norm": 1.219582438468933,
      "learning_rate": 2.5919818358741487e-05,
      "loss": 2.6223,
      "step": 743700
    },
    {
      "epoch": 241.10210696920583,
      "grad_norm": 1.412219524383545,
      "learning_rate": 2.5916574764839442e-05,
      "loss": 2.6505,
      "step": 743800
    },
    {
      "epoch": 241.13452188006482,
      "grad_norm": 1.4857518672943115,
      "learning_rate": 2.5913331170937398e-05,
      "loss": 2.6448,
      "step": 743900
    },
    {
      "epoch": 241.1669367909238,
      "grad_norm": 1.099816918373108,
      "learning_rate": 2.5910087577035356e-05,
      "loss": 2.6415,
      "step": 744000
    },
    {
      "epoch": 241.19935170178283,
      "grad_norm": 1.043739914894104,
      "learning_rate": 2.5906843983133312e-05,
      "loss": 2.615,
      "step": 744100
    },
    {
      "epoch": 241.23176661264182,
      "grad_norm": 1.1797603368759155,
      "learning_rate": 2.590360038923127e-05,
      "loss": 2.6332,
      "step": 744200
    },
    {
      "epoch": 241.2641815235008,
      "grad_norm": 1.2858837842941284,
      "learning_rate": 2.590035679532923e-05,
      "loss": 2.6436,
      "step": 744300
    },
    {
      "epoch": 241.2965964343598,
      "grad_norm": 1.3075315952301025,
      "learning_rate": 2.589711320142718e-05,
      "loss": 2.6199,
      "step": 744400
    },
    {
      "epoch": 241.3290113452188,
      "grad_norm": 1.3130278587341309,
      "learning_rate": 2.589386960752514e-05,
      "loss": 2.6326,
      "step": 744500
    },
    {
      "epoch": 241.3614262560778,
      "grad_norm": 1.3447659015655518,
      "learning_rate": 2.58906260136231e-05,
      "loss": 2.6364,
      "step": 744600
    },
    {
      "epoch": 241.3938411669368,
      "grad_norm": 1.4362047910690308,
      "learning_rate": 2.588738241972105e-05,
      "loss": 2.6258,
      "step": 744700
    },
    {
      "epoch": 241.42625607779578,
      "grad_norm": 1.3225170373916626,
      "learning_rate": 2.588413882581901e-05,
      "loss": 2.6435,
      "step": 744800
    },
    {
      "epoch": 241.45867098865477,
      "grad_norm": 1.193414568901062,
      "learning_rate": 2.588089523191696e-05,
      "loss": 2.6403,
      "step": 744900
    },
    {
      "epoch": 241.49108589951376,
      "grad_norm": 1.287002444267273,
      "learning_rate": 2.587765163801492e-05,
      "loss": 2.6069,
      "step": 745000
    },
    {
      "epoch": 241.52350081037278,
      "grad_norm": 1.3766672611236572,
      "learning_rate": 2.587440804411288e-05,
      "loss": 2.6346,
      "step": 745100
    },
    {
      "epoch": 241.55591572123177,
      "grad_norm": 1.4326597452163696,
      "learning_rate": 2.5871164450210834e-05,
      "loss": 2.643,
      "step": 745200
    },
    {
      "epoch": 241.58833063209076,
      "grad_norm": 1.2114120721817017,
      "learning_rate": 2.5867920856308793e-05,
      "loss": 2.6475,
      "step": 745300
    },
    {
      "epoch": 241.62074554294975,
      "grad_norm": 1.368568778038025,
      "learning_rate": 2.5864677262406752e-05,
      "loss": 2.6357,
      "step": 745400
    },
    {
      "epoch": 241.65316045380874,
      "grad_norm": 1.3148818016052246,
      "learning_rate": 2.5861433668504704e-05,
      "loss": 2.6419,
      "step": 745500
    },
    {
      "epoch": 241.68557536466776,
      "grad_norm": 1.2807400226593018,
      "learning_rate": 2.5858190074602662e-05,
      "loss": 2.6423,
      "step": 745600
    },
    {
      "epoch": 241.71799027552674,
      "grad_norm": 1.2773222923278809,
      "learning_rate": 2.5854946480700614e-05,
      "loss": 2.6641,
      "step": 745700
    },
    {
      "epoch": 241.75040518638573,
      "grad_norm": 1.199934720993042,
      "learning_rate": 2.5851702886798573e-05,
      "loss": 2.6456,
      "step": 745800
    },
    {
      "epoch": 241.78282009724472,
      "grad_norm": 1.1636576652526855,
      "learning_rate": 2.5848459292896532e-05,
      "loss": 2.6373,
      "step": 745900
    },
    {
      "epoch": 241.81523500810374,
      "grad_norm": 1.3631603717803955,
      "learning_rate": 2.5845215698994484e-05,
      "loss": 2.6439,
      "step": 746000
    },
    {
      "epoch": 241.84764991896273,
      "grad_norm": 1.043550729751587,
      "learning_rate": 2.5841972105092443e-05,
      "loss": 2.6399,
      "step": 746100
    },
    {
      "epoch": 241.88006482982172,
      "grad_norm": 1.1125446557998657,
      "learning_rate": 2.58387285111904e-05,
      "loss": 2.6464,
      "step": 746200
    },
    {
      "epoch": 241.9124797406807,
      "grad_norm": 1.360863447189331,
      "learning_rate": 2.5835484917288357e-05,
      "loss": 2.636,
      "step": 746300
    },
    {
      "epoch": 241.9448946515397,
      "grad_norm": 1.25265634059906,
      "learning_rate": 2.5832241323386312e-05,
      "loss": 2.647,
      "step": 746400
    },
    {
      "epoch": 241.97730956239872,
      "grad_norm": 1.4542757272720337,
      "learning_rate": 2.582899772948427e-05,
      "loss": 2.6386,
      "step": 746500
    },
    {
      "epoch": 242.0,
      "eval_bleu": 1.0725352109765947,
      "eval_loss": 4.074095726013184,
      "eval_runtime": 4.8476,
      "eval_samples_per_second": 101.494,
      "eval_steps_per_second": 1.65,
      "step": 746570
    },
    {
      "epoch": 242.0097244732577,
      "grad_norm": 1.1493401527404785,
      "learning_rate": 2.582578657152125e-05,
      "loss": 2.6395,
      "step": 746600
    },
    {
      "epoch": 242.0421393841167,
      "grad_norm": 1.1823804378509521,
      "learning_rate": 2.58225429776192e-05,
      "loss": 2.6381,
      "step": 746700
    },
    {
      "epoch": 242.07455429497568,
      "grad_norm": 1.1153967380523682,
      "learning_rate": 2.581929938371716e-05,
      "loss": 2.6183,
      "step": 746800
    },
    {
      "epoch": 242.10696920583467,
      "grad_norm": 1.43474543094635,
      "learning_rate": 2.5816055789815118e-05,
      "loss": 2.6338,
      "step": 746900
    },
    {
      "epoch": 242.1393841166937,
      "grad_norm": 1.344824194908142,
      "learning_rate": 2.5812812195913073e-05,
      "loss": 2.6174,
      "step": 747000
    },
    {
      "epoch": 242.17179902755268,
      "grad_norm": 1.3164912462234497,
      "learning_rate": 2.5809568602011032e-05,
      "loss": 2.5996,
      "step": 747100
    },
    {
      "epoch": 242.20421393841167,
      "grad_norm": 1.3069169521331787,
      "learning_rate": 2.5806325008108984e-05,
      "loss": 2.6175,
      "step": 747200
    },
    {
      "epoch": 242.23662884927066,
      "grad_norm": 1.2710758447647095,
      "learning_rate": 2.5803081414206943e-05,
      "loss": 2.6344,
      "step": 747300
    },
    {
      "epoch": 242.26904376012965,
      "grad_norm": 1.2408666610717773,
      "learning_rate": 2.57998378203049e-05,
      "loss": 2.6306,
      "step": 747400
    },
    {
      "epoch": 242.30145867098867,
      "grad_norm": 1.4485052824020386,
      "learning_rate": 2.5796594226402854e-05,
      "loss": 2.6224,
      "step": 747500
    },
    {
      "epoch": 242.33387358184766,
      "grad_norm": 1.246525764465332,
      "learning_rate": 2.5793350632500812e-05,
      "loss": 2.6413,
      "step": 747600
    },
    {
      "epoch": 242.36628849270664,
      "grad_norm": 1.1735212802886963,
      "learning_rate": 2.579010703859877e-05,
      "loss": 2.6527,
      "step": 747700
    },
    {
      "epoch": 242.39870340356563,
      "grad_norm": 1.229965329170227,
      "learning_rate": 2.5786863444696723e-05,
      "loss": 2.6336,
      "step": 747800
    },
    {
      "epoch": 242.43111831442462,
      "grad_norm": 1.1456921100616455,
      "learning_rate": 2.5783619850794682e-05,
      "loss": 2.6499,
      "step": 747900
    },
    {
      "epoch": 242.46353322528364,
      "grad_norm": 1.363645315170288,
      "learning_rate": 2.578037625689264e-05,
      "loss": 2.6328,
      "step": 748000
    },
    {
      "epoch": 242.49594813614263,
      "grad_norm": 1.5318481922149658,
      "learning_rate": 2.5777132662990593e-05,
      "loss": 2.6285,
      "step": 748100
    },
    {
      "epoch": 242.52836304700162,
      "grad_norm": 1.3697779178619385,
      "learning_rate": 2.577388906908855e-05,
      "loss": 2.6393,
      "step": 748200
    },
    {
      "epoch": 242.5607779578606,
      "grad_norm": 1.3669463396072388,
      "learning_rate": 2.5770645475186507e-05,
      "loss": 2.6363,
      "step": 748300
    },
    {
      "epoch": 242.5931928687196,
      "grad_norm": 1.1411421298980713,
      "learning_rate": 2.5767401881284465e-05,
      "loss": 2.6256,
      "step": 748400
    },
    {
      "epoch": 242.62560777957862,
      "grad_norm": 1.3412994146347046,
      "learning_rate": 2.5764158287382424e-05,
      "loss": 2.6414,
      "step": 748500
    },
    {
      "epoch": 242.6580226904376,
      "grad_norm": 1.1235969066619873,
      "learning_rate": 2.57609471294194e-05,
      "loss": 2.657,
      "step": 748600
    },
    {
      "epoch": 242.6904376012966,
      "grad_norm": 1.278239369392395,
      "learning_rate": 2.5757703535517354e-05,
      "loss": 2.6499,
      "step": 748700
    },
    {
      "epoch": 242.72285251215558,
      "grad_norm": 1.20552659034729,
      "learning_rate": 2.5754459941615313e-05,
      "loss": 2.6272,
      "step": 748800
    },
    {
      "epoch": 242.75526742301457,
      "grad_norm": 1.1760722398757935,
      "learning_rate": 2.5751216347713268e-05,
      "loss": 2.6365,
      "step": 748900
    },
    {
      "epoch": 242.7876823338736,
      "grad_norm": 1.4972275495529175,
      "learning_rate": 2.5747972753811223e-05,
      "loss": 2.6355,
      "step": 749000
    },
    {
      "epoch": 242.82009724473258,
      "grad_norm": 1.4594963788986206,
      "learning_rate": 2.5744729159909182e-05,
      "loss": 2.6183,
      "step": 749100
    },
    {
      "epoch": 242.85251215559157,
      "grad_norm": 1.239436149597168,
      "learning_rate": 2.574148556600714e-05,
      "loss": 2.6349,
      "step": 749200
    },
    {
      "epoch": 242.88492706645056,
      "grad_norm": 1.3396799564361572,
      "learning_rate": 2.5738241972105093e-05,
      "loss": 2.6523,
      "step": 749300
    },
    {
      "epoch": 242.91734197730958,
      "grad_norm": 1.4034196138381958,
      "learning_rate": 2.573499837820305e-05,
      "loss": 2.6625,
      "step": 749400
    },
    {
      "epoch": 242.94975688816857,
      "grad_norm": 1.1520391702651978,
      "learning_rate": 2.5731754784301003e-05,
      "loss": 2.6412,
      "step": 749500
    },
    {
      "epoch": 242.98217179902755,
      "grad_norm": 1.149100661277771,
      "learning_rate": 2.5728511190398962e-05,
      "loss": 2.6549,
      "step": 749600
    },
    {
      "epoch": 243.0,
      "eval_bleu": 1.188476151370537,
      "eval_loss": 4.085279941558838,
      "eval_runtime": 3.9909,
      "eval_samples_per_second": 123.279,
      "eval_steps_per_second": 2.005,
      "step": 749655
    },
    {
      "epoch": 243.01458670988654,
      "grad_norm": 1.2772332429885864,
      "learning_rate": 2.572526759649692e-05,
      "loss": 2.6459,
      "step": 749700
    },
    {
      "epoch": 243.04700162074553,
      "grad_norm": 1.3341734409332275,
      "learning_rate": 2.5722024002594873e-05,
      "loss": 2.6383,
      "step": 749800
    },
    {
      "epoch": 243.07941653160455,
      "grad_norm": 1.1766839027404785,
      "learning_rate": 2.571878040869283e-05,
      "loss": 2.6222,
      "step": 749900
    },
    {
      "epoch": 243.11183144246354,
      "grad_norm": 1.177681565284729,
      "learning_rate": 2.571553681479079e-05,
      "loss": 2.6301,
      "step": 750000
    },
    {
      "epoch": 243.14424635332253,
      "grad_norm": 1.323122262954712,
      "learning_rate": 2.5712293220888746e-05,
      "loss": 2.6306,
      "step": 750100
    },
    {
      "epoch": 243.17666126418152,
      "grad_norm": 1.1330434083938599,
      "learning_rate": 2.5709049626986705e-05,
      "loss": 2.631,
      "step": 750200
    },
    {
      "epoch": 243.2090761750405,
      "grad_norm": 1.353775978088379,
      "learning_rate": 2.5705806033084663e-05,
      "loss": 2.6218,
      "step": 750300
    },
    {
      "epoch": 243.24149108589953,
      "grad_norm": 1.4017337560653687,
      "learning_rate": 2.5702562439182615e-05,
      "loss": 2.6397,
      "step": 750400
    },
    {
      "epoch": 243.27390599675851,
      "grad_norm": 1.276645302772522,
      "learning_rate": 2.5699318845280574e-05,
      "loss": 2.6252,
      "step": 750500
    },
    {
      "epoch": 243.3063209076175,
      "grad_norm": 1.3057457208633423,
      "learning_rate": 2.569610768731755e-05,
      "loss": 2.6524,
      "step": 750600
    },
    {
      "epoch": 243.3387358184765,
      "grad_norm": 1.1039800643920898,
      "learning_rate": 2.5692864093415504e-05,
      "loss": 2.6406,
      "step": 750700
    },
    {
      "epoch": 243.37115072933548,
      "grad_norm": 1.2481883764266968,
      "learning_rate": 2.5689620499513462e-05,
      "loss": 2.6115,
      "step": 750800
    },
    {
      "epoch": 243.4035656401945,
      "grad_norm": 1.2937006950378418,
      "learning_rate": 2.568637690561142e-05,
      "loss": 2.638,
      "step": 750900
    },
    {
      "epoch": 243.4359805510535,
      "grad_norm": 1.3645118474960327,
      "learning_rate": 2.5683133311709373e-05,
      "loss": 2.6309,
      "step": 751000
    },
    {
      "epoch": 243.46839546191248,
      "grad_norm": 1.4015990495681763,
      "learning_rate": 2.5679889717807332e-05,
      "loss": 2.6343,
      "step": 751100
    },
    {
      "epoch": 243.50081037277147,
      "grad_norm": 1.3042625188827515,
      "learning_rate": 2.567664612390529e-05,
      "loss": 2.6248,
      "step": 751200
    },
    {
      "epoch": 243.53322528363046,
      "grad_norm": 1.2187221050262451,
      "learning_rate": 2.5673402530003243e-05,
      "loss": 2.6335,
      "step": 751300
    },
    {
      "epoch": 243.56564019448948,
      "grad_norm": 1.426946997642517,
      "learning_rate": 2.56701589361012e-05,
      "loss": 2.6308,
      "step": 751400
    },
    {
      "epoch": 243.59805510534846,
      "grad_norm": 1.1618937253952026,
      "learning_rate": 2.566691534219916e-05,
      "loss": 2.6223,
      "step": 751500
    },
    {
      "epoch": 243.63047001620745,
      "grad_norm": 1.3364479541778564,
      "learning_rate": 2.5663671748297112e-05,
      "loss": 2.6327,
      "step": 751600
    },
    {
      "epoch": 243.66288492706644,
      "grad_norm": 1.2803248167037964,
      "learning_rate": 2.566042815439507e-05,
      "loss": 2.6376,
      "step": 751700
    },
    {
      "epoch": 243.69529983792543,
      "grad_norm": 1.2193567752838135,
      "learning_rate": 2.5657184560493026e-05,
      "loss": 2.6301,
      "step": 751800
    },
    {
      "epoch": 243.72771474878445,
      "grad_norm": 1.2660391330718994,
      "learning_rate": 2.5653940966590985e-05,
      "loss": 2.6452,
      "step": 751900
    },
    {
      "epoch": 243.76012965964344,
      "grad_norm": 1.5841315984725952,
      "learning_rate": 2.5650697372688944e-05,
      "loss": 2.6242,
      "step": 752000
    },
    {
      "epoch": 243.79254457050243,
      "grad_norm": 1.3156365156173706,
      "learning_rate": 2.5647453778786896e-05,
      "loss": 2.6432,
      "step": 752100
    },
    {
      "epoch": 243.82495948136142,
      "grad_norm": 1.3042068481445312,
      "learning_rate": 2.5644210184884854e-05,
      "loss": 2.6222,
      "step": 752200
    },
    {
      "epoch": 243.8573743922204,
      "grad_norm": 1.3788236379623413,
      "learning_rate": 2.5640966590982813e-05,
      "loss": 2.6556,
      "step": 752300
    },
    {
      "epoch": 243.88978930307943,
      "grad_norm": 1.3289270401000977,
      "learning_rate": 2.5637722997080765e-05,
      "loss": 2.6356,
      "step": 752400
    },
    {
      "epoch": 243.92220421393841,
      "grad_norm": 1.2863433361053467,
      "learning_rate": 2.5634479403178724e-05,
      "loss": 2.6559,
      "step": 752500
    },
    {
      "epoch": 243.9546191247974,
      "grad_norm": 1.1431454420089722,
      "learning_rate": 2.56312682452157e-05,
      "loss": 2.6289,
      "step": 752600
    },
    {
      "epoch": 243.9870340356564,
      "grad_norm": 1.250127911567688,
      "learning_rate": 2.562802465131366e-05,
      "loss": 2.6399,
      "step": 752700
    },
    {
      "epoch": 244.0,
      "eval_bleu": 1.0968641872228888,
      "eval_loss": 4.085362434387207,
      "eval_runtime": 4.3482,
      "eval_samples_per_second": 113.151,
      "eval_steps_per_second": 1.84,
      "step": 752740
    },
    {
      "epoch": 244.0194489465154,
      "grad_norm": 1.1640573740005493,
      "learning_rate": 2.5624781057411612e-05,
      "loss": 2.6187,
      "step": 752800
    },
    {
      "epoch": 244.0518638573744,
      "grad_norm": 1.2690426111221313,
      "learning_rate": 2.562153746350957e-05,
      "loss": 2.6235,
      "step": 752900
    },
    {
      "epoch": 244.0842787682334,
      "grad_norm": 1.3819077014923096,
      "learning_rate": 2.561832630554655e-05,
      "loss": 2.6394,
      "step": 753000
    },
    {
      "epoch": 244.11669367909238,
      "grad_norm": 1.3003028631210327,
      "learning_rate": 2.5615082711644504e-05,
      "loss": 2.615,
      "step": 753100
    },
    {
      "epoch": 244.14910858995137,
      "grad_norm": 1.2572487592697144,
      "learning_rate": 2.561183911774246e-05,
      "loss": 2.6268,
      "step": 753200
    },
    {
      "epoch": 244.18152350081039,
      "grad_norm": 1.3970293998718262,
      "learning_rate": 2.560859552384042e-05,
      "loss": 2.6236,
      "step": 753300
    },
    {
      "epoch": 244.21393841166937,
      "grad_norm": 1.4187945127487183,
      "learning_rate": 2.560535192993837e-05,
      "loss": 2.6597,
      "step": 753400
    },
    {
      "epoch": 244.24635332252836,
      "grad_norm": 1.2493886947631836,
      "learning_rate": 2.560210833603633e-05,
      "loss": 2.6117,
      "step": 753500
    },
    {
      "epoch": 244.27876823338735,
      "grad_norm": 1.2375048398971558,
      "learning_rate": 2.5598864742134288e-05,
      "loss": 2.6433,
      "step": 753600
    },
    {
      "epoch": 244.31118314424634,
      "grad_norm": 1.5466437339782715,
      "learning_rate": 2.559562114823224e-05,
      "loss": 2.6308,
      "step": 753700
    },
    {
      "epoch": 244.34359805510536,
      "grad_norm": 1.263683795928955,
      "learning_rate": 2.55923775543302e-05,
      "loss": 2.6142,
      "step": 753800
    },
    {
      "epoch": 244.37601296596435,
      "grad_norm": 1.277734398841858,
      "learning_rate": 2.5589133960428157e-05,
      "loss": 2.6345,
      "step": 753900
    },
    {
      "epoch": 244.40842787682334,
      "grad_norm": 1.3563339710235596,
      "learning_rate": 2.5585922802465135e-05,
      "loss": 2.6417,
      "step": 754000
    },
    {
      "epoch": 244.44084278768233,
      "grad_norm": 1.283003568649292,
      "learning_rate": 2.5582679208563087e-05,
      "loss": 2.6302,
      "step": 754100
    },
    {
      "epoch": 244.47325769854132,
      "grad_norm": 1.2828868627548218,
      "learning_rate": 2.5579435614661046e-05,
      "loss": 2.6353,
      "step": 754200
    },
    {
      "epoch": 244.50567260940034,
      "grad_norm": 1.201557993888855,
      "learning_rate": 2.5576192020759004e-05,
      "loss": 2.6355,
      "step": 754300
    },
    {
      "epoch": 244.53808752025932,
      "grad_norm": 1.3887523412704468,
      "learning_rate": 2.5572948426856956e-05,
      "loss": 2.6332,
      "step": 754400
    },
    {
      "epoch": 244.5705024311183,
      "grad_norm": 1.2426424026489258,
      "learning_rate": 2.5569704832954915e-05,
      "loss": 2.6228,
      "step": 754500
    },
    {
      "epoch": 244.6029173419773,
      "grad_norm": 1.402520775794983,
      "learning_rate": 2.556646123905287e-05,
      "loss": 2.6309,
      "step": 754600
    },
    {
      "epoch": 244.6353322528363,
      "grad_norm": 1.2131332159042358,
      "learning_rate": 2.556321764515083e-05,
      "loss": 2.6356,
      "step": 754700
    },
    {
      "epoch": 244.6677471636953,
      "grad_norm": 1.1906559467315674,
      "learning_rate": 2.5559974051248785e-05,
      "loss": 2.6335,
      "step": 754800
    },
    {
      "epoch": 244.7001620745543,
      "grad_norm": 1.3094698190689087,
      "learning_rate": 2.555673045734674e-05,
      "loss": 2.6347,
      "step": 754900
    },
    {
      "epoch": 244.7325769854133,
      "grad_norm": 1.3951313495635986,
      "learning_rate": 2.55534868634447e-05,
      "loss": 2.6553,
      "step": 755000
    },
    {
      "epoch": 244.76499189627228,
      "grad_norm": 1.4556572437286377,
      "learning_rate": 2.5550243269542657e-05,
      "loss": 2.613,
      "step": 755100
    },
    {
      "epoch": 244.79740680713127,
      "grad_norm": 1.2088497877120972,
      "learning_rate": 2.554699967564061e-05,
      "loss": 2.6425,
      "step": 755200
    },
    {
      "epoch": 244.82982171799028,
      "grad_norm": 1.5649845600128174,
      "learning_rate": 2.5543756081738568e-05,
      "loss": 2.6164,
      "step": 755300
    },
    {
      "epoch": 244.86223662884927,
      "grad_norm": 1.323384165763855,
      "learning_rate": 2.5540512487836527e-05,
      "loss": 2.6198,
      "step": 755400
    },
    {
      "epoch": 244.89465153970826,
      "grad_norm": 1.4182571172714233,
      "learning_rate": 2.553726889393448e-05,
      "loss": 2.6667,
      "step": 755500
    },
    {
      "epoch": 244.92706645056725,
      "grad_norm": 1.321184754371643,
      "learning_rate": 2.5534025300032438e-05,
      "loss": 2.6541,
      "step": 755600
    },
    {
      "epoch": 244.95948136142624,
      "grad_norm": 1.4673892259597778,
      "learning_rate": 2.553078170613039e-05,
      "loss": 2.6248,
      "step": 755700
    },
    {
      "epoch": 244.99189627228526,
      "grad_norm": 1.254446268081665,
      "learning_rate": 2.552753811222835e-05,
      "loss": 2.6265,
      "step": 755800
    },
    {
      "epoch": 245.0,
      "eval_bleu": 1.0996694708774302,
      "eval_loss": 4.082738399505615,
      "eval_runtime": 4.4247,
      "eval_samples_per_second": 111.195,
      "eval_steps_per_second": 1.808,
      "step": 755825
    },
    {
      "epoch": 245.02431118314425,
      "grad_norm": 1.1131329536437988,
      "learning_rate": 2.5524294518326307e-05,
      "loss": 2.628,
      "step": 755900
    },
    {
      "epoch": 245.05672609400324,
      "grad_norm": 1.6549911499023438,
      "learning_rate": 2.5521050924424262e-05,
      "loss": 2.6308,
      "step": 756000
    },
    {
      "epoch": 245.08914100486223,
      "grad_norm": 1.151039958000183,
      "learning_rate": 2.551780733052222e-05,
      "loss": 2.6449,
      "step": 756100
    },
    {
      "epoch": 245.12155591572125,
      "grad_norm": 1.1686842441558838,
      "learning_rate": 2.551456373662018e-05,
      "loss": 2.6245,
      "step": 756200
    },
    {
      "epoch": 245.15397082658023,
      "grad_norm": 1.182180643081665,
      "learning_rate": 2.5511320142718132e-05,
      "loss": 2.6224,
      "step": 756300
    },
    {
      "epoch": 245.18638573743922,
      "grad_norm": 1.345167875289917,
      "learning_rate": 2.550807654881609e-05,
      "loss": 2.6235,
      "step": 756400
    },
    {
      "epoch": 245.2188006482982,
      "grad_norm": 1.1777809858322144,
      "learning_rate": 2.550483295491405e-05,
      "loss": 2.6443,
      "step": 756500
    },
    {
      "epoch": 245.2512155591572,
      "grad_norm": 1.2466199398040771,
      "learning_rate": 2.5501589361012e-05,
      "loss": 2.6124,
      "step": 756600
    },
    {
      "epoch": 245.28363047001622,
      "grad_norm": 1.1240556240081787,
      "learning_rate": 2.549834576710996e-05,
      "loss": 2.6244,
      "step": 756700
    },
    {
      "epoch": 245.3160453808752,
      "grad_norm": 1.4319766759872437,
      "learning_rate": 2.5495102173207912e-05,
      "loss": 2.6294,
      "step": 756800
    },
    {
      "epoch": 245.3484602917342,
      "grad_norm": 1.2433390617370605,
      "learning_rate": 2.549185857930587e-05,
      "loss": 2.6526,
      "step": 756900
    },
    {
      "epoch": 245.3808752025932,
      "grad_norm": 1.2733612060546875,
      "learning_rate": 2.548861498540383e-05,
      "loss": 2.6325,
      "step": 757000
    },
    {
      "epoch": 245.41329011345218,
      "grad_norm": 1.21898353099823,
      "learning_rate": 2.5485371391501785e-05,
      "loss": 2.6205,
      "step": 757100
    },
    {
      "epoch": 245.4457050243112,
      "grad_norm": 1.2710587978363037,
      "learning_rate": 2.548212779759974e-05,
      "loss": 2.6386,
      "step": 757200
    },
    {
      "epoch": 245.47811993517018,
      "grad_norm": 1.3373448848724365,
      "learning_rate": 2.54788842036977e-05,
      "loss": 2.6274,
      "step": 757300
    },
    {
      "epoch": 245.51053484602917,
      "grad_norm": 1.3666826486587524,
      "learning_rate": 2.5475640609795654e-05,
      "loss": 2.6338,
      "step": 757400
    },
    {
      "epoch": 245.54294975688816,
      "grad_norm": 1.2135769128799438,
      "learning_rate": 2.5472397015893613e-05,
      "loss": 2.6232,
      "step": 757500
    },
    {
      "epoch": 245.57536466774715,
      "grad_norm": 1.31293523311615,
      "learning_rate": 2.5469185857930588e-05,
      "loss": 2.6183,
      "step": 757600
    },
    {
      "epoch": 245.60777957860617,
      "grad_norm": 1.082385540008545,
      "learning_rate": 2.5465942264028546e-05,
      "loss": 2.6261,
      "step": 757700
    },
    {
      "epoch": 245.64019448946516,
      "grad_norm": 1.197246789932251,
      "learning_rate": 2.54626986701265e-05,
      "loss": 2.6301,
      "step": 757800
    },
    {
      "epoch": 245.67260940032415,
      "grad_norm": 1.4305970668792725,
      "learning_rate": 2.545945507622446e-05,
      "loss": 2.6156,
      "step": 757900
    },
    {
      "epoch": 245.70502431118314,
      "grad_norm": 1.374718427658081,
      "learning_rate": 2.5456211482322412e-05,
      "loss": 2.6426,
      "step": 758000
    },
    {
      "epoch": 245.73743922204213,
      "grad_norm": 1.3238017559051514,
      "learning_rate": 2.545296788842037e-05,
      "loss": 2.6353,
      "step": 758100
    },
    {
      "epoch": 245.76985413290114,
      "grad_norm": 1.3012827634811401,
      "learning_rate": 2.544972429451833e-05,
      "loss": 2.6226,
      "step": 758200
    },
    {
      "epoch": 245.80226904376013,
      "grad_norm": 1.39275062084198,
      "learning_rate": 2.5446480700616282e-05,
      "loss": 2.6267,
      "step": 758300
    },
    {
      "epoch": 245.83468395461912,
      "grad_norm": 1.2760944366455078,
      "learning_rate": 2.544323710671424e-05,
      "loss": 2.6626,
      "step": 758400
    },
    {
      "epoch": 245.8670988654781,
      "grad_norm": 1.2787493467330933,
      "learning_rate": 2.54399935128122e-05,
      "loss": 2.6446,
      "step": 758500
    },
    {
      "epoch": 245.8995137763371,
      "grad_norm": 1.3168799877166748,
      "learning_rate": 2.543674991891015e-05,
      "loss": 2.6511,
      "step": 758600
    },
    {
      "epoch": 245.93192868719612,
      "grad_norm": 1.2958160638809204,
      "learning_rate": 2.543350632500811e-05,
      "loss": 2.6063,
      "step": 758700
    },
    {
      "epoch": 245.9643435980551,
      "grad_norm": 1.2247586250305176,
      "learning_rate": 2.543026273110607e-05,
      "loss": 2.6416,
      "step": 758800
    },
    {
      "epoch": 245.9967585089141,
      "grad_norm": 1.2478998899459839,
      "learning_rate": 2.542701913720402e-05,
      "loss": 2.6298,
      "step": 758900
    },
    {
      "epoch": 246.0,
      "eval_bleu": 1.140117343821558,
      "eval_loss": 4.0848002433776855,
      "eval_runtime": 4.2408,
      "eval_samples_per_second": 116.015,
      "eval_steps_per_second": 1.886,
      "step": 758910
    },
    {
      "epoch": 246.0291734197731,
      "grad_norm": 1.2391085624694824,
      "learning_rate": 2.542377554330198e-05,
      "loss": 2.6132,
      "step": 759000
    },
    {
      "epoch": 246.06158833063208,
      "grad_norm": 1.1837745904922485,
      "learning_rate": 2.5420531949399935e-05,
      "loss": 2.6233,
      "step": 759100
    },
    {
      "epoch": 246.0940032414911,
      "grad_norm": 1.21886146068573,
      "learning_rate": 2.5417288355497894e-05,
      "loss": 2.6465,
      "step": 759200
    },
    {
      "epoch": 246.12641815235008,
      "grad_norm": 1.1620268821716309,
      "learning_rate": 2.5414044761595852e-05,
      "loss": 2.6328,
      "step": 759300
    },
    {
      "epoch": 246.15883306320907,
      "grad_norm": 1.2596914768218994,
      "learning_rate": 2.5410801167693804e-05,
      "loss": 2.6174,
      "step": 759400
    },
    {
      "epoch": 246.19124797406806,
      "grad_norm": 1.264399766921997,
      "learning_rate": 2.5407557573791763e-05,
      "loss": 2.622,
      "step": 759500
    },
    {
      "epoch": 246.22366288492708,
      "grad_norm": 1.3644180297851562,
      "learning_rate": 2.5404313979889722e-05,
      "loss": 2.6148,
      "step": 759600
    },
    {
      "epoch": 246.25607779578607,
      "grad_norm": 1.1858007907867432,
      "learning_rate": 2.5401070385987674e-05,
      "loss": 2.6344,
      "step": 759700
    },
    {
      "epoch": 246.28849270664506,
      "grad_norm": 1.3264671564102173,
      "learning_rate": 2.5397826792085632e-05,
      "loss": 2.6212,
      "step": 759800
    },
    {
      "epoch": 246.32090761750405,
      "grad_norm": 1.2327501773834229,
      "learning_rate": 2.539458319818359e-05,
      "loss": 2.6401,
      "step": 759900
    },
    {
      "epoch": 246.35332252836304,
      "grad_norm": 1.1830836534500122,
      "learning_rate": 2.5391339604281543e-05,
      "loss": 2.6154,
      "step": 760000
    },
    {
      "epoch": 246.38573743922205,
      "grad_norm": 1.3405224084854126,
      "learning_rate": 2.5388096010379502e-05,
      "loss": 2.6456,
      "step": 760100
    },
    {
      "epoch": 246.41815235008104,
      "grad_norm": 1.2557250261306763,
      "learning_rate": 2.5384852416477457e-05,
      "loss": 2.6351,
      "step": 760200
    },
    {
      "epoch": 246.45056726094003,
      "grad_norm": 1.4671565294265747,
      "learning_rate": 2.5381608822575416e-05,
      "loss": 2.64,
      "step": 760300
    },
    {
      "epoch": 246.48298217179902,
      "grad_norm": 1.4307421445846558,
      "learning_rate": 2.5378365228673375e-05,
      "loss": 2.6366,
      "step": 760400
    },
    {
      "epoch": 246.515397082658,
      "grad_norm": 1.3726726770401,
      "learning_rate": 2.5375121634771327e-05,
      "loss": 2.6359,
      "step": 760500
    },
    {
      "epoch": 246.54781199351703,
      "grad_norm": 1.3159430027008057,
      "learning_rate": 2.5371878040869285e-05,
      "loss": 2.6332,
      "step": 760600
    },
    {
      "epoch": 246.58022690437602,
      "grad_norm": 1.428481936454773,
      "learning_rate": 2.5368634446967244e-05,
      "loss": 2.6172,
      "step": 760700
    },
    {
      "epoch": 246.612641815235,
      "grad_norm": 1.241295576095581,
      "learning_rate": 2.5365390853065196e-05,
      "loss": 2.6502,
      "step": 760800
    },
    {
      "epoch": 246.645056726094,
      "grad_norm": 1.3257185220718384,
      "learning_rate": 2.5362147259163155e-05,
      "loss": 2.6294,
      "step": 760900
    },
    {
      "epoch": 246.677471636953,
      "grad_norm": 1.2628748416900635,
      "learning_rate": 2.5358903665261107e-05,
      "loss": 2.6282,
      "step": 761000
    },
    {
      "epoch": 246.709886547812,
      "grad_norm": 1.3371825218200684,
      "learning_rate": 2.5355660071359066e-05,
      "loss": 2.639,
      "step": 761100
    },
    {
      "epoch": 246.742301458671,
      "grad_norm": 1.258745551109314,
      "learning_rate": 2.5352416477457024e-05,
      "loss": 2.6158,
      "step": 761200
    },
    {
      "epoch": 246.77471636952998,
      "grad_norm": 1.3078200817108154,
      "learning_rate": 2.5349205319494002e-05,
      "loss": 2.6225,
      "step": 761300
    },
    {
      "epoch": 246.80713128038897,
      "grad_norm": 1.5164093971252441,
      "learning_rate": 2.5345961725591954e-05,
      "loss": 2.6152,
      "step": 761400
    },
    {
      "epoch": 246.83954619124796,
      "grad_norm": 1.1337672472000122,
      "learning_rate": 2.5342718131689913e-05,
      "loss": 2.649,
      "step": 761500
    },
    {
      "epoch": 246.87196110210698,
      "grad_norm": 1.2100639343261719,
      "learning_rate": 2.533947453778787e-05,
      "loss": 2.6178,
      "step": 761600
    },
    {
      "epoch": 246.90437601296597,
      "grad_norm": 1.149674415588379,
      "learning_rate": 2.5336230943885824e-05,
      "loss": 2.6173,
      "step": 761700
    },
    {
      "epoch": 246.93679092382496,
      "grad_norm": 1.3350743055343628,
      "learning_rate": 2.5332987349983782e-05,
      "loss": 2.6523,
      "step": 761800
    },
    {
      "epoch": 246.96920583468395,
      "grad_norm": 1.3454266786575317,
      "learning_rate": 2.532977619202076e-05,
      "loss": 2.6383,
      "step": 761900
    },
    {
      "epoch": 247.0,
      "eval_bleu": 1.0241869588623183,
      "eval_loss": 4.087136745452881,
      "eval_runtime": 4.2795,
      "eval_samples_per_second": 114.966,
      "eval_steps_per_second": 1.869,
      "step": 761995
    },
    {
      "epoch": 247.00162074554294,
      "grad_norm": 1.2966647148132324,
      "learning_rate": 2.532653259811872e-05,
      "loss": 2.6468,
      "step": 762000
    },
    {
      "epoch": 247.03403565640195,
      "grad_norm": 1.1773903369903564,
      "learning_rate": 2.532328900421667e-05,
      "loss": 2.6256,
      "step": 762100
    },
    {
      "epoch": 247.06645056726094,
      "grad_norm": 1.4902615547180176,
      "learning_rate": 2.532004541031463e-05,
      "loss": 2.6134,
      "step": 762200
    },
    {
      "epoch": 247.09886547811993,
      "grad_norm": 1.3735054731369019,
      "learning_rate": 2.5316801816412588e-05,
      "loss": 2.6255,
      "step": 762300
    },
    {
      "epoch": 247.13128038897892,
      "grad_norm": 1.2518571615219116,
      "learning_rate": 2.531355822251054e-05,
      "loss": 2.6222,
      "step": 762400
    },
    {
      "epoch": 247.1636952998379,
      "grad_norm": 1.2217222452163696,
      "learning_rate": 2.53103146286085e-05,
      "loss": 2.6285,
      "step": 762500
    },
    {
      "epoch": 247.19611021069693,
      "grad_norm": 1.1793596744537354,
      "learning_rate": 2.5307071034706454e-05,
      "loss": 2.6122,
      "step": 762600
    },
    {
      "epoch": 247.22852512155592,
      "grad_norm": 1.1704435348510742,
      "learning_rate": 2.5303827440804413e-05,
      "loss": 2.6197,
      "step": 762700
    },
    {
      "epoch": 247.2609400324149,
      "grad_norm": 1.3172860145568848,
      "learning_rate": 2.5300583846902372e-05,
      "loss": 2.626,
      "step": 762800
    },
    {
      "epoch": 247.2933549432739,
      "grad_norm": 1.2802860736846924,
      "learning_rate": 2.5297340253000324e-05,
      "loss": 2.6263,
      "step": 762900
    },
    {
      "epoch": 247.32576985413291,
      "grad_norm": 1.228210687637329,
      "learning_rate": 2.5294096659098283e-05,
      "loss": 2.6345,
      "step": 763000
    },
    {
      "epoch": 247.3581847649919,
      "grad_norm": 1.564903736114502,
      "learning_rate": 2.529085306519624e-05,
      "loss": 2.619,
      "step": 763100
    },
    {
      "epoch": 247.3905996758509,
      "grad_norm": 1.2181720733642578,
      "learning_rate": 2.5287609471294193e-05,
      "loss": 2.6244,
      "step": 763200
    },
    {
      "epoch": 247.42301458670988,
      "grad_norm": 1.368344783782959,
      "learning_rate": 2.5284365877392152e-05,
      "loss": 2.6068,
      "step": 763300
    },
    {
      "epoch": 247.45542949756887,
      "grad_norm": 1.5999364852905273,
      "learning_rate": 2.528112228349011e-05,
      "loss": 2.6265,
      "step": 763400
    },
    {
      "epoch": 247.4878444084279,
      "grad_norm": 1.2552707195281982,
      "learning_rate": 2.5277878689588063e-05,
      "loss": 2.6297,
      "step": 763500
    },
    {
      "epoch": 247.52025931928688,
      "grad_norm": 1.224421739578247,
      "learning_rate": 2.527463509568602e-05,
      "loss": 2.6291,
      "step": 763600
    },
    {
      "epoch": 247.55267423014587,
      "grad_norm": 1.3332266807556152,
      "learning_rate": 2.5271391501783977e-05,
      "loss": 2.6284,
      "step": 763700
    },
    {
      "epoch": 247.58508914100486,
      "grad_norm": 1.4923139810562134,
      "learning_rate": 2.5268147907881932e-05,
      "loss": 2.6316,
      "step": 763800
    },
    {
      "epoch": 247.61750405186385,
      "grad_norm": 1.1398266553878784,
      "learning_rate": 2.526490431397989e-05,
      "loss": 2.6505,
      "step": 763900
    },
    {
      "epoch": 247.64991896272286,
      "grad_norm": 1.2910031080245972,
      "learning_rate": 2.5261660720077846e-05,
      "loss": 2.6206,
      "step": 764000
    },
    {
      "epoch": 247.68233387358185,
      "grad_norm": 1.1115849018096924,
      "learning_rate": 2.5258417126175805e-05,
      "loss": 2.6268,
      "step": 764100
    },
    {
      "epoch": 247.71474878444084,
      "grad_norm": 1.1952323913574219,
      "learning_rate": 2.5255173532273764e-05,
      "loss": 2.6317,
      "step": 764200
    },
    {
      "epoch": 247.74716369529983,
      "grad_norm": 1.1945133209228516,
      "learning_rate": 2.5251929938371716e-05,
      "loss": 2.6551,
      "step": 764300
    },
    {
      "epoch": 247.77957860615882,
      "grad_norm": 1.1516059637069702,
      "learning_rate": 2.5248686344469674e-05,
      "loss": 2.626,
      "step": 764400
    },
    {
      "epoch": 247.81199351701784,
      "grad_norm": 1.183941125869751,
      "learning_rate": 2.5245442750567633e-05,
      "loss": 2.6276,
      "step": 764500
    },
    {
      "epoch": 247.84440842787683,
      "grad_norm": 1.3299463987350464,
      "learning_rate": 2.5242199156665585e-05,
      "loss": 2.6564,
      "step": 764600
    },
    {
      "epoch": 247.87682333873582,
      "grad_norm": 1.275497555732727,
      "learning_rate": 2.5238955562763544e-05,
      "loss": 2.6209,
      "step": 764700
    },
    {
      "epoch": 247.9092382495948,
      "grad_norm": 1.222121238708496,
      "learning_rate": 2.5235711968861496e-05,
      "loss": 2.6186,
      "step": 764800
    },
    {
      "epoch": 247.9416531604538,
      "grad_norm": 1.3244611024856567,
      "learning_rate": 2.5232468374959455e-05,
      "loss": 2.6196,
      "step": 764900
    },
    {
      "epoch": 247.97406807131281,
      "grad_norm": 1.5342416763305664,
      "learning_rate": 2.5229224781057413e-05,
      "loss": 2.6471,
      "step": 765000
    },
    {
      "epoch": 248.0,
      "eval_bleu": 0.9657266431611943,
      "eval_loss": 4.081826686859131,
      "eval_runtime": 4.1988,
      "eval_samples_per_second": 117.177,
      "eval_steps_per_second": 1.905,
      "step": 765080
    },
    {
      "epoch": 248.0064829821718,
      "grad_norm": 1.2241703271865845,
      "learning_rate": 2.522598118715537e-05,
      "loss": 2.6374,
      "step": 765100
    },
    {
      "epoch": 248.0388978930308,
      "grad_norm": 1.122747540473938,
      "learning_rate": 2.5222737593253327e-05,
      "loss": 2.6224,
      "step": 765200
    },
    {
      "epoch": 248.07131280388978,
      "grad_norm": 1.3594504594802856,
      "learning_rate": 2.5219493999351286e-05,
      "loss": 2.6183,
      "step": 765300
    },
    {
      "epoch": 248.10372771474877,
      "grad_norm": 1.2021598815917969,
      "learning_rate": 2.5216250405449238e-05,
      "loss": 2.6261,
      "step": 765400
    },
    {
      "epoch": 248.1361426256078,
      "grad_norm": 1.282484769821167,
      "learning_rate": 2.5213006811547197e-05,
      "loss": 2.6219,
      "step": 765500
    },
    {
      "epoch": 248.16855753646678,
      "grad_norm": 1.2306640148162842,
      "learning_rate": 2.5209763217645156e-05,
      "loss": 2.6221,
      "step": 765600
    },
    {
      "epoch": 248.20097244732577,
      "grad_norm": 1.3249711990356445,
      "learning_rate": 2.5206519623743108e-05,
      "loss": 2.6145,
      "step": 765700
    },
    {
      "epoch": 248.23338735818476,
      "grad_norm": 1.212523102760315,
      "learning_rate": 2.5203276029841066e-05,
      "loss": 2.6241,
      "step": 765800
    },
    {
      "epoch": 248.26580226904375,
      "grad_norm": 1.2988241910934448,
      "learning_rate": 2.520003243593902e-05,
      "loss": 2.6242,
      "step": 765900
    },
    {
      "epoch": 248.29821717990276,
      "grad_norm": 1.257482886314392,
      "learning_rate": 2.5196788842036977e-05,
      "loss": 2.6158,
      "step": 766000
    },
    {
      "epoch": 248.33063209076175,
      "grad_norm": 1.263142704963684,
      "learning_rate": 2.5193545248134936e-05,
      "loss": 2.6442,
      "step": 766100
    },
    {
      "epoch": 248.36304700162074,
      "grad_norm": 1.2488399744033813,
      "learning_rate": 2.519030165423289e-05,
      "loss": 2.599,
      "step": 766200
    },
    {
      "epoch": 248.39546191247973,
      "grad_norm": 1.2307603359222412,
      "learning_rate": 2.5187058060330847e-05,
      "loss": 2.6432,
      "step": 766300
    },
    {
      "epoch": 248.42787682333875,
      "grad_norm": 1.298394799232483,
      "learning_rate": 2.5183814466428805e-05,
      "loss": 2.6218,
      "step": 766400
    },
    {
      "epoch": 248.46029173419774,
      "grad_norm": 1.2908251285552979,
      "learning_rate": 2.518057087252676e-05,
      "loss": 2.6269,
      "step": 766500
    },
    {
      "epoch": 248.49270664505673,
      "grad_norm": 1.375464916229248,
      "learning_rate": 2.517732727862472e-05,
      "loss": 2.6419,
      "step": 766600
    },
    {
      "epoch": 248.52512155591572,
      "grad_norm": 1.3087254762649536,
      "learning_rate": 2.517408368472267e-05,
      "loss": 2.6384,
      "step": 766700
    },
    {
      "epoch": 248.5575364667747,
      "grad_norm": 1.4838875532150269,
      "learning_rate": 2.517084009082063e-05,
      "loss": 2.6053,
      "step": 766800
    },
    {
      "epoch": 248.58995137763372,
      "grad_norm": 1.4980106353759766,
      "learning_rate": 2.516759649691859e-05,
      "loss": 2.609,
      "step": 766900
    },
    {
      "epoch": 248.6223662884927,
      "grad_norm": 1.1888777017593384,
      "learning_rate": 2.516435290301654e-05,
      "loss": 2.6217,
      "step": 767000
    },
    {
      "epoch": 248.6547811993517,
      "grad_norm": 1.2919342517852783,
      "learning_rate": 2.51611093091145e-05,
      "loss": 2.6189,
      "step": 767100
    },
    {
      "epoch": 248.6871961102107,
      "grad_norm": 1.2912344932556152,
      "learning_rate": 2.5157865715212458e-05,
      "loss": 2.6367,
      "step": 767200
    },
    {
      "epoch": 248.71961102106968,
      "grad_norm": 1.177708387374878,
      "learning_rate": 2.515462212131041e-05,
      "loss": 2.6182,
      "step": 767300
    },
    {
      "epoch": 248.7520259319287,
      "grad_norm": 1.3508799076080322,
      "learning_rate": 2.515137852740837e-05,
      "loss": 2.6481,
      "step": 767400
    },
    {
      "epoch": 248.7844408427877,
      "grad_norm": 1.3592439889907837,
      "learning_rate": 2.5148134933506328e-05,
      "loss": 2.6391,
      "step": 767500
    },
    {
      "epoch": 248.81685575364668,
      "grad_norm": 1.2509853839874268,
      "learning_rate": 2.5144891339604283e-05,
      "loss": 2.6347,
      "step": 767600
    },
    {
      "epoch": 248.84927066450567,
      "grad_norm": 1.1726300716400146,
      "learning_rate": 2.5141647745702242e-05,
      "loss": 2.6261,
      "step": 767700
    },
    {
      "epoch": 248.88168557536466,
      "grad_norm": 1.2718089818954468,
      "learning_rate": 2.5138404151800194e-05,
      "loss": 2.6324,
      "step": 767800
    },
    {
      "epoch": 248.91410048622367,
      "grad_norm": 1.3650994300842285,
      "learning_rate": 2.5135192993837175e-05,
      "loss": 2.6436,
      "step": 767900
    },
    {
      "epoch": 248.94651539708266,
      "grad_norm": 1.1588059663772583,
      "learning_rate": 2.5131949399935127e-05,
      "loss": 2.635,
      "step": 768000
    },
    {
      "epoch": 248.97893030794165,
      "grad_norm": 1.4600729942321777,
      "learning_rate": 2.5128705806033086e-05,
      "loss": 2.6505,
      "step": 768100
    },
    {
      "epoch": 249.0,
      "eval_bleu": 1.1448069203686186,
      "eval_loss": 4.093033790588379,
      "eval_runtime": 4.0805,
      "eval_samples_per_second": 120.574,
      "eval_steps_per_second": 1.961,
      "step": 768165
    },
    {
      "epoch": 249.01134521880064,
      "grad_norm": 1.2316242456436157,
      "learning_rate": 2.512546221213104e-05,
      "loss": 2.6309,
      "step": 768200
    },
    {
      "epoch": 249.04376012965963,
      "grad_norm": 1.3846794366836548,
      "learning_rate": 2.5122218618229e-05,
      "loss": 2.6163,
      "step": 768300
    },
    {
      "epoch": 249.07617504051865,
      "grad_norm": 1.4092321395874023,
      "learning_rate": 2.511897502432696e-05,
      "loss": 2.6022,
      "step": 768400
    },
    {
      "epoch": 249.10858995137764,
      "grad_norm": 1.2103897333145142,
      "learning_rate": 2.5115763866363933e-05,
      "loss": 2.6306,
      "step": 768500
    },
    {
      "epoch": 249.14100486223663,
      "grad_norm": 1.1175440549850464,
      "learning_rate": 2.5112520272461888e-05,
      "loss": 2.6131,
      "step": 768600
    },
    {
      "epoch": 249.17341977309562,
      "grad_norm": 1.3475795984268188,
      "learning_rate": 2.5109276678559847e-05,
      "loss": 2.6079,
      "step": 768700
    },
    {
      "epoch": 249.2058346839546,
      "grad_norm": 1.5001050233840942,
      "learning_rate": 2.5106033084657802e-05,
      "loss": 2.6508,
      "step": 768800
    },
    {
      "epoch": 249.23824959481362,
      "grad_norm": 1.2491116523742676,
      "learning_rate": 2.5102789490755758e-05,
      "loss": 2.6294,
      "step": 768900
    },
    {
      "epoch": 249.2706645056726,
      "grad_norm": 1.1541272401809692,
      "learning_rate": 2.5099545896853716e-05,
      "loss": 2.5954,
      "step": 769000
    },
    {
      "epoch": 249.3030794165316,
      "grad_norm": 1.3559250831604004,
      "learning_rate": 2.5096302302951675e-05,
      "loss": 2.6299,
      "step": 769100
    },
    {
      "epoch": 249.3354943273906,
      "grad_norm": 1.598747968673706,
      "learning_rate": 2.5093058709049627e-05,
      "loss": 2.6192,
      "step": 769200
    },
    {
      "epoch": 249.36790923824958,
      "grad_norm": 1.3896077871322632,
      "learning_rate": 2.5089815115147586e-05,
      "loss": 2.6227,
      "step": 769300
    },
    {
      "epoch": 249.4003241491086,
      "grad_norm": 1.2024506330490112,
      "learning_rate": 2.5086571521245538e-05,
      "loss": 2.617,
      "step": 769400
    },
    {
      "epoch": 249.4327390599676,
      "grad_norm": 1.205960750579834,
      "learning_rate": 2.5083327927343497e-05,
      "loss": 2.6226,
      "step": 769500
    },
    {
      "epoch": 249.46515397082658,
      "grad_norm": 1.2766516208648682,
      "learning_rate": 2.5080084333441455e-05,
      "loss": 2.6206,
      "step": 769600
    },
    {
      "epoch": 249.49756888168557,
      "grad_norm": 1.3848706483840942,
      "learning_rate": 2.5076840739539407e-05,
      "loss": 2.6347,
      "step": 769700
    },
    {
      "epoch": 249.52998379254458,
      "grad_norm": 1.4594146013259888,
      "learning_rate": 2.5073597145637366e-05,
      "loss": 2.6191,
      "step": 769800
    },
    {
      "epoch": 249.56239870340357,
      "grad_norm": 1.3957806825637817,
      "learning_rate": 2.5070353551735325e-05,
      "loss": 2.6301,
      "step": 769900
    },
    {
      "epoch": 249.59481361426256,
      "grad_norm": 1.2763285636901855,
      "learning_rate": 2.506710995783328e-05,
      "loss": 2.6375,
      "step": 770000
    },
    {
      "epoch": 249.62722852512155,
      "grad_norm": 1.2845029830932617,
      "learning_rate": 2.506386636393124e-05,
      "loss": 2.6439,
      "step": 770100
    },
    {
      "epoch": 249.65964343598054,
      "grad_norm": 1.22207510471344,
      "learning_rate": 2.5060622770029198e-05,
      "loss": 2.6327,
      "step": 770200
    },
    {
      "epoch": 249.69205834683956,
      "grad_norm": 1.34294593334198,
      "learning_rate": 2.505737917612715e-05,
      "loss": 2.6356,
      "step": 770300
    },
    {
      "epoch": 249.72447325769855,
      "grad_norm": 1.224556803703308,
      "learning_rate": 2.505413558222511e-05,
      "loss": 2.6317,
      "step": 770400
    },
    {
      "epoch": 249.75688816855754,
      "grad_norm": 1.4178248643875122,
      "learning_rate": 2.505089198832306e-05,
      "loss": 2.6298,
      "step": 770500
    },
    {
      "epoch": 249.78930307941653,
      "grad_norm": 1.316864013671875,
      "learning_rate": 2.504764839442102e-05,
      "loss": 2.6452,
      "step": 770600
    },
    {
      "epoch": 249.82171799027552,
      "grad_norm": 1.1906324625015259,
      "learning_rate": 2.5044404800518978e-05,
      "loss": 2.6307,
      "step": 770700
    },
    {
      "epoch": 249.85413290113453,
      "grad_norm": 1.2614561319351196,
      "learning_rate": 2.504116120661693e-05,
      "loss": 2.617,
      "step": 770800
    },
    {
      "epoch": 249.88654781199352,
      "grad_norm": 1.3461271524429321,
      "learning_rate": 2.503791761271489e-05,
      "loss": 2.6081,
      "step": 770900
    },
    {
      "epoch": 249.9189627228525,
      "grad_norm": 1.4413487911224365,
      "learning_rate": 2.5034674018812847e-05,
      "loss": 2.6326,
      "step": 771000
    },
    {
      "epoch": 249.9513776337115,
      "grad_norm": 1.2125619649887085,
      "learning_rate": 2.5031430424910803e-05,
      "loss": 2.625,
      "step": 771100
    },
    {
      "epoch": 249.9837925445705,
      "grad_norm": 1.1229164600372314,
      "learning_rate": 2.502818683100876e-05,
      "loss": 2.6296,
      "step": 771200
    },
    {
      "epoch": 250.0,
      "eval_bleu": 0.8261279412815874,
      "eval_loss": 4.094529151916504,
      "eval_runtime": 4.0096,
      "eval_samples_per_second": 122.705,
      "eval_steps_per_second": 1.995,
      "step": 771250
    },
    {
      "epoch": 250.0162074554295,
      "grad_norm": 1.5821831226348877,
      "learning_rate": 2.5024943237106713e-05,
      "loss": 2.6372,
      "step": 771300
    },
    {
      "epoch": 250.0486223662885,
      "grad_norm": 1.3329988718032837,
      "learning_rate": 2.5021699643204672e-05,
      "loss": 2.6265,
      "step": 771400
    },
    {
      "epoch": 250.0810372771475,
      "grad_norm": 1.1591253280639648,
      "learning_rate": 2.501845604930263e-05,
      "loss": 2.6291,
      "step": 771500
    },
    {
      "epoch": 250.11345218800648,
      "grad_norm": 1.1916707754135132,
      "learning_rate": 2.5015212455400583e-05,
      "loss": 2.6108,
      "step": 771600
    },
    {
      "epoch": 250.14586709886547,
      "grad_norm": 1.2063074111938477,
      "learning_rate": 2.501196886149854e-05,
      "loss": 2.635,
      "step": 771700
    },
    {
      "epoch": 250.17828200972448,
      "grad_norm": 1.3981472253799438,
      "learning_rate": 2.50087252675965e-05,
      "loss": 2.5949,
      "step": 771800
    },
    {
      "epoch": 250.21069692058347,
      "grad_norm": 1.5150078535079956,
      "learning_rate": 2.5005481673694452e-05,
      "loss": 2.613,
      "step": 771900
    },
    {
      "epoch": 250.24311183144246,
      "grad_norm": 1.2423683404922485,
      "learning_rate": 2.500223807979241e-05,
      "loss": 2.6275,
      "step": 772000
    },
    {
      "epoch": 250.27552674230145,
      "grad_norm": 1.2462518215179443,
      "learning_rate": 2.4998994485890366e-05,
      "loss": 2.6263,
      "step": 772100
    },
    {
      "epoch": 250.30794165316044,
      "grad_norm": 1.1979529857635498,
      "learning_rate": 2.4995750891988322e-05,
      "loss": 2.6123,
      "step": 772200
    },
    {
      "epoch": 250.34035656401946,
      "grad_norm": 1.2040455341339111,
      "learning_rate": 2.499250729808628e-05,
      "loss": 2.6187,
      "step": 772300
    },
    {
      "epoch": 250.37277147487845,
      "grad_norm": 1.2760580778121948,
      "learning_rate": 2.498926370418424e-05,
      "loss": 2.6139,
      "step": 772400
    },
    {
      "epoch": 250.40518638573744,
      "grad_norm": 1.1867722272872925,
      "learning_rate": 2.4986020110282195e-05,
      "loss": 2.6356,
      "step": 772500
    },
    {
      "epoch": 250.43760129659643,
      "grad_norm": 1.4130187034606934,
      "learning_rate": 2.498277651638015e-05,
      "loss": 2.6115,
      "step": 772600
    },
    {
      "epoch": 250.47001620745542,
      "grad_norm": 1.2949541807174683,
      "learning_rate": 2.497953292247811e-05,
      "loss": 2.6163,
      "step": 772700
    },
    {
      "epoch": 250.50243111831443,
      "grad_norm": 1.1299701929092407,
      "learning_rate": 2.4976289328576064e-05,
      "loss": 2.6343,
      "step": 772800
    },
    {
      "epoch": 250.53484602917342,
      "grad_norm": 1.152177333831787,
      "learning_rate": 2.497304573467402e-05,
      "loss": 2.6186,
      "step": 772900
    },
    {
      "epoch": 250.5672609400324,
      "grad_norm": 1.2522268295288086,
      "learning_rate": 2.4969802140771978e-05,
      "loss": 2.6329,
      "step": 773000
    },
    {
      "epoch": 250.5996758508914,
      "grad_norm": 1.5323460102081299,
      "learning_rate": 2.4966558546869933e-05,
      "loss": 2.6321,
      "step": 773100
    },
    {
      "epoch": 250.63209076175042,
      "grad_norm": 1.2058311700820923,
      "learning_rate": 2.496331495296789e-05,
      "loss": 2.604,
      "step": 773200
    },
    {
      "epoch": 250.6645056726094,
      "grad_norm": 1.2513363361358643,
      "learning_rate": 2.4960071359065844e-05,
      "loss": 2.6278,
      "step": 773300
    },
    {
      "epoch": 250.6969205834684,
      "grad_norm": 1.2092245817184448,
      "learning_rate": 2.4956827765163803e-05,
      "loss": 2.6251,
      "step": 773400
    },
    {
      "epoch": 250.7293354943274,
      "grad_norm": 1.2053720951080322,
      "learning_rate": 2.4953584171261758e-05,
      "loss": 2.6448,
      "step": 773500
    },
    {
      "epoch": 250.76175040518638,
      "grad_norm": 1.3696497678756714,
      "learning_rate": 2.4950340577359717e-05,
      "loss": 2.6286,
      "step": 773600
    },
    {
      "epoch": 250.7941653160454,
      "grad_norm": 1.3413562774658203,
      "learning_rate": 2.4947096983457672e-05,
      "loss": 2.6039,
      "step": 773700
    },
    {
      "epoch": 250.82658022690438,
      "grad_norm": 1.3268407583236694,
      "learning_rate": 2.494385338955563e-05,
      "loss": 2.6434,
      "step": 773800
    },
    {
      "epoch": 250.85899513776337,
      "grad_norm": 1.2106986045837402,
      "learning_rate": 2.4940609795653586e-05,
      "loss": 2.624,
      "step": 773900
    },
    {
      "epoch": 250.89141004862236,
      "grad_norm": 1.3785381317138672,
      "learning_rate": 2.4937366201751542e-05,
      "loss": 2.6094,
      "step": 774000
    },
    {
      "epoch": 250.92382495948135,
      "grad_norm": 1.2954614162445068,
      "learning_rate": 2.4934122607849497e-05,
      "loss": 2.6425,
      "step": 774100
    },
    {
      "epoch": 250.95623987034037,
      "grad_norm": 1.4108508825302124,
      "learning_rate": 2.4930879013947456e-05,
      "loss": 2.6319,
      "step": 774200
    },
    {
      "epoch": 250.98865478119936,
      "grad_norm": 1.5911002159118652,
      "learning_rate": 2.492763542004541e-05,
      "loss": 2.6347,
      "step": 774300
    },
    {
      "epoch": 251.0,
      "eval_bleu": 1.0681680401280385,
      "eval_loss": 4.097356796264648,
      "eval_runtime": 3.9779,
      "eval_samples_per_second": 123.684,
      "eval_steps_per_second": 2.011,
      "step": 774335
    },
    {
      "epoch": 251.02106969205835,
      "grad_norm": 1.3499501943588257,
      "learning_rate": 2.4924391826143367e-05,
      "loss": 2.6038,
      "step": 774400
    },
    {
      "epoch": 251.05348460291734,
      "grad_norm": 1.248609185218811,
      "learning_rate": 2.4921180668180344e-05,
      "loss": 2.6149,
      "step": 774500
    },
    {
      "epoch": 251.08589951377633,
      "grad_norm": 1.4420007467269897,
      "learning_rate": 2.4917937074278303e-05,
      "loss": 2.6134,
      "step": 774600
    },
    {
      "epoch": 251.11831442463534,
      "grad_norm": 1.2449591159820557,
      "learning_rate": 2.491469348037626e-05,
      "loss": 2.6137,
      "step": 774700
    },
    {
      "epoch": 251.15072933549433,
      "grad_norm": 1.3254250288009644,
      "learning_rate": 2.4911449886474214e-05,
      "loss": 2.6013,
      "step": 774800
    },
    {
      "epoch": 251.18314424635332,
      "grad_norm": 1.324416160583496,
      "learning_rate": 2.490820629257217e-05,
      "loss": 2.6252,
      "step": 774900
    },
    {
      "epoch": 251.2155591572123,
      "grad_norm": 1.2157628536224365,
      "learning_rate": 2.4904962698670128e-05,
      "loss": 2.625,
      "step": 775000
    },
    {
      "epoch": 251.2479740680713,
      "grad_norm": 1.1586830615997314,
      "learning_rate": 2.4901719104768083e-05,
      "loss": 2.6392,
      "step": 775100
    },
    {
      "epoch": 251.28038897893032,
      "grad_norm": 1.5252048969268799,
      "learning_rate": 2.489847551086604e-05,
      "loss": 2.6194,
      "step": 775200
    },
    {
      "epoch": 251.3128038897893,
      "grad_norm": 1.328818917274475,
      "learning_rate": 2.4895231916963997e-05,
      "loss": 2.6203,
      "step": 775300
    },
    {
      "epoch": 251.3452188006483,
      "grad_norm": 1.2996653318405151,
      "learning_rate": 2.4891988323061953e-05,
      "loss": 2.6336,
      "step": 775400
    },
    {
      "epoch": 251.3776337115073,
      "grad_norm": 1.1266753673553467,
      "learning_rate": 2.488874472915991e-05,
      "loss": 2.6171,
      "step": 775500
    },
    {
      "epoch": 251.41004862236628,
      "grad_norm": 1.1833555698394775,
      "learning_rate": 2.4885501135257867e-05,
      "loss": 2.6203,
      "step": 775600
    },
    {
      "epoch": 251.4424635332253,
      "grad_norm": 1.3958303928375244,
      "learning_rate": 2.4882257541355826e-05,
      "loss": 2.6371,
      "step": 775700
    },
    {
      "epoch": 251.47487844408428,
      "grad_norm": 1.227529525756836,
      "learning_rate": 2.487901394745378e-05,
      "loss": 2.6297,
      "step": 775800
    },
    {
      "epoch": 251.50729335494327,
      "grad_norm": 1.2452759742736816,
      "learning_rate": 2.4875770353551736e-05,
      "loss": 2.6278,
      "step": 775900
    },
    {
      "epoch": 251.53970826580226,
      "grad_norm": 1.3659098148345947,
      "learning_rate": 2.4872526759649692e-05,
      "loss": 2.6463,
      "step": 776000
    },
    {
      "epoch": 251.57212317666125,
      "grad_norm": 1.321394681930542,
      "learning_rate": 2.486928316574765e-05,
      "loss": 2.6216,
      "step": 776100
    },
    {
      "epoch": 251.60453808752027,
      "grad_norm": 1.2522176504135132,
      "learning_rate": 2.4866039571845606e-05,
      "loss": 2.6292,
      "step": 776200
    },
    {
      "epoch": 251.63695299837926,
      "grad_norm": 1.423141360282898,
      "learning_rate": 2.486279597794356e-05,
      "loss": 2.6221,
      "step": 776300
    },
    {
      "epoch": 251.66936790923825,
      "grad_norm": 1.1288632154464722,
      "learning_rate": 2.4859552384041517e-05,
      "loss": 2.6179,
      "step": 776400
    },
    {
      "epoch": 251.70178282009724,
      "grad_norm": 1.3483790159225464,
      "learning_rate": 2.4856341226078498e-05,
      "loss": 2.6262,
      "step": 776500
    },
    {
      "epoch": 251.73419773095625,
      "grad_norm": 1.1926472187042236,
      "learning_rate": 2.4853097632176453e-05,
      "loss": 2.6197,
      "step": 776600
    },
    {
      "epoch": 251.76661264181524,
      "grad_norm": 1.334734320640564,
      "learning_rate": 2.484985403827441e-05,
      "loss": 2.6071,
      "step": 776700
    },
    {
      "epoch": 251.79902755267423,
      "grad_norm": 1.2453265190124512,
      "learning_rate": 2.4846642880311386e-05,
      "loss": 2.653,
      "step": 776800
    },
    {
      "epoch": 251.83144246353322,
      "grad_norm": 1.1051174402236938,
      "learning_rate": 2.4843399286409345e-05,
      "loss": 2.6245,
      "step": 776900
    },
    {
      "epoch": 251.8638573743922,
      "grad_norm": 1.4584659337997437,
      "learning_rate": 2.48401556925073e-05,
      "loss": 2.6199,
      "step": 777000
    },
    {
      "epoch": 251.89627228525123,
      "grad_norm": 1.2686691284179688,
      "learning_rate": 2.4836912098605256e-05,
      "loss": 2.6145,
      "step": 777100
    },
    {
      "epoch": 251.92868719611022,
      "grad_norm": 1.4642125368118286,
      "learning_rate": 2.483366850470321e-05,
      "loss": 2.6153,
      "step": 777200
    },
    {
      "epoch": 251.9611021069692,
      "grad_norm": 1.479215383529663,
      "learning_rate": 2.483042491080117e-05,
      "loss": 2.6448,
      "step": 777300
    },
    {
      "epoch": 251.9935170178282,
      "grad_norm": 1.243667721748352,
      "learning_rate": 2.4827181316899125e-05,
      "loss": 2.6369,
      "step": 777400
    },
    {
      "epoch": 252.0,
      "eval_bleu": 0.967374451108524,
      "eval_loss": 4.0964531898498535,
      "eval_runtime": 4.4406,
      "eval_samples_per_second": 110.796,
      "eval_steps_per_second": 1.802,
      "step": 777420
    },
    {
      "epoch": 252.02593192868719,
      "grad_norm": 1.2350852489471436,
      "learning_rate": 2.482393772299708e-05,
      "loss": 2.6048,
      "step": 777500
    },
    {
      "epoch": 252.0583468395462,
      "grad_norm": 1.243253469467163,
      "learning_rate": 2.4820694129095036e-05,
      "loss": 2.626,
      "step": 777600
    },
    {
      "epoch": 252.0907617504052,
      "grad_norm": 1.3613520860671997,
      "learning_rate": 2.4817450535192995e-05,
      "loss": 2.6126,
      "step": 777700
    },
    {
      "epoch": 252.12317666126418,
      "grad_norm": 1.3043323755264282,
      "learning_rate": 2.4814206941290953e-05,
      "loss": 2.6258,
      "step": 777800
    },
    {
      "epoch": 252.15559157212317,
      "grad_norm": 1.2603135108947754,
      "learning_rate": 2.481096334738891e-05,
      "loss": 2.5851,
      "step": 777900
    },
    {
      "epoch": 252.18800648298216,
      "grad_norm": 1.3757648468017578,
      "learning_rate": 2.4807719753486864e-05,
      "loss": 2.5981,
      "step": 778000
    },
    {
      "epoch": 252.22042139384118,
      "grad_norm": 1.3109796047210693,
      "learning_rate": 2.4804476159584823e-05,
      "loss": 2.63,
      "step": 778100
    },
    {
      "epoch": 252.25283630470017,
      "grad_norm": 1.301214575767517,
      "learning_rate": 2.4801232565682778e-05,
      "loss": 2.6333,
      "step": 778200
    },
    {
      "epoch": 252.28525121555916,
      "grad_norm": 1.1843442916870117,
      "learning_rate": 2.4797988971780733e-05,
      "loss": 2.6246,
      "step": 778300
    },
    {
      "epoch": 252.31766612641815,
      "grad_norm": 1.3444242477416992,
      "learning_rate": 2.4794745377878692e-05,
      "loss": 2.6256,
      "step": 778400
    },
    {
      "epoch": 252.35008103727714,
      "grad_norm": 1.2436416149139404,
      "learning_rate": 2.4791501783976648e-05,
      "loss": 2.6412,
      "step": 778500
    },
    {
      "epoch": 252.38249594813615,
      "grad_norm": 1.298374891281128,
      "learning_rate": 2.4788258190074603e-05,
      "loss": 2.6258,
      "step": 778600
    },
    {
      "epoch": 252.41491085899514,
      "grad_norm": 1.5606184005737305,
      "learning_rate": 2.4785014596172558e-05,
      "loss": 2.6047,
      "step": 778700
    },
    {
      "epoch": 252.44732576985413,
      "grad_norm": 1.2995318174362183,
      "learning_rate": 2.4781771002270517e-05,
      "loss": 2.6165,
      "step": 778800
    },
    {
      "epoch": 252.47974068071312,
      "grad_norm": 1.4787135124206543,
      "learning_rate": 2.4778527408368472e-05,
      "loss": 2.5967,
      "step": 778900
    },
    {
      "epoch": 252.5121555915721,
      "grad_norm": 1.1767585277557373,
      "learning_rate": 2.477528381446643e-05,
      "loss": 2.6311,
      "step": 779000
    },
    {
      "epoch": 252.54457050243113,
      "grad_norm": 1.120144248008728,
      "learning_rate": 2.4772040220564386e-05,
      "loss": 2.6378,
      "step": 779100
    },
    {
      "epoch": 252.57698541329012,
      "grad_norm": 1.313682198524475,
      "learning_rate": 2.4768796626662345e-05,
      "loss": 2.6052,
      "step": 779200
    },
    {
      "epoch": 252.6094003241491,
      "grad_norm": 1.4564480781555176,
      "learning_rate": 2.47655530327603e-05,
      "loss": 2.6262,
      "step": 779300
    },
    {
      "epoch": 252.6418152350081,
      "grad_norm": 1.3269864320755005,
      "learning_rate": 2.4762309438858256e-05,
      "loss": 2.6428,
      "step": 779400
    },
    {
      "epoch": 252.67423014586709,
      "grad_norm": 1.3230143785476685,
      "learning_rate": 2.475906584495621e-05,
      "loss": 2.6332,
      "step": 779500
    },
    {
      "epoch": 252.7066450567261,
      "grad_norm": 1.3595006465911865,
      "learning_rate": 2.475582225105417e-05,
      "loss": 2.6079,
      "step": 779600
    },
    {
      "epoch": 252.7390599675851,
      "grad_norm": 1.3002995252609253,
      "learning_rate": 2.4752578657152125e-05,
      "loss": 2.6273,
      "step": 779700
    },
    {
      "epoch": 252.77147487844408,
      "grad_norm": 1.195678949356079,
      "learning_rate": 2.474933506325008e-05,
      "loss": 2.6141,
      "step": 779800
    },
    {
      "epoch": 252.80388978930307,
      "grad_norm": 1.328970193862915,
      "learning_rate": 2.474609146934804e-05,
      "loss": 2.6242,
      "step": 779900
    },
    {
      "epoch": 252.8363047001621,
      "grad_norm": 1.4723868370056152,
      "learning_rate": 2.4742847875445995e-05,
      "loss": 2.6233,
      "step": 780000
    },
    {
      "epoch": 252.86871961102108,
      "grad_norm": 1.2892171144485474,
      "learning_rate": 2.473960428154395e-05,
      "loss": 2.6338,
      "step": 780100
    },
    {
      "epoch": 252.90113452188007,
      "grad_norm": 1.417158603668213,
      "learning_rate": 2.473636068764191e-05,
      "loss": 2.6188,
      "step": 780200
    },
    {
      "epoch": 252.93354943273906,
      "grad_norm": 1.6170506477355957,
      "learning_rate": 2.4733117093739864e-05,
      "loss": 2.6263,
      "step": 780300
    },
    {
      "epoch": 252.96596434359805,
      "grad_norm": 1.7075793743133545,
      "learning_rate": 2.4729873499837823e-05,
      "loss": 2.6045,
      "step": 780400
    },
    {
      "epoch": 252.99837925445706,
      "grad_norm": 1.3983409404754639,
      "learning_rate": 2.472662990593578e-05,
      "loss": 2.6412,
      "step": 780500
    },
    {
      "epoch": 253.0,
      "eval_bleu": 1.0103591360170927,
      "eval_loss": 4.094039440155029,
      "eval_runtime": 4.1155,
      "eval_samples_per_second": 119.549,
      "eval_steps_per_second": 1.944,
      "step": 780505
    },
    {
      "epoch": 253.03079416531605,
      "grad_norm": 1.2312289476394653,
      "learning_rate": 2.4723386312033734e-05,
      "loss": 2.6054,
      "step": 780600
    },
    {
      "epoch": 253.06320907617504,
      "grad_norm": 1.4328633546829224,
      "learning_rate": 2.4720142718131692e-05,
      "loss": 2.6124,
      "step": 780700
    },
    {
      "epoch": 253.09562398703403,
      "grad_norm": 1.2156999111175537,
      "learning_rate": 2.4716899124229648e-05,
      "loss": 2.616,
      "step": 780800
    },
    {
      "epoch": 253.12803889789302,
      "grad_norm": 1.2198792695999146,
      "learning_rate": 2.4713687966266626e-05,
      "loss": 2.6273,
      "step": 780900
    },
    {
      "epoch": 253.16045380875204,
      "grad_norm": 1.1896061897277832,
      "learning_rate": 2.471044437236458e-05,
      "loss": 2.6281,
      "step": 781000
    },
    {
      "epoch": 253.19286871961103,
      "grad_norm": 1.4411779642105103,
      "learning_rate": 2.470720077846254e-05,
      "loss": 2.6047,
      "step": 781100
    },
    {
      "epoch": 253.22528363047002,
      "grad_norm": 1.3405777215957642,
      "learning_rate": 2.4703957184560495e-05,
      "loss": 2.6195,
      "step": 781200
    },
    {
      "epoch": 253.257698541329,
      "grad_norm": 1.2756507396697998,
      "learning_rate": 2.470071359065845e-05,
      "loss": 2.6212,
      "step": 781300
    },
    {
      "epoch": 253.290113452188,
      "grad_norm": 1.3136996030807495,
      "learning_rate": 2.4697469996756406e-05,
      "loss": 2.6171,
      "step": 781400
    },
    {
      "epoch": 253.322528363047,
      "grad_norm": 1.3320159912109375,
      "learning_rate": 2.4694226402854365e-05,
      "loss": 2.6402,
      "step": 781500
    },
    {
      "epoch": 253.354943273906,
      "grad_norm": 1.1830466985702515,
      "learning_rate": 2.469098280895232e-05,
      "loss": 2.6359,
      "step": 781600
    },
    {
      "epoch": 253.387358184765,
      "grad_norm": 1.3628329038619995,
      "learning_rate": 2.4687739215050275e-05,
      "loss": 2.6243,
      "step": 781700
    },
    {
      "epoch": 253.41977309562398,
      "grad_norm": 1.696560263633728,
      "learning_rate": 2.4684495621148234e-05,
      "loss": 2.6145,
      "step": 781800
    },
    {
      "epoch": 253.45218800648297,
      "grad_norm": 1.2785202264785767,
      "learning_rate": 2.468125202724619e-05,
      "loss": 2.6267,
      "step": 781900
    },
    {
      "epoch": 253.484602917342,
      "grad_norm": 1.3770111799240112,
      "learning_rate": 2.4678008433344145e-05,
      "loss": 2.6329,
      "step": 782000
    },
    {
      "epoch": 253.51701782820098,
      "grad_norm": 1.3153412342071533,
      "learning_rate": 2.4674764839442103e-05,
      "loss": 2.6133,
      "step": 782100
    },
    {
      "epoch": 253.54943273905997,
      "grad_norm": 1.2249691486358643,
      "learning_rate": 2.4671521245540062e-05,
      "loss": 2.6294,
      "step": 782200
    },
    {
      "epoch": 253.58184764991896,
      "grad_norm": 1.3696632385253906,
      "learning_rate": 2.4668277651638018e-05,
      "loss": 2.6048,
      "step": 782300
    },
    {
      "epoch": 253.61426256077795,
      "grad_norm": 1.4441555738449097,
      "learning_rate": 2.4665034057735973e-05,
      "loss": 2.6357,
      "step": 782400
    },
    {
      "epoch": 253.64667747163696,
      "grad_norm": 1.5138230323791504,
      "learning_rate": 2.4661790463833928e-05,
      "loss": 2.6395,
      "step": 782500
    },
    {
      "epoch": 253.67909238249595,
      "grad_norm": 1.327429175376892,
      "learning_rate": 2.4658579305870906e-05,
      "loss": 2.6228,
      "step": 782600
    },
    {
      "epoch": 253.71150729335494,
      "grad_norm": 1.2738430500030518,
      "learning_rate": 2.4655335711968865e-05,
      "loss": 2.6145,
      "step": 782700
    },
    {
      "epoch": 253.74392220421393,
      "grad_norm": 1.2655503749847412,
      "learning_rate": 2.465209211806682e-05,
      "loss": 2.6242,
      "step": 782800
    },
    {
      "epoch": 253.77633711507292,
      "grad_norm": 1.3670021295547485,
      "learning_rate": 2.4648848524164775e-05,
      "loss": 2.6174,
      "step": 782900
    },
    {
      "epoch": 253.80875202593194,
      "grad_norm": 1.2235817909240723,
      "learning_rate": 2.4645604930262734e-05,
      "loss": 2.6037,
      "step": 783000
    },
    {
      "epoch": 253.84116693679093,
      "grad_norm": 1.3722383975982666,
      "learning_rate": 2.464236133636069e-05,
      "loss": 2.621,
      "step": 783100
    },
    {
      "epoch": 253.87358184764992,
      "grad_norm": 1.1963173151016235,
      "learning_rate": 2.4639117742458645e-05,
      "loss": 2.6007,
      "step": 783200
    },
    {
      "epoch": 253.9059967585089,
      "grad_norm": 1.2032921314239502,
      "learning_rate": 2.46358741485566e-05,
      "loss": 2.6195,
      "step": 783300
    },
    {
      "epoch": 253.93841166936792,
      "grad_norm": 1.4259636402130127,
      "learning_rate": 2.463263055465456e-05,
      "loss": 2.6052,
      "step": 783400
    },
    {
      "epoch": 253.9708265802269,
      "grad_norm": 1.1475586891174316,
      "learning_rate": 2.4629386960752514e-05,
      "loss": 2.6285,
      "step": 783500
    },
    {
      "epoch": 254.0,
      "eval_bleu": 1.071004081385737,
      "eval_loss": 4.094521522521973,
      "eval_runtime": 4.418,
      "eval_samples_per_second": 111.362,
      "eval_steps_per_second": 1.811,
      "step": 783590
    },
    {
      "epoch": 254.0032414910859,
      "grad_norm": 1.176276445388794,
      "learning_rate": 2.462614336685047e-05,
      "loss": 2.6302,
      "step": 783600
    },
    {
      "epoch": 254.0356564019449,
      "grad_norm": 1.211657166481018,
      "learning_rate": 2.4622899772948425e-05,
      "loss": 2.6233,
      "step": 783700
    },
    {
      "epoch": 254.06807131280388,
      "grad_norm": 1.4815539121627808,
      "learning_rate": 2.4619656179046384e-05,
      "loss": 2.6377,
      "step": 783800
    },
    {
      "epoch": 254.1004862236629,
      "grad_norm": 1.3280011415481567,
      "learning_rate": 2.4616412585144343e-05,
      "loss": 2.6244,
      "step": 783900
    },
    {
      "epoch": 254.1329011345219,
      "grad_norm": 1.3784836530685425,
      "learning_rate": 2.4613168991242298e-05,
      "loss": 2.5957,
      "step": 784000
    },
    {
      "epoch": 254.16531604538088,
      "grad_norm": 1.2799403667449951,
      "learning_rate": 2.4609925397340257e-05,
      "loss": 2.6294,
      "step": 784100
    },
    {
      "epoch": 254.19773095623987,
      "grad_norm": 1.3322763442993164,
      "learning_rate": 2.4606681803438212e-05,
      "loss": 2.6129,
      "step": 784200
    },
    {
      "epoch": 254.23014586709886,
      "grad_norm": 1.4726026058197021,
      "learning_rate": 2.4603438209536167e-05,
      "loss": 2.6284,
      "step": 784300
    },
    {
      "epoch": 254.26256077795787,
      "grad_norm": 1.4664032459259033,
      "learning_rate": 2.4600194615634123e-05,
      "loss": 2.6249,
      "step": 784400
    },
    {
      "epoch": 254.29497568881686,
      "grad_norm": 1.213763952255249,
      "learning_rate": 2.459695102173208e-05,
      "loss": 2.5932,
      "step": 784500
    },
    {
      "epoch": 254.32739059967585,
      "grad_norm": 1.5669227838516235,
      "learning_rate": 2.4593707427830037e-05,
      "loss": 2.6096,
      "step": 784600
    },
    {
      "epoch": 254.35980551053484,
      "grad_norm": 1.625285029411316,
      "learning_rate": 2.4590463833927992e-05,
      "loss": 2.6076,
      "step": 784700
    },
    {
      "epoch": 254.39222042139383,
      "grad_norm": 1.0770236253738403,
      "learning_rate": 2.4587220240025948e-05,
      "loss": 2.6007,
      "step": 784800
    },
    {
      "epoch": 254.42463533225285,
      "grad_norm": 1.2228045463562012,
      "learning_rate": 2.4583976646123906e-05,
      "loss": 2.6142,
      "step": 784900
    },
    {
      "epoch": 254.45705024311184,
      "grad_norm": 1.4264318943023682,
      "learning_rate": 2.458073305222186e-05,
      "loss": 2.6113,
      "step": 785000
    },
    {
      "epoch": 254.48946515397083,
      "grad_norm": 1.320762276649475,
      "learning_rate": 2.457748945831982e-05,
      "loss": 2.6204,
      "step": 785100
    },
    {
      "epoch": 254.52188006482982,
      "grad_norm": 1.3967244625091553,
      "learning_rate": 2.4574245864417776e-05,
      "loss": 2.6107,
      "step": 785200
    },
    {
      "epoch": 254.5542949756888,
      "grad_norm": 1.15181303024292,
      "learning_rate": 2.4571002270515735e-05,
      "loss": 2.6233,
      "step": 785300
    },
    {
      "epoch": 254.58670988654782,
      "grad_norm": 1.2273882627487183,
      "learning_rate": 2.456775867661369e-05,
      "loss": 2.6333,
      "step": 785400
    },
    {
      "epoch": 254.6191247974068,
      "grad_norm": 1.23731529712677,
      "learning_rate": 2.4564515082711645e-05,
      "loss": 2.651,
      "step": 785500
    },
    {
      "epoch": 254.6515397082658,
      "grad_norm": 1.2331651449203491,
      "learning_rate": 2.4561271488809604e-05,
      "loss": 2.6334,
      "step": 785600
    },
    {
      "epoch": 254.6839546191248,
      "grad_norm": 1.3763467073440552,
      "learning_rate": 2.455802789490756e-05,
      "loss": 2.5983,
      "step": 785700
    },
    {
      "epoch": 254.71636952998378,
      "grad_norm": 1.3173495531082153,
      "learning_rate": 2.4554784301005515e-05,
      "loss": 2.6166,
      "step": 785800
    },
    {
      "epoch": 254.7487844408428,
      "grad_norm": 1.166621208190918,
      "learning_rate": 2.455154070710347e-05,
      "loss": 2.6332,
      "step": 785900
    },
    {
      "epoch": 254.7811993517018,
      "grad_norm": 1.1920857429504395,
      "learning_rate": 2.454829711320143e-05,
      "loss": 2.6303,
      "step": 786000
    },
    {
      "epoch": 254.81361426256078,
      "grad_norm": 1.2142138481140137,
      "learning_rate": 2.4545053519299384e-05,
      "loss": 2.6289,
      "step": 786100
    },
    {
      "epoch": 254.84602917341977,
      "grad_norm": 1.1238645315170288,
      "learning_rate": 2.454180992539734e-05,
      "loss": 2.6334,
      "step": 786200
    },
    {
      "epoch": 254.87844408427875,
      "grad_norm": 1.390499234199524,
      "learning_rate": 2.4538566331495298e-05,
      "loss": 2.5942,
      "step": 786300
    },
    {
      "epoch": 254.91085899513777,
      "grad_norm": 1.2049229145050049,
      "learning_rate": 2.4535322737593257e-05,
      "loss": 2.6288,
      "step": 786400
    },
    {
      "epoch": 254.94327390599676,
      "grad_norm": 1.1053556203842163,
      "learning_rate": 2.4532079143691212e-05,
      "loss": 2.6211,
      "step": 786500
    },
    {
      "epoch": 254.97568881685575,
      "grad_norm": 1.277093529701233,
      "learning_rate": 2.4528867985728187e-05,
      "loss": 2.6021,
      "step": 786600
    },
    {
      "epoch": 255.0,
      "eval_bleu": 1.081699937231241,
      "eval_loss": 4.097776412963867,
      "eval_runtime": 4.3613,
      "eval_samples_per_second": 112.809,
      "eval_steps_per_second": 1.834,
      "step": 786675
    },
    {
      "epoch": 255.00810372771474,
      "grad_norm": 1.1710400581359863,
      "learning_rate": 2.4525624391826142e-05,
      "loss": 2.6163,
      "step": 786700
    },
    {
      "epoch": 255.04051863857376,
      "grad_norm": 1.2162339687347412,
      "learning_rate": 2.45223807979241e-05,
      "loss": 2.6298,
      "step": 786800
    },
    {
      "epoch": 255.07293354943275,
      "grad_norm": 1.169750452041626,
      "learning_rate": 2.451913720402206e-05,
      "loss": 2.6222,
      "step": 786900
    },
    {
      "epoch": 255.10534846029174,
      "grad_norm": 1.492821216583252,
      "learning_rate": 2.4515893610120015e-05,
      "loss": 2.6104,
      "step": 787000
    },
    {
      "epoch": 255.13776337115073,
      "grad_norm": 1.3617161512374878,
      "learning_rate": 2.451265001621797e-05,
      "loss": 2.6014,
      "step": 787100
    },
    {
      "epoch": 255.17017828200972,
      "grad_norm": 1.1697683334350586,
      "learning_rate": 2.450940642231593e-05,
      "loss": 2.6293,
      "step": 787200
    },
    {
      "epoch": 255.20259319286873,
      "grad_norm": 1.2300325632095337,
      "learning_rate": 2.4506162828413884e-05,
      "loss": 2.6024,
      "step": 787300
    },
    {
      "epoch": 255.23500810372772,
      "grad_norm": 1.1711994409561157,
      "learning_rate": 2.450291923451184e-05,
      "loss": 2.5983,
      "step": 787400
    },
    {
      "epoch": 255.2674230145867,
      "grad_norm": 1.3643831014633179,
      "learning_rate": 2.4499675640609795e-05,
      "loss": 2.6228,
      "step": 787500
    },
    {
      "epoch": 255.2998379254457,
      "grad_norm": 1.2829489707946777,
      "learning_rate": 2.4496432046707754e-05,
      "loss": 2.5964,
      "step": 787600
    },
    {
      "epoch": 255.3322528363047,
      "grad_norm": 1.271077036857605,
      "learning_rate": 2.449318845280571e-05,
      "loss": 2.6154,
      "step": 787700
    },
    {
      "epoch": 255.3646677471637,
      "grad_norm": 1.1501699686050415,
      "learning_rate": 2.4489944858903665e-05,
      "loss": 2.5986,
      "step": 787800
    },
    {
      "epoch": 255.3970826580227,
      "grad_norm": 1.4041719436645508,
      "learning_rate": 2.4486701265001623e-05,
      "loss": 2.6168,
      "step": 787900
    },
    {
      "epoch": 255.4294975688817,
      "grad_norm": 1.2158136367797852,
      "learning_rate": 2.448345767109958e-05,
      "loss": 2.6063,
      "step": 788000
    },
    {
      "epoch": 255.46191247974068,
      "grad_norm": 1.2508296966552734,
      "learning_rate": 2.4480214077197537e-05,
      "loss": 2.6251,
      "step": 788100
    },
    {
      "epoch": 255.49432739059966,
      "grad_norm": 1.2037631273269653,
      "learning_rate": 2.4476970483295493e-05,
      "loss": 2.6215,
      "step": 788200
    },
    {
      "epoch": 255.52674230145868,
      "grad_norm": 1.4861345291137695,
      "learning_rate": 2.447372688939345e-05,
      "loss": 2.6349,
      "step": 788300
    },
    {
      "epoch": 255.55915721231767,
      "grad_norm": 1.1817768812179565,
      "learning_rate": 2.4470483295491407e-05,
      "loss": 2.5948,
      "step": 788400
    },
    {
      "epoch": 255.59157212317666,
      "grad_norm": 1.1348142623901367,
      "learning_rate": 2.4467239701589362e-05,
      "loss": 2.6209,
      "step": 788500
    },
    {
      "epoch": 255.62398703403565,
      "grad_norm": 1.2767821550369263,
      "learning_rate": 2.446402854362634e-05,
      "loss": 2.6154,
      "step": 788600
    },
    {
      "epoch": 255.65640194489464,
      "grad_norm": 1.413743257522583,
      "learning_rate": 2.4460784949724295e-05,
      "loss": 2.6168,
      "step": 788700
    },
    {
      "epoch": 255.68881685575366,
      "grad_norm": 1.1577858924865723,
      "learning_rate": 2.4457541355822254e-05,
      "loss": 2.6283,
      "step": 788800
    },
    {
      "epoch": 255.72123176661265,
      "grad_norm": 1.414504051208496,
      "learning_rate": 2.445429776192021e-05,
      "loss": 2.6122,
      "step": 788900
    },
    {
      "epoch": 255.75364667747164,
      "grad_norm": 1.4043118953704834,
      "learning_rate": 2.4451054168018165e-05,
      "loss": 2.5989,
      "step": 789000
    },
    {
      "epoch": 255.78606158833063,
      "grad_norm": 1.3893884420394897,
      "learning_rate": 2.4447810574116124e-05,
      "loss": 2.6366,
      "step": 789100
    },
    {
      "epoch": 255.81847649918961,
      "grad_norm": 1.2136539220809937,
      "learning_rate": 2.444456698021408e-05,
      "loss": 2.6386,
      "step": 789200
    },
    {
      "epoch": 255.85089141004863,
      "grad_norm": 1.3267993927001953,
      "learning_rate": 2.4441323386312034e-05,
      "loss": 2.6303,
      "step": 789300
    },
    {
      "epoch": 255.88330632090762,
      "grad_norm": 1.3955914974212646,
      "learning_rate": 2.443807979240999e-05,
      "loss": 2.6081,
      "step": 789400
    },
    {
      "epoch": 255.9157212317666,
      "grad_norm": 1.2618014812469482,
      "learning_rate": 2.443483619850795e-05,
      "loss": 2.6156,
      "step": 789500
    },
    {
      "epoch": 255.9481361426256,
      "grad_norm": 1.1811968088150024,
      "learning_rate": 2.4431592604605904e-05,
      "loss": 2.65,
      "step": 789600
    },
    {
      "epoch": 255.9805510534846,
      "grad_norm": 1.2795058488845825,
      "learning_rate": 2.442834901070386e-05,
      "loss": 2.6135,
      "step": 789700
    },
    {
      "epoch": 256.0,
      "eval_bleu": 1.1630085115484312,
      "eval_loss": 4.099703788757324,
      "eval_runtime": 4.2989,
      "eval_samples_per_second": 114.447,
      "eval_steps_per_second": 1.861,
      "step": 789760
    },
    {
      "epoch": 256.0129659643436,
      "grad_norm": 1.4775282144546509,
      "learning_rate": 2.4425105416801818e-05,
      "loss": 2.6366,
      "step": 789800
    },
    {
      "epoch": 256.04538087520257,
      "grad_norm": 1.739870309829712,
      "learning_rate": 2.4421894258838796e-05,
      "loss": 2.6374,
      "step": 789900
    },
    {
      "epoch": 256.0777957860616,
      "grad_norm": 1.2421678304672241,
      "learning_rate": 2.441865066493675e-05,
      "loss": 2.5956,
      "step": 790000
    },
    {
      "epoch": 256.1102106969206,
      "grad_norm": 1.233085036277771,
      "learning_rate": 2.4415407071034706e-05,
      "loss": 2.6142,
      "step": 790100
    },
    {
      "epoch": 256.14262560777956,
      "grad_norm": 1.0816363096237183,
      "learning_rate": 2.441216347713266e-05,
      "loss": 2.5764,
      "step": 790200
    },
    {
      "epoch": 256.1750405186386,
      "grad_norm": 1.1390149593353271,
      "learning_rate": 2.440891988323062e-05,
      "loss": 2.6179,
      "step": 790300
    },
    {
      "epoch": 256.20745542949754,
      "grad_norm": 1.3420522212982178,
      "learning_rate": 2.4405676289328576e-05,
      "loss": 2.5926,
      "step": 790400
    },
    {
      "epoch": 256.23987034035656,
      "grad_norm": 1.1389955282211304,
      "learning_rate": 2.4402432695426534e-05,
      "loss": 2.5966,
      "step": 790500
    },
    {
      "epoch": 256.2722852512156,
      "grad_norm": 1.2540416717529297,
      "learning_rate": 2.4399189101524493e-05,
      "loss": 2.629,
      "step": 790600
    },
    {
      "epoch": 256.30470016207454,
      "grad_norm": 1.1129729747772217,
      "learning_rate": 2.439594550762245e-05,
      "loss": 2.6184,
      "step": 790700
    },
    {
      "epoch": 256.33711507293356,
      "grad_norm": 1.2106423377990723,
      "learning_rate": 2.4392701913720404e-05,
      "loss": 2.622,
      "step": 790800
    },
    {
      "epoch": 256.3695299837925,
      "grad_norm": 1.1640864610671997,
      "learning_rate": 2.438945831981836e-05,
      "loss": 2.6344,
      "step": 790900
    },
    {
      "epoch": 256.40194489465154,
      "grad_norm": 1.4282218217849731,
      "learning_rate": 2.4386214725916318e-05,
      "loss": 2.6318,
      "step": 791000
    },
    {
      "epoch": 256.43435980551055,
      "grad_norm": 1.315240502357483,
      "learning_rate": 2.4382971132014273e-05,
      "loss": 2.6239,
      "step": 791100
    },
    {
      "epoch": 256.4667747163695,
      "grad_norm": 1.5503636598587036,
      "learning_rate": 2.437972753811223e-05,
      "loss": 2.6087,
      "step": 791200
    },
    {
      "epoch": 256.49918962722853,
      "grad_norm": 1.3474962711334229,
      "learning_rate": 2.4376483944210184e-05,
      "loss": 2.6163,
      "step": 791300
    },
    {
      "epoch": 256.5316045380875,
      "grad_norm": 1.3937710523605347,
      "learning_rate": 2.4373240350308143e-05,
      "loss": 2.6012,
      "step": 791400
    },
    {
      "epoch": 256.5640194489465,
      "grad_norm": 1.295723795890808,
      "learning_rate": 2.4369996756406098e-05,
      "loss": 2.6214,
      "step": 791500
    },
    {
      "epoch": 256.5964343598055,
      "grad_norm": 1.2622579336166382,
      "learning_rate": 2.4366753162504054e-05,
      "loss": 2.6149,
      "step": 791600
    },
    {
      "epoch": 256.6288492706645,
      "grad_norm": 1.1870485544204712,
      "learning_rate": 2.4363509568602012e-05,
      "loss": 2.6252,
      "step": 791700
    },
    {
      "epoch": 256.6612641815235,
      "grad_norm": 1.2647125720977783,
      "learning_rate": 2.436026597469997e-05,
      "loss": 2.6267,
      "step": 791800
    },
    {
      "epoch": 256.6936790923825,
      "grad_norm": 1.1496567726135254,
      "learning_rate": 2.4357022380797926e-05,
      "loss": 2.6126,
      "step": 791900
    },
    {
      "epoch": 256.7260940032415,
      "grad_norm": 1.39536452293396,
      "learning_rate": 2.4353778786895882e-05,
      "loss": 2.5987,
      "step": 792000
    },
    {
      "epoch": 256.7585089141005,
      "grad_norm": 1.373352289199829,
      "learning_rate": 2.435053519299384e-05,
      "loss": 2.6383,
      "step": 792100
    },
    {
      "epoch": 256.79092382495946,
      "grad_norm": 1.3399970531463623,
      "learning_rate": 2.4347291599091796e-05,
      "loss": 2.6003,
      "step": 792200
    },
    {
      "epoch": 256.8233387358185,
      "grad_norm": 1.180657982826233,
      "learning_rate": 2.434404800518975e-05,
      "loss": 2.6104,
      "step": 792300
    },
    {
      "epoch": 256.8557536466775,
      "grad_norm": 1.4383670091629028,
      "learning_rate": 2.4340804411287707e-05,
      "loss": 2.6328,
      "step": 792400
    },
    {
      "epoch": 256.88816855753646,
      "grad_norm": 1.2317911386489868,
      "learning_rate": 2.4337560817385665e-05,
      "loss": 2.6237,
      "step": 792500
    },
    {
      "epoch": 256.9205834683955,
      "grad_norm": 1.1023470163345337,
      "learning_rate": 2.4334382095361662e-05,
      "loss": 2.6175,
      "step": 792600
    },
    {
      "epoch": 256.95299837925444,
      "grad_norm": 1.1315913200378418,
      "learning_rate": 2.4331138501459618e-05,
      "loss": 2.6292,
      "step": 792700
    },
    {
      "epoch": 256.98541329011346,
      "grad_norm": 1.3594958782196045,
      "learning_rate": 2.4327894907557576e-05,
      "loss": 2.6098,
      "step": 792800
    },
    {
      "epoch": 257.0,
      "eval_bleu": 1.1011898693528206,
      "eval_loss": 4.099843978881836,
      "eval_runtime": 4.4601,
      "eval_samples_per_second": 110.312,
      "eval_steps_per_second": 1.794,
      "step": 792845
    },
    {
      "epoch": 257.0178282009725,
      "grad_norm": 1.1326544284820557,
      "learning_rate": 2.432465131365553e-05,
      "loss": 2.6049,
      "step": 792900
    },
    {
      "epoch": 257.05024311183143,
      "grad_norm": 1.2836902141571045,
      "learning_rate": 2.432140771975349e-05,
      "loss": 2.5776,
      "step": 793000
    },
    {
      "epoch": 257.08265802269045,
      "grad_norm": 1.3206677436828613,
      "learning_rate": 2.4318164125851446e-05,
      "loss": 2.6057,
      "step": 793100
    },
    {
      "epoch": 257.1150729335494,
      "grad_norm": 1.2977813482284546,
      "learning_rate": 2.43149205319494e-05,
      "loss": 2.6146,
      "step": 793200
    },
    {
      "epoch": 257.14748784440843,
      "grad_norm": 1.3499479293823242,
      "learning_rate": 2.4311676938047356e-05,
      "loss": 2.6077,
      "step": 793300
    },
    {
      "epoch": 257.17990275526745,
      "grad_norm": 1.1744297742843628,
      "learning_rate": 2.4308433344145315e-05,
      "loss": 2.6206,
      "step": 793400
    },
    {
      "epoch": 257.2123176661264,
      "grad_norm": 1.1529115438461304,
      "learning_rate": 2.430518975024327e-05,
      "loss": 2.6103,
      "step": 793500
    },
    {
      "epoch": 257.2447325769854,
      "grad_norm": 1.1458275318145752,
      "learning_rate": 2.4301946156341226e-05,
      "loss": 2.6147,
      "step": 793600
    },
    {
      "epoch": 257.2771474878444,
      "grad_norm": 1.313333511352539,
      "learning_rate": 2.4298702562439185e-05,
      "loss": 2.6101,
      "step": 793700
    },
    {
      "epoch": 257.3095623987034,
      "grad_norm": 1.326734185218811,
      "learning_rate": 2.429545896853714e-05,
      "loss": 2.6374,
      "step": 793800
    },
    {
      "epoch": 257.3419773095624,
      "grad_norm": 1.1530159711837769,
      "learning_rate": 2.4292215374635095e-05,
      "loss": 2.6006,
      "step": 793900
    },
    {
      "epoch": 257.3743922204214,
      "grad_norm": 1.1801939010620117,
      "learning_rate": 2.4288971780733054e-05,
      "loss": 2.6174,
      "step": 794000
    },
    {
      "epoch": 257.4068071312804,
      "grad_norm": 1.2037889957427979,
      "learning_rate": 2.428572818683101e-05,
      "loss": 2.632,
      "step": 794100
    },
    {
      "epoch": 257.43922204213936,
      "grad_norm": 1.2082151174545288,
      "learning_rate": 2.4282484592928968e-05,
      "loss": 2.607,
      "step": 794200
    },
    {
      "epoch": 257.4716369529984,
      "grad_norm": 1.3800194263458252,
      "learning_rate": 2.4279240999026924e-05,
      "loss": 2.5948,
      "step": 794300
    },
    {
      "epoch": 257.5040518638574,
      "grad_norm": 1.1368457078933716,
      "learning_rate": 2.427599740512488e-05,
      "loss": 2.597,
      "step": 794400
    },
    {
      "epoch": 257.53646677471636,
      "grad_norm": 1.210121750831604,
      "learning_rate": 2.4272753811222838e-05,
      "loss": 2.61,
      "step": 794500
    },
    {
      "epoch": 257.5688816855754,
      "grad_norm": 1.2559257745742798,
      "learning_rate": 2.4269510217320793e-05,
      "loss": 2.6204,
      "step": 794600
    },
    {
      "epoch": 257.60129659643434,
      "grad_norm": 1.180503487586975,
      "learning_rate": 2.426626662341875e-05,
      "loss": 2.6214,
      "step": 794700
    },
    {
      "epoch": 257.63371150729336,
      "grad_norm": 1.2645196914672852,
      "learning_rate": 2.4263023029516704e-05,
      "loss": 2.605,
      "step": 794800
    },
    {
      "epoch": 257.6661264181524,
      "grad_norm": 1.3143911361694336,
      "learning_rate": 2.4259779435614662e-05,
      "loss": 2.6245,
      "step": 794900
    },
    {
      "epoch": 257.69854132901133,
      "grad_norm": 1.2217553853988647,
      "learning_rate": 2.4256535841712618e-05,
      "loss": 2.6176,
      "step": 795000
    },
    {
      "epoch": 257.73095623987035,
      "grad_norm": 1.4211721420288086,
      "learning_rate": 2.4253292247810573e-05,
      "loss": 2.6316,
      "step": 795100
    },
    {
      "epoch": 257.7633711507293,
      "grad_norm": 1.3329994678497314,
      "learning_rate": 2.4250048653908532e-05,
      "loss": 2.6287,
      "step": 795200
    },
    {
      "epoch": 257.79578606158833,
      "grad_norm": 1.3858267068862915,
      "learning_rate": 2.4246805060006487e-05,
      "loss": 2.6341,
      "step": 795300
    },
    {
      "epoch": 257.82820097244735,
      "grad_norm": 1.282460331916809,
      "learning_rate": 2.4243561466104446e-05,
      "loss": 2.6135,
      "step": 795400
    },
    {
      "epoch": 257.8606158833063,
      "grad_norm": 1.4646066427230835,
      "learning_rate": 2.42403178722024e-05,
      "loss": 2.6101,
      "step": 795500
    },
    {
      "epoch": 257.8930307941653,
      "grad_norm": 1.2627429962158203,
      "learning_rate": 2.423707427830036e-05,
      "loss": 2.6355,
      "step": 795600
    },
    {
      "epoch": 257.9254457050243,
      "grad_norm": 1.446571707725525,
      "learning_rate": 2.4233830684398315e-05,
      "loss": 2.6377,
      "step": 795700
    },
    {
      "epoch": 257.9578606158833,
      "grad_norm": 1.4243773221969604,
      "learning_rate": 2.423058709049627e-05,
      "loss": 2.5968,
      "step": 795800
    },
    {
      "epoch": 257.9902755267423,
      "grad_norm": 1.4892818927764893,
      "learning_rate": 2.4227343496594226e-05,
      "loss": 2.6257,
      "step": 795900
    },
    {
      "epoch": 258.0,
      "eval_bleu": 1.0186682349367637,
      "eval_loss": 4.101884841918945,
      "eval_runtime": 4.4659,
      "eval_samples_per_second": 110.169,
      "eval_steps_per_second": 1.791,
      "step": 795930
    },
    {
      "epoch": 258.0226904376013,
      "grad_norm": 1.3350383043289185,
      "learning_rate": 2.4224099902692185e-05,
      "loss": 2.6034,
      "step": 796000
    },
    {
      "epoch": 258.0551053484603,
      "grad_norm": 1.23678719997406,
      "learning_rate": 2.422085630879014e-05,
      "loss": 2.6229,
      "step": 796100
    },
    {
      "epoch": 258.08752025931926,
      "grad_norm": 1.3397393226623535,
      "learning_rate": 2.4217612714888096e-05,
      "loss": 2.6072,
      "step": 796200
    },
    {
      "epoch": 258.1199351701783,
      "grad_norm": 1.3689149618148804,
      "learning_rate": 2.421436912098605e-05,
      "loss": 2.6167,
      "step": 796300
    },
    {
      "epoch": 258.1523500810373,
      "grad_norm": 1.2296391725540161,
      "learning_rate": 2.421112552708401e-05,
      "loss": 2.5846,
      "step": 796400
    },
    {
      "epoch": 258.18476499189626,
      "grad_norm": 1.1778953075408936,
      "learning_rate": 2.4207881933181965e-05,
      "loss": 2.6006,
      "step": 796500
    },
    {
      "epoch": 258.2171799027553,
      "grad_norm": 1.185115098953247,
      "learning_rate": 2.4204638339279924e-05,
      "loss": 2.5773,
      "step": 796600
    },
    {
      "epoch": 258.24959481361424,
      "grad_norm": 1.2163547277450562,
      "learning_rate": 2.4201427181316898e-05,
      "loss": 2.5977,
      "step": 796700
    },
    {
      "epoch": 258.28200972447326,
      "grad_norm": 1.2068309783935547,
      "learning_rate": 2.4198183587414857e-05,
      "loss": 2.6141,
      "step": 796800
    },
    {
      "epoch": 258.3144246353323,
      "grad_norm": 1.1566755771636963,
      "learning_rate": 2.4194939993512812e-05,
      "loss": 2.6144,
      "step": 796900
    },
    {
      "epoch": 258.34683954619123,
      "grad_norm": 1.1908975839614868,
      "learning_rate": 2.4191696399610768e-05,
      "loss": 2.6074,
      "step": 797000
    },
    {
      "epoch": 258.37925445705025,
      "grad_norm": 1.3487766981124878,
      "learning_rate": 2.4188452805708726e-05,
      "loss": 2.6257,
      "step": 797100
    },
    {
      "epoch": 258.4116693679092,
      "grad_norm": 1.3103461265563965,
      "learning_rate": 2.4185209211806685e-05,
      "loss": 2.5999,
      "step": 797200
    },
    {
      "epoch": 258.44408427876823,
      "grad_norm": 1.3602526187896729,
      "learning_rate": 2.418196561790464e-05,
      "loss": 2.6232,
      "step": 797300
    },
    {
      "epoch": 258.47649918962725,
      "grad_norm": 1.4395123720169067,
      "learning_rate": 2.4178722024002596e-05,
      "loss": 2.6136,
      "step": 797400
    },
    {
      "epoch": 258.5089141004862,
      "grad_norm": 1.372949242591858,
      "learning_rate": 2.4175478430100555e-05,
      "loss": 2.6125,
      "step": 797500
    },
    {
      "epoch": 258.5413290113452,
      "grad_norm": 1.260561466217041,
      "learning_rate": 2.417223483619851e-05,
      "loss": 2.6106,
      "step": 797600
    },
    {
      "epoch": 258.5737439222042,
      "grad_norm": 1.5075724124908447,
      "learning_rate": 2.4168991242296465e-05,
      "loss": 2.6158,
      "step": 797700
    },
    {
      "epoch": 258.6061588330632,
      "grad_norm": 1.4677894115447998,
      "learning_rate": 2.416574764839442e-05,
      "loss": 2.6175,
      "step": 797800
    },
    {
      "epoch": 258.6385737439222,
      "grad_norm": 1.5668708086013794,
      "learning_rate": 2.416250405449238e-05,
      "loss": 2.6166,
      "step": 797900
    },
    {
      "epoch": 258.6709886547812,
      "grad_norm": 1.3336695432662964,
      "learning_rate": 2.4159260460590335e-05,
      "loss": 2.6331,
      "step": 798000
    },
    {
      "epoch": 258.7034035656402,
      "grad_norm": 1.1803057193756104,
      "learning_rate": 2.415601686668829e-05,
      "loss": 2.6206,
      "step": 798100
    },
    {
      "epoch": 258.73581847649916,
      "grad_norm": 1.2025458812713623,
      "learning_rate": 2.4152773272786245e-05,
      "loss": 2.5859,
      "step": 798200
    },
    {
      "epoch": 258.7682333873582,
      "grad_norm": 1.4189966917037964,
      "learning_rate": 2.4149529678884204e-05,
      "loss": 2.6271,
      "step": 798300
    },
    {
      "epoch": 258.8006482982172,
      "grad_norm": 1.3402637243270874,
      "learning_rate": 2.4146286084982163e-05,
      "loss": 2.6343,
      "step": 798400
    },
    {
      "epoch": 258.83306320907616,
      "grad_norm": 1.2691941261291504,
      "learning_rate": 2.4143042491080118e-05,
      "loss": 2.6139,
      "step": 798500
    },
    {
      "epoch": 258.8654781199352,
      "grad_norm": 1.274357795715332,
      "learning_rate": 2.4139798897178074e-05,
      "loss": 2.6442,
      "step": 798600
    },
    {
      "epoch": 258.8978930307942,
      "grad_norm": 1.3004285097122192,
      "learning_rate": 2.4136555303276032e-05,
      "loss": 2.5959,
      "step": 798700
    },
    {
      "epoch": 258.93030794165315,
      "grad_norm": 1.2981404066085815,
      "learning_rate": 2.4133311709373988e-05,
      "loss": 2.6113,
      "step": 798800
    },
    {
      "epoch": 258.9627228525122,
      "grad_norm": 1.334525227546692,
      "learning_rate": 2.4130068115471943e-05,
      "loss": 2.618,
      "step": 798900
    },
    {
      "epoch": 258.99513776337113,
      "grad_norm": 1.494793176651001,
      "learning_rate": 2.4126824521569902e-05,
      "loss": 2.6149,
      "step": 799000
    },
    {
      "epoch": 259.0,
      "eval_bleu": 1.0818265836627374,
      "eval_loss": 4.107663154602051,
      "eval_runtime": 4.1457,
      "eval_samples_per_second": 118.678,
      "eval_steps_per_second": 1.93,
      "step": 799015
    },
    {
      "epoch": 259.02755267423015,
      "grad_norm": 1.379430890083313,
      "learning_rate": 2.4123580927667857e-05,
      "loss": 2.6436,
      "step": 799100
    },
    {
      "epoch": 259.05996758508917,
      "grad_norm": 1.3287158012390137,
      "learning_rate": 2.4120337333765813e-05,
      "loss": 2.603,
      "step": 799200
    },
    {
      "epoch": 259.09238249594813,
      "grad_norm": 1.4929759502410889,
      "learning_rate": 2.4117093739863768e-05,
      "loss": 2.6265,
      "step": 799300
    },
    {
      "epoch": 259.12479740680715,
      "grad_norm": 1.1771624088287354,
      "learning_rate": 2.4113850145961727e-05,
      "loss": 2.6233,
      "step": 799400
    },
    {
      "epoch": 259.1572123176661,
      "grad_norm": 1.3686683177947998,
      "learning_rate": 2.4110606552059682e-05,
      "loss": 2.6064,
      "step": 799500
    },
    {
      "epoch": 259.1896272285251,
      "grad_norm": 1.276679515838623,
      "learning_rate": 2.410736295815764e-05,
      "loss": 2.6212,
      "step": 799600
    },
    {
      "epoch": 259.22204213938414,
      "grad_norm": 1.4904649257659912,
      "learning_rate": 2.4104119364255596e-05,
      "loss": 2.6082,
      "step": 799700
    },
    {
      "epoch": 259.2544570502431,
      "grad_norm": 1.2974368333816528,
      "learning_rate": 2.4100875770353555e-05,
      "loss": 2.6116,
      "step": 799800
    },
    {
      "epoch": 259.2868719611021,
      "grad_norm": 1.4081711769104004,
      "learning_rate": 2.409763217645151e-05,
      "loss": 2.6016,
      "step": 799900
    },
    {
      "epoch": 259.3192868719611,
      "grad_norm": 1.2487177848815918,
      "learning_rate": 2.4094388582549466e-05,
      "loss": 2.6062,
      "step": 800000
    },
    {
      "epoch": 259.3517017828201,
      "grad_norm": 1.4159754514694214,
      "learning_rate": 2.4091144988647424e-05,
      "loss": 2.5969,
      "step": 800100
    },
    {
      "epoch": 259.3841166936791,
      "grad_norm": 1.3772213459014893,
      "learning_rate": 2.408790139474538e-05,
      "loss": 2.5921,
      "step": 800200
    },
    {
      "epoch": 259.4165316045381,
      "grad_norm": 1.5114436149597168,
      "learning_rate": 2.4084657800843335e-05,
      "loss": 2.6248,
      "step": 800300
    },
    {
      "epoch": 259.4489465153971,
      "grad_norm": 1.3251899480819702,
      "learning_rate": 2.408141420694129e-05,
      "loss": 2.604,
      "step": 800400
    },
    {
      "epoch": 259.48136142625606,
      "grad_norm": 1.3021750450134277,
      "learning_rate": 2.407817061303925e-05,
      "loss": 2.6025,
      "step": 800500
    },
    {
      "epoch": 259.5137763371151,
      "grad_norm": 1.3778105974197388,
      "learning_rate": 2.4074927019137204e-05,
      "loss": 2.6062,
      "step": 800600
    },
    {
      "epoch": 259.5461912479741,
      "grad_norm": 1.2698835134506226,
      "learning_rate": 2.4071715861174182e-05,
      "loss": 2.6164,
      "step": 800700
    },
    {
      "epoch": 259.57860615883305,
      "grad_norm": 1.2977333068847656,
      "learning_rate": 2.4068472267272138e-05,
      "loss": 2.6198,
      "step": 800800
    },
    {
      "epoch": 259.6110210696921,
      "grad_norm": 1.1258234977722168,
      "learning_rate": 2.4065228673370096e-05,
      "loss": 2.6163,
      "step": 800900
    },
    {
      "epoch": 259.64343598055103,
      "grad_norm": 1.2499922513961792,
      "learning_rate": 2.4061985079468052e-05,
      "loss": 2.6259,
      "step": 801000
    },
    {
      "epoch": 259.67585089141005,
      "grad_norm": 1.3271653652191162,
      "learning_rate": 2.4058741485566007e-05,
      "loss": 2.6187,
      "step": 801100
    },
    {
      "epoch": 259.70826580226907,
      "grad_norm": 1.2759135961532593,
      "learning_rate": 2.4055497891663962e-05,
      "loss": 2.6045,
      "step": 801200
    },
    {
      "epoch": 259.74068071312803,
      "grad_norm": 1.195888876914978,
      "learning_rate": 2.405225429776192e-05,
      "loss": 2.6108,
      "step": 801300
    },
    {
      "epoch": 259.77309562398705,
      "grad_norm": 1.1503006219863892,
      "learning_rate": 2.404901070385988e-05,
      "loss": 2.6169,
      "step": 801400
    },
    {
      "epoch": 259.805510534846,
      "grad_norm": 1.3588485717773438,
      "learning_rate": 2.4045767109957835e-05,
      "loss": 2.6272,
      "step": 801500
    },
    {
      "epoch": 259.837925445705,
      "grad_norm": 1.279475450515747,
      "learning_rate": 2.404252351605579e-05,
      "loss": 2.6213,
      "step": 801600
    },
    {
      "epoch": 259.87034035656404,
      "grad_norm": 1.3582850694656372,
      "learning_rate": 2.403927992215375e-05,
      "loss": 2.5918,
      "step": 801700
    },
    {
      "epoch": 259.902755267423,
      "grad_norm": 1.3876301050186157,
      "learning_rate": 2.4036036328251705e-05,
      "loss": 2.6132,
      "step": 801800
    },
    {
      "epoch": 259.935170178282,
      "grad_norm": 1.3096598386764526,
      "learning_rate": 2.403279273434966e-05,
      "loss": 2.6024,
      "step": 801900
    },
    {
      "epoch": 259.967585089141,
      "grad_norm": 1.5475821495056152,
      "learning_rate": 2.4029549140447615e-05,
      "loss": 2.6097,
      "step": 802000
    },
    {
      "epoch": 260.0,
      "grad_norm": 1.227144718170166,
      "learning_rate": 2.4026305546545574e-05,
      "loss": 2.615,
      "step": 802100
    },
    {
      "epoch": 260.0,
      "eval_bleu": 1.0204462330567095,
      "eval_loss": 4.104010581970215,
      "eval_runtime": 4.5259,
      "eval_samples_per_second": 108.707,
      "eval_steps_per_second": 1.768,
      "step": 802100
    },
    {
      "epoch": 260.032414910859,
      "grad_norm": 1.439987063407898,
      "learning_rate": 2.402306195264353e-05,
      "loss": 2.6113,
      "step": 802200
    },
    {
      "epoch": 260.064829821718,
      "grad_norm": 1.4772758483886719,
      "learning_rate": 2.4019818358741485e-05,
      "loss": 2.6025,
      "step": 802300
    },
    {
      "epoch": 260.097244732577,
      "grad_norm": 1.316859245300293,
      "learning_rate": 2.4016574764839444e-05,
      "loss": 2.5984,
      "step": 802400
    },
    {
      "epoch": 260.12965964343596,
      "grad_norm": 1.5719746351242065,
      "learning_rate": 2.40133311709374e-05,
      "loss": 2.6068,
      "step": 802500
    },
    {
      "epoch": 260.162074554295,
      "grad_norm": 1.2728952169418335,
      "learning_rate": 2.4010087577035358e-05,
      "loss": 2.6186,
      "step": 802600
    },
    {
      "epoch": 260.194489465154,
      "grad_norm": 1.2794625759124756,
      "learning_rate": 2.4006876419072332e-05,
      "loss": 2.6054,
      "step": 802700
    },
    {
      "epoch": 260.22690437601295,
      "grad_norm": 1.395215630531311,
      "learning_rate": 2.4003632825170287e-05,
      "loss": 2.6185,
      "step": 802800
    },
    {
      "epoch": 260.25931928687197,
      "grad_norm": 1.2128163576126099,
      "learning_rate": 2.4000389231268246e-05,
      "loss": 2.6258,
      "step": 802900
    },
    {
      "epoch": 260.29173419773093,
      "grad_norm": 1.4006924629211426,
      "learning_rate": 2.39971456373662e-05,
      "loss": 2.6331,
      "step": 803000
    },
    {
      "epoch": 260.32414910858995,
      "grad_norm": 1.5354297161102295,
      "learning_rate": 2.399390204346416e-05,
      "loss": 2.6255,
      "step": 803100
    },
    {
      "epoch": 260.35656401944897,
      "grad_norm": 1.2487986087799072,
      "learning_rate": 2.3990658449562116e-05,
      "loss": 2.6304,
      "step": 803200
    },
    {
      "epoch": 260.3889789303079,
      "grad_norm": 1.1241888999938965,
      "learning_rate": 2.3987414855660074e-05,
      "loss": 2.5977,
      "step": 803300
    },
    {
      "epoch": 260.42139384116695,
      "grad_norm": 1.2178524732589722,
      "learning_rate": 2.398417126175803e-05,
      "loss": 2.6139,
      "step": 803400
    },
    {
      "epoch": 260.4538087520259,
      "grad_norm": 1.179978847503662,
      "learning_rate": 2.3980960103795004e-05,
      "loss": 2.6236,
      "step": 803500
    },
    {
      "epoch": 260.4862236628849,
      "grad_norm": 1.344618797302246,
      "learning_rate": 2.3977716509892963e-05,
      "loss": 2.6124,
      "step": 803600
    },
    {
      "epoch": 260.51863857374394,
      "grad_norm": 1.2556860446929932,
      "learning_rate": 2.3974472915990918e-05,
      "loss": 2.5994,
      "step": 803700
    },
    {
      "epoch": 260.5510534846029,
      "grad_norm": 1.4002586603164673,
      "learning_rate": 2.3971229322088877e-05,
      "loss": 2.5879,
      "step": 803800
    },
    {
      "epoch": 260.5834683954619,
      "grad_norm": 1.3044250011444092,
      "learning_rate": 2.3967985728186832e-05,
      "loss": 2.5946,
      "step": 803900
    },
    {
      "epoch": 260.6158833063209,
      "grad_norm": 1.411816954612732,
      "learning_rate": 2.396474213428479e-05,
      "loss": 2.6376,
      "step": 804000
    },
    {
      "epoch": 260.6482982171799,
      "grad_norm": 1.2465715408325195,
      "learning_rate": 2.3961498540382746e-05,
      "loss": 2.6112,
      "step": 804100
    },
    {
      "epoch": 260.6807131280389,
      "grad_norm": 1.2352259159088135,
      "learning_rate": 2.3958254946480702e-05,
      "loss": 2.6015,
      "step": 804200
    },
    {
      "epoch": 260.7131280388979,
      "grad_norm": 1.3362337350845337,
      "learning_rate": 2.3955011352578657e-05,
      "loss": 2.6018,
      "step": 804300
    },
    {
      "epoch": 260.7455429497569,
      "grad_norm": 1.1893744468688965,
      "learning_rate": 2.3951767758676616e-05,
      "loss": 2.5983,
      "step": 804400
    },
    {
      "epoch": 260.77795786061586,
      "grad_norm": 1.238915205001831,
      "learning_rate": 2.394852416477457e-05,
      "loss": 2.6231,
      "step": 804500
    },
    {
      "epoch": 260.8103727714749,
      "grad_norm": 1.244089126586914,
      "learning_rate": 2.3945280570872527e-05,
      "loss": 2.6098,
      "step": 804600
    },
    {
      "epoch": 260.8427876823339,
      "grad_norm": 1.2246222496032715,
      "learning_rate": 2.3942036976970482e-05,
      "loss": 2.5957,
      "step": 804700
    },
    {
      "epoch": 260.87520259319285,
      "grad_norm": 1.2062159776687622,
      "learning_rate": 2.393879338306844e-05,
      "loss": 2.6333,
      "step": 804800
    },
    {
      "epoch": 260.90761750405187,
      "grad_norm": 1.2540779113769531,
      "learning_rate": 2.3935549789166396e-05,
      "loss": 2.5992,
      "step": 804900
    },
    {
      "epoch": 260.94003241491083,
      "grad_norm": 1.265492558479309,
      "learning_rate": 2.3932306195264355e-05,
      "loss": 2.6355,
      "step": 805000
    },
    {
      "epoch": 260.97244732576985,
      "grad_norm": 1.222574234008789,
      "learning_rate": 2.392906260136231e-05,
      "loss": 2.5923,
      "step": 805100
    },
    {
      "epoch": 261.0,
      "eval_bleu": 1.178745584523083,
      "eval_loss": 4.111169815063477,
      "eval_runtime": 4.1517,
      "eval_samples_per_second": 118.504,
      "eval_steps_per_second": 1.927,
      "step": 805185
    },
    {
      "epoch": 261.00486223662887,
      "grad_norm": 1.2001489400863647,
      "learning_rate": 2.392581900746027e-05,
      "loss": 2.5888,
      "step": 805200
    },
    {
      "epoch": 261.0372771474878,
      "grad_norm": 1.4161237478256226,
      "learning_rate": 2.3922575413558224e-05,
      "loss": 2.6008,
      "step": 805300
    },
    {
      "epoch": 261.06969205834685,
      "grad_norm": 1.2486575841903687,
      "learning_rate": 2.391933181965618e-05,
      "loss": 2.599,
      "step": 805400
    },
    {
      "epoch": 261.1021069692058,
      "grad_norm": 1.258134365081787,
      "learning_rate": 2.391608822575414e-05,
      "loss": 2.6201,
      "step": 805500
    },
    {
      "epoch": 261.1345218800648,
      "grad_norm": 1.2832187414169312,
      "learning_rate": 2.3912844631852094e-05,
      "loss": 2.6022,
      "step": 805600
    },
    {
      "epoch": 261.16693679092384,
      "grad_norm": 1.2847473621368408,
      "learning_rate": 2.390960103795005e-05,
      "loss": 2.5822,
      "step": 805700
    },
    {
      "epoch": 261.1993517017828,
      "grad_norm": 1.2793389558792114,
      "learning_rate": 2.3906357444048004e-05,
      "loss": 2.6095,
      "step": 805800
    },
    {
      "epoch": 261.2317666126418,
      "grad_norm": 1.244852066040039,
      "learning_rate": 2.3903113850145963e-05,
      "loss": 2.6047,
      "step": 805900
    },
    {
      "epoch": 261.26418152350084,
      "grad_norm": 1.4269640445709229,
      "learning_rate": 2.389987025624392e-05,
      "loss": 2.6232,
      "step": 806000
    },
    {
      "epoch": 261.2965964343598,
      "grad_norm": 1.5219544172286987,
      "learning_rate": 2.3896626662341874e-05,
      "loss": 2.6209,
      "step": 806100
    },
    {
      "epoch": 261.3290113452188,
      "grad_norm": 1.1890158653259277,
      "learning_rate": 2.3893383068439833e-05,
      "loss": 2.5954,
      "step": 806200
    },
    {
      "epoch": 261.3614262560778,
      "grad_norm": 1.23195219039917,
      "learning_rate": 2.389013947453779e-05,
      "loss": 2.5959,
      "step": 806300
    },
    {
      "epoch": 261.3938411669368,
      "grad_norm": 1.1823575496673584,
      "learning_rate": 2.3886895880635747e-05,
      "loss": 2.6032,
      "step": 806400
    },
    {
      "epoch": 261.4262560777958,
      "grad_norm": 1.2025678157806396,
      "learning_rate": 2.3883652286733702e-05,
      "loss": 2.6075,
      "step": 806500
    },
    {
      "epoch": 261.4586709886548,
      "grad_norm": 1.3518736362457275,
      "learning_rate": 2.3880408692831657e-05,
      "loss": 2.594,
      "step": 806600
    },
    {
      "epoch": 261.4910858995138,
      "grad_norm": 1.1403557062149048,
      "learning_rate": 2.3877165098929616e-05,
      "loss": 2.6126,
      "step": 806700
    },
    {
      "epoch": 261.52350081037275,
      "grad_norm": 1.4021683931350708,
      "learning_rate": 2.387392150502757e-05,
      "loss": 2.6182,
      "step": 806800
    },
    {
      "epoch": 261.55591572123177,
      "grad_norm": 1.3225476741790771,
      "learning_rate": 2.3870677911125527e-05,
      "loss": 2.6128,
      "step": 806900
    },
    {
      "epoch": 261.5883306320908,
      "grad_norm": 1.2250701189041138,
      "learning_rate": 2.3867434317223486e-05,
      "loss": 2.6226,
      "step": 807000
    },
    {
      "epoch": 261.62074554294975,
      "grad_norm": 1.4623905420303345,
      "learning_rate": 2.386419072332144e-05,
      "loss": 2.6,
      "step": 807100
    },
    {
      "epoch": 261.65316045380877,
      "grad_norm": 1.3373817205429077,
      "learning_rate": 2.3860947129419396e-05,
      "loss": 2.5956,
      "step": 807200
    },
    {
      "epoch": 261.6855753646677,
      "grad_norm": 1.2652201652526855,
      "learning_rate": 2.3857703535517352e-05,
      "loss": 2.6076,
      "step": 807300
    },
    {
      "epoch": 261.71799027552674,
      "grad_norm": 1.1153934001922607,
      "learning_rate": 2.385445994161531e-05,
      "loss": 2.6184,
      "step": 807400
    },
    {
      "epoch": 261.75040518638576,
      "grad_norm": 1.2720105648040771,
      "learning_rate": 2.3851248783652288e-05,
      "loss": 2.6224,
      "step": 807500
    },
    {
      "epoch": 261.7828200972447,
      "grad_norm": 1.4004145860671997,
      "learning_rate": 2.3848005189750244e-05,
      "loss": 2.64,
      "step": 807600
    },
    {
      "epoch": 261.81523500810374,
      "grad_norm": 1.2187641859054565,
      "learning_rate": 2.38447615958482e-05,
      "loss": 2.6151,
      "step": 807700
    },
    {
      "epoch": 261.8476499189627,
      "grad_norm": 1.3771744966506958,
      "learning_rate": 2.3841518001946158e-05,
      "loss": 2.5945,
      "step": 807800
    },
    {
      "epoch": 261.8800648298217,
      "grad_norm": 1.2818419933319092,
      "learning_rate": 2.3838274408044113e-05,
      "loss": 2.6071,
      "step": 807900
    },
    {
      "epoch": 261.91247974068074,
      "grad_norm": 1.347538709640503,
      "learning_rate": 2.3835030814142072e-05,
      "loss": 2.5903,
      "step": 808000
    },
    {
      "epoch": 261.9448946515397,
      "grad_norm": 1.2365760803222656,
      "learning_rate": 2.3831787220240027e-05,
      "loss": 2.6336,
      "step": 808100
    },
    {
      "epoch": 261.9773095623987,
      "grad_norm": 1.2147608995437622,
      "learning_rate": 2.3828543626337986e-05,
      "loss": 2.6041,
      "step": 808200
    },
    {
      "epoch": 262.0,
      "eval_bleu": 1.0377808329416522,
      "eval_loss": 4.1104278564453125,
      "eval_runtime": 4.3342,
      "eval_samples_per_second": 113.517,
      "eval_steps_per_second": 1.846,
      "step": 808270
    },
    {
      "epoch": 262.0097244732577,
      "grad_norm": 1.2934207916259766,
      "learning_rate": 2.382530003243594e-05,
      "loss": 2.6081,
      "step": 808300
    },
    {
      "epoch": 262.0421393841167,
      "grad_norm": 1.1974537372589111,
      "learning_rate": 2.3822056438533897e-05,
      "loss": 2.6003,
      "step": 808400
    },
    {
      "epoch": 262.0745542949757,
      "grad_norm": 1.259792447090149,
      "learning_rate": 2.3818812844631852e-05,
      "loss": 2.623,
      "step": 808500
    },
    {
      "epoch": 262.1069692058347,
      "grad_norm": 1.3456460237503052,
      "learning_rate": 2.381556925072981e-05,
      "loss": 2.5709,
      "step": 808600
    },
    {
      "epoch": 262.1393841166937,
      "grad_norm": 1.1986515522003174,
      "learning_rate": 2.3812325656827766e-05,
      "loss": 2.5932,
      "step": 808700
    },
    {
      "epoch": 262.17179902755265,
      "grad_norm": 1.2317560911178589,
      "learning_rate": 2.3809114498864744e-05,
      "loss": 2.5895,
      "step": 808800
    },
    {
      "epoch": 262.20421393841167,
      "grad_norm": 1.553283929824829,
      "learning_rate": 2.38058709049627e-05,
      "loss": 2.5821,
      "step": 808900
    },
    {
      "epoch": 262.2366288492707,
      "grad_norm": 1.4863979816436768,
      "learning_rate": 2.3802627311060658e-05,
      "loss": 2.6092,
      "step": 809000
    },
    {
      "epoch": 262.26904376012965,
      "grad_norm": 1.2040154933929443,
      "learning_rate": 2.3799383717158613e-05,
      "loss": 2.5948,
      "step": 809100
    },
    {
      "epoch": 262.30145867098867,
      "grad_norm": 1.3773572444915771,
      "learning_rate": 2.379614012325657e-05,
      "loss": 2.6045,
      "step": 809200
    },
    {
      "epoch": 262.3338735818476,
      "grad_norm": 1.4393929243087769,
      "learning_rate": 2.3792896529354524e-05,
      "loss": 2.6228,
      "step": 809300
    },
    {
      "epoch": 262.36628849270664,
      "grad_norm": 1.302708387374878,
      "learning_rate": 2.3789652935452483e-05,
      "loss": 2.6028,
      "step": 809400
    },
    {
      "epoch": 262.39870340356566,
      "grad_norm": 1.3206924200057983,
      "learning_rate": 2.378644177748946e-05,
      "loss": 2.5945,
      "step": 809500
    },
    {
      "epoch": 262.4311183144246,
      "grad_norm": 1.2674514055252075,
      "learning_rate": 2.3783198183587416e-05,
      "loss": 2.6151,
      "step": 809600
    },
    {
      "epoch": 262.46353322528364,
      "grad_norm": 1.1265312433242798,
      "learning_rate": 2.377995458968537e-05,
      "loss": 2.6097,
      "step": 809700
    },
    {
      "epoch": 262.4959481361426,
      "grad_norm": 1.3746875524520874,
      "learning_rate": 2.377671099578333e-05,
      "loss": 2.6136,
      "step": 809800
    },
    {
      "epoch": 262.5283630470016,
      "grad_norm": 1.650403380393982,
      "learning_rate": 2.3773467401881285e-05,
      "loss": 2.6163,
      "step": 809900
    },
    {
      "epoch": 262.56077795786064,
      "grad_norm": 1.3211385011672974,
      "learning_rate": 2.377022380797924e-05,
      "loss": 2.6071,
      "step": 810000
    },
    {
      "epoch": 262.5931928687196,
      "grad_norm": 1.3754091262817383,
      "learning_rate": 2.3766980214077196e-05,
      "loss": 2.6114,
      "step": 810100
    },
    {
      "epoch": 262.6256077795786,
      "grad_norm": 1.2367945909500122,
      "learning_rate": 2.3763736620175155e-05,
      "loss": 2.5992,
      "step": 810200
    },
    {
      "epoch": 262.6580226904376,
      "grad_norm": 1.2344176769256592,
      "learning_rate": 2.376049302627311e-05,
      "loss": 2.622,
      "step": 810300
    },
    {
      "epoch": 262.6904376012966,
      "grad_norm": 1.4841614961624146,
      "learning_rate": 2.375724943237107e-05,
      "loss": 2.606,
      "step": 810400
    },
    {
      "epoch": 262.7228525121556,
      "grad_norm": 1.317717432975769,
      "learning_rate": 2.3754005838469028e-05,
      "loss": 2.6118,
      "step": 810500
    },
    {
      "epoch": 262.7552674230146,
      "grad_norm": 1.2520675659179688,
      "learning_rate": 2.3750762244566983e-05,
      "loss": 2.6325,
      "step": 810600
    },
    {
      "epoch": 262.7876823338736,
      "grad_norm": 1.2909239530563354,
      "learning_rate": 2.374751865066494e-05,
      "loss": 2.6049,
      "step": 810700
    },
    {
      "epoch": 262.82009724473255,
      "grad_norm": 1.2363837957382202,
      "learning_rate": 2.3744275056762894e-05,
      "loss": 2.6052,
      "step": 810800
    },
    {
      "epoch": 262.85251215559157,
      "grad_norm": 1.130570888519287,
      "learning_rate": 2.3741031462860852e-05,
      "loss": 2.6005,
      "step": 810900
    },
    {
      "epoch": 262.8849270664506,
      "grad_norm": 1.250221610069275,
      "learning_rate": 2.3737787868958808e-05,
      "loss": 2.6133,
      "step": 811000
    },
    {
      "epoch": 262.91734197730955,
      "grad_norm": 1.273382306098938,
      "learning_rate": 2.3734544275056763e-05,
      "loss": 2.6142,
      "step": 811100
    },
    {
      "epoch": 262.94975688816857,
      "grad_norm": 1.3185104131698608,
      "learning_rate": 2.373130068115472e-05,
      "loss": 2.6063,
      "step": 811200
    },
    {
      "epoch": 262.9821717990275,
      "grad_norm": 1.3167407512664795,
      "learning_rate": 2.3728057087252677e-05,
      "loss": 2.6379,
      "step": 811300
    },
    {
      "epoch": 263.0,
      "eval_bleu": 1.0604419016355282,
      "eval_loss": 4.109675407409668,
      "eval_runtime": 4.4045,
      "eval_samples_per_second": 111.705,
      "eval_steps_per_second": 1.816,
      "step": 811355
    },
    {
      "epoch": 263.01458670988654,
      "grad_norm": 1.4069585800170898,
      "learning_rate": 2.3724813493350633e-05,
      "loss": 2.625,
      "step": 811400
    },
    {
      "epoch": 263.04700162074556,
      "grad_norm": 1.1984527111053467,
      "learning_rate": 2.3721569899448588e-05,
      "loss": 2.6021,
      "step": 811500
    },
    {
      "epoch": 263.0794165316045,
      "grad_norm": 1.205896258354187,
      "learning_rate": 2.3718326305546547e-05,
      "loss": 2.5897,
      "step": 811600
    },
    {
      "epoch": 263.11183144246354,
      "grad_norm": 1.4410879611968994,
      "learning_rate": 2.3715082711644505e-05,
      "loss": 2.6173,
      "step": 811700
    },
    {
      "epoch": 263.1442463533225,
      "grad_norm": 1.2409504652023315,
      "learning_rate": 2.371183911774246e-05,
      "loss": 2.6072,
      "step": 811800
    },
    {
      "epoch": 263.1766612641815,
      "grad_norm": 1.3630095720291138,
      "learning_rate": 2.3708595523840416e-05,
      "loss": 2.6173,
      "step": 811900
    },
    {
      "epoch": 263.20907617504054,
      "grad_norm": 1.5013920068740845,
      "learning_rate": 2.3705351929938375e-05,
      "loss": 2.6028,
      "step": 812000
    },
    {
      "epoch": 263.2414910858995,
      "grad_norm": 1.1910837888717651,
      "learning_rate": 2.370210833603633e-05,
      "loss": 2.6368,
      "step": 812100
    },
    {
      "epoch": 263.2739059967585,
      "grad_norm": 1.2358171939849854,
      "learning_rate": 2.3698864742134286e-05,
      "loss": 2.583,
      "step": 812200
    },
    {
      "epoch": 263.3063209076175,
      "grad_norm": 1.3863297700881958,
      "learning_rate": 2.369562114823224e-05,
      "loss": 2.6215,
      "step": 812300
    },
    {
      "epoch": 263.3387358184765,
      "grad_norm": 1.1978435516357422,
      "learning_rate": 2.36923775543302e-05,
      "loss": 2.6143,
      "step": 812400
    },
    {
      "epoch": 263.3711507293355,
      "grad_norm": 1.275692105293274,
      "learning_rate": 2.3689133960428155e-05,
      "loss": 2.6096,
      "step": 812500
    },
    {
      "epoch": 263.4035656401945,
      "grad_norm": 1.2884998321533203,
      "learning_rate": 2.368589036652611e-05,
      "loss": 2.5955,
      "step": 812600
    },
    {
      "epoch": 263.4359805510535,
      "grad_norm": 1.2691423892974854,
      "learning_rate": 2.3682646772624066e-05,
      "loss": 2.603,
      "step": 812700
    },
    {
      "epoch": 263.4683954619125,
      "grad_norm": 1.4250295162200928,
      "learning_rate": 2.3679403178722025e-05,
      "loss": 2.5894,
      "step": 812800
    },
    {
      "epoch": 263.50081037277147,
      "grad_norm": 1.1725797653198242,
      "learning_rate": 2.3676159584819983e-05,
      "loss": 2.6102,
      "step": 812900
    },
    {
      "epoch": 263.5332252836305,
      "grad_norm": 1.720230221748352,
      "learning_rate": 2.367291599091794e-05,
      "loss": 2.6009,
      "step": 813000
    },
    {
      "epoch": 263.56564019448945,
      "grad_norm": 1.4330815076828003,
      "learning_rate": 2.3669672397015894e-05,
      "loss": 2.6095,
      "step": 813100
    },
    {
      "epoch": 263.59805510534846,
      "grad_norm": 1.3001819849014282,
      "learning_rate": 2.3666428803113853e-05,
      "loss": 2.6355,
      "step": 813200
    },
    {
      "epoch": 263.6304700162075,
      "grad_norm": 1.2824028730392456,
      "learning_rate": 2.3663185209211808e-05,
      "loss": 2.5997,
      "step": 813300
    },
    {
      "epoch": 263.66288492706644,
      "grad_norm": 1.1447117328643799,
      "learning_rate": 2.3659941615309763e-05,
      "loss": 2.5869,
      "step": 813400
    },
    {
      "epoch": 263.69529983792546,
      "grad_norm": 1.3428183794021606,
      "learning_rate": 2.3656698021407722e-05,
      "loss": 2.6069,
      "step": 813500
    },
    {
      "epoch": 263.7277147487844,
      "grad_norm": 1.3938642740249634,
      "learning_rate": 2.3653454427505678e-05,
      "loss": 2.6062,
      "step": 813600
    },
    {
      "epoch": 263.76012965964344,
      "grad_norm": 1.5182234048843384,
      "learning_rate": 2.3650210833603633e-05,
      "loss": 2.6018,
      "step": 813700
    },
    {
      "epoch": 263.79254457050246,
      "grad_norm": 1.6446645259857178,
      "learning_rate": 2.3646967239701588e-05,
      "loss": 2.6031,
      "step": 813800
    },
    {
      "epoch": 263.8249594813614,
      "grad_norm": 1.2583098411560059,
      "learning_rate": 2.3643723645799547e-05,
      "loss": 2.6065,
      "step": 813900
    },
    {
      "epoch": 263.85737439222044,
      "grad_norm": 1.2974436283111572,
      "learning_rate": 2.3640480051897502e-05,
      "loss": 2.6116,
      "step": 814000
    },
    {
      "epoch": 263.8897893030794,
      "grad_norm": 1.3671385049819946,
      "learning_rate": 2.363723645799546e-05,
      "loss": 2.5954,
      "step": 814100
    },
    {
      "epoch": 263.9222042139384,
      "grad_norm": 1.221099615097046,
      "learning_rate": 2.3633992864093416e-05,
      "loss": 2.6267,
      "step": 814200
    },
    {
      "epoch": 263.95461912479743,
      "grad_norm": 1.2434295415878296,
      "learning_rate": 2.3630749270191375e-05,
      "loss": 2.6047,
      "step": 814300
    },
    {
      "epoch": 263.9870340356564,
      "grad_norm": 1.30385422706604,
      "learning_rate": 2.362750567628933e-05,
      "loss": 2.5922,
      "step": 814400
    },
    {
      "epoch": 264.0,
      "eval_bleu": 1.032635876606413,
      "eval_loss": 4.107585430145264,
      "eval_runtime": 4.3425,
      "eval_samples_per_second": 113.298,
      "eval_steps_per_second": 1.842,
      "step": 814440
    },
    {
      "epoch": 264.0194489465154,
      "grad_norm": 1.616775393486023,
      "learning_rate": 2.3624262082387286e-05,
      "loss": 2.6045,
      "step": 814500
    },
    {
      "epoch": 264.05186385737437,
      "grad_norm": 1.465466022491455,
      "learning_rate": 2.362101848848524e-05,
      "loss": 2.581,
      "step": 814600
    },
    {
      "epoch": 264.0842787682334,
      "grad_norm": 1.3753373622894287,
      "learning_rate": 2.36177748945832e-05,
      "loss": 2.595,
      "step": 814700
    },
    {
      "epoch": 264.1166936790924,
      "grad_norm": 1.2819812297821045,
      "learning_rate": 2.3614531300681155e-05,
      "loss": 2.6152,
      "step": 814800
    },
    {
      "epoch": 264.14910858995137,
      "grad_norm": 1.3274340629577637,
      "learning_rate": 2.361128770677911e-05,
      "loss": 2.6074,
      "step": 814900
    },
    {
      "epoch": 264.1815235008104,
      "grad_norm": 1.3321689367294312,
      "learning_rate": 2.360804411287707e-05,
      "loss": 2.597,
      "step": 815000
    },
    {
      "epoch": 264.21393841166935,
      "grad_norm": 1.1945998668670654,
      "learning_rate": 2.3604800518975025e-05,
      "loss": 2.6003,
      "step": 815100
    },
    {
      "epoch": 264.24635332252836,
      "grad_norm": 1.1959724426269531,
      "learning_rate": 2.360155692507298e-05,
      "loss": 2.6137,
      "step": 815200
    },
    {
      "epoch": 264.2787682333874,
      "grad_norm": 1.3300827741622925,
      "learning_rate": 2.359831333117094e-05,
      "loss": 2.6065,
      "step": 815300
    },
    {
      "epoch": 264.31118314424634,
      "grad_norm": 1.254280686378479,
      "learning_rate": 2.3595069737268898e-05,
      "loss": 2.6059,
      "step": 815400
    },
    {
      "epoch": 264.34359805510536,
      "grad_norm": 1.1750819683074951,
      "learning_rate": 2.3591858579305872e-05,
      "loss": 2.5795,
      "step": 815500
    },
    {
      "epoch": 264.3760129659643,
      "grad_norm": 1.3079428672790527,
      "learning_rate": 2.3588614985403827e-05,
      "loss": 2.602,
      "step": 815600
    },
    {
      "epoch": 264.40842787682334,
      "grad_norm": 1.1602675914764404,
      "learning_rate": 2.3585371391501783e-05,
      "loss": 2.5989,
      "step": 815700
    },
    {
      "epoch": 264.44084278768236,
      "grad_norm": 1.2692747116088867,
      "learning_rate": 2.358212779759974e-05,
      "loss": 2.6022,
      "step": 815800
    },
    {
      "epoch": 264.4732576985413,
      "grad_norm": 1.4514881372451782,
      "learning_rate": 2.35788842036977e-05,
      "loss": 2.629,
      "step": 815900
    },
    {
      "epoch": 264.50567260940034,
      "grad_norm": 1.319640040397644,
      "learning_rate": 2.3575640609795656e-05,
      "loss": 2.6058,
      "step": 816000
    },
    {
      "epoch": 264.5380875202593,
      "grad_norm": 1.2927486896514893,
      "learning_rate": 2.357239701589361e-05,
      "loss": 2.5968,
      "step": 816100
    },
    {
      "epoch": 264.5705024311183,
      "grad_norm": 1.2782692909240723,
      "learning_rate": 2.356915342199157e-05,
      "loss": 2.6224,
      "step": 816200
    },
    {
      "epoch": 264.60291734197733,
      "grad_norm": 1.2451428174972534,
      "learning_rate": 2.3565909828089525e-05,
      "loss": 2.6244,
      "step": 816300
    },
    {
      "epoch": 264.6353322528363,
      "grad_norm": 1.2861127853393555,
      "learning_rate": 2.356266623418748e-05,
      "loss": 2.6263,
      "step": 816400
    },
    {
      "epoch": 264.6677471636953,
      "grad_norm": 1.153018593788147,
      "learning_rate": 2.3559455076224458e-05,
      "loss": 2.6053,
      "step": 816500
    },
    {
      "epoch": 264.70016207455427,
      "grad_norm": 1.5465044975280762,
      "learning_rate": 2.3556211482322417e-05,
      "loss": 2.6119,
      "step": 816600
    },
    {
      "epoch": 264.7325769854133,
      "grad_norm": 1.4677164554595947,
      "learning_rate": 2.3552967888420372e-05,
      "loss": 2.6085,
      "step": 816700
    },
    {
      "epoch": 264.7649918962723,
      "grad_norm": 1.4545214176177979,
      "learning_rate": 2.3549724294518328e-05,
      "loss": 2.5846,
      "step": 816800
    },
    {
      "epoch": 264.79740680713127,
      "grad_norm": 1.2040190696716309,
      "learning_rate": 2.3546480700616283e-05,
      "loss": 2.581,
      "step": 816900
    },
    {
      "epoch": 264.8298217179903,
      "grad_norm": 1.2552684545516968,
      "learning_rate": 2.3543237106714242e-05,
      "loss": 2.6036,
      "step": 817000
    },
    {
      "epoch": 264.86223662884925,
      "grad_norm": 1.5014443397521973,
      "learning_rate": 2.3539993512812197e-05,
      "loss": 2.6304,
      "step": 817100
    },
    {
      "epoch": 264.89465153970826,
      "grad_norm": 1.3884057998657227,
      "learning_rate": 2.3536749918910152e-05,
      "loss": 2.599,
      "step": 817200
    },
    {
      "epoch": 264.9270664505673,
      "grad_norm": 1.1690372228622437,
      "learning_rate": 2.3533506325008108e-05,
      "loss": 2.5969,
      "step": 817300
    },
    {
      "epoch": 264.95948136142624,
      "grad_norm": 1.2225574254989624,
      "learning_rate": 2.3530262731106067e-05,
      "loss": 2.6116,
      "step": 817400
    },
    {
      "epoch": 264.99189627228526,
      "grad_norm": 1.3344467878341675,
      "learning_rate": 2.3527019137204022e-05,
      "loss": 2.6276,
      "step": 817500
    },
    {
      "epoch": 265.0,
      "eval_bleu": 1.1874891318202336,
      "eval_loss": 4.111593723297119,
      "eval_runtime": 4.5925,
      "eval_samples_per_second": 107.132,
      "eval_steps_per_second": 1.742,
      "step": 817525
    },
    {
      "epoch": 265.0243111831442,
      "grad_norm": 1.1593416929244995,
      "learning_rate": 2.352377554330198e-05,
      "loss": 2.5985,
      "step": 817600
    },
    {
      "epoch": 265.05672609400324,
      "grad_norm": 1.3587498664855957,
      "learning_rate": 2.3520531949399936e-05,
      "loss": 2.5914,
      "step": 817700
    },
    {
      "epoch": 265.08914100486226,
      "grad_norm": 1.4809622764587402,
      "learning_rate": 2.3517288355497895e-05,
      "loss": 2.617,
      "step": 817800
    },
    {
      "epoch": 265.1215559157212,
      "grad_norm": 1.2690374851226807,
      "learning_rate": 2.351404476159585e-05,
      "loss": 2.5936,
      "step": 817900
    },
    {
      "epoch": 265.15397082658023,
      "grad_norm": 1.471758484840393,
      "learning_rate": 2.3510801167693805e-05,
      "loss": 2.6166,
      "step": 818000
    },
    {
      "epoch": 265.1863857374392,
      "grad_norm": 1.2740051746368408,
      "learning_rate": 2.3507557573791764e-05,
      "loss": 2.6142,
      "step": 818100
    },
    {
      "epoch": 265.2188006482982,
      "grad_norm": 1.1615208387374878,
      "learning_rate": 2.350431397988972e-05,
      "loss": 2.5977,
      "step": 818200
    },
    {
      "epoch": 265.25121555915723,
      "grad_norm": 1.1518027782440186,
      "learning_rate": 2.3501070385987675e-05,
      "loss": 2.6015,
      "step": 818300
    },
    {
      "epoch": 265.2836304700162,
      "grad_norm": 1.423624038696289,
      "learning_rate": 2.349782679208563e-05,
      "loss": 2.6106,
      "step": 818400
    },
    {
      "epoch": 265.3160453808752,
      "grad_norm": 1.2155606746673584,
      "learning_rate": 2.349458319818359e-05,
      "loss": 2.6141,
      "step": 818500
    },
    {
      "epoch": 265.34846029173417,
      "grad_norm": 1.300885796546936,
      "learning_rate": 2.3491339604281544e-05,
      "loss": 2.5942,
      "step": 818600
    },
    {
      "epoch": 265.3808752025932,
      "grad_norm": 1.2487030029296875,
      "learning_rate": 2.34880960103795e-05,
      "loss": 2.5945,
      "step": 818700
    },
    {
      "epoch": 265.4132901134522,
      "grad_norm": 1.271066427230835,
      "learning_rate": 2.348485241647746e-05,
      "loss": 2.591,
      "step": 818800
    },
    {
      "epoch": 265.44570502431117,
      "grad_norm": 1.2306638956069946,
      "learning_rate": 2.3481608822575414e-05,
      "loss": 2.6021,
      "step": 818900
    },
    {
      "epoch": 265.4781199351702,
      "grad_norm": 1.2547024488449097,
      "learning_rate": 2.3478365228673373e-05,
      "loss": 2.5981,
      "step": 819000
    },
    {
      "epoch": 265.51053484602915,
      "grad_norm": 1.3475576639175415,
      "learning_rate": 2.3475121634771328e-05,
      "loss": 2.6063,
      "step": 819100
    },
    {
      "epoch": 265.54294975688816,
      "grad_norm": 1.4163155555725098,
      "learning_rate": 2.3471878040869287e-05,
      "loss": 2.6075,
      "step": 819200
    },
    {
      "epoch": 265.5753646677472,
      "grad_norm": 1.2191321849822998,
      "learning_rate": 2.3468634446967242e-05,
      "loss": 2.6003,
      "step": 819300
    },
    {
      "epoch": 265.60777957860614,
      "grad_norm": 1.3603402376174927,
      "learning_rate": 2.3465390853065197e-05,
      "loss": 2.6077,
      "step": 819400
    },
    {
      "epoch": 265.64019448946516,
      "grad_norm": 1.4523895978927612,
      "learning_rate": 2.3462147259163153e-05,
      "loss": 2.6237,
      "step": 819500
    },
    {
      "epoch": 265.6726094003242,
      "grad_norm": 1.231985092163086,
      "learning_rate": 2.345890366526111e-05,
      "loss": 2.5986,
      "step": 819600
    },
    {
      "epoch": 265.70502431118314,
      "grad_norm": 1.1060422658920288,
      "learning_rate": 2.3455660071359067e-05,
      "loss": 2.6123,
      "step": 819700
    },
    {
      "epoch": 265.73743922204216,
      "grad_norm": 1.470159649848938,
      "learning_rate": 2.3452416477457022e-05,
      "loss": 2.5935,
      "step": 819800
    },
    {
      "epoch": 265.7698541329011,
      "grad_norm": 1.400734782218933,
      "learning_rate": 2.3449172883554978e-05,
      "loss": 2.6233,
      "step": 819900
    },
    {
      "epoch": 265.80226904376013,
      "grad_norm": 1.2949626445770264,
      "learning_rate": 2.3445929289652936e-05,
      "loss": 2.5997,
      "step": 820000
    },
    {
      "epoch": 265.83468395461915,
      "grad_norm": 1.3990534543991089,
      "learning_rate": 2.344268569575089e-05,
      "loss": 2.607,
      "step": 820100
    },
    {
      "epoch": 265.8670988654781,
      "grad_norm": 1.3025423288345337,
      "learning_rate": 2.343944210184885e-05,
      "loss": 2.581,
      "step": 820200
    },
    {
      "epoch": 265.89951377633713,
      "grad_norm": 1.161133885383606,
      "learning_rate": 2.3436198507946806e-05,
      "loss": 2.6063,
      "step": 820300
    },
    {
      "epoch": 265.9319286871961,
      "grad_norm": 1.1709063053131104,
      "learning_rate": 2.3432954914044764e-05,
      "loss": 2.5943,
      "step": 820400
    },
    {
      "epoch": 265.9643435980551,
      "grad_norm": 1.2970774173736572,
      "learning_rate": 2.342974375608174e-05,
      "loss": 2.606,
      "step": 820500
    },
    {
      "epoch": 265.9967585089141,
      "grad_norm": 1.3499407768249512,
      "learning_rate": 2.3426500162179694e-05,
      "loss": 2.6247,
      "step": 820600
    },
    {
      "epoch": 266.0,
      "eval_bleu": 1.1230564424257639,
      "eval_loss": 4.111647605895996,
      "eval_runtime": 4.2348,
      "eval_samples_per_second": 116.18,
      "eval_steps_per_second": 1.889,
      "step": 820610
    },
    {
      "epoch": 266.0291734197731,
      "grad_norm": 1.3368505239486694,
      "learning_rate": 2.3423256568277653e-05,
      "loss": 2.6248,
      "step": 820700
    },
    {
      "epoch": 266.0615883306321,
      "grad_norm": 1.078231692314148,
      "learning_rate": 2.3420012974375612e-05,
      "loss": 2.587,
      "step": 820800
    },
    {
      "epoch": 266.09400324149107,
      "grad_norm": 1.2088836431503296,
      "learning_rate": 2.3416769380473567e-05,
      "loss": 2.5938,
      "step": 820900
    },
    {
      "epoch": 266.1264181523501,
      "grad_norm": 1.39315664768219,
      "learning_rate": 2.3413525786571522e-05,
      "loss": 2.6023,
      "step": 821000
    },
    {
      "epoch": 266.1588330632091,
      "grad_norm": 1.569291114807129,
      "learning_rate": 2.3410282192669478e-05,
      "loss": 2.596,
      "step": 821100
    },
    {
      "epoch": 266.19124797406806,
      "grad_norm": 1.3263565301895142,
      "learning_rate": 2.3407038598767437e-05,
      "loss": 2.6057,
      "step": 821200
    },
    {
      "epoch": 266.2236628849271,
      "grad_norm": 1.3930846452713013,
      "learning_rate": 2.3403795004865392e-05,
      "loss": 2.6188,
      "step": 821300
    },
    {
      "epoch": 266.25607779578604,
      "grad_norm": 1.1997902393341064,
      "learning_rate": 2.3400551410963347e-05,
      "loss": 2.6055,
      "step": 821400
    },
    {
      "epoch": 266.28849270664506,
      "grad_norm": 1.2946878671646118,
      "learning_rate": 2.3397307817061306e-05,
      "loss": 2.5984,
      "step": 821500
    },
    {
      "epoch": 266.3209076175041,
      "grad_norm": 1.328730583190918,
      "learning_rate": 2.339406422315926e-05,
      "loss": 2.5993,
      "step": 821600
    },
    {
      "epoch": 266.35332252836304,
      "grad_norm": 1.2678858041763306,
      "learning_rate": 2.3390820629257217e-05,
      "loss": 2.6004,
      "step": 821700
    },
    {
      "epoch": 266.38573743922205,
      "grad_norm": 1.2042860984802246,
      "learning_rate": 2.3387577035355172e-05,
      "loss": 2.5996,
      "step": 821800
    },
    {
      "epoch": 266.418152350081,
      "grad_norm": 1.4373611211776733,
      "learning_rate": 2.338433344145313e-05,
      "loss": 2.6116,
      "step": 821900
    },
    {
      "epoch": 266.45056726094003,
      "grad_norm": 1.2815642356872559,
      "learning_rate": 2.338108984755109e-05,
      "loss": 2.6045,
      "step": 822000
    },
    {
      "epoch": 266.48298217179905,
      "grad_norm": 1.2887557744979858,
      "learning_rate": 2.3377846253649045e-05,
      "loss": 2.5954,
      "step": 822100
    },
    {
      "epoch": 266.515397082658,
      "grad_norm": 1.2787485122680664,
      "learning_rate": 2.3374602659747e-05,
      "loss": 2.5975,
      "step": 822200
    },
    {
      "epoch": 266.54781199351703,
      "grad_norm": 1.4100778102874756,
      "learning_rate": 2.337135906584496e-05,
      "loss": 2.5939,
      "step": 822300
    },
    {
      "epoch": 266.580226904376,
      "grad_norm": 1.2089056968688965,
      "learning_rate": 2.3368115471942914e-05,
      "loss": 2.6021,
      "step": 822400
    },
    {
      "epoch": 266.612641815235,
      "grad_norm": 1.2002851963043213,
      "learning_rate": 2.3364904313979892e-05,
      "loss": 2.6026,
      "step": 822500
    },
    {
      "epoch": 266.645056726094,
      "grad_norm": 1.3142576217651367,
      "learning_rate": 2.3361660720077848e-05,
      "loss": 2.5998,
      "step": 822600
    },
    {
      "epoch": 266.677471636953,
      "grad_norm": 1.1326082944869995,
      "learning_rate": 2.3358417126175806e-05,
      "loss": 2.5962,
      "step": 822700
    },
    {
      "epoch": 266.709886547812,
      "grad_norm": 1.1864408254623413,
      "learning_rate": 2.335517353227376e-05,
      "loss": 2.6093,
      "step": 822800
    },
    {
      "epoch": 266.74230145867097,
      "grad_norm": 1.4346551895141602,
      "learning_rate": 2.3351929938371717e-05,
      "loss": 2.5819,
      "step": 822900
    },
    {
      "epoch": 266.77471636953,
      "grad_norm": 1.6648139953613281,
      "learning_rate": 2.3348686344469672e-05,
      "loss": 2.6354,
      "step": 823000
    },
    {
      "epoch": 266.807131280389,
      "grad_norm": 1.2728681564331055,
      "learning_rate": 2.334544275056763e-05,
      "loss": 2.6193,
      "step": 823100
    },
    {
      "epoch": 266.83954619124796,
      "grad_norm": 1.28988516330719,
      "learning_rate": 2.3342199156665586e-05,
      "loss": 2.6082,
      "step": 823200
    },
    {
      "epoch": 266.871961102107,
      "grad_norm": 1.2722543478012085,
      "learning_rate": 2.3338955562763542e-05,
      "loss": 2.6031,
      "step": 823300
    },
    {
      "epoch": 266.90437601296594,
      "grad_norm": 1.5562032461166382,
      "learning_rate": 2.3335711968861497e-05,
      "loss": 2.592,
      "step": 823400
    },
    {
      "epoch": 266.93679092382496,
      "grad_norm": 1.460706353187561,
      "learning_rate": 2.3332468374959456e-05,
      "loss": 2.5987,
      "step": 823500
    },
    {
      "epoch": 266.969205834684,
      "grad_norm": 1.2594945430755615,
      "learning_rate": 2.332922478105741e-05,
      "loss": 2.6037,
      "step": 823600
    },
    {
      "epoch": 267.0,
      "eval_bleu": 0.8651671371081138,
      "eval_loss": 4.114676475524902,
      "eval_runtime": 4.3944,
      "eval_samples_per_second": 111.961,
      "eval_steps_per_second": 1.821,
      "step": 823695
    },
    {
      "epoch": 267.00162074554294,
      "grad_norm": 1.3332197666168213,
      "learning_rate": 2.332598118715537e-05,
      "loss": 2.5961,
      "step": 823700
    },
    {
      "epoch": 267.03403565640195,
      "grad_norm": 1.40233314037323,
      "learning_rate": 2.332273759325333e-05,
      "loss": 2.5947,
      "step": 823800
    },
    {
      "epoch": 267.0664505672609,
      "grad_norm": 1.442887306213379,
      "learning_rate": 2.3319493999351284e-05,
      "loss": 2.6058,
      "step": 823900
    },
    {
      "epoch": 267.09886547811993,
      "grad_norm": 1.2864654064178467,
      "learning_rate": 2.331625040544924e-05,
      "loss": 2.5818,
      "step": 824000
    },
    {
      "epoch": 267.13128038897895,
      "grad_norm": 1.2494633197784424,
      "learning_rate": 2.3313006811547195e-05,
      "loss": 2.6021,
      "step": 824100
    },
    {
      "epoch": 267.1636952998379,
      "grad_norm": 1.3967057466506958,
      "learning_rate": 2.3309763217645154e-05,
      "loss": 2.5939,
      "step": 824200
    },
    {
      "epoch": 267.19611021069693,
      "grad_norm": 1.2600244283676147,
      "learning_rate": 2.330651962374311e-05,
      "loss": 2.6025,
      "step": 824300
    },
    {
      "epoch": 267.2285251215559,
      "grad_norm": 1.2932239770889282,
      "learning_rate": 2.3303276029841064e-05,
      "loss": 2.621,
      "step": 824400
    },
    {
      "epoch": 267.2609400324149,
      "grad_norm": 1.3158693313598633,
      "learning_rate": 2.330003243593902e-05,
      "loss": 2.6052,
      "step": 824500
    },
    {
      "epoch": 267.2933549432739,
      "grad_norm": 1.2539445161819458,
      "learning_rate": 2.329678884203698e-05,
      "loss": 2.6156,
      "step": 824600
    },
    {
      "epoch": 267.3257698541329,
      "grad_norm": 1.4971694946289062,
      "learning_rate": 2.3293545248134934e-05,
      "loss": 2.5979,
      "step": 824700
    },
    {
      "epoch": 267.3581847649919,
      "grad_norm": 1.3671159744262695,
      "learning_rate": 2.329030165423289e-05,
      "loss": 2.6031,
      "step": 824800
    },
    {
      "epoch": 267.39059967585086,
      "grad_norm": 1.2939653396606445,
      "learning_rate": 2.3287058060330848e-05,
      "loss": 2.6147,
      "step": 824900
    },
    {
      "epoch": 267.4230145867099,
      "grad_norm": 1.2818523645401,
      "learning_rate": 2.3283814466428807e-05,
      "loss": 2.5825,
      "step": 825000
    },
    {
      "epoch": 267.4554294975689,
      "grad_norm": 1.2286063432693481,
      "learning_rate": 2.3280570872526762e-05,
      "loss": 2.5864,
      "step": 825100
    },
    {
      "epoch": 267.48784440842786,
      "grad_norm": 1.4730876684188843,
      "learning_rate": 2.3277359714563736e-05,
      "loss": 2.6051,
      "step": 825200
    },
    {
      "epoch": 267.5202593192869,
      "grad_norm": 1.1273807287216187,
      "learning_rate": 2.327411612066169e-05,
      "loss": 2.5896,
      "step": 825300
    },
    {
      "epoch": 267.55267423014584,
      "grad_norm": 1.3189977407455444,
      "learning_rate": 2.327087252675965e-05,
      "loss": 2.6069,
      "step": 825400
    },
    {
      "epoch": 267.58508914100486,
      "grad_norm": 1.428071141242981,
      "learning_rate": 2.326762893285761e-05,
      "loss": 2.6017,
      "step": 825500
    },
    {
      "epoch": 267.6175040518639,
      "grad_norm": 1.3071826696395874,
      "learning_rate": 2.3264385338955564e-05,
      "loss": 2.6148,
      "step": 825600
    },
    {
      "epoch": 267.64991896272284,
      "grad_norm": 1.272122859954834,
      "learning_rate": 2.326114174505352e-05,
      "loss": 2.5972,
      "step": 825700
    },
    {
      "epoch": 267.68233387358185,
      "grad_norm": 1.2290109395980835,
      "learning_rate": 2.325789815115148e-05,
      "loss": 2.5971,
      "step": 825800
    },
    {
      "epoch": 267.7147487844408,
      "grad_norm": 1.2367215156555176,
      "learning_rate": 2.3254654557249434e-05,
      "loss": 2.5904,
      "step": 825900
    },
    {
      "epoch": 267.74716369529983,
      "grad_norm": 1.2219946384429932,
      "learning_rate": 2.325141096334739e-05,
      "loss": 2.6155,
      "step": 826000
    },
    {
      "epoch": 267.77957860615885,
      "grad_norm": 1.2900453805923462,
      "learning_rate": 2.3248167369445348e-05,
      "loss": 2.6029,
      "step": 826100
    },
    {
      "epoch": 267.8119935170178,
      "grad_norm": 1.5350803136825562,
      "learning_rate": 2.3244923775543303e-05,
      "loss": 2.6206,
      "step": 826200
    },
    {
      "epoch": 267.84440842787683,
      "grad_norm": 1.191161036491394,
      "learning_rate": 2.324168018164126e-05,
      "loss": 2.6036,
      "step": 826300
    },
    {
      "epoch": 267.87682333873585,
      "grad_norm": 1.5238845348358154,
      "learning_rate": 2.3238436587739214e-05,
      "loss": 2.6049,
      "step": 826400
    },
    {
      "epoch": 267.9092382495948,
      "grad_norm": 1.3124650716781616,
      "learning_rate": 2.3235192993837173e-05,
      "loss": 2.5837,
      "step": 826500
    },
    {
      "epoch": 267.9416531604538,
      "grad_norm": 1.3074764013290405,
      "learning_rate": 2.3231949399935128e-05,
      "loss": 2.5989,
      "step": 826600
    },
    {
      "epoch": 267.9740680713128,
      "grad_norm": 1.2140493392944336,
      "learning_rate": 2.3228705806033087e-05,
      "loss": 2.6069,
      "step": 826700
    },
    {
      "epoch": 268.0,
      "eval_bleu": 1.0935836912441035,
      "eval_loss": 4.118286609649658,
      "eval_runtime": 4.0643,
      "eval_samples_per_second": 121.053,
      "eval_steps_per_second": 1.968,
      "step": 826780
    },
    {
      "epoch": 268.0064829821718,
      "grad_norm": 1.3243614435195923,
      "learning_rate": 2.3225462212131042e-05,
      "loss": 2.6155,
      "step": 826800
    },
    {
      "epoch": 268.0388978930308,
      "grad_norm": 1.5990492105484009,
      "learning_rate": 2.3222218618229e-05,
      "loss": 2.5884,
      "step": 826900
    },
    {
      "epoch": 268.0713128038898,
      "grad_norm": 1.4688152074813843,
      "learning_rate": 2.3218975024326956e-05,
      "loss": 2.5942,
      "step": 827000
    },
    {
      "epoch": 268.1037277147488,
      "grad_norm": 1.5099254846572876,
      "learning_rate": 2.3215731430424912e-05,
      "loss": 2.5888,
      "step": 827100
    },
    {
      "epoch": 268.13614262560776,
      "grad_norm": 1.5329722166061401,
      "learning_rate": 2.3212487836522867e-05,
      "loss": 2.606,
      "step": 827200
    },
    {
      "epoch": 268.1685575364668,
      "grad_norm": 1.3656944036483765,
      "learning_rate": 2.3209244242620826e-05,
      "loss": 2.6029,
      "step": 827300
    },
    {
      "epoch": 268.2009724473258,
      "grad_norm": 1.2294408082962036,
      "learning_rate": 2.320600064871878e-05,
      "loss": 2.5928,
      "step": 827400
    },
    {
      "epoch": 268.23338735818476,
      "grad_norm": 1.4709473848342896,
      "learning_rate": 2.3202757054816737e-05,
      "loss": 2.5877,
      "step": 827500
    },
    {
      "epoch": 268.2658022690438,
      "grad_norm": 1.1334134340286255,
      "learning_rate": 2.3199513460914695e-05,
      "loss": 2.6094,
      "step": 827600
    },
    {
      "epoch": 268.29821717990274,
      "grad_norm": 1.3009772300720215,
      "learning_rate": 2.319626986701265e-05,
      "loss": 2.6031,
      "step": 827700
    },
    {
      "epoch": 268.33063209076175,
      "grad_norm": 1.2326302528381348,
      "learning_rate": 2.3193026273110606e-05,
      "loss": 2.5922,
      "step": 827800
    },
    {
      "epoch": 268.36304700162077,
      "grad_norm": 1.4811099767684937,
      "learning_rate": 2.3189782679208565e-05,
      "loss": 2.5993,
      "step": 827900
    },
    {
      "epoch": 268.39546191247973,
      "grad_norm": 1.2459685802459717,
      "learning_rate": 2.318653908530652e-05,
      "loss": 2.6081,
      "step": 828000
    },
    {
      "epoch": 268.42787682333875,
      "grad_norm": 1.3998645544052124,
      "learning_rate": 2.318329549140448e-05,
      "loss": 2.5974,
      "step": 828100
    },
    {
      "epoch": 268.4602917341977,
      "grad_norm": 1.2588939666748047,
      "learning_rate": 2.3180051897502434e-05,
      "loss": 2.595,
      "step": 828200
    },
    {
      "epoch": 268.4927066450567,
      "grad_norm": 1.3386822938919067,
      "learning_rate": 2.317680830360039e-05,
      "loss": 2.6016,
      "step": 828300
    },
    {
      "epoch": 268.52512155591575,
      "grad_norm": 1.131736397743225,
      "learning_rate": 2.3173564709698348e-05,
      "loss": 2.585,
      "step": 828400
    },
    {
      "epoch": 268.5575364667747,
      "grad_norm": 1.2508571147918701,
      "learning_rate": 2.3170321115796304e-05,
      "loss": 2.5969,
      "step": 828500
    },
    {
      "epoch": 268.5899513776337,
      "grad_norm": 1.2525920867919922,
      "learning_rate": 2.316707752189426e-05,
      "loss": 2.5967,
      "step": 828600
    },
    {
      "epoch": 268.6223662884927,
      "grad_norm": 1.0898993015289307,
      "learning_rate": 2.3163833927992218e-05,
      "loss": 2.6089,
      "step": 828700
    },
    {
      "epoch": 268.6547811993517,
      "grad_norm": 1.5388904809951782,
      "learning_rate": 2.3160590334090173e-05,
      "loss": 2.5999,
      "step": 828800
    },
    {
      "epoch": 268.6871961102107,
      "grad_norm": 1.2397922277450562,
      "learning_rate": 2.315734674018813e-05,
      "loss": 2.6042,
      "step": 828900
    },
    {
      "epoch": 268.7196110210697,
      "grad_norm": 1.282080888748169,
      "learning_rate": 2.3154103146286084e-05,
      "loss": 2.5897,
      "step": 829000
    },
    {
      "epoch": 268.7520259319287,
      "grad_norm": 1.2225288152694702,
      "learning_rate": 2.3150859552384043e-05,
      "loss": 2.5991,
      "step": 829100
    },
    {
      "epoch": 268.78444084278766,
      "grad_norm": 1.464347004890442,
      "learning_rate": 2.314764839442102e-05,
      "loss": 2.5864,
      "step": 829200
    },
    {
      "epoch": 268.8168557536467,
      "grad_norm": 1.4782097339630127,
      "learning_rate": 2.3144404800518976e-05,
      "loss": 2.6099,
      "step": 829300
    },
    {
      "epoch": 268.8492706645057,
      "grad_norm": 1.3693981170654297,
      "learning_rate": 2.314116120661693e-05,
      "loss": 2.6291,
      "step": 829400
    },
    {
      "epoch": 268.88168557536466,
      "grad_norm": 1.1554616689682007,
      "learning_rate": 2.313791761271489e-05,
      "loss": 2.5927,
      "step": 829500
    },
    {
      "epoch": 268.9141004862237,
      "grad_norm": 1.4825159311294556,
      "learning_rate": 2.3134674018812845e-05,
      "loss": 2.5893,
      "step": 829600
    },
    {
      "epoch": 268.94651539708263,
      "grad_norm": 1.4098585844039917,
      "learning_rate": 2.31314304249108e-05,
      "loss": 2.6172,
      "step": 829700
    },
    {
      "epoch": 268.97893030794165,
      "grad_norm": 1.1881177425384521,
      "learning_rate": 2.312818683100876e-05,
      "loss": 2.6027,
      "step": 829800
    },
    {
      "epoch": 269.0,
      "eval_bleu": 1.155020978924197,
      "eval_loss": 4.1180739402771,
      "eval_runtime": 4.3623,
      "eval_samples_per_second": 112.784,
      "eval_steps_per_second": 1.834,
      "step": 829865
    },
    {
      "epoch": 269.01134521880067,
      "grad_norm": 1.5318517684936523,
      "learning_rate": 2.3124943237106718e-05,
      "loss": 2.6178,
      "step": 829900
    },
    {
      "epoch": 269.04376012965963,
      "grad_norm": 1.325461983680725,
      "learning_rate": 2.3121699643204673e-05,
      "loss": 2.5976,
      "step": 830000
    },
    {
      "epoch": 269.07617504051865,
      "grad_norm": 1.3721582889556885,
      "learning_rate": 2.311845604930263e-05,
      "loss": 2.6006,
      "step": 830100
    },
    {
      "epoch": 269.1085899513776,
      "grad_norm": 1.4628671407699585,
      "learning_rate": 2.3115212455400584e-05,
      "loss": 2.5956,
      "step": 830200
    },
    {
      "epoch": 269.1410048622366,
      "grad_norm": 1.2453258037567139,
      "learning_rate": 2.3111968861498543e-05,
      "loss": 2.5694,
      "step": 830300
    },
    {
      "epoch": 269.17341977309565,
      "grad_norm": 1.1527140140533447,
      "learning_rate": 2.3108725267596498e-05,
      "loss": 2.6101,
      "step": 830400
    },
    {
      "epoch": 269.2058346839546,
      "grad_norm": 1.4017146825790405,
      "learning_rate": 2.3105481673694454e-05,
      "loss": 2.576,
      "step": 830500
    },
    {
      "epoch": 269.2382495948136,
      "grad_norm": 1.3483973741531372,
      "learning_rate": 2.310223807979241e-05,
      "loss": 2.5839,
      "step": 830600
    },
    {
      "epoch": 269.2706645056726,
      "grad_norm": 1.1588878631591797,
      "learning_rate": 2.3098994485890368e-05,
      "loss": 2.5733,
      "step": 830700
    },
    {
      "epoch": 269.3030794165316,
      "grad_norm": 1.4470328092575073,
      "learning_rate": 2.3095750891988323e-05,
      "loss": 2.6048,
      "step": 830800
    },
    {
      "epoch": 269.3354943273906,
      "grad_norm": 1.2850840091705322,
      "learning_rate": 2.309250729808628e-05,
      "loss": 2.6216,
      "step": 830900
    },
    {
      "epoch": 269.3679092382496,
      "grad_norm": 1.4999762773513794,
      "learning_rate": 2.3089263704184237e-05,
      "loss": 2.6087,
      "step": 831000
    },
    {
      "epoch": 269.4003241491086,
      "grad_norm": 1.3140465021133423,
      "learning_rate": 2.3086020110282196e-05,
      "loss": 2.5825,
      "step": 831100
    },
    {
      "epoch": 269.43273905996756,
      "grad_norm": 1.147631287574768,
      "learning_rate": 2.308277651638015e-05,
      "loss": 2.5733,
      "step": 831200
    },
    {
      "epoch": 269.4651539708266,
      "grad_norm": 1.2900139093399048,
      "learning_rate": 2.3079565358417126e-05,
      "loss": 2.6002,
      "step": 831300
    },
    {
      "epoch": 269.4975688816856,
      "grad_norm": 1.1954375505447388,
      "learning_rate": 2.307632176451508e-05,
      "loss": 2.6034,
      "step": 831400
    },
    {
      "epoch": 269.52998379254456,
      "grad_norm": 1.2980386018753052,
      "learning_rate": 2.307307817061304e-05,
      "loss": 2.598,
      "step": 831500
    },
    {
      "epoch": 269.5623987034036,
      "grad_norm": 1.2632145881652832,
      "learning_rate": 2.3069834576711e-05,
      "loss": 2.592,
      "step": 831600
    },
    {
      "epoch": 269.59481361426253,
      "grad_norm": 1.299412488937378,
      "learning_rate": 2.3066590982808954e-05,
      "loss": 2.5978,
      "step": 831700
    },
    {
      "epoch": 269.62722852512155,
      "grad_norm": 1.2827023267745972,
      "learning_rate": 2.3063347388906913e-05,
      "loss": 2.5963,
      "step": 831800
    },
    {
      "epoch": 269.65964343598057,
      "grad_norm": 1.1819932460784912,
      "learning_rate": 2.3060103795004868e-05,
      "loss": 2.617,
      "step": 831900
    },
    {
      "epoch": 269.69205834683953,
      "grad_norm": 1.3734345436096191,
      "learning_rate": 2.3056860201102823e-05,
      "loss": 2.6051,
      "step": 832000
    },
    {
      "epoch": 269.72447325769855,
      "grad_norm": 1.440966248512268,
      "learning_rate": 2.305361660720078e-05,
      "loss": 2.5989,
      "step": 832100
    },
    {
      "epoch": 269.7568881685575,
      "grad_norm": 1.263988733291626,
      "learning_rate": 2.3050373013298737e-05,
      "loss": 2.6187,
      "step": 832200
    },
    {
      "epoch": 269.7893030794165,
      "grad_norm": 1.229680061340332,
      "learning_rate": 2.3047129419396693e-05,
      "loss": 2.6159,
      "step": 832300
    },
    {
      "epoch": 269.82171799027554,
      "grad_norm": 1.332912564277649,
      "learning_rate": 2.3043885825494648e-05,
      "loss": 2.6013,
      "step": 832400
    },
    {
      "epoch": 269.8541329011345,
      "grad_norm": 1.2517248392105103,
      "learning_rate": 2.3040642231592603e-05,
      "loss": 2.6267,
      "step": 832500
    },
    {
      "epoch": 269.8865478119935,
      "grad_norm": 1.3693816661834717,
      "learning_rate": 2.3037398637690562e-05,
      "loss": 2.5989,
      "step": 832600
    },
    {
      "epoch": 269.9189627228525,
      "grad_norm": 1.2442991733551025,
      "learning_rate": 2.3034155043788517e-05,
      "loss": 2.5981,
      "step": 832700
    },
    {
      "epoch": 269.9513776337115,
      "grad_norm": 1.2455463409423828,
      "learning_rate": 2.3030911449886476e-05,
      "loss": 2.5833,
      "step": 832800
    },
    {
      "epoch": 269.9837925445705,
      "grad_norm": 1.255264163017273,
      "learning_rate": 2.302770029192345e-05,
      "loss": 2.6177,
      "step": 832900
    },
    {
      "epoch": 270.0,
      "eval_bleu": 1.0631032090823718,
      "eval_loss": 4.122354507446289,
      "eval_runtime": 4.4414,
      "eval_samples_per_second": 110.777,
      "eval_steps_per_second": 1.801,
      "step": 832950
    },
    {
      "epoch": 270.0162074554295,
      "grad_norm": 1.4772069454193115,
      "learning_rate": 2.302445669802141e-05,
      "loss": 2.6083,
      "step": 833000
    },
    {
      "epoch": 270.0486223662885,
      "grad_norm": 1.5209875106811523,
      "learning_rate": 2.3021213104119365e-05,
      "loss": 2.5881,
      "step": 833100
    },
    {
      "epoch": 270.0810372771475,
      "grad_norm": 1.1493024826049805,
      "learning_rate": 2.301796951021732e-05,
      "loss": 2.5863,
      "step": 833200
    },
    {
      "epoch": 270.1134521880065,
      "grad_norm": 1.247031807899475,
      "learning_rate": 2.301472591631528e-05,
      "loss": 2.5911,
      "step": 833300
    },
    {
      "epoch": 270.1458670988655,
      "grad_norm": 1.223960518836975,
      "learning_rate": 2.3011482322413234e-05,
      "loss": 2.5669,
      "step": 833400
    },
    {
      "epoch": 270.17828200972446,
      "grad_norm": 1.182653784751892,
      "learning_rate": 2.3008238728511193e-05,
      "loss": 2.6244,
      "step": 833500
    },
    {
      "epoch": 270.2106969205835,
      "grad_norm": 1.3551262617111206,
      "learning_rate": 2.3004995134609148e-05,
      "loss": 2.5902,
      "step": 833600
    },
    {
      "epoch": 270.2431118314425,
      "grad_norm": 1.4770114421844482,
      "learning_rate": 2.3001751540707104e-05,
      "loss": 2.6062,
      "step": 833700
    },
    {
      "epoch": 270.27552674230145,
      "grad_norm": 1.357640266418457,
      "learning_rate": 2.2998507946805062e-05,
      "loss": 2.5814,
      "step": 833800
    },
    {
      "epoch": 270.30794165316047,
      "grad_norm": 1.4396800994873047,
      "learning_rate": 2.2995264352903018e-05,
      "loss": 2.6051,
      "step": 833900
    },
    {
      "epoch": 270.34035656401943,
      "grad_norm": 1.1539628505706787,
      "learning_rate": 2.2992020759000973e-05,
      "loss": 2.5967,
      "step": 834000
    },
    {
      "epoch": 270.37277147487845,
      "grad_norm": 1.3560543060302734,
      "learning_rate": 2.2988777165098932e-05,
      "loss": 2.598,
      "step": 834100
    },
    {
      "epoch": 270.40518638573747,
      "grad_norm": 1.234263300895691,
      "learning_rate": 2.2985533571196887e-05,
      "loss": 2.5905,
      "step": 834200
    },
    {
      "epoch": 270.4376012965964,
      "grad_norm": 1.3452194929122925,
      "learning_rate": 2.2982289977294843e-05,
      "loss": 2.5856,
      "step": 834300
    },
    {
      "epoch": 270.47001620745544,
      "grad_norm": 1.3988450765609741,
      "learning_rate": 2.2979046383392798e-05,
      "loss": 2.5975,
      "step": 834400
    },
    {
      "epoch": 270.5024311183144,
      "grad_norm": 1.3789156675338745,
      "learning_rate": 2.2975802789490757e-05,
      "loss": 2.5835,
      "step": 834500
    },
    {
      "epoch": 270.5348460291734,
      "grad_norm": 1.266327977180481,
      "learning_rate": 2.2972559195588715e-05,
      "loss": 2.6217,
      "step": 834600
    },
    {
      "epoch": 270.56726094003244,
      "grad_norm": 1.1791101694107056,
      "learning_rate": 2.296931560168667e-05,
      "loss": 2.6005,
      "step": 834700
    },
    {
      "epoch": 270.5996758508914,
      "grad_norm": 1.2895127534866333,
      "learning_rate": 2.2966072007784626e-05,
      "loss": 2.6026,
      "step": 834800
    },
    {
      "epoch": 270.6320907617504,
      "grad_norm": 1.3392871618270874,
      "learning_rate": 2.2962828413882585e-05,
      "loss": 2.5817,
      "step": 834900
    },
    {
      "epoch": 270.6645056726094,
      "grad_norm": 1.2486978769302368,
      "learning_rate": 2.295958481998054e-05,
      "loss": 2.5966,
      "step": 835000
    },
    {
      "epoch": 270.6969205834684,
      "grad_norm": 1.4475669860839844,
      "learning_rate": 2.2956341226078496e-05,
      "loss": 2.6069,
      "step": 835100
    },
    {
      "epoch": 270.7293354943274,
      "grad_norm": 1.1839699745178223,
      "learning_rate": 2.295309763217645e-05,
      "loss": 2.6064,
      "step": 835200
    },
    {
      "epoch": 270.7617504051864,
      "grad_norm": 1.1570688486099243,
      "learning_rate": 2.294985403827441e-05,
      "loss": 2.601,
      "step": 835300
    },
    {
      "epoch": 270.7941653160454,
      "grad_norm": 1.222084641456604,
      "learning_rate": 2.2946610444372365e-05,
      "loss": 2.6097,
      "step": 835400
    },
    {
      "epoch": 270.82658022690435,
      "grad_norm": 1.379515290260315,
      "learning_rate": 2.294336685047032e-05,
      "loss": 2.5755,
      "step": 835500
    },
    {
      "epoch": 270.8589951377634,
      "grad_norm": 1.3266196250915527,
      "learning_rate": 2.294012325656828e-05,
      "loss": 2.6158,
      "step": 835600
    },
    {
      "epoch": 270.8914100486224,
      "grad_norm": 1.4338563680648804,
      "learning_rate": 2.2936879662666234e-05,
      "loss": 2.6124,
      "step": 835700
    },
    {
      "epoch": 270.92382495948135,
      "grad_norm": 1.207186222076416,
      "learning_rate": 2.2933636068764193e-05,
      "loss": 2.6014,
      "step": 835800
    },
    {
      "epoch": 270.95623987034037,
      "grad_norm": 1.3649932146072388,
      "learning_rate": 2.293039247486215e-05,
      "loss": 2.6034,
      "step": 835900
    },
    {
      "epoch": 270.98865478119933,
      "grad_norm": 1.117998480796814,
      "learning_rate": 2.2927148880960107e-05,
      "loss": 2.6045,
      "step": 836000
    },
    {
      "epoch": 271.0,
      "eval_bleu": 1.0416710660668023,
      "eval_loss": 4.124814510345459,
      "eval_runtime": 4.3383,
      "eval_samples_per_second": 113.409,
      "eval_steps_per_second": 1.844,
      "step": 836035
    },
    {
      "epoch": 271.02106969205835,
      "grad_norm": 1.25040602684021,
      "learning_rate": 2.2923905287058063e-05,
      "loss": 2.5912,
      "step": 836100
    },
    {
      "epoch": 271.05348460291737,
      "grad_norm": 1.335578203201294,
      "learning_rate": 2.2920661693156018e-05,
      "loss": 2.5927,
      "step": 836200
    },
    {
      "epoch": 271.0858995137763,
      "grad_norm": 1.4083919525146484,
      "learning_rate": 2.2917418099253973e-05,
      "loss": 2.6013,
      "step": 836300
    },
    {
      "epoch": 271.11831442463534,
      "grad_norm": 1.443784475326538,
      "learning_rate": 2.291420694129095e-05,
      "loss": 2.5947,
      "step": 836400
    },
    {
      "epoch": 271.1507293354943,
      "grad_norm": 1.2635223865509033,
      "learning_rate": 2.291096334738891e-05,
      "loss": 2.576,
      "step": 836500
    },
    {
      "epoch": 271.1831442463533,
      "grad_norm": 1.1645026206970215,
      "learning_rate": 2.2907719753486865e-05,
      "loss": 2.6179,
      "step": 836600
    },
    {
      "epoch": 271.21555915721234,
      "grad_norm": 1.2995415925979614,
      "learning_rate": 2.290447615958482e-05,
      "loss": 2.6015,
      "step": 836700
    },
    {
      "epoch": 271.2479740680713,
      "grad_norm": 1.2246626615524292,
      "learning_rate": 2.290123256568278e-05,
      "loss": 2.589,
      "step": 836800
    },
    {
      "epoch": 271.2803889789303,
      "grad_norm": 1.494782567024231,
      "learning_rate": 2.2897988971780735e-05,
      "loss": 2.6016,
      "step": 836900
    },
    {
      "epoch": 271.3128038897893,
      "grad_norm": 1.1495084762573242,
      "learning_rate": 2.289474537787869e-05,
      "loss": 2.596,
      "step": 837000
    },
    {
      "epoch": 271.3452188006483,
      "grad_norm": 1.3576411008834839,
      "learning_rate": 2.2891501783976645e-05,
      "loss": 2.5906,
      "step": 837100
    },
    {
      "epoch": 271.3776337115073,
      "grad_norm": 1.3019312620162964,
      "learning_rate": 2.2888258190074604e-05,
      "loss": 2.5801,
      "step": 837200
    },
    {
      "epoch": 271.4100486223663,
      "grad_norm": 1.50620698928833,
      "learning_rate": 2.288501459617256e-05,
      "loss": 2.5925,
      "step": 837300
    },
    {
      "epoch": 271.4424635332253,
      "grad_norm": 1.1884759664535522,
      "learning_rate": 2.2881771002270515e-05,
      "loss": 2.5876,
      "step": 837400
    },
    {
      "epoch": 271.47487844408425,
      "grad_norm": 1.315977692604065,
      "learning_rate": 2.2878527408368474e-05,
      "loss": 2.5775,
      "step": 837500
    },
    {
      "epoch": 271.5072933549433,
      "grad_norm": 1.2395471334457397,
      "learning_rate": 2.287528381446643e-05,
      "loss": 2.5897,
      "step": 837600
    },
    {
      "epoch": 271.5397082658023,
      "grad_norm": 1.6295305490493774,
      "learning_rate": 2.2872040220564388e-05,
      "loss": 2.5943,
      "step": 837700
    },
    {
      "epoch": 271.57212317666125,
      "grad_norm": 1.398340106010437,
      "learning_rate": 2.2868796626662343e-05,
      "loss": 2.606,
      "step": 837800
    },
    {
      "epoch": 271.60453808752027,
      "grad_norm": 1.3247029781341553,
      "learning_rate": 2.2865553032760302e-05,
      "loss": 2.6029,
      "step": 837900
    },
    {
      "epoch": 271.63695299837923,
      "grad_norm": 1.2144689559936523,
      "learning_rate": 2.2862309438858257e-05,
      "loss": 2.593,
      "step": 838000
    },
    {
      "epoch": 271.66936790923825,
      "grad_norm": 1.2657546997070312,
      "learning_rate": 2.2859065844956213e-05,
      "loss": 2.5941,
      "step": 838100
    },
    {
      "epoch": 271.70178282009726,
      "grad_norm": 1.4017810821533203,
      "learning_rate": 2.2855822251054168e-05,
      "loss": 2.5933,
      "step": 838200
    },
    {
      "epoch": 271.7341977309562,
      "grad_norm": 1.3046190738677979,
      "learning_rate": 2.2852578657152127e-05,
      "loss": 2.6085,
      "step": 838300
    },
    {
      "epoch": 271.76661264181524,
      "grad_norm": 1.3259519338607788,
      "learning_rate": 2.2849335063250082e-05,
      "loss": 2.6057,
      "step": 838400
    },
    {
      "epoch": 271.7990275526742,
      "grad_norm": 1.2181966304779053,
      "learning_rate": 2.2846091469348037e-05,
      "loss": 2.5822,
      "step": 838500
    },
    {
      "epoch": 271.8314424635332,
      "grad_norm": 1.4008369445800781,
      "learning_rate": 2.2842847875445993e-05,
      "loss": 2.5929,
      "step": 838600
    },
    {
      "epoch": 271.86385737439224,
      "grad_norm": 1.2703323364257812,
      "learning_rate": 2.283960428154395e-05,
      "loss": 2.6031,
      "step": 838700
    },
    {
      "epoch": 271.8962722852512,
      "grad_norm": 1.4956443309783936,
      "learning_rate": 2.2836360687641907e-05,
      "loss": 2.6007,
      "step": 838800
    },
    {
      "epoch": 271.9286871961102,
      "grad_norm": 1.3685141801834106,
      "learning_rate": 2.2833117093739866e-05,
      "loss": 2.6145,
      "step": 838900
    },
    {
      "epoch": 271.9611021069692,
      "grad_norm": 1.3475278615951538,
      "learning_rate": 2.2829873499837824e-05,
      "loss": 2.6037,
      "step": 839000
    },
    {
      "epoch": 271.9935170178282,
      "grad_norm": 1.1092849969863892,
      "learning_rate": 2.282662990593578e-05,
      "loss": 2.6056,
      "step": 839100
    },
    {
      "epoch": 272.0,
      "eval_bleu": 1.2146587789411394,
      "eval_loss": 4.125784873962402,
      "eval_runtime": 4.6785,
      "eval_samples_per_second": 105.162,
      "eval_steps_per_second": 1.71,
      "step": 839120
    },
    {
      "epoch": 272.0259319286872,
      "grad_norm": 1.2507952451705933,
      "learning_rate": 2.2823418747972754e-05,
      "loss": 2.6013,
      "step": 839200
    },
    {
      "epoch": 272.0583468395462,
      "grad_norm": 1.3461276292800903,
      "learning_rate": 2.282017515407071e-05,
      "loss": 2.6009,
      "step": 839300
    },
    {
      "epoch": 272.0907617504052,
      "grad_norm": 1.5214823484420776,
      "learning_rate": 2.2816931560168668e-05,
      "loss": 2.5835,
      "step": 839400
    },
    {
      "epoch": 272.12317666126415,
      "grad_norm": 1.332512617111206,
      "learning_rate": 2.2813687966266627e-05,
      "loss": 2.5887,
      "step": 839500
    },
    {
      "epoch": 272.15559157212317,
      "grad_norm": 1.5809650421142578,
      "learning_rate": 2.2810444372364582e-05,
      "loss": 2.581,
      "step": 839600
    },
    {
      "epoch": 272.1880064829822,
      "grad_norm": 1.1957868337631226,
      "learning_rate": 2.2807200778462538e-05,
      "loss": 2.6197,
      "step": 839700
    },
    {
      "epoch": 272.22042139384115,
      "grad_norm": 1.2063614130020142,
      "learning_rate": 2.2803957184560496e-05,
      "loss": 2.5956,
      "step": 839800
    },
    {
      "epoch": 272.25283630470017,
      "grad_norm": 1.316016435623169,
      "learning_rate": 2.280071359065845e-05,
      "loss": 2.558,
      "step": 839900
    },
    {
      "epoch": 272.2852512155592,
      "grad_norm": 1.196346402168274,
      "learning_rate": 2.2797469996756407e-05,
      "loss": 2.6021,
      "step": 840000
    },
    {
      "epoch": 272.31766612641815,
      "grad_norm": 1.209276556968689,
      "learning_rate": 2.2794226402854362e-05,
      "loss": 2.6051,
      "step": 840100
    },
    {
      "epoch": 272.35008103727716,
      "grad_norm": 1.2673341035842896,
      "learning_rate": 2.279098280895232e-05,
      "loss": 2.5956,
      "step": 840200
    },
    {
      "epoch": 272.3824959481361,
      "grad_norm": 1.3779566287994385,
      "learning_rate": 2.2787739215050276e-05,
      "loss": 2.5647,
      "step": 840300
    },
    {
      "epoch": 272.41491085899514,
      "grad_norm": 1.293452262878418,
      "learning_rate": 2.2784495621148232e-05,
      "loss": 2.5665,
      "step": 840400
    },
    {
      "epoch": 272.44732576985416,
      "grad_norm": 1.2857192754745483,
      "learning_rate": 2.2781252027246187e-05,
      "loss": 2.5915,
      "step": 840500
    },
    {
      "epoch": 272.4797406807131,
      "grad_norm": 1.3567219972610474,
      "learning_rate": 2.2778008433344146e-05,
      "loss": 2.6011,
      "step": 840600
    },
    {
      "epoch": 272.51215559157214,
      "grad_norm": 1.4559473991394043,
      "learning_rate": 2.2774764839442105e-05,
      "loss": 2.5943,
      "step": 840700
    },
    {
      "epoch": 272.5445705024311,
      "grad_norm": 1.2997829914093018,
      "learning_rate": 2.277152124554006e-05,
      "loss": 2.6001,
      "step": 840800
    },
    {
      "epoch": 272.5769854132901,
      "grad_norm": 1.4114991426467896,
      "learning_rate": 2.2768277651638015e-05,
      "loss": 2.6028,
      "step": 840900
    },
    {
      "epoch": 272.60940032414914,
      "grad_norm": 1.2527984380722046,
      "learning_rate": 2.2765034057735974e-05,
      "loss": 2.5962,
      "step": 841000
    },
    {
      "epoch": 272.6418152350081,
      "grad_norm": 1.3378316164016724,
      "learning_rate": 2.276179046383393e-05,
      "loss": 2.5941,
      "step": 841100
    },
    {
      "epoch": 272.6742301458671,
      "grad_norm": 1.2325981855392456,
      "learning_rate": 2.2758546869931885e-05,
      "loss": 2.601,
      "step": 841200
    },
    {
      "epoch": 272.7066450567261,
      "grad_norm": 1.395010232925415,
      "learning_rate": 2.2755303276029844e-05,
      "loss": 2.6023,
      "step": 841300
    },
    {
      "epoch": 272.7390599675851,
      "grad_norm": 1.2156987190246582,
      "learning_rate": 2.27520596821278e-05,
      "loss": 2.6159,
      "step": 841400
    },
    {
      "epoch": 272.7714748784441,
      "grad_norm": 1.3726445436477661,
      "learning_rate": 2.2748816088225754e-05,
      "loss": 2.6004,
      "step": 841500
    },
    {
      "epoch": 272.80388978930307,
      "grad_norm": 1.2812573909759521,
      "learning_rate": 2.274557249432371e-05,
      "loss": 2.6107,
      "step": 841600
    },
    {
      "epoch": 272.8363047001621,
      "grad_norm": 1.2348747253417969,
      "learning_rate": 2.274232890042167e-05,
      "loss": 2.5973,
      "step": 841700
    },
    {
      "epoch": 272.86871961102105,
      "grad_norm": 1.25802743434906,
      "learning_rate": 2.2739085306519624e-05,
      "loss": 2.5862,
      "step": 841800
    },
    {
      "epoch": 272.90113452188007,
      "grad_norm": 1.2579253911972046,
      "learning_rate": 2.2735841712617582e-05,
      "loss": 2.5901,
      "step": 841900
    },
    {
      "epoch": 272.9335494327391,
      "grad_norm": 1.3969286680221558,
      "learning_rate": 2.2732598118715538e-05,
      "loss": 2.5879,
      "step": 842000
    },
    {
      "epoch": 272.96596434359805,
      "grad_norm": 1.216575264930725,
      "learning_rate": 2.2729354524813497e-05,
      "loss": 2.6192,
      "step": 842100
    },
    {
      "epoch": 272.99837925445706,
      "grad_norm": 1.2457355260849,
      "learning_rate": 2.2726110930911452e-05,
      "loss": 2.5928,
      "step": 842200
    },
    {
      "epoch": 273.0,
      "eval_bleu": 0.9551594839599382,
      "eval_loss": 4.122618198394775,
      "eval_runtime": 4.9145,
      "eval_samples_per_second": 100.111,
      "eval_steps_per_second": 1.628,
      "step": 842205
    },
    {
      "epoch": 273.030794165316,
      "grad_norm": 1.3362702131271362,
      "learning_rate": 2.2722867337009407e-05,
      "loss": 2.578,
      "step": 842300
    },
    {
      "epoch": 273.06320907617504,
      "grad_norm": 1.2320138216018677,
      "learning_rate": 2.2719623743107363e-05,
      "loss": 2.5868,
      "step": 842400
    },
    {
      "epoch": 273.09562398703406,
      "grad_norm": 1.3646713495254517,
      "learning_rate": 2.271638014920532e-05,
      "loss": 2.5868,
      "step": 842500
    },
    {
      "epoch": 273.128038897893,
      "grad_norm": 1.298018217086792,
      "learning_rate": 2.2713136555303277e-05,
      "loss": 2.58,
      "step": 842600
    },
    {
      "epoch": 273.16045380875204,
      "grad_norm": 1.3029106855392456,
      "learning_rate": 2.2709892961401232e-05,
      "loss": 2.577,
      "step": 842700
    },
    {
      "epoch": 273.192868719611,
      "grad_norm": 1.2930134534835815,
      "learning_rate": 2.270664936749919e-05,
      "loss": 2.5929,
      "step": 842800
    },
    {
      "epoch": 273.22528363047,
      "grad_norm": 1.2078059911727905,
      "learning_rate": 2.2703405773597146e-05,
      "loss": 2.5882,
      "step": 842900
    },
    {
      "epoch": 273.25769854132903,
      "grad_norm": 1.2545521259307861,
      "learning_rate": 2.27001621796951e-05,
      "loss": 2.6004,
      "step": 843000
    },
    {
      "epoch": 273.290113452188,
      "grad_norm": 1.4071074724197388,
      "learning_rate": 2.269691858579306e-05,
      "loss": 2.5848,
      "step": 843100
    },
    {
      "epoch": 273.322528363047,
      "grad_norm": 1.3176093101501465,
      "learning_rate": 2.2693707427830035e-05,
      "loss": 2.5965,
      "step": 843200
    },
    {
      "epoch": 273.354943273906,
      "grad_norm": 1.159692406654358,
      "learning_rate": 2.2690463833927993e-05,
      "loss": 2.5894,
      "step": 843300
    },
    {
      "epoch": 273.387358184765,
      "grad_norm": 1.2235150337219238,
      "learning_rate": 2.268722024002595e-05,
      "loss": 2.5778,
      "step": 843400
    },
    {
      "epoch": 273.419773095624,
      "grad_norm": 1.2035284042358398,
      "learning_rate": 2.2683976646123904e-05,
      "loss": 2.5829,
      "step": 843500
    },
    {
      "epoch": 273.45218800648297,
      "grad_norm": 1.3828456401824951,
      "learning_rate": 2.2680733052221863e-05,
      "loss": 2.5953,
      "step": 843600
    },
    {
      "epoch": 273.484602917342,
      "grad_norm": 1.405179738998413,
      "learning_rate": 2.267748945831982e-05,
      "loss": 2.6109,
      "step": 843700
    },
    {
      "epoch": 273.51701782820095,
      "grad_norm": 1.221177101135254,
      "learning_rate": 2.2674245864417777e-05,
      "loss": 2.5803,
      "step": 843800
    },
    {
      "epoch": 273.54943273905997,
      "grad_norm": 1.3078168630599976,
      "learning_rate": 2.2671002270515732e-05,
      "loss": 2.5996,
      "step": 843900
    },
    {
      "epoch": 273.581847649919,
      "grad_norm": 1.3427931070327759,
      "learning_rate": 2.266775867661369e-05,
      "loss": 2.6083,
      "step": 844000
    },
    {
      "epoch": 273.61426256077795,
      "grad_norm": 1.2445731163024902,
      "learning_rate": 2.2664515082711646e-05,
      "loss": 2.5939,
      "step": 844100
    },
    {
      "epoch": 273.64667747163696,
      "grad_norm": 1.175585150718689,
      "learning_rate": 2.2661271488809602e-05,
      "loss": 2.5988,
      "step": 844200
    },
    {
      "epoch": 273.6790923824959,
      "grad_norm": 1.1737793684005737,
      "learning_rate": 2.2658027894907557e-05,
      "loss": 2.5844,
      "step": 844300
    },
    {
      "epoch": 273.71150729335494,
      "grad_norm": 1.4392467737197876,
      "learning_rate": 2.2654784301005516e-05,
      "loss": 2.5912,
      "step": 844400
    },
    {
      "epoch": 273.74392220421396,
      "grad_norm": 1.3134257793426514,
      "learning_rate": 2.265154070710347e-05,
      "loss": 2.5981,
      "step": 844500
    },
    {
      "epoch": 273.7763371150729,
      "grad_norm": 1.3697139024734497,
      "learning_rate": 2.2648297113201427e-05,
      "loss": 2.6021,
      "step": 844600
    },
    {
      "epoch": 273.80875202593194,
      "grad_norm": 1.4111441373825073,
      "learning_rate": 2.2645053519299382e-05,
      "loss": 2.6064,
      "step": 844700
    },
    {
      "epoch": 273.8411669367909,
      "grad_norm": 1.3227548599243164,
      "learning_rate": 2.264180992539734e-05,
      "loss": 2.6116,
      "step": 844800
    },
    {
      "epoch": 273.8735818476499,
      "grad_norm": 1.4413467645645142,
      "learning_rate": 2.26385663314953e-05,
      "loss": 2.5903,
      "step": 844900
    },
    {
      "epoch": 273.90599675850893,
      "grad_norm": 1.200613021850586,
      "learning_rate": 2.2635322737593255e-05,
      "loss": 2.6035,
      "step": 845000
    },
    {
      "epoch": 273.9384116693679,
      "grad_norm": 1.4524788856506348,
      "learning_rate": 2.2632079143691214e-05,
      "loss": 2.5937,
      "step": 845100
    },
    {
      "epoch": 273.9708265802269,
      "grad_norm": 1.2747513055801392,
      "learning_rate": 2.2628867985728188e-05,
      "loss": 2.5889,
      "step": 845200
    },
    {
      "epoch": 274.0,
      "eval_bleu": 0.9853723932431943,
      "eval_loss": 4.122632026672363,
      "eval_runtime": 4.4924,
      "eval_samples_per_second": 109.517,
      "eval_steps_per_second": 1.781,
      "step": 845290
    },
    {
      "epoch": 274.0032414910859,
      "grad_norm": 1.3281471729278564,
      "learning_rate": 2.2625624391826143e-05,
      "loss": 2.6137,
      "step": 845300
    },
    {
      "epoch": 274.0356564019449,
      "grad_norm": 1.2828420400619507,
      "learning_rate": 2.26223807979241e-05,
      "loss": 2.5953,
      "step": 845400
    },
    {
      "epoch": 274.0680713128039,
      "grad_norm": 1.3234211206436157,
      "learning_rate": 2.2619137204022057e-05,
      "loss": 2.5985,
      "step": 845500
    },
    {
      "epoch": 274.10048622366287,
      "grad_norm": 1.3166271448135376,
      "learning_rate": 2.2615893610120016e-05,
      "loss": 2.5956,
      "step": 845600
    },
    {
      "epoch": 274.1329011345219,
      "grad_norm": 1.3624566793441772,
      "learning_rate": 2.261265001621797e-05,
      "loss": 2.6164,
      "step": 845700
    },
    {
      "epoch": 274.16531604538085,
      "grad_norm": 1.4833053350448608,
      "learning_rate": 2.2609406422315927e-05,
      "loss": 2.5927,
      "step": 845800
    },
    {
      "epoch": 274.19773095623987,
      "grad_norm": 1.1890345811843872,
      "learning_rate": 2.2606162828413886e-05,
      "loss": 2.5853,
      "step": 845900
    },
    {
      "epoch": 274.2301458670989,
      "grad_norm": 1.2422980070114136,
      "learning_rate": 2.260291923451184e-05,
      "loss": 2.597,
      "step": 846000
    },
    {
      "epoch": 274.26256077795784,
      "grad_norm": 1.1977018117904663,
      "learning_rate": 2.2599675640609796e-05,
      "loss": 2.5822,
      "step": 846100
    },
    {
      "epoch": 274.29497568881686,
      "grad_norm": 1.3714919090270996,
      "learning_rate": 2.259643204670775e-05,
      "loss": 2.6033,
      "step": 846200
    },
    {
      "epoch": 274.3273905996758,
      "grad_norm": 1.2328811883926392,
      "learning_rate": 2.259318845280571e-05,
      "loss": 2.5747,
      "step": 846300
    },
    {
      "epoch": 274.35980551053484,
      "grad_norm": 1.2516863346099854,
      "learning_rate": 2.2589944858903666e-05,
      "loss": 2.6081,
      "step": 846400
    },
    {
      "epoch": 274.39222042139386,
      "grad_norm": 1.5564706325531006,
      "learning_rate": 2.258670126500162e-05,
      "loss": 2.5917,
      "step": 846500
    },
    {
      "epoch": 274.4246353322528,
      "grad_norm": 1.4981942176818848,
      "learning_rate": 2.258345767109958e-05,
      "loss": 2.5934,
      "step": 846600
    },
    {
      "epoch": 274.45705024311184,
      "grad_norm": 1.293959140777588,
      "learning_rate": 2.2580214077197535e-05,
      "loss": 2.5974,
      "step": 846700
    },
    {
      "epoch": 274.48946515397085,
      "grad_norm": 1.2464783191680908,
      "learning_rate": 2.2576970483295494e-05,
      "loss": 2.5806,
      "step": 846800
    },
    {
      "epoch": 274.5218800648298,
      "grad_norm": 1.2672901153564453,
      "learning_rate": 2.257372688939345e-05,
      "loss": 2.59,
      "step": 846900
    },
    {
      "epoch": 274.55429497568883,
      "grad_norm": 1.216495394706726,
      "learning_rate": 2.2570483295491408e-05,
      "loss": 2.5911,
      "step": 847000
    },
    {
      "epoch": 274.5867098865478,
      "grad_norm": 1.569277286529541,
      "learning_rate": 2.2567239701589363e-05,
      "loss": 2.6122,
      "step": 847100
    },
    {
      "epoch": 274.6191247974068,
      "grad_norm": 1.4358818531036377,
      "learning_rate": 2.2564028543626338e-05,
      "loss": 2.5924,
      "step": 847200
    },
    {
      "epoch": 274.65153970826583,
      "grad_norm": 1.3818751573562622,
      "learning_rate": 2.2560784949724297e-05,
      "loss": 2.5772,
      "step": 847300
    },
    {
      "epoch": 274.6839546191248,
      "grad_norm": 1.2033755779266357,
      "learning_rate": 2.2557541355822252e-05,
      "loss": 2.5872,
      "step": 847400
    },
    {
      "epoch": 274.7163695299838,
      "grad_norm": 1.2133082151412964,
      "learning_rate": 2.255429776192021e-05,
      "loss": 2.5719,
      "step": 847500
    },
    {
      "epoch": 274.74878444084277,
      "grad_norm": 1.5282039642333984,
      "learning_rate": 2.2551054168018166e-05,
      "loss": 2.584,
      "step": 847600
    },
    {
      "epoch": 274.7811993517018,
      "grad_norm": 1.3624330759048462,
      "learning_rate": 2.254784301005514e-05,
      "loss": 2.5962,
      "step": 847700
    },
    {
      "epoch": 274.8136142625608,
      "grad_norm": 1.3181746006011963,
      "learning_rate": 2.25445994161531e-05,
      "loss": 2.6008,
      "step": 847800
    },
    {
      "epoch": 274.84602917341977,
      "grad_norm": 1.2082010507583618,
      "learning_rate": 2.2541355822251055e-05,
      "loss": 2.6023,
      "step": 847900
    },
    {
      "epoch": 274.8784440842788,
      "grad_norm": 1.4142770767211914,
      "learning_rate": 2.2538112228349013e-05,
      "loss": 2.5996,
      "step": 848000
    },
    {
      "epoch": 274.91085899513774,
      "grad_norm": 1.1454952955245972,
      "learning_rate": 2.253486863444697e-05,
      "loss": 2.5922,
      "step": 848100
    },
    {
      "epoch": 274.94327390599676,
      "grad_norm": 1.4202886819839478,
      "learning_rate": 2.2531625040544924e-05,
      "loss": 2.5847,
      "step": 848200
    },
    {
      "epoch": 274.9756888168558,
      "grad_norm": 1.3218666315078735,
      "learning_rate": 2.2528381446642883e-05,
      "loss": 2.6135,
      "step": 848300
    },
    {
      "epoch": 275.0,
      "eval_bleu": 1.0977971388191134,
      "eval_loss": 4.136385917663574,
      "eval_runtime": 4.1524,
      "eval_samples_per_second": 118.487,
      "eval_steps_per_second": 1.927,
      "step": 848375
    },
    {
      "epoch": 275.00810372771474,
      "grad_norm": 1.4287054538726807,
      "learning_rate": 2.2525137852740838e-05,
      "loss": 2.5906,
      "step": 848400
    },
    {
      "epoch": 275.04051863857376,
      "grad_norm": 1.2638155221939087,
      "learning_rate": 2.2521894258838793e-05,
      "loss": 2.6022,
      "step": 848500
    },
    {
      "epoch": 275.0729335494327,
      "grad_norm": 1.247098445892334,
      "learning_rate": 2.2518650664936752e-05,
      "loss": 2.5921,
      "step": 848600
    },
    {
      "epoch": 275.10534846029174,
      "grad_norm": 1.261224389076233,
      "learning_rate": 2.2515407071034708e-05,
      "loss": 2.6069,
      "step": 848700
    },
    {
      "epoch": 275.13776337115075,
      "grad_norm": 1.3578191995620728,
      "learning_rate": 2.2512163477132663e-05,
      "loss": 2.5989,
      "step": 848800
    },
    {
      "epoch": 275.1701782820097,
      "grad_norm": 1.182812213897705,
      "learning_rate": 2.2508919883230618e-05,
      "loss": 2.6002,
      "step": 848900
    },
    {
      "epoch": 275.20259319286873,
      "grad_norm": 1.3756424188613892,
      "learning_rate": 2.2505676289328577e-05,
      "loss": 2.5883,
      "step": 849000
    },
    {
      "epoch": 275.2350081037277,
      "grad_norm": 1.229099154472351,
      "learning_rate": 2.2502432695426536e-05,
      "loss": 2.5837,
      "step": 849100
    },
    {
      "epoch": 275.2674230145867,
      "grad_norm": 1.3175005912780762,
      "learning_rate": 2.249918910152449e-05,
      "loss": 2.599,
      "step": 849200
    },
    {
      "epoch": 275.29983792544573,
      "grad_norm": 1.4681024551391602,
      "learning_rate": 2.2495945507622446e-05,
      "loss": 2.5895,
      "step": 849300
    },
    {
      "epoch": 275.3322528363047,
      "grad_norm": 1.2314081192016602,
      "learning_rate": 2.2492701913720405e-05,
      "loss": 2.5666,
      "step": 849400
    },
    {
      "epoch": 275.3646677471637,
      "grad_norm": 1.30682373046875,
      "learning_rate": 2.248945831981836e-05,
      "loss": 2.5808,
      "step": 849500
    },
    {
      "epoch": 275.39708265802267,
      "grad_norm": 1.3533365726470947,
      "learning_rate": 2.2486214725916316e-05,
      "loss": 2.5929,
      "step": 849600
    },
    {
      "epoch": 275.4294975688817,
      "grad_norm": 1.381738543510437,
      "learning_rate": 2.248297113201427e-05,
      "loss": 2.5836,
      "step": 849700
    },
    {
      "epoch": 275.4619124797407,
      "grad_norm": 1.284278392791748,
      "learning_rate": 2.247972753811223e-05,
      "loss": 2.5913,
      "step": 849800
    },
    {
      "epoch": 275.49432739059966,
      "grad_norm": 1.3083986043930054,
      "learning_rate": 2.2476483944210185e-05,
      "loss": 2.5717,
      "step": 849900
    },
    {
      "epoch": 275.5267423014587,
      "grad_norm": 1.524172306060791,
      "learning_rate": 2.247324035030814e-05,
      "loss": 2.5989,
      "step": 850000
    },
    {
      "epoch": 275.55915721231764,
      "grad_norm": 1.1954879760742188,
      "learning_rate": 2.24699967564061e-05,
      "loss": 2.5806,
      "step": 850100
    },
    {
      "epoch": 275.59157212317666,
      "grad_norm": 1.4595955610275269,
      "learning_rate": 2.2466753162504055e-05,
      "loss": 2.6042,
      "step": 850200
    },
    {
      "epoch": 275.6239870340357,
      "grad_norm": 1.189064860343933,
      "learning_rate": 2.2463509568602014e-05,
      "loss": 2.5976,
      "step": 850300
    },
    {
      "epoch": 275.65640194489464,
      "grad_norm": 1.420180082321167,
      "learning_rate": 2.246026597469997e-05,
      "loss": 2.6082,
      "step": 850400
    },
    {
      "epoch": 275.68881685575366,
      "grad_norm": 1.1439363956451416,
      "learning_rate": 2.2457022380797928e-05,
      "loss": 2.5978,
      "step": 850500
    },
    {
      "epoch": 275.7212317666126,
      "grad_norm": 1.2672525644302368,
      "learning_rate": 2.2453778786895883e-05,
      "loss": 2.5861,
      "step": 850600
    },
    {
      "epoch": 275.75364667747164,
      "grad_norm": 1.3093818426132202,
      "learning_rate": 2.245053519299384e-05,
      "loss": 2.5999,
      "step": 850700
    },
    {
      "epoch": 275.78606158833065,
      "grad_norm": 1.3597218990325928,
      "learning_rate": 2.2447291599091794e-05,
      "loss": 2.5897,
      "step": 850800
    },
    {
      "epoch": 275.8184764991896,
      "grad_norm": 1.4474754333496094,
      "learning_rate": 2.2444048005189752e-05,
      "loss": 2.5949,
      "step": 850900
    },
    {
      "epoch": 275.85089141004863,
      "grad_norm": 1.300104022026062,
      "learning_rate": 2.2440804411287708e-05,
      "loss": 2.5781,
      "step": 851000
    },
    {
      "epoch": 275.8833063209076,
      "grad_norm": 1.1590744256973267,
      "learning_rate": 2.2437560817385663e-05,
      "loss": 2.5723,
      "step": 851100
    },
    {
      "epoch": 275.9157212317666,
      "grad_norm": 1.4254262447357178,
      "learning_rate": 2.243431722348362e-05,
      "loss": 2.6199,
      "step": 851200
    },
    {
      "epoch": 275.94813614262563,
      "grad_norm": 1.3517473936080933,
      "learning_rate": 2.2431073629581577e-05,
      "loss": 2.581,
      "step": 851300
    },
    {
      "epoch": 275.9805510534846,
      "grad_norm": 1.307847499847412,
      "learning_rate": 2.2427830035679533e-05,
      "loss": 2.5924,
      "step": 851400
    },
    {
      "epoch": 276.0,
      "eval_bleu": 1.0999372694366052,
      "eval_loss": 4.131634712219238,
      "eval_runtime": 4.4308,
      "eval_samples_per_second": 111.04,
      "eval_steps_per_second": 1.806,
      "step": 851460
    },
    {
      "epoch": 276.0129659643436,
      "grad_norm": 1.4162527322769165,
      "learning_rate": 2.242458644177749e-05,
      "loss": 2.595,
      "step": 851500
    },
    {
      "epoch": 276.04538087520257,
      "grad_norm": 1.3950176239013672,
      "learning_rate": 2.2421342847875447e-05,
      "loss": 2.5775,
      "step": 851600
    },
    {
      "epoch": 276.0777957860616,
      "grad_norm": 1.346532940864563,
      "learning_rate": 2.2418131689912425e-05,
      "loss": 2.592,
      "step": 851700
    },
    {
      "epoch": 276.1102106969206,
      "grad_norm": 1.2612864971160889,
      "learning_rate": 2.241488809601038e-05,
      "loss": 2.5855,
      "step": 851800
    },
    {
      "epoch": 276.14262560777956,
      "grad_norm": 1.2503451108932495,
      "learning_rate": 2.2411644502108335e-05,
      "loss": 2.5771,
      "step": 851900
    },
    {
      "epoch": 276.1750405186386,
      "grad_norm": 1.438658595085144,
      "learning_rate": 2.2408400908206294e-05,
      "loss": 2.5937,
      "step": 852000
    },
    {
      "epoch": 276.20745542949754,
      "grad_norm": 1.3061293363571167,
      "learning_rate": 2.240515731430425e-05,
      "loss": 2.5746,
      "step": 852100
    },
    {
      "epoch": 276.23987034035656,
      "grad_norm": 1.3549578189849854,
      "learning_rate": 2.2401913720402208e-05,
      "loss": 2.5783,
      "step": 852200
    },
    {
      "epoch": 276.2722852512156,
      "grad_norm": 1.3485758304595947,
      "learning_rate": 2.2398670126500163e-05,
      "loss": 2.5946,
      "step": 852300
    },
    {
      "epoch": 276.30470016207454,
      "grad_norm": 1.3432565927505493,
      "learning_rate": 2.2395426532598122e-05,
      "loss": 2.6029,
      "step": 852400
    },
    {
      "epoch": 276.33711507293356,
      "grad_norm": 1.2703224420547485,
      "learning_rate": 2.2392182938696078e-05,
      "loss": 2.6031,
      "step": 852500
    },
    {
      "epoch": 276.3695299837925,
      "grad_norm": 1.2305504083633423,
      "learning_rate": 2.2388939344794033e-05,
      "loss": 2.588,
      "step": 852600
    },
    {
      "epoch": 276.40194489465154,
      "grad_norm": 1.3945730924606323,
      "learning_rate": 2.2385695750891988e-05,
      "loss": 2.5987,
      "step": 852700
    },
    {
      "epoch": 276.43435980551055,
      "grad_norm": 1.347281813621521,
      "learning_rate": 2.2382452156989947e-05,
      "loss": 2.5882,
      "step": 852800
    },
    {
      "epoch": 276.4667747163695,
      "grad_norm": 1.5234273672103882,
      "learning_rate": 2.2379208563087902e-05,
      "loss": 2.5857,
      "step": 852900
    },
    {
      "epoch": 276.49918962722853,
      "grad_norm": 1.5483940839767456,
      "learning_rate": 2.2375964969185858e-05,
      "loss": 2.596,
      "step": 853000
    },
    {
      "epoch": 276.5316045380875,
      "grad_norm": 1.3090946674346924,
      "learning_rate": 2.2372721375283813e-05,
      "loss": 2.6072,
      "step": 853100
    },
    {
      "epoch": 276.5640194489465,
      "grad_norm": 1.370846152305603,
      "learning_rate": 2.2369477781381772e-05,
      "loss": 2.5667,
      "step": 853200
    },
    {
      "epoch": 276.5964343598055,
      "grad_norm": 1.4173394441604614,
      "learning_rate": 2.2366234187479727e-05,
      "loss": 2.5871,
      "step": 853300
    },
    {
      "epoch": 276.6288492706645,
      "grad_norm": 1.3300833702087402,
      "learning_rate": 2.2362990593577686e-05,
      "loss": 2.5759,
      "step": 853400
    },
    {
      "epoch": 276.6612641815235,
      "grad_norm": 1.2292860746383667,
      "learning_rate": 2.235974699967564e-05,
      "loss": 2.5688,
      "step": 853500
    },
    {
      "epoch": 276.6936790923825,
      "grad_norm": 1.4990787506103516,
      "learning_rate": 2.23565034057736e-05,
      "loss": 2.5887,
      "step": 853600
    },
    {
      "epoch": 276.7260940032415,
      "grad_norm": 1.314137578010559,
      "learning_rate": 2.2353259811871555e-05,
      "loss": 2.6078,
      "step": 853700
    },
    {
      "epoch": 276.7585089141005,
      "grad_norm": 1.566699504852295,
      "learning_rate": 2.235004865390853e-05,
      "loss": 2.5928,
      "step": 853800
    },
    {
      "epoch": 276.79092382495946,
      "grad_norm": 1.3165905475616455,
      "learning_rate": 2.234680506000649e-05,
      "loss": 2.6016,
      "step": 853900
    },
    {
      "epoch": 276.8233387358185,
      "grad_norm": 1.3434698581695557,
      "learning_rate": 2.2343561466104447e-05,
      "loss": 2.5863,
      "step": 854000
    },
    {
      "epoch": 276.8557536466775,
      "grad_norm": 1.6712100505828857,
      "learning_rate": 2.2340317872202403e-05,
      "loss": 2.5889,
      "step": 854100
    },
    {
      "epoch": 276.88816855753646,
      "grad_norm": 1.4078245162963867,
      "learning_rate": 2.2337074278300358e-05,
      "loss": 2.5926,
      "step": 854200
    },
    {
      "epoch": 276.9205834683955,
      "grad_norm": 1.3379839658737183,
      "learning_rate": 2.2333830684398313e-05,
      "loss": 2.594,
      "step": 854300
    },
    {
      "epoch": 276.95299837925444,
      "grad_norm": 1.4201743602752686,
      "learning_rate": 2.2330587090496272e-05,
      "loss": 2.5944,
      "step": 854400
    },
    {
      "epoch": 276.98541329011346,
      "grad_norm": 1.2866992950439453,
      "learning_rate": 2.2327343496594227e-05,
      "loss": 2.6028,
      "step": 854500
    },
    {
      "epoch": 277.0,
      "eval_bleu": 1.0874760496436362,
      "eval_loss": 4.127565860748291,
      "eval_runtime": 4.4166,
      "eval_samples_per_second": 111.397,
      "eval_steps_per_second": 1.811,
      "step": 854545
    },
    {
      "epoch": 277.0178282009725,
      "grad_norm": 1.1917003393173218,
      "learning_rate": 2.2324099902692183e-05,
      "loss": 2.5924,
      "step": 854600
    },
    {
      "epoch": 277.05024311183143,
      "grad_norm": 1.2172125577926636,
      "learning_rate": 2.232085630879014e-05,
      "loss": 2.5644,
      "step": 854700
    },
    {
      "epoch": 277.08265802269045,
      "grad_norm": 1.329457402229309,
      "learning_rate": 2.2317612714888097e-05,
      "loss": 2.5835,
      "step": 854800
    },
    {
      "epoch": 277.1150729335494,
      "grad_norm": 1.0906376838684082,
      "learning_rate": 2.2314369120986052e-05,
      "loss": 2.5868,
      "step": 854900
    },
    {
      "epoch": 277.14748784440843,
      "grad_norm": 1.3227297067642212,
      "learning_rate": 2.2311125527084008e-05,
      "loss": 2.5794,
      "step": 855000
    },
    {
      "epoch": 277.17990275526745,
      "grad_norm": 1.3158565759658813,
      "learning_rate": 2.2307881933181966e-05,
      "loss": 2.5804,
      "step": 855100
    },
    {
      "epoch": 277.2123176661264,
      "grad_norm": 1.4775222539901733,
      "learning_rate": 2.2304638339279925e-05,
      "loss": 2.5967,
      "step": 855200
    },
    {
      "epoch": 277.2447325769854,
      "grad_norm": 1.1419916152954102,
      "learning_rate": 2.230139474537788e-05,
      "loss": 2.578,
      "step": 855300
    },
    {
      "epoch": 277.2771474878444,
      "grad_norm": 1.2229480743408203,
      "learning_rate": 2.2298151151475836e-05,
      "loss": 2.5881,
      "step": 855400
    },
    {
      "epoch": 277.3095623987034,
      "grad_norm": 1.3753728866577148,
      "learning_rate": 2.2294907557573794e-05,
      "loss": 2.5781,
      "step": 855500
    },
    {
      "epoch": 277.3419773095624,
      "grad_norm": 1.1327054500579834,
      "learning_rate": 2.229166396367175e-05,
      "loss": 2.5898,
      "step": 855600
    },
    {
      "epoch": 277.3743922204214,
      "grad_norm": 1.1922353506088257,
      "learning_rate": 2.2288420369769705e-05,
      "loss": 2.6051,
      "step": 855700
    },
    {
      "epoch": 277.4068071312804,
      "grad_norm": 1.3842597007751465,
      "learning_rate": 2.2285209211806683e-05,
      "loss": 2.5812,
      "step": 855800
    },
    {
      "epoch": 277.43922204213936,
      "grad_norm": 1.4147510528564453,
      "learning_rate": 2.2281965617904642e-05,
      "loss": 2.5826,
      "step": 855900
    },
    {
      "epoch": 277.4716369529984,
      "grad_norm": 1.3593021631240845,
      "learning_rate": 2.2278754459941616e-05,
      "loss": 2.5761,
      "step": 856000
    },
    {
      "epoch": 277.5040518638574,
      "grad_norm": 1.4760079383850098,
      "learning_rate": 2.227551086603957e-05,
      "loss": 2.5873,
      "step": 856100
    },
    {
      "epoch": 277.53646677471636,
      "grad_norm": 1.2795346975326538,
      "learning_rate": 2.227226727213753e-05,
      "loss": 2.6065,
      "step": 856200
    },
    {
      "epoch": 277.5688816855754,
      "grad_norm": 1.476355791091919,
      "learning_rate": 2.2269023678235486e-05,
      "loss": 2.5813,
      "step": 856300
    },
    {
      "epoch": 277.60129659643434,
      "grad_norm": 1.5299321413040161,
      "learning_rate": 2.2265780084333444e-05,
      "loss": 2.6236,
      "step": 856400
    },
    {
      "epoch": 277.63371150729336,
      "grad_norm": 1.466848373413086,
      "learning_rate": 2.22625364904314e-05,
      "loss": 2.5924,
      "step": 856500
    },
    {
      "epoch": 277.6661264181524,
      "grad_norm": 1.1964712142944336,
      "learning_rate": 2.2259292896529355e-05,
      "loss": 2.5869,
      "step": 856600
    },
    {
      "epoch": 277.69854132901133,
      "grad_norm": 1.4649358987808228,
      "learning_rate": 2.2256049302627314e-05,
      "loss": 2.5938,
      "step": 856700
    },
    {
      "epoch": 277.73095623987035,
      "grad_norm": 1.3519916534423828,
      "learning_rate": 2.225280570872527e-05,
      "loss": 2.5931,
      "step": 856800
    },
    {
      "epoch": 277.7633711507293,
      "grad_norm": 1.3807849884033203,
      "learning_rate": 2.2249562114823225e-05,
      "loss": 2.604,
      "step": 856900
    },
    {
      "epoch": 277.79578606158833,
      "grad_norm": 1.251172423362732,
      "learning_rate": 2.224631852092118e-05,
      "loss": 2.5869,
      "step": 857000
    },
    {
      "epoch": 277.82820097244735,
      "grad_norm": 1.433031439781189,
      "learning_rate": 2.224307492701914e-05,
      "loss": 2.5796,
      "step": 857100
    },
    {
      "epoch": 277.8606158833063,
      "grad_norm": 1.4107404947280884,
      "learning_rate": 2.2239831333117094e-05,
      "loss": 2.5783,
      "step": 857200
    },
    {
      "epoch": 277.8930307941653,
      "grad_norm": 1.2277820110321045,
      "learning_rate": 2.223658773921505e-05,
      "loss": 2.5967,
      "step": 857300
    },
    {
      "epoch": 277.9254457050243,
      "grad_norm": 1.6908705234527588,
      "learning_rate": 2.2233344145313008e-05,
      "loss": 2.6025,
      "step": 857400
    },
    {
      "epoch": 277.9578606158833,
      "grad_norm": 1.4119386672973633,
      "learning_rate": 2.2230100551410963e-05,
      "loss": 2.588,
      "step": 857500
    },
    {
      "epoch": 277.9902755267423,
      "grad_norm": 1.3255521059036255,
      "learning_rate": 2.2226856957508922e-05,
      "loss": 2.6028,
      "step": 857600
    },
    {
      "epoch": 278.0,
      "eval_bleu": 0.9830367508841827,
      "eval_loss": 4.130795478820801,
      "eval_runtime": 4.9243,
      "eval_samples_per_second": 99.914,
      "eval_steps_per_second": 1.625,
      "step": 857630
    },
    {
      "epoch": 278.0226904376013,
      "grad_norm": 1.3756803274154663,
      "learning_rate": 2.2223613363606878e-05,
      "loss": 2.5961,
      "step": 857700
    },
    {
      "epoch": 278.0551053484603,
      "grad_norm": 1.2947217226028442,
      "learning_rate": 2.2220369769704836e-05,
      "loss": 2.5805,
      "step": 857800
    },
    {
      "epoch": 278.08752025931926,
      "grad_norm": 1.3524945974349976,
      "learning_rate": 2.221712617580279e-05,
      "loss": 2.5847,
      "step": 857900
    },
    {
      "epoch": 278.1199351701783,
      "grad_norm": 1.2906992435455322,
      "learning_rate": 2.2213882581900747e-05,
      "loss": 2.5932,
      "step": 858000
    },
    {
      "epoch": 278.1523500810373,
      "grad_norm": 1.2665560245513916,
      "learning_rate": 2.2210638987998702e-05,
      "loss": 2.5828,
      "step": 858100
    },
    {
      "epoch": 278.18476499189626,
      "grad_norm": 1.4815261363983154,
      "learning_rate": 2.220739539409666e-05,
      "loss": 2.5834,
      "step": 858200
    },
    {
      "epoch": 278.2171799027553,
      "grad_norm": 1.2253046035766602,
      "learning_rate": 2.2204151800194616e-05,
      "loss": 2.5697,
      "step": 858300
    },
    {
      "epoch": 278.24959481361424,
      "grad_norm": 1.2115765810012817,
      "learning_rate": 2.2200908206292572e-05,
      "loss": 2.5995,
      "step": 858400
    },
    {
      "epoch": 278.28200972447326,
      "grad_norm": 1.1766773462295532,
      "learning_rate": 2.2197664612390527e-05,
      "loss": 2.5732,
      "step": 858500
    },
    {
      "epoch": 278.3144246353323,
      "grad_norm": 1.2637099027633667,
      "learning_rate": 2.2194421018488486e-05,
      "loss": 2.5917,
      "step": 858600
    },
    {
      "epoch": 278.34683954619123,
      "grad_norm": 1.2146220207214355,
      "learning_rate": 2.219117742458644e-05,
      "loss": 2.5731,
      "step": 858700
    },
    {
      "epoch": 278.37925445705025,
      "grad_norm": 1.776519775390625,
      "learning_rate": 2.21879338306844e-05,
      "loss": 2.5638,
      "step": 858800
    },
    {
      "epoch": 278.4116693679092,
      "grad_norm": 1.1571476459503174,
      "learning_rate": 2.218469023678236e-05,
      "loss": 2.5797,
      "step": 858900
    },
    {
      "epoch": 278.44408427876823,
      "grad_norm": 1.2567251920700073,
      "learning_rate": 2.2181446642880314e-05,
      "loss": 2.6132,
      "step": 859000
    },
    {
      "epoch": 278.47649918962725,
      "grad_norm": 1.122155785560608,
      "learning_rate": 2.217820304897827e-05,
      "loss": 2.594,
      "step": 859100
    },
    {
      "epoch": 278.5089141004862,
      "grad_norm": 1.1874382495880127,
      "learning_rate": 2.2174959455076225e-05,
      "loss": 2.603,
      "step": 859200
    },
    {
      "epoch": 278.5413290113452,
      "grad_norm": 1.4464384317398071,
      "learning_rate": 2.2171715861174184e-05,
      "loss": 2.604,
      "step": 859300
    },
    {
      "epoch": 278.5737439222042,
      "grad_norm": 1.4868533611297607,
      "learning_rate": 2.216847226727214e-05,
      "loss": 2.583,
      "step": 859400
    },
    {
      "epoch": 278.6061588330632,
      "grad_norm": 1.3491222858428955,
      "learning_rate": 2.2165228673370094e-05,
      "loss": 2.5697,
      "step": 859500
    },
    {
      "epoch": 278.6385737439222,
      "grad_norm": 1.1650235652923584,
      "learning_rate": 2.216198507946805e-05,
      "loss": 2.5939,
      "step": 859600
    },
    {
      "epoch": 278.6709886547812,
      "grad_norm": 1.3052693605422974,
      "learning_rate": 2.215874148556601e-05,
      "loss": 2.5691,
      "step": 859700
    },
    {
      "epoch": 278.7034035656402,
      "grad_norm": 1.464507818222046,
      "learning_rate": 2.2155530327602986e-05,
      "loss": 2.6059,
      "step": 859800
    },
    {
      "epoch": 278.73581847649916,
      "grad_norm": 1.2548511028289795,
      "learning_rate": 2.215228673370094e-05,
      "loss": 2.6031,
      "step": 859900
    },
    {
      "epoch": 278.7682333873582,
      "grad_norm": 1.3576886653900146,
      "learning_rate": 2.2149043139798897e-05,
      "loss": 2.5887,
      "step": 860000
    },
    {
      "epoch": 278.8006482982172,
      "grad_norm": 1.5052971839904785,
      "learning_rate": 2.2145799545896856e-05,
      "loss": 2.582,
      "step": 860100
    },
    {
      "epoch": 278.83306320907616,
      "grad_norm": 1.4031567573547363,
      "learning_rate": 2.214255595199481e-05,
      "loss": 2.5919,
      "step": 860200
    },
    {
      "epoch": 278.8654781199352,
      "grad_norm": 1.337101936340332,
      "learning_rate": 2.2139312358092766e-05,
      "loss": 2.5762,
      "step": 860300
    },
    {
      "epoch": 278.8978930307942,
      "grad_norm": 1.563003420829773,
      "learning_rate": 2.213606876419072e-05,
      "loss": 2.584,
      "step": 860400
    },
    {
      "epoch": 278.93030794165315,
      "grad_norm": 1.3214927911758423,
      "learning_rate": 2.213282517028868e-05,
      "loss": 2.6024,
      "step": 860500
    },
    {
      "epoch": 278.9627228525122,
      "grad_norm": 1.20833158493042,
      "learning_rate": 2.212958157638664e-05,
      "loss": 2.6004,
      "step": 860600
    },
    {
      "epoch": 278.99513776337113,
      "grad_norm": 1.5876257419586182,
      "learning_rate": 2.2126337982484594e-05,
      "loss": 2.5833,
      "step": 860700
    },
    {
      "epoch": 279.0,
      "eval_bleu": 1.105778211299347,
      "eval_loss": 4.133793830871582,
      "eval_runtime": 4.2875,
      "eval_samples_per_second": 114.753,
      "eval_steps_per_second": 1.866,
      "step": 860715
    },
    {
      "epoch": 279.02755267423015,
      "grad_norm": 1.3516240119934082,
      "learning_rate": 2.212309438858255e-05,
      "loss": 2.5828,
      "step": 860800
    },
    {
      "epoch": 279.05996758508917,
      "grad_norm": 1.35114586353302,
      "learning_rate": 2.211985079468051e-05,
      "loss": 2.5773,
      "step": 860900
    },
    {
      "epoch": 279.09238249594813,
      "grad_norm": 1.4742344617843628,
      "learning_rate": 2.2116607200778464e-05,
      "loss": 2.5784,
      "step": 861000
    },
    {
      "epoch": 279.12479740680715,
      "grad_norm": 1.1936362981796265,
      "learning_rate": 2.211336360687642e-05,
      "loss": 2.5878,
      "step": 861100
    },
    {
      "epoch": 279.1572123176661,
      "grad_norm": 1.464651107788086,
      "learning_rate": 2.2110120012974378e-05,
      "loss": 2.5817,
      "step": 861200
    },
    {
      "epoch": 279.1896272285251,
      "grad_norm": 1.1928750276565552,
      "learning_rate": 2.2106876419072333e-05,
      "loss": 2.5804,
      "step": 861300
    },
    {
      "epoch": 279.22204213938414,
      "grad_norm": 1.3604098558425903,
      "learning_rate": 2.210363282517029e-05,
      "loss": 2.5913,
      "step": 861400
    },
    {
      "epoch": 279.2544570502431,
      "grad_norm": 1.4573941230773926,
      "learning_rate": 2.2100389231268244e-05,
      "loss": 2.5966,
      "step": 861500
    },
    {
      "epoch": 279.2868719611021,
      "grad_norm": 1.3647212982177734,
      "learning_rate": 2.2097145637366203e-05,
      "loss": 2.5859,
      "step": 861600
    },
    {
      "epoch": 279.3192868719611,
      "grad_norm": 1.6893017292022705,
      "learning_rate": 2.2093902043464158e-05,
      "loss": 2.5869,
      "step": 861700
    },
    {
      "epoch": 279.3517017828201,
      "grad_norm": 1.3790724277496338,
      "learning_rate": 2.2090658449562117e-05,
      "loss": 2.5403,
      "step": 861800
    },
    {
      "epoch": 279.3841166936791,
      "grad_norm": 1.4695854187011719,
      "learning_rate": 2.2087414855660072e-05,
      "loss": 2.5851,
      "step": 861900
    },
    {
      "epoch": 279.4165316045381,
      "grad_norm": 1.1272053718566895,
      "learning_rate": 2.208417126175803e-05,
      "loss": 2.579,
      "step": 862000
    },
    {
      "epoch": 279.4489465153971,
      "grad_norm": 1.185774803161621,
      "learning_rate": 2.2080927667855986e-05,
      "loss": 2.5777,
      "step": 862100
    },
    {
      "epoch": 279.48136142625606,
      "grad_norm": 1.2166495323181152,
      "learning_rate": 2.2077684073953942e-05,
      "loss": 2.5747,
      "step": 862200
    },
    {
      "epoch": 279.5137763371151,
      "grad_norm": 1.49820876121521,
      "learning_rate": 2.2074440480051897e-05,
      "loss": 2.5822,
      "step": 862300
    },
    {
      "epoch": 279.5461912479741,
      "grad_norm": 1.0820648670196533,
      "learning_rate": 2.2071196886149856e-05,
      "loss": 2.569,
      "step": 862400
    },
    {
      "epoch": 279.57860615883305,
      "grad_norm": 1.256061315536499,
      "learning_rate": 2.206795329224781e-05,
      "loss": 2.5846,
      "step": 862500
    },
    {
      "epoch": 279.6110210696921,
      "grad_norm": 1.3510884046554565,
      "learning_rate": 2.2064709698345767e-05,
      "loss": 2.5849,
      "step": 862600
    },
    {
      "epoch": 279.64343598055103,
      "grad_norm": 1.2365288734436035,
      "learning_rate": 2.2061466104443725e-05,
      "loss": 2.5942,
      "step": 862700
    },
    {
      "epoch": 279.67585089141005,
      "grad_norm": 1.4336316585540771,
      "learning_rate": 2.205822251054168e-05,
      "loss": 2.6156,
      "step": 862800
    },
    {
      "epoch": 279.70826580226907,
      "grad_norm": 1.5005983114242554,
      "learning_rate": 2.2054978916639636e-05,
      "loss": 2.603,
      "step": 862900
    },
    {
      "epoch": 279.74068071312803,
      "grad_norm": 1.3882379531860352,
      "learning_rate": 2.2051735322737595e-05,
      "loss": 2.5692,
      "step": 863000
    },
    {
      "epoch": 279.77309562398705,
      "grad_norm": 1.4131247997283936,
      "learning_rate": 2.2048491728835553e-05,
      "loss": 2.5877,
      "step": 863100
    },
    {
      "epoch": 279.805510534846,
      "grad_norm": 1.3812015056610107,
      "learning_rate": 2.204524813493351e-05,
      "loss": 2.6098,
      "step": 863200
    },
    {
      "epoch": 279.837925445705,
      "grad_norm": 1.4285247325897217,
      "learning_rate": 2.2042004541031464e-05,
      "loss": 2.5915,
      "step": 863300
    },
    {
      "epoch": 279.87034035656404,
      "grad_norm": 1.5406231880187988,
      "learning_rate": 2.203876094712942e-05,
      "loss": 2.5977,
      "step": 863400
    },
    {
      "epoch": 279.902755267423,
      "grad_norm": 1.3112297058105469,
      "learning_rate": 2.2035517353227378e-05,
      "loss": 2.587,
      "step": 863500
    },
    {
      "epoch": 279.935170178282,
      "grad_norm": 1.3220645189285278,
      "learning_rate": 2.2032306195264356e-05,
      "loss": 2.6235,
      "step": 863600
    },
    {
      "epoch": 279.967585089141,
      "grad_norm": 1.247904896736145,
      "learning_rate": 2.202906260136231e-05,
      "loss": 2.5909,
      "step": 863700
    },
    {
      "epoch": 280.0,
      "grad_norm": 1.1446847915649414,
      "learning_rate": 2.2025819007460267e-05,
      "loss": 2.5911,
      "step": 863800
    },
    {
      "epoch": 280.0,
      "eval_bleu": 0.9716489099883847,
      "eval_loss": 4.131707191467285,
      "eval_runtime": 4.3783,
      "eval_samples_per_second": 112.372,
      "eval_steps_per_second": 1.827,
      "step": 863800
    },
    {
      "epoch": 280.032414910859,
      "grad_norm": 1.249341607093811,
      "learning_rate": 2.2022575413558226e-05,
      "loss": 2.5644,
      "step": 863900
    },
    {
      "epoch": 280.064829821718,
      "grad_norm": 1.4526633024215698,
      "learning_rate": 2.201933181965618e-05,
      "loss": 2.5791,
      "step": 864000
    },
    {
      "epoch": 280.097244732577,
      "grad_norm": 1.1954888105392456,
      "learning_rate": 2.2016088225754136e-05,
      "loss": 2.5765,
      "step": 864100
    },
    {
      "epoch": 280.12965964343596,
      "grad_norm": 1.3494267463684082,
      "learning_rate": 2.201284463185209e-05,
      "loss": 2.5841,
      "step": 864200
    },
    {
      "epoch": 280.162074554295,
      "grad_norm": 1.3233927488327026,
      "learning_rate": 2.200960103795005e-05,
      "loss": 2.5935,
      "step": 864300
    },
    {
      "epoch": 280.194489465154,
      "grad_norm": 1.4498302936553955,
      "learning_rate": 2.2006357444048006e-05,
      "loss": 2.5951,
      "step": 864400
    },
    {
      "epoch": 280.22690437601295,
      "grad_norm": 1.267964482307434,
      "learning_rate": 2.200311385014596e-05,
      "loss": 2.5901,
      "step": 864500
    },
    {
      "epoch": 280.25931928687197,
      "grad_norm": 1.300525188446045,
      "learning_rate": 2.1999870256243916e-05,
      "loss": 2.5805,
      "step": 864600
    },
    {
      "epoch": 280.29173419773093,
      "grad_norm": 1.260805606842041,
      "learning_rate": 2.1996626662341875e-05,
      "loss": 2.6056,
      "step": 864700
    },
    {
      "epoch": 280.32414910858995,
      "grad_norm": 1.4390010833740234,
      "learning_rate": 2.1993383068439834e-05,
      "loss": 2.6014,
      "step": 864800
    },
    {
      "epoch": 280.35656401944897,
      "grad_norm": 1.3399028778076172,
      "learning_rate": 2.199013947453779e-05,
      "loss": 2.5775,
      "step": 864900
    },
    {
      "epoch": 280.3889789303079,
      "grad_norm": 1.2032537460327148,
      "learning_rate": 2.1986895880635748e-05,
      "loss": 2.5817,
      "step": 865000
    },
    {
      "epoch": 280.42139384116695,
      "grad_norm": 1.3527203798294067,
      "learning_rate": 2.1983652286733703e-05,
      "loss": 2.5785,
      "step": 865100
    },
    {
      "epoch": 280.4538087520259,
      "grad_norm": 1.4520312547683716,
      "learning_rate": 2.198040869283166e-05,
      "loss": 2.5835,
      "step": 865200
    },
    {
      "epoch": 280.4862236628849,
      "grad_norm": 1.2015358209609985,
      "learning_rate": 2.1977165098929614e-05,
      "loss": 2.5986,
      "step": 865300
    },
    {
      "epoch": 280.51863857374394,
      "grad_norm": 1.370384693145752,
      "learning_rate": 2.1973921505027573e-05,
      "loss": 2.5753,
      "step": 865400
    },
    {
      "epoch": 280.5510534846029,
      "grad_norm": 1.3218144178390503,
      "learning_rate": 2.1970677911125528e-05,
      "loss": 2.5893,
      "step": 865500
    },
    {
      "epoch": 280.5834683954619,
      "grad_norm": 1.2061142921447754,
      "learning_rate": 2.1967434317223484e-05,
      "loss": 2.5799,
      "step": 865600
    },
    {
      "epoch": 280.6158833063209,
      "grad_norm": 1.2539477348327637,
      "learning_rate": 2.196422315926046e-05,
      "loss": 2.5704,
      "step": 865700
    },
    {
      "epoch": 280.6482982171799,
      "grad_norm": 1.4591624736785889,
      "learning_rate": 2.196097956535842e-05,
      "loss": 2.5777,
      "step": 865800
    },
    {
      "epoch": 280.6807131280389,
      "grad_norm": 1.2982807159423828,
      "learning_rate": 2.1957768407395394e-05,
      "loss": 2.6218,
      "step": 865900
    },
    {
      "epoch": 280.7131280388979,
      "grad_norm": 1.2569791078567505,
      "learning_rate": 2.1954524813493353e-05,
      "loss": 2.5593,
      "step": 866000
    },
    {
      "epoch": 280.7455429497569,
      "grad_norm": 1.3831840753555298,
      "learning_rate": 2.195128121959131e-05,
      "loss": 2.5954,
      "step": 866100
    },
    {
      "epoch": 280.77795786061586,
      "grad_norm": 1.4332685470581055,
      "learning_rate": 2.1948037625689267e-05,
      "loss": 2.5778,
      "step": 866200
    },
    {
      "epoch": 280.8103727714749,
      "grad_norm": 1.5416293144226074,
      "learning_rate": 2.1944794031787223e-05,
      "loss": 2.5926,
      "step": 866300
    },
    {
      "epoch": 280.8427876823339,
      "grad_norm": 1.234803557395935,
      "learning_rate": 2.1941550437885178e-05,
      "loss": 2.5864,
      "step": 866400
    },
    {
      "epoch": 280.87520259319285,
      "grad_norm": 1.4767699241638184,
      "learning_rate": 2.1938306843983133e-05,
      "loss": 2.581,
      "step": 866500
    },
    {
      "epoch": 280.90761750405187,
      "grad_norm": 1.3568381071090698,
      "learning_rate": 2.1935063250081092e-05,
      "loss": 2.5923,
      "step": 866600
    },
    {
      "epoch": 280.94003241491083,
      "grad_norm": 1.2252342700958252,
      "learning_rate": 2.1931819656179047e-05,
      "loss": 2.6045,
      "step": 866700
    },
    {
      "epoch": 280.97244732576985,
      "grad_norm": 1.268441915512085,
      "learning_rate": 2.1928576062277003e-05,
      "loss": 2.5739,
      "step": 866800
    },
    {
      "epoch": 281.0,
      "eval_bleu": 1.1857723111695668,
      "eval_loss": 4.131796360015869,
      "eval_runtime": 4.353,
      "eval_samples_per_second": 113.024,
      "eval_steps_per_second": 1.838,
      "step": 866885
    },
    {
      "epoch": 281.00486223662887,
      "grad_norm": 1.4020100831985474,
      "learning_rate": 2.1925332468374958e-05,
      "loss": 2.5702,
      "step": 866900
    },
    {
      "epoch": 281.0372771474878,
      "grad_norm": 1.2802040576934814,
      "learning_rate": 2.1922088874472917e-05,
      "loss": 2.5444,
      "step": 867000
    },
    {
      "epoch": 281.06969205834685,
      "grad_norm": 1.3410159349441528,
      "learning_rate": 2.1918845280570872e-05,
      "loss": 2.5831,
      "step": 867100
    },
    {
      "epoch": 281.1021069692058,
      "grad_norm": 1.461584210395813,
      "learning_rate": 2.191560168666883e-05,
      "loss": 2.5719,
      "step": 867200
    },
    {
      "epoch": 281.1345218800648,
      "grad_norm": 1.3435521125793457,
      "learning_rate": 2.1912358092766786e-05,
      "loss": 2.5783,
      "step": 867300
    },
    {
      "epoch": 281.16693679092384,
      "grad_norm": 1.3683485984802246,
      "learning_rate": 2.1909114498864745e-05,
      "loss": 2.5511,
      "step": 867400
    },
    {
      "epoch": 281.1993517017828,
      "grad_norm": 1.3270214796066284,
      "learning_rate": 2.19058709049627e-05,
      "loss": 2.5531,
      "step": 867500
    },
    {
      "epoch": 281.2317666126418,
      "grad_norm": 1.2304035425186157,
      "learning_rate": 2.1902627311060656e-05,
      "loss": 2.5716,
      "step": 867600
    },
    {
      "epoch": 281.26418152350084,
      "grad_norm": 1.2994025945663452,
      "learning_rate": 2.1899383717158615e-05,
      "loss": 2.5798,
      "step": 867700
    },
    {
      "epoch": 281.2965964343598,
      "grad_norm": 1.4182201623916626,
      "learning_rate": 2.189614012325657e-05,
      "loss": 2.5731,
      "step": 867800
    },
    {
      "epoch": 281.3290113452188,
      "grad_norm": 1.1389093399047852,
      "learning_rate": 2.1892896529354525e-05,
      "loss": 2.5941,
      "step": 867900
    },
    {
      "epoch": 281.3614262560778,
      "grad_norm": 1.7783277034759521,
      "learning_rate": 2.188965293545248e-05,
      "loss": 2.592,
      "step": 868000
    },
    {
      "epoch": 281.3938411669368,
      "grad_norm": 1.223557710647583,
      "learning_rate": 2.188640934155044e-05,
      "loss": 2.5922,
      "step": 868100
    },
    {
      "epoch": 281.4262560777958,
      "grad_norm": 1.297696828842163,
      "learning_rate": 2.1883165747648395e-05,
      "loss": 2.5917,
      "step": 868200
    },
    {
      "epoch": 281.4586709886548,
      "grad_norm": 1.245406150817871,
      "learning_rate": 2.187992215374635e-05,
      "loss": 2.5888,
      "step": 868300
    },
    {
      "epoch": 281.4910858995138,
      "grad_norm": 1.4807523488998413,
      "learning_rate": 2.187667855984431e-05,
      "loss": 2.5769,
      "step": 868400
    },
    {
      "epoch": 281.52350081037275,
      "grad_norm": 1.3468555212020874,
      "learning_rate": 2.1873434965942268e-05,
      "loss": 2.5842,
      "step": 868500
    },
    {
      "epoch": 281.55591572123177,
      "grad_norm": 1.2644833326339722,
      "learning_rate": 2.1870191372040223e-05,
      "loss": 2.5737,
      "step": 868600
    },
    {
      "epoch": 281.5883306320908,
      "grad_norm": 1.3702821731567383,
      "learning_rate": 2.1866947778138178e-05,
      "loss": 2.6017,
      "step": 868700
    },
    {
      "epoch": 281.62074554294975,
      "grad_norm": 1.303597331047058,
      "learning_rate": 2.1863704184236134e-05,
      "loss": 2.5872,
      "step": 868800
    },
    {
      "epoch": 281.65316045380877,
      "grad_norm": 1.3581372499465942,
      "learning_rate": 2.1860460590334092e-05,
      "loss": 2.5808,
      "step": 868900
    },
    {
      "epoch": 281.6855753646677,
      "grad_norm": 1.3648278713226318,
      "learning_rate": 2.1857216996432048e-05,
      "loss": 2.5849,
      "step": 869000
    },
    {
      "epoch": 281.71799027552674,
      "grad_norm": 1.0919047594070435,
      "learning_rate": 2.1853973402530003e-05,
      "loss": 2.5927,
      "step": 869100
    },
    {
      "epoch": 281.75040518638576,
      "grad_norm": 1.2949646711349487,
      "learning_rate": 2.1850729808627962e-05,
      "loss": 2.6102,
      "step": 869200
    },
    {
      "epoch": 281.7828200972447,
      "grad_norm": 1.51869535446167,
      "learning_rate": 2.1847486214725917e-05,
      "loss": 2.5947,
      "step": 869300
    },
    {
      "epoch": 281.81523500810374,
      "grad_norm": 1.4463436603546143,
      "learning_rate": 2.1844242620823873e-05,
      "loss": 2.5935,
      "step": 869400
    },
    {
      "epoch": 281.8476499189627,
      "grad_norm": 1.3084943294525146,
      "learning_rate": 2.1840999026921828e-05,
      "loss": 2.6124,
      "step": 869500
    },
    {
      "epoch": 281.8800648298217,
      "grad_norm": 1.3676451444625854,
      "learning_rate": 2.1837755433019787e-05,
      "loss": 2.5776,
      "step": 869600
    },
    {
      "epoch": 281.91247974068074,
      "grad_norm": 1.2036317586898804,
      "learning_rate": 2.1834511839117745e-05,
      "loss": 2.5846,
      "step": 869700
    },
    {
      "epoch": 281.9448946515397,
      "grad_norm": 1.4746228456497192,
      "learning_rate": 2.18312682452157e-05,
      "loss": 2.6024,
      "step": 869800
    },
    {
      "epoch": 281.9773095623987,
      "grad_norm": 1.4402929544448853,
      "learning_rate": 2.1828024651313656e-05,
      "loss": 2.5826,
      "step": 869900
    },
    {
      "epoch": 282.0,
      "eval_bleu": 1.134688863981087,
      "eval_loss": 4.133864879608154,
      "eval_runtime": 3.8277,
      "eval_samples_per_second": 128.536,
      "eval_steps_per_second": 2.09,
      "step": 869970
    },
    {
      "epoch": 282.0097244732577,
      "grad_norm": 1.1401302814483643,
      "learning_rate": 2.1824781057411615e-05,
      "loss": 2.5811,
      "step": 870000
    },
    {
      "epoch": 282.0421393841167,
      "grad_norm": 1.631258249282837,
      "learning_rate": 2.182153746350957e-05,
      "loss": 2.5863,
      "step": 870100
    },
    {
      "epoch": 282.0745542949757,
      "grad_norm": 1.3310610055923462,
      "learning_rate": 2.1818293869607526e-05,
      "loss": 2.5973,
      "step": 870200
    },
    {
      "epoch": 282.1069692058347,
      "grad_norm": 1.387220025062561,
      "learning_rate": 2.181505027570548e-05,
      "loss": 2.5804,
      "step": 870300
    },
    {
      "epoch": 282.1393841166937,
      "grad_norm": 1.3717763423919678,
      "learning_rate": 2.181180668180344e-05,
      "loss": 2.5905,
      "step": 870400
    },
    {
      "epoch": 282.17179902755265,
      "grad_norm": 1.2920621633529663,
      "learning_rate": 2.1808563087901395e-05,
      "loss": 2.5834,
      "step": 870500
    },
    {
      "epoch": 282.20421393841167,
      "grad_norm": 1.32410728931427,
      "learning_rate": 2.180531949399935e-05,
      "loss": 2.574,
      "step": 870600
    },
    {
      "epoch": 282.2366288492707,
      "grad_norm": 1.284447193145752,
      "learning_rate": 2.180207590009731e-05,
      "loss": 2.5611,
      "step": 870700
    },
    {
      "epoch": 282.26904376012965,
      "grad_norm": 1.186700701713562,
      "learning_rate": 2.1798832306195264e-05,
      "loss": 2.5772,
      "step": 870800
    },
    {
      "epoch": 282.30145867098867,
      "grad_norm": 1.5069341659545898,
      "learning_rate": 2.1795588712293223e-05,
      "loss": 2.6121,
      "step": 870900
    },
    {
      "epoch": 282.3338735818476,
      "grad_norm": 1.3091927766799927,
      "learning_rate": 2.179234511839118e-05,
      "loss": 2.5795,
      "step": 871000
    },
    {
      "epoch": 282.36628849270664,
      "grad_norm": 1.5948072671890259,
      "learning_rate": 2.1789101524489137e-05,
      "loss": 2.5716,
      "step": 871100
    },
    {
      "epoch": 282.39870340356566,
      "grad_norm": 1.2873706817626953,
      "learning_rate": 2.1785857930587093e-05,
      "loss": 2.5947,
      "step": 871200
    },
    {
      "epoch": 282.4311183144246,
      "grad_norm": 1.5044971704483032,
      "learning_rate": 2.1782614336685048e-05,
      "loss": 2.5868,
      "step": 871300
    },
    {
      "epoch": 282.46353322528364,
      "grad_norm": 1.3860251903533936,
      "learning_rate": 2.1779370742783003e-05,
      "loss": 2.5655,
      "step": 871400
    },
    {
      "epoch": 282.4959481361426,
      "grad_norm": 1.2773542404174805,
      "learning_rate": 2.1776127148880962e-05,
      "loss": 2.5792,
      "step": 871500
    },
    {
      "epoch": 282.5283630470016,
      "grad_norm": 1.7130591869354248,
      "learning_rate": 2.177291599091794e-05,
      "loss": 2.5989,
      "step": 871600
    },
    {
      "epoch": 282.56077795786064,
      "grad_norm": 1.2745615243911743,
      "learning_rate": 2.1769672397015895e-05,
      "loss": 2.5836,
      "step": 871700
    },
    {
      "epoch": 282.5931928687196,
      "grad_norm": 1.1677201986312866,
      "learning_rate": 2.176642880311385e-05,
      "loss": 2.5739,
      "step": 871800
    },
    {
      "epoch": 282.6256077795786,
      "grad_norm": 1.334543228149414,
      "learning_rate": 2.176318520921181e-05,
      "loss": 2.5929,
      "step": 871900
    },
    {
      "epoch": 282.6580226904376,
      "grad_norm": 1.2817543745040894,
      "learning_rate": 2.1759941615309765e-05,
      "loss": 2.5805,
      "step": 872000
    },
    {
      "epoch": 282.6904376012966,
      "grad_norm": 1.3259567022323608,
      "learning_rate": 2.175669802140772e-05,
      "loss": 2.5633,
      "step": 872100
    },
    {
      "epoch": 282.7228525121556,
      "grad_norm": 1.444298505783081,
      "learning_rate": 2.1753454427505675e-05,
      "loss": 2.589,
      "step": 872200
    },
    {
      "epoch": 282.7552674230146,
      "grad_norm": 1.4441111087799072,
      "learning_rate": 2.1750210833603634e-05,
      "loss": 2.5682,
      "step": 872300
    },
    {
      "epoch": 282.7876823338736,
      "grad_norm": 1.3402928113937378,
      "learning_rate": 2.174696723970159e-05,
      "loss": 2.5873,
      "step": 872400
    },
    {
      "epoch": 282.82009724473255,
      "grad_norm": 1.5301942825317383,
      "learning_rate": 2.1743723645799545e-05,
      "loss": 2.5829,
      "step": 872500
    },
    {
      "epoch": 282.85251215559157,
      "grad_norm": 1.4794046878814697,
      "learning_rate": 2.1740480051897504e-05,
      "loss": 2.579,
      "step": 872600
    },
    {
      "epoch": 282.8849270664506,
      "grad_norm": 1.390070915222168,
      "learning_rate": 2.1737236457995462e-05,
      "loss": 2.6001,
      "step": 872700
    },
    {
      "epoch": 282.91734197730955,
      "grad_norm": 1.4554439783096313,
      "learning_rate": 2.1733992864093418e-05,
      "loss": 2.5808,
      "step": 872800
    },
    {
      "epoch": 282.94975688816857,
      "grad_norm": 1.1894584894180298,
      "learning_rate": 2.1730749270191373e-05,
      "loss": 2.5891,
      "step": 872900
    },
    {
      "epoch": 282.9821717990275,
      "grad_norm": 1.4552044868469238,
      "learning_rate": 2.1727505676289332e-05,
      "loss": 2.5854,
      "step": 873000
    },
    {
      "epoch": 283.0,
      "eval_bleu": 0.9746517273509087,
      "eval_loss": 4.141571044921875,
      "eval_runtime": 4.0752,
      "eval_samples_per_second": 120.73,
      "eval_steps_per_second": 1.963,
      "step": 873055
    },
    {
      "epoch": 283.01458670988654,
      "grad_norm": 1.478192687034607,
      "learning_rate": 2.1724262082387287e-05,
      "loss": 2.5667,
      "step": 873100
    },
    {
      "epoch": 283.04700162074556,
      "grad_norm": 1.6196688413619995,
      "learning_rate": 2.1721018488485243e-05,
      "loss": 2.5743,
      "step": 873200
    },
    {
      "epoch": 283.0794165316045,
      "grad_norm": 1.4665976762771606,
      "learning_rate": 2.1717774894583198e-05,
      "loss": 2.5642,
      "step": 873300
    },
    {
      "epoch": 283.11183144246354,
      "grad_norm": 1.489894151687622,
      "learning_rate": 2.1714531300681157e-05,
      "loss": 2.5556,
      "step": 873400
    },
    {
      "epoch": 283.1442463533225,
      "grad_norm": 1.3072136640548706,
      "learning_rate": 2.1711287706779112e-05,
      "loss": 2.5839,
      "step": 873500
    },
    {
      "epoch": 283.1766612641815,
      "grad_norm": 1.1808619499206543,
      "learning_rate": 2.1708044112877067e-05,
      "loss": 2.5644,
      "step": 873600
    },
    {
      "epoch": 283.20907617504054,
      "grad_norm": 1.306915521621704,
      "learning_rate": 2.1704800518975023e-05,
      "loss": 2.584,
      "step": 873700
    },
    {
      "epoch": 283.2414910858995,
      "grad_norm": 1.276898980140686,
      "learning_rate": 2.170155692507298e-05,
      "loss": 2.5972,
      "step": 873800
    },
    {
      "epoch": 283.2739059967585,
      "grad_norm": 1.252192497253418,
      "learning_rate": 2.169831333117094e-05,
      "loss": 2.5785,
      "step": 873900
    },
    {
      "epoch": 283.3063209076175,
      "grad_norm": 1.4585732221603394,
      "learning_rate": 2.1695102173207915e-05,
      "loss": 2.5814,
      "step": 874000
    },
    {
      "epoch": 283.3387358184765,
      "grad_norm": 1.4794477224349976,
      "learning_rate": 2.169185857930587e-05,
      "loss": 2.5732,
      "step": 874100
    },
    {
      "epoch": 283.3711507293355,
      "grad_norm": 1.2679643630981445,
      "learning_rate": 2.168861498540383e-05,
      "loss": 2.5851,
      "step": 874200
    },
    {
      "epoch": 283.4035656401945,
      "grad_norm": 1.5402641296386719,
      "learning_rate": 2.1685371391501784e-05,
      "loss": 2.5723,
      "step": 874300
    },
    {
      "epoch": 283.4359805510535,
      "grad_norm": 1.5146070718765259,
      "learning_rate": 2.1682127797599743e-05,
      "loss": 2.5672,
      "step": 874400
    },
    {
      "epoch": 283.4683954619125,
      "grad_norm": 1.597019076347351,
      "learning_rate": 2.1678884203697698e-05,
      "loss": 2.6099,
      "step": 874500
    },
    {
      "epoch": 283.50081037277147,
      "grad_norm": 1.1287750005722046,
      "learning_rate": 2.1675640609795657e-05,
      "loss": 2.5854,
      "step": 874600
    },
    {
      "epoch": 283.5332252836305,
      "grad_norm": 1.3000956773757935,
      "learning_rate": 2.1672397015893612e-05,
      "loss": 2.5866,
      "step": 874700
    },
    {
      "epoch": 283.56564019448945,
      "grad_norm": 1.588632345199585,
      "learning_rate": 2.1669153421991568e-05,
      "loss": 2.595,
      "step": 874800
    },
    {
      "epoch": 283.59805510534846,
      "grad_norm": 1.4695377349853516,
      "learning_rate": 2.1665909828089523e-05,
      "loss": 2.5881,
      "step": 874900
    },
    {
      "epoch": 283.6304700162075,
      "grad_norm": 1.3289518356323242,
      "learning_rate": 2.166266623418748e-05,
      "loss": 2.582,
      "step": 875000
    },
    {
      "epoch": 283.66288492706644,
      "grad_norm": 1.3013116121292114,
      "learning_rate": 2.1659422640285437e-05,
      "loss": 2.5876,
      "step": 875100
    },
    {
      "epoch": 283.69529983792546,
      "grad_norm": 1.6245664358139038,
      "learning_rate": 2.1656179046383392e-05,
      "loss": 2.5821,
      "step": 875200
    },
    {
      "epoch": 283.7277147487844,
      "grad_norm": 1.3251233100891113,
      "learning_rate": 2.165293545248135e-05,
      "loss": 2.6045,
      "step": 875300
    },
    {
      "epoch": 283.76012965964344,
      "grad_norm": 1.2925968170166016,
      "learning_rate": 2.1649691858579306e-05,
      "loss": 2.5781,
      "step": 875400
    },
    {
      "epoch": 283.79254457050246,
      "grad_norm": 1.2762564420700073,
      "learning_rate": 2.1646448264677262e-05,
      "loss": 2.563,
      "step": 875500
    },
    {
      "epoch": 283.8249594813614,
      "grad_norm": 1.295508861541748,
      "learning_rate": 2.164320467077522e-05,
      "loss": 2.5904,
      "step": 875600
    },
    {
      "epoch": 283.85737439222044,
      "grad_norm": 1.5107885599136353,
      "learning_rate": 2.1639961076873176e-05,
      "loss": 2.5877,
      "step": 875700
    },
    {
      "epoch": 283.8897893030794,
      "grad_norm": 1.3785274028778076,
      "learning_rate": 2.1636717482971135e-05,
      "loss": 2.5915,
      "step": 875800
    },
    {
      "epoch": 283.9222042139384,
      "grad_norm": 1.1568557024002075,
      "learning_rate": 2.163347388906909e-05,
      "loss": 2.5769,
      "step": 875900
    },
    {
      "epoch": 283.95461912479743,
      "grad_norm": 1.4106171131134033,
      "learning_rate": 2.1630230295167045e-05,
      "loss": 2.5733,
      "step": 876000
    },
    {
      "epoch": 283.9870340356564,
      "grad_norm": 1.5022908449172974,
      "learning_rate": 2.1626986701265004e-05,
      "loss": 2.5976,
      "step": 876100
    },
    {
      "epoch": 284.0,
      "eval_bleu": 1.1355132473002398,
      "eval_loss": 4.13659143447876,
      "eval_runtime": 4.1558,
      "eval_samples_per_second": 118.389,
      "eval_steps_per_second": 1.925,
      "step": 876140
    },
    {
      "epoch": 284.0194489465154,
      "grad_norm": 1.4039825201034546,
      "learning_rate": 2.162374310736296e-05,
      "loss": 2.5903,
      "step": 876200
    },
    {
      "epoch": 284.05186385737437,
      "grad_norm": 1.2306383848190308,
      "learning_rate": 2.1620499513460915e-05,
      "loss": 2.5887,
      "step": 876300
    },
    {
      "epoch": 284.0842787682334,
      "grad_norm": 1.3018547296524048,
      "learning_rate": 2.1617255919558874e-05,
      "loss": 2.5724,
      "step": 876400
    },
    {
      "epoch": 284.1166936790924,
      "grad_norm": 1.359266996383667,
      "learning_rate": 2.161401232565683e-05,
      "loss": 2.571,
      "step": 876500
    },
    {
      "epoch": 284.14910858995137,
      "grad_norm": 1.2291208505630493,
      "learning_rate": 2.1610768731754784e-05,
      "loss": 2.5757,
      "step": 876600
    },
    {
      "epoch": 284.1815235008104,
      "grad_norm": 1.3550480604171753,
      "learning_rate": 2.160752513785274e-05,
      "loss": 2.5723,
      "step": 876700
    },
    {
      "epoch": 284.21393841166935,
      "grad_norm": 1.2692043781280518,
      "learning_rate": 2.16042815439507e-05,
      "loss": 2.5881,
      "step": 876800
    },
    {
      "epoch": 284.24635332252836,
      "grad_norm": 1.2941608428955078,
      "learning_rate": 2.1601037950048654e-05,
      "loss": 2.5733,
      "step": 876900
    },
    {
      "epoch": 284.2787682333874,
      "grad_norm": 1.2739816904067993,
      "learning_rate": 2.1597794356146612e-05,
      "loss": 2.5861,
      "step": 877000
    },
    {
      "epoch": 284.31118314424634,
      "grad_norm": 1.2703707218170166,
      "learning_rate": 2.1594550762244568e-05,
      "loss": 2.5949,
      "step": 877100
    },
    {
      "epoch": 284.34359805510536,
      "grad_norm": 1.402105450630188,
      "learning_rate": 2.1591339604281546e-05,
      "loss": 2.5704,
      "step": 877200
    },
    {
      "epoch": 284.3760129659643,
      "grad_norm": 1.276012659072876,
      "learning_rate": 2.15880960103795e-05,
      "loss": 2.5737,
      "step": 877300
    },
    {
      "epoch": 284.40842787682334,
      "grad_norm": 1.427071213722229,
      "learning_rate": 2.1584852416477456e-05,
      "loss": 2.5845,
      "step": 877400
    },
    {
      "epoch": 284.44084278768236,
      "grad_norm": 1.1949583292007446,
      "learning_rate": 2.1581608822575415e-05,
      "loss": 2.5902,
      "step": 877500
    },
    {
      "epoch": 284.4732576985413,
      "grad_norm": 1.4007762670516968,
      "learning_rate": 2.1578365228673374e-05,
      "loss": 2.5886,
      "step": 877600
    },
    {
      "epoch": 284.50567260940034,
      "grad_norm": 1.3957443237304688,
      "learning_rate": 2.157512163477133e-05,
      "loss": 2.5733,
      "step": 877700
    },
    {
      "epoch": 284.5380875202593,
      "grad_norm": 1.4499198198318481,
      "learning_rate": 2.1571878040869285e-05,
      "loss": 2.5831,
      "step": 877800
    },
    {
      "epoch": 284.5705024311183,
      "grad_norm": 1.1926466226577759,
      "learning_rate": 2.156863444696724e-05,
      "loss": 2.5797,
      "step": 877900
    },
    {
      "epoch": 284.60291734197733,
      "grad_norm": 1.2881755828857422,
      "learning_rate": 2.15653908530652e-05,
      "loss": 2.5825,
      "step": 878000
    },
    {
      "epoch": 284.6353322528363,
      "grad_norm": 1.234076976776123,
      "learning_rate": 2.1562147259163154e-05,
      "loss": 2.59,
      "step": 878100
    },
    {
      "epoch": 284.6677471636953,
      "grad_norm": 1.2247862815856934,
      "learning_rate": 2.155890366526111e-05,
      "loss": 2.5765,
      "step": 878200
    },
    {
      "epoch": 284.70016207455427,
      "grad_norm": 1.2877017259597778,
      "learning_rate": 2.1555660071359065e-05,
      "loss": 2.5912,
      "step": 878300
    },
    {
      "epoch": 284.7325769854133,
      "grad_norm": 1.4190735816955566,
      "learning_rate": 2.1552416477457023e-05,
      "loss": 2.5706,
      "step": 878400
    },
    {
      "epoch": 284.7649918962723,
      "grad_norm": 1.5229768753051758,
      "learning_rate": 2.154917288355498e-05,
      "loss": 2.5835,
      "step": 878500
    },
    {
      "epoch": 284.79740680713127,
      "grad_norm": 1.272355079650879,
      "learning_rate": 2.1545929289652934e-05,
      "loss": 2.5759,
      "step": 878600
    },
    {
      "epoch": 284.8298217179903,
      "grad_norm": 1.3515146970748901,
      "learning_rate": 2.1542685695750893e-05,
      "loss": 2.5673,
      "step": 878700
    },
    {
      "epoch": 284.86223662884925,
      "grad_norm": 1.296398401260376,
      "learning_rate": 2.153944210184885e-05,
      "loss": 2.5849,
      "step": 878800
    },
    {
      "epoch": 284.89465153970826,
      "grad_norm": 1.3943380117416382,
      "learning_rate": 2.1536198507946807e-05,
      "loss": 2.5798,
      "step": 878900
    },
    {
      "epoch": 284.9270664505673,
      "grad_norm": 1.2128819227218628,
      "learning_rate": 2.1532954914044762e-05,
      "loss": 2.5902,
      "step": 879000
    },
    {
      "epoch": 284.95948136142624,
      "grad_norm": 1.2628188133239746,
      "learning_rate": 2.152971132014272e-05,
      "loss": 2.5848,
      "step": 879100
    },
    {
      "epoch": 284.99189627228526,
      "grad_norm": 1.2315644025802612,
      "learning_rate": 2.1526467726240676e-05,
      "loss": 2.5781,
      "step": 879200
    },
    {
      "epoch": 285.0,
      "eval_bleu": 0.9745511710562957,
      "eval_loss": 4.14224910736084,
      "eval_runtime": 4.1086,
      "eval_samples_per_second": 119.749,
      "eval_steps_per_second": 1.947,
      "step": 879225
    },
    {
      "epoch": 285.0243111831442,
      "grad_norm": 1.4762018918991089,
      "learning_rate": 2.1523224132338632e-05,
      "loss": 2.5599,
      "step": 879300
    },
    {
      "epoch": 285.05672609400324,
      "grad_norm": 1.368564486503601,
      "learning_rate": 2.1519980538436587e-05,
      "loss": 2.5969,
      "step": 879400
    },
    {
      "epoch": 285.08914100486226,
      "grad_norm": 1.3257145881652832,
      "learning_rate": 2.1516736944534546e-05,
      "loss": 2.5966,
      "step": 879500
    },
    {
      "epoch": 285.1215559157212,
      "grad_norm": 1.2646541595458984,
      "learning_rate": 2.15134933506325e-05,
      "loss": 2.5893,
      "step": 879600
    },
    {
      "epoch": 285.15397082658023,
      "grad_norm": 1.3484346866607666,
      "learning_rate": 2.1510249756730457e-05,
      "loss": 2.5877,
      "step": 879700
    },
    {
      "epoch": 285.1863857374392,
      "grad_norm": 1.3266899585723877,
      "learning_rate": 2.1507006162828412e-05,
      "loss": 2.5905,
      "step": 879800
    },
    {
      "epoch": 285.2188006482982,
      "grad_norm": 1.1800239086151123,
      "learning_rate": 2.150376256892637e-05,
      "loss": 2.5511,
      "step": 879900
    },
    {
      "epoch": 285.25121555915723,
      "grad_norm": 1.3756322860717773,
      "learning_rate": 2.150055141096335e-05,
      "loss": 2.5791,
      "step": 880000
    },
    {
      "epoch": 285.2836304700162,
      "grad_norm": 1.3789801597595215,
      "learning_rate": 2.1497307817061304e-05,
      "loss": 2.5774,
      "step": 880100
    },
    {
      "epoch": 285.3160453808752,
      "grad_norm": 1.2312787771224976,
      "learning_rate": 2.149406422315926e-05,
      "loss": 2.5927,
      "step": 880200
    },
    {
      "epoch": 285.34846029173417,
      "grad_norm": 1.3159070014953613,
      "learning_rate": 2.1490820629257218e-05,
      "loss": 2.5632,
      "step": 880300
    },
    {
      "epoch": 285.3808752025932,
      "grad_norm": 1.5426161289215088,
      "learning_rate": 2.1487577035355173e-05,
      "loss": 2.5576,
      "step": 880400
    },
    {
      "epoch": 285.4132901134522,
      "grad_norm": 1.3127151727676392,
      "learning_rate": 2.1484333441453132e-05,
      "loss": 2.5725,
      "step": 880500
    },
    {
      "epoch": 285.44570502431117,
      "grad_norm": 1.3396908044815063,
      "learning_rate": 2.1481089847551087e-05,
      "loss": 2.5742,
      "step": 880600
    },
    {
      "epoch": 285.4781199351702,
      "grad_norm": 1.3598333597183228,
      "learning_rate": 2.1477846253649046e-05,
      "loss": 2.5841,
      "step": 880700
    },
    {
      "epoch": 285.51053484602915,
      "grad_norm": 1.2515934705734253,
      "learning_rate": 2.1474602659747e-05,
      "loss": 2.566,
      "step": 880800
    },
    {
      "epoch": 285.54294975688816,
      "grad_norm": 1.370954155921936,
      "learning_rate": 2.1471359065844957e-05,
      "loss": 2.5677,
      "step": 880900
    },
    {
      "epoch": 285.5753646677472,
      "grad_norm": 1.3513926267623901,
      "learning_rate": 2.1468115471942916e-05,
      "loss": 2.5792,
      "step": 881000
    },
    {
      "epoch": 285.60777957860614,
      "grad_norm": 1.3469024896621704,
      "learning_rate": 2.146487187804087e-05,
      "loss": 2.5799,
      "step": 881100
    },
    {
      "epoch": 285.64019448946516,
      "grad_norm": 1.3397853374481201,
      "learning_rate": 2.1461628284138826e-05,
      "loss": 2.5786,
      "step": 881200
    },
    {
      "epoch": 285.6726094003242,
      "grad_norm": 1.6953145265579224,
      "learning_rate": 2.145838469023678e-05,
      "loss": 2.584,
      "step": 881300
    },
    {
      "epoch": 285.70502431118314,
      "grad_norm": 1.7146576642990112,
      "learning_rate": 2.145514109633474e-05,
      "loss": 2.5702,
      "step": 881400
    },
    {
      "epoch": 285.73743922204216,
      "grad_norm": 1.2875829935073853,
      "learning_rate": 2.1451897502432696e-05,
      "loss": 2.5697,
      "step": 881500
    },
    {
      "epoch": 285.7698541329011,
      "grad_norm": 1.1992607116699219,
      "learning_rate": 2.144865390853065e-05,
      "loss": 2.5943,
      "step": 881600
    },
    {
      "epoch": 285.80226904376013,
      "grad_norm": 1.21272873878479,
      "learning_rate": 2.144541031462861e-05,
      "loss": 2.5833,
      "step": 881700
    },
    {
      "epoch": 285.83468395461915,
      "grad_norm": 1.3488463163375854,
      "learning_rate": 2.144216672072657e-05,
      "loss": 2.5741,
      "step": 881800
    },
    {
      "epoch": 285.8670988654781,
      "grad_norm": 1.316638469696045,
      "learning_rate": 2.1438923126824524e-05,
      "loss": 2.5928,
      "step": 881900
    },
    {
      "epoch": 285.89951377633713,
      "grad_norm": 1.2970844507217407,
      "learning_rate": 2.143567953292248e-05,
      "loss": 2.6054,
      "step": 882000
    },
    {
      "epoch": 285.9319286871961,
      "grad_norm": 1.4277290105819702,
      "learning_rate": 2.1432468374959454e-05,
      "loss": 2.5838,
      "step": 882100
    },
    {
      "epoch": 285.9643435980551,
      "grad_norm": 1.5386855602264404,
      "learning_rate": 2.1429224781057412e-05,
      "loss": 2.5686,
      "step": 882200
    },
    {
      "epoch": 285.9967585089141,
      "grad_norm": 1.587105631828308,
      "learning_rate": 2.1425981187155368e-05,
      "loss": 2.5771,
      "step": 882300
    },
    {
      "epoch": 286.0,
      "eval_bleu": 1.1116191588560511,
      "eval_loss": 4.1361083984375,
      "eval_runtime": 4.4422,
      "eval_samples_per_second": 110.755,
      "eval_steps_per_second": 1.801,
      "step": 882310
    },
    {
      "epoch": 286.0291734197731,
      "grad_norm": 1.45363450050354,
      "learning_rate": 2.1422737593253327e-05,
      "loss": 2.5844,
      "step": 882400
    },
    {
      "epoch": 286.0615883306321,
      "grad_norm": 1.332387089729309,
      "learning_rate": 2.1419493999351282e-05,
      "loss": 2.5564,
      "step": 882500
    },
    {
      "epoch": 286.09400324149107,
      "grad_norm": 1.312436819076538,
      "learning_rate": 2.141625040544924e-05,
      "loss": 2.5665,
      "step": 882600
    },
    {
      "epoch": 286.1264181523501,
      "grad_norm": 1.2210774421691895,
      "learning_rate": 2.1413006811547196e-05,
      "loss": 2.5749,
      "step": 882700
    },
    {
      "epoch": 286.1588330632091,
      "grad_norm": 1.273644208908081,
      "learning_rate": 2.140976321764515e-05,
      "loss": 2.6014,
      "step": 882800
    },
    {
      "epoch": 286.19124797406806,
      "grad_norm": 1.4165266752243042,
      "learning_rate": 2.1406519623743107e-05,
      "loss": 2.5682,
      "step": 882900
    },
    {
      "epoch": 286.2236628849271,
      "grad_norm": 1.421987771987915,
      "learning_rate": 2.1403308465780088e-05,
      "loss": 2.5826,
      "step": 883000
    },
    {
      "epoch": 286.25607779578604,
      "grad_norm": 1.490107774734497,
      "learning_rate": 2.1400064871878043e-05,
      "loss": 2.5912,
      "step": 883100
    },
    {
      "epoch": 286.28849270664506,
      "grad_norm": 1.3867149353027344,
      "learning_rate": 2.1396821277976e-05,
      "loss": 2.5738,
      "step": 883200
    },
    {
      "epoch": 286.3209076175041,
      "grad_norm": 1.5340380668640137,
      "learning_rate": 2.1393577684073954e-05,
      "loss": 2.5543,
      "step": 883300
    },
    {
      "epoch": 286.35332252836304,
      "grad_norm": 1.44647216796875,
      "learning_rate": 2.1390334090171913e-05,
      "loss": 2.5726,
      "step": 883400
    },
    {
      "epoch": 286.38573743922205,
      "grad_norm": 1.393303632736206,
      "learning_rate": 2.1387090496269868e-05,
      "loss": 2.5645,
      "step": 883500
    },
    {
      "epoch": 286.418152350081,
      "grad_norm": 1.3539925813674927,
      "learning_rate": 2.1383846902367823e-05,
      "loss": 2.5853,
      "step": 883600
    },
    {
      "epoch": 286.45056726094003,
      "grad_norm": 1.2861990928649902,
      "learning_rate": 2.138060330846578e-05,
      "loss": 2.5913,
      "step": 883700
    },
    {
      "epoch": 286.48298217179905,
      "grad_norm": 1.3403303623199463,
      "learning_rate": 2.1377359714563738e-05,
      "loss": 2.5671,
      "step": 883800
    },
    {
      "epoch": 286.515397082658,
      "grad_norm": 1.2980692386627197,
      "learning_rate": 2.1374116120661693e-05,
      "loss": 2.5766,
      "step": 883900
    },
    {
      "epoch": 286.54781199351703,
      "grad_norm": 1.5374714136123657,
      "learning_rate": 2.1370872526759648e-05,
      "loss": 2.5661,
      "step": 884000
    },
    {
      "epoch": 286.580226904376,
      "grad_norm": 1.3534722328186035,
      "learning_rate": 2.1367628932857607e-05,
      "loss": 2.5651,
      "step": 884100
    },
    {
      "epoch": 286.612641815235,
      "grad_norm": 1.2691316604614258,
      "learning_rate": 2.1364385338955566e-05,
      "loss": 2.5758,
      "step": 884200
    },
    {
      "epoch": 286.645056726094,
      "grad_norm": 1.2273527383804321,
      "learning_rate": 2.136114174505352e-05,
      "loss": 2.5965,
      "step": 884300
    },
    {
      "epoch": 286.677471636953,
      "grad_norm": 1.4719992876052856,
      "learning_rate": 2.1357898151151476e-05,
      "loss": 2.5805,
      "step": 884400
    },
    {
      "epoch": 286.709886547812,
      "grad_norm": 1.2883000373840332,
      "learning_rate": 2.1354654557249435e-05,
      "loss": 2.5879,
      "step": 884500
    },
    {
      "epoch": 286.74230145867097,
      "grad_norm": 1.4363288879394531,
      "learning_rate": 2.135141096334739e-05,
      "loss": 2.5762,
      "step": 884600
    },
    {
      "epoch": 286.77471636953,
      "grad_norm": 1.213944673538208,
      "learning_rate": 2.1348167369445346e-05,
      "loss": 2.5713,
      "step": 884700
    },
    {
      "epoch": 286.807131280389,
      "grad_norm": 1.414737343788147,
      "learning_rate": 2.13449237755433e-05,
      "loss": 2.5768,
      "step": 884800
    },
    {
      "epoch": 286.83954619124796,
      "grad_norm": 1.3352831602096558,
      "learning_rate": 2.134168018164126e-05,
      "loss": 2.5836,
      "step": 884900
    },
    {
      "epoch": 286.871961102107,
      "grad_norm": 1.3407623767852783,
      "learning_rate": 2.1338436587739215e-05,
      "loss": 2.5982,
      "step": 885000
    },
    {
      "epoch": 286.90437601296594,
      "grad_norm": 1.3581342697143555,
      "learning_rate": 2.133519299383717e-05,
      "loss": 2.5648,
      "step": 885100
    },
    {
      "epoch": 286.93679092382496,
      "grad_norm": 1.2342437505722046,
      "learning_rate": 2.133194939993513e-05,
      "loss": 2.5839,
      "step": 885200
    },
    {
      "epoch": 286.969205834684,
      "grad_norm": 1.3538309335708618,
      "learning_rate": 2.1328705806033085e-05,
      "loss": 2.5957,
      "step": 885300
    },
    {
      "epoch": 287.0,
      "eval_bleu": 1.016597353787068,
      "eval_loss": 4.137632369995117,
      "eval_runtime": 4.2884,
      "eval_samples_per_second": 114.728,
      "eval_steps_per_second": 1.865,
      "step": 885395
    },
    {
      "epoch": 287.00162074554294,
      "grad_norm": 1.3587301969528198,
      "learning_rate": 2.1325462212131044e-05,
      "loss": 2.6002,
      "step": 885400
    },
    {
      "epoch": 287.03403565640195,
      "grad_norm": 1.3676536083221436,
      "learning_rate": 2.1322218618229e-05,
      "loss": 2.5547,
      "step": 885500
    },
    {
      "epoch": 287.0664505672609,
      "grad_norm": 1.2358635663986206,
      "learning_rate": 2.1318975024326958e-05,
      "loss": 2.5683,
      "step": 885600
    },
    {
      "epoch": 287.09886547811993,
      "grad_norm": 1.2510051727294922,
      "learning_rate": 2.1315731430424913e-05,
      "loss": 2.5881,
      "step": 885700
    },
    {
      "epoch": 287.13128038897895,
      "grad_norm": 1.3164042234420776,
      "learning_rate": 2.131248783652287e-05,
      "loss": 2.5782,
      "step": 885800
    },
    {
      "epoch": 287.1636952998379,
      "grad_norm": 1.3585211038589478,
      "learning_rate": 2.1309244242620824e-05,
      "loss": 2.5617,
      "step": 885900
    },
    {
      "epoch": 287.19611021069693,
      "grad_norm": 1.300659418106079,
      "learning_rate": 2.1306000648718782e-05,
      "loss": 2.5674,
      "step": 886000
    },
    {
      "epoch": 287.2285251215559,
      "grad_norm": 1.1700615882873535,
      "learning_rate": 2.1302757054816738e-05,
      "loss": 2.5673,
      "step": 886100
    },
    {
      "epoch": 287.2609400324149,
      "grad_norm": 1.2298319339752197,
      "learning_rate": 2.1299513460914693e-05,
      "loss": 2.5916,
      "step": 886200
    },
    {
      "epoch": 287.2933549432739,
      "grad_norm": 1.5358269214630127,
      "learning_rate": 2.129626986701265e-05,
      "loss": 2.5797,
      "step": 886300
    },
    {
      "epoch": 287.3257698541329,
      "grad_norm": 1.4190187454223633,
      "learning_rate": 2.1293026273110607e-05,
      "loss": 2.5884,
      "step": 886400
    },
    {
      "epoch": 287.3581847649919,
      "grad_norm": 1.3648806810379028,
      "learning_rate": 2.1289782679208563e-05,
      "loss": 2.5946,
      "step": 886500
    },
    {
      "epoch": 287.39059967585086,
      "grad_norm": 1.3443080186843872,
      "learning_rate": 2.128653908530652e-05,
      "loss": 2.5608,
      "step": 886600
    },
    {
      "epoch": 287.4230145867099,
      "grad_norm": 1.1104141473770142,
      "learning_rate": 2.128329549140448e-05,
      "loss": 2.5683,
      "step": 886700
    },
    {
      "epoch": 287.4554294975689,
      "grad_norm": 1.4499207735061646,
      "learning_rate": 2.1280051897502435e-05,
      "loss": 2.5617,
      "step": 886800
    },
    {
      "epoch": 287.48784440842786,
      "grad_norm": 1.2804429531097412,
      "learning_rate": 2.127680830360039e-05,
      "loss": 2.5739,
      "step": 886900
    },
    {
      "epoch": 287.5202593192869,
      "grad_norm": 1.2435717582702637,
      "learning_rate": 2.1273564709698346e-05,
      "loss": 2.5844,
      "step": 887000
    },
    {
      "epoch": 287.55267423014584,
      "grad_norm": 1.295055866241455,
      "learning_rate": 2.1270321115796305e-05,
      "loss": 2.5748,
      "step": 887100
    },
    {
      "epoch": 287.58508914100486,
      "grad_norm": 1.3653948307037354,
      "learning_rate": 2.126707752189426e-05,
      "loss": 2.5827,
      "step": 887200
    },
    {
      "epoch": 287.6175040518639,
      "grad_norm": 1.621922492980957,
      "learning_rate": 2.1263833927992216e-05,
      "loss": 2.578,
      "step": 887300
    },
    {
      "epoch": 287.64991896272284,
      "grad_norm": 1.3740699291229248,
      "learning_rate": 2.126059033409017e-05,
      "loss": 2.5876,
      "step": 887400
    },
    {
      "epoch": 287.68233387358185,
      "grad_norm": 1.4384056329727173,
      "learning_rate": 2.125734674018813e-05,
      "loss": 2.5832,
      "step": 887500
    },
    {
      "epoch": 287.7147487844408,
      "grad_norm": 1.4245895147323608,
      "learning_rate": 2.1254103146286085e-05,
      "loss": 2.5858,
      "step": 887600
    },
    {
      "epoch": 287.74716369529983,
      "grad_norm": 1.5637174844741821,
      "learning_rate": 2.125085955238404e-05,
      "loss": 2.5752,
      "step": 887700
    },
    {
      "epoch": 287.77957860615885,
      "grad_norm": 1.2301291227340698,
      "learning_rate": 2.1247615958482e-05,
      "loss": 2.5811,
      "step": 887800
    },
    {
      "epoch": 287.8119935170178,
      "grad_norm": 1.4189250469207764,
      "learning_rate": 2.1244372364579958e-05,
      "loss": 2.5766,
      "step": 887900
    },
    {
      "epoch": 287.84440842787683,
      "grad_norm": 1.2888460159301758,
      "learning_rate": 2.1241128770677913e-05,
      "loss": 2.5749,
      "step": 888000
    },
    {
      "epoch": 287.87682333873585,
      "grad_norm": 1.5566023588180542,
      "learning_rate": 2.123788517677587e-05,
      "loss": 2.5725,
      "step": 888100
    },
    {
      "epoch": 287.9092382495948,
      "grad_norm": 1.3867919445037842,
      "learning_rate": 2.1234641582873827e-05,
      "loss": 2.5965,
      "step": 888200
    },
    {
      "epoch": 287.9416531604538,
      "grad_norm": 1.4749996662139893,
      "learning_rate": 2.1231397988971783e-05,
      "loss": 2.5862,
      "step": 888300
    },
    {
      "epoch": 287.9740680713128,
      "grad_norm": 1.2638570070266724,
      "learning_rate": 2.1228154395069738e-05,
      "loss": 2.5699,
      "step": 888400
    },
    {
      "epoch": 288.0,
      "eval_bleu": 1.194741631720278,
      "eval_loss": 4.144143581390381,
      "eval_runtime": 4.3193,
      "eval_samples_per_second": 113.907,
      "eval_steps_per_second": 1.852,
      "step": 888480
    },
    {
      "epoch": 288.0064829821718,
      "grad_norm": 1.351073145866394,
      "learning_rate": 2.1224943237106716e-05,
      "loss": 2.5619,
      "step": 888500
    },
    {
      "epoch": 288.0388978930308,
      "grad_norm": 1.419141173362732,
      "learning_rate": 2.122169964320467e-05,
      "loss": 2.5495,
      "step": 888600
    },
    {
      "epoch": 288.0713128038898,
      "grad_norm": 1.2460508346557617,
      "learning_rate": 2.121845604930263e-05,
      "loss": 2.5439,
      "step": 888700
    },
    {
      "epoch": 288.1037277147488,
      "grad_norm": 1.2588305473327637,
      "learning_rate": 2.1215212455400585e-05,
      "loss": 2.5626,
      "step": 888800
    },
    {
      "epoch": 288.13614262560776,
      "grad_norm": 1.3675600290298462,
      "learning_rate": 2.121196886149854e-05,
      "loss": 2.5812,
      "step": 888900
    },
    {
      "epoch": 288.1685575364668,
      "grad_norm": 1.3330014944076538,
      "learning_rate": 2.12087252675965e-05,
      "loss": 2.5574,
      "step": 889000
    },
    {
      "epoch": 288.2009724473258,
      "grad_norm": 1.2280999422073364,
      "learning_rate": 2.1205481673694455e-05,
      "loss": 2.5595,
      "step": 889100
    },
    {
      "epoch": 288.23338735818476,
      "grad_norm": 1.1709692478179932,
      "learning_rate": 2.120223807979241e-05,
      "loss": 2.5822,
      "step": 889200
    },
    {
      "epoch": 288.2658022690438,
      "grad_norm": 1.4246057271957397,
      "learning_rate": 2.1198994485890365e-05,
      "loss": 2.5808,
      "step": 889300
    },
    {
      "epoch": 288.29821717990274,
      "grad_norm": 1.5100771188735962,
      "learning_rate": 2.1195750891988324e-05,
      "loss": 2.5853,
      "step": 889400
    },
    {
      "epoch": 288.33063209076175,
      "grad_norm": 1.3014798164367676,
      "learning_rate": 2.119250729808628e-05,
      "loss": 2.5844,
      "step": 889500
    },
    {
      "epoch": 288.36304700162077,
      "grad_norm": 1.1889582872390747,
      "learning_rate": 2.118926370418424e-05,
      "loss": 2.5688,
      "step": 889600
    },
    {
      "epoch": 288.39546191247973,
      "grad_norm": 1.448777437210083,
      "learning_rate": 2.1186020110282194e-05,
      "loss": 2.582,
      "step": 889700
    },
    {
      "epoch": 288.42787682333875,
      "grad_norm": 1.7379264831542969,
      "learning_rate": 2.1182776516380152e-05,
      "loss": 2.5834,
      "step": 889800
    },
    {
      "epoch": 288.4602917341977,
      "grad_norm": 1.1454358100891113,
      "learning_rate": 2.1179532922478108e-05,
      "loss": 2.5936,
      "step": 889900
    },
    {
      "epoch": 288.4927066450567,
      "grad_norm": 1.4020740985870361,
      "learning_rate": 2.1176289328576063e-05,
      "loss": 2.5727,
      "step": 890000
    },
    {
      "epoch": 288.52512155591575,
      "grad_norm": 1.3171956539154053,
      "learning_rate": 2.117304573467402e-05,
      "loss": 2.5651,
      "step": 890100
    },
    {
      "epoch": 288.5575364667747,
      "grad_norm": 1.49441397190094,
      "learning_rate": 2.1169802140771977e-05,
      "loss": 2.5872,
      "step": 890200
    },
    {
      "epoch": 288.5899513776337,
      "grad_norm": 1.3222554922103882,
      "learning_rate": 2.1166558546869933e-05,
      "loss": 2.5733,
      "step": 890300
    },
    {
      "epoch": 288.6223662884927,
      "grad_norm": 1.1495786905288696,
      "learning_rate": 2.1163314952967888e-05,
      "loss": 2.5707,
      "step": 890400
    },
    {
      "epoch": 288.6547811993517,
      "grad_norm": 1.3620306253433228,
      "learning_rate": 2.1160071359065847e-05,
      "loss": 2.5795,
      "step": 890500
    },
    {
      "epoch": 288.6871961102107,
      "grad_norm": 1.2851698398590088,
      "learning_rate": 2.1156827765163802e-05,
      "loss": 2.5662,
      "step": 890600
    },
    {
      "epoch": 288.7196110210697,
      "grad_norm": 1.202629566192627,
      "learning_rate": 2.1153584171261757e-05,
      "loss": 2.5934,
      "step": 890700
    },
    {
      "epoch": 288.7520259319287,
      "grad_norm": 1.4232829809188843,
      "learning_rate": 2.1150340577359716e-05,
      "loss": 2.5861,
      "step": 890800
    },
    {
      "epoch": 288.78444084278766,
      "grad_norm": 1.1893811225891113,
      "learning_rate": 2.1147096983457675e-05,
      "loss": 2.5763,
      "step": 890900
    },
    {
      "epoch": 288.8168557536467,
      "grad_norm": 1.463883876800537,
      "learning_rate": 2.114385338955563e-05,
      "loss": 2.5853,
      "step": 891000
    },
    {
      "epoch": 288.8492706645057,
      "grad_norm": 1.2699800729751587,
      "learning_rate": 2.1140609795653586e-05,
      "loss": 2.5771,
      "step": 891100
    },
    {
      "epoch": 288.88168557536466,
      "grad_norm": 1.1847723722457886,
      "learning_rate": 2.113736620175154e-05,
      "loss": 2.5833,
      "step": 891200
    },
    {
      "epoch": 288.9141004862237,
      "grad_norm": 1.2814713716506958,
      "learning_rate": 2.11341226078495e-05,
      "loss": 2.5818,
      "step": 891300
    },
    {
      "epoch": 288.94651539708263,
      "grad_norm": 1.38499116897583,
      "learning_rate": 2.1130879013947455e-05,
      "loss": 2.5742,
      "step": 891400
    },
    {
      "epoch": 288.97893030794165,
      "grad_norm": 1.3319456577301025,
      "learning_rate": 2.112763542004541e-05,
      "loss": 2.5679,
      "step": 891500
    },
    {
      "epoch": 289.0,
      "eval_bleu": 1.110961349573606,
      "eval_loss": 4.140491962432861,
      "eval_runtime": 4.41,
      "eval_samples_per_second": 111.564,
      "eval_steps_per_second": 1.814,
      "step": 891565
    },
    {
      "epoch": 289.01134521880067,
      "grad_norm": 1.3267325162887573,
      "learning_rate": 2.1124391826143366e-05,
      "loss": 2.5934,
      "step": 891600
    },
    {
      "epoch": 289.04376012965963,
      "grad_norm": 1.460654616355896,
      "learning_rate": 2.1121148232241324e-05,
      "loss": 2.5622,
      "step": 891700
    },
    {
      "epoch": 289.07617504051865,
      "grad_norm": 1.3218265771865845,
      "learning_rate": 2.111790463833928e-05,
      "loss": 2.5719,
      "step": 891800
    },
    {
      "epoch": 289.1085899513776,
      "grad_norm": 1.2701767683029175,
      "learning_rate": 2.1114661044437235e-05,
      "loss": 2.5908,
      "step": 891900
    },
    {
      "epoch": 289.1410048622366,
      "grad_norm": 1.4204151630401611,
      "learning_rate": 2.1111417450535194e-05,
      "loss": 2.5552,
      "step": 892000
    },
    {
      "epoch": 289.17341977309565,
      "grad_norm": 1.143409013748169,
      "learning_rate": 2.1108173856633153e-05,
      "loss": 2.559,
      "step": 892100
    },
    {
      "epoch": 289.2058346839546,
      "grad_norm": 1.2476662397384644,
      "learning_rate": 2.1104930262731108e-05,
      "loss": 2.5788,
      "step": 892200
    },
    {
      "epoch": 289.2382495948136,
      "grad_norm": 1.5074442625045776,
      "learning_rate": 2.1101686668829063e-05,
      "loss": 2.6076,
      "step": 892300
    },
    {
      "epoch": 289.2706645056726,
      "grad_norm": 1.4651885032653809,
      "learning_rate": 2.1098443074927022e-05,
      "loss": 2.5671,
      "step": 892400
    },
    {
      "epoch": 289.3030794165316,
      "grad_norm": 1.177526831626892,
      "learning_rate": 2.1095199481024977e-05,
      "loss": 2.589,
      "step": 892500
    },
    {
      "epoch": 289.3354943273906,
      "grad_norm": 1.144627571105957,
      "learning_rate": 2.1091988323061952e-05,
      "loss": 2.5515,
      "step": 892600
    },
    {
      "epoch": 289.3679092382496,
      "grad_norm": 1.2634166479110718,
      "learning_rate": 2.108874472915991e-05,
      "loss": 2.5806,
      "step": 892700
    },
    {
      "epoch": 289.4003241491086,
      "grad_norm": 1.294978380203247,
      "learning_rate": 2.108550113525787e-05,
      "loss": 2.5743,
      "step": 892800
    },
    {
      "epoch": 289.43273905996756,
      "grad_norm": 1.3497728109359741,
      "learning_rate": 2.1082257541355825e-05,
      "loss": 2.5649,
      "step": 892900
    },
    {
      "epoch": 289.4651539708266,
      "grad_norm": 1.2810038328170776,
      "learning_rate": 2.107901394745378e-05,
      "loss": 2.5659,
      "step": 893000
    },
    {
      "epoch": 289.4975688816856,
      "grad_norm": 1.3727877140045166,
      "learning_rate": 2.1075770353551735e-05,
      "loss": 2.5857,
      "step": 893100
    },
    {
      "epoch": 289.52998379254456,
      "grad_norm": 1.422755479812622,
      "learning_rate": 2.1072526759649694e-05,
      "loss": 2.5709,
      "step": 893200
    },
    {
      "epoch": 289.5623987034036,
      "grad_norm": 1.9674980640411377,
      "learning_rate": 2.106928316574765e-05,
      "loss": 2.5717,
      "step": 893300
    },
    {
      "epoch": 289.59481361426253,
      "grad_norm": 1.5247634649276733,
      "learning_rate": 2.1066039571845605e-05,
      "loss": 2.5746,
      "step": 893400
    },
    {
      "epoch": 289.62722852512155,
      "grad_norm": 1.158730149269104,
      "learning_rate": 2.106279597794356e-05,
      "loss": 2.5738,
      "step": 893500
    },
    {
      "epoch": 289.65964343598057,
      "grad_norm": 1.3774223327636719,
      "learning_rate": 2.105955238404152e-05,
      "loss": 2.5845,
      "step": 893600
    },
    {
      "epoch": 289.69205834683953,
      "grad_norm": 1.5016670227050781,
      "learning_rate": 2.1056308790139474e-05,
      "loss": 2.586,
      "step": 893700
    },
    {
      "epoch": 289.72447325769855,
      "grad_norm": 1.601676106452942,
      "learning_rate": 2.1053065196237433e-05,
      "loss": 2.5734,
      "step": 893800
    },
    {
      "epoch": 289.7568881685575,
      "grad_norm": 1.1392676830291748,
      "learning_rate": 2.104982160233539e-05,
      "loss": 2.5704,
      "step": 893900
    },
    {
      "epoch": 289.7893030794165,
      "grad_norm": 1.4425806999206543,
      "learning_rate": 2.1046578008433347e-05,
      "loss": 2.5809,
      "step": 894000
    },
    {
      "epoch": 289.82171799027554,
      "grad_norm": 1.1293188333511353,
      "learning_rate": 2.1043334414531303e-05,
      "loss": 2.5716,
      "step": 894100
    },
    {
      "epoch": 289.8541329011345,
      "grad_norm": 1.8584840297698975,
      "learning_rate": 2.1040090820629258e-05,
      "loss": 2.5772,
      "step": 894200
    },
    {
      "epoch": 289.8865478119935,
      "grad_norm": 1.302721619606018,
      "learning_rate": 2.1036879662666232e-05,
      "loss": 2.5921,
      "step": 894300
    },
    {
      "epoch": 289.9189627228525,
      "grad_norm": 1.6382774114608765,
      "learning_rate": 2.103363606876419e-05,
      "loss": 2.5648,
      "step": 894400
    },
    {
      "epoch": 289.9513776337115,
      "grad_norm": 1.1662240028381348,
      "learning_rate": 2.103039247486215e-05,
      "loss": 2.5673,
      "step": 894500
    },
    {
      "epoch": 289.9837925445705,
      "grad_norm": 1.5428991317749023,
      "learning_rate": 2.1027148880960105e-05,
      "loss": 2.5824,
      "step": 894600
    },
    {
      "epoch": 290.0,
      "eval_bleu": 1.067723050408269,
      "eval_loss": 4.150318145751953,
      "eval_runtime": 4.3852,
      "eval_samples_per_second": 112.196,
      "eval_steps_per_second": 1.824,
      "step": 894650
    },
    {
      "epoch": 290.0162074554295,
      "grad_norm": 1.273895502090454,
      "learning_rate": 2.102390528705806e-05,
      "loss": 2.5622,
      "step": 894700
    },
    {
      "epoch": 290.0486223662885,
      "grad_norm": 1.299534797668457,
      "learning_rate": 2.102066169315602e-05,
      "loss": 2.5653,
      "step": 894800
    },
    {
      "epoch": 290.0810372771475,
      "grad_norm": 1.2760709524154663,
      "learning_rate": 2.1017418099253975e-05,
      "loss": 2.5638,
      "step": 894900
    },
    {
      "epoch": 290.1134521880065,
      "grad_norm": 1.2118000984191895,
      "learning_rate": 2.101417450535193e-05,
      "loss": 2.5973,
      "step": 895000
    },
    {
      "epoch": 290.1458670988655,
      "grad_norm": 1.3173924684524536,
      "learning_rate": 2.101093091144989e-05,
      "loss": 2.5611,
      "step": 895100
    },
    {
      "epoch": 290.17828200972446,
      "grad_norm": 1.5346882343292236,
      "learning_rate": 2.1007719753486867e-05,
      "loss": 2.5668,
      "step": 895200
    },
    {
      "epoch": 290.2106969205835,
      "grad_norm": 1.333000659942627,
      "learning_rate": 2.1004476159584822e-05,
      "loss": 2.5675,
      "step": 895300
    },
    {
      "epoch": 290.2431118314425,
      "grad_norm": 1.2415850162506104,
      "learning_rate": 2.1001232565682777e-05,
      "loss": 2.5779,
      "step": 895400
    },
    {
      "epoch": 290.27552674230145,
      "grad_norm": 1.3883695602416992,
      "learning_rate": 2.0997988971780736e-05,
      "loss": 2.5699,
      "step": 895500
    },
    {
      "epoch": 290.30794165316047,
      "grad_norm": 1.291337013244629,
      "learning_rate": 2.099474537787869e-05,
      "loss": 2.5773,
      "step": 895600
    },
    {
      "epoch": 290.34035656401943,
      "grad_norm": 1.4041873216629028,
      "learning_rate": 2.0991501783976647e-05,
      "loss": 2.5876,
      "step": 895700
    },
    {
      "epoch": 290.37277147487845,
      "grad_norm": 1.1628903150558472,
      "learning_rate": 2.0988258190074602e-05,
      "loss": 2.5747,
      "step": 895800
    },
    {
      "epoch": 290.40518638573747,
      "grad_norm": 1.3520723581314087,
      "learning_rate": 2.098501459617256e-05,
      "loss": 2.5675,
      "step": 895900
    },
    {
      "epoch": 290.4376012965964,
      "grad_norm": 1.3405818939208984,
      "learning_rate": 2.0981771002270516e-05,
      "loss": 2.5722,
      "step": 896000
    },
    {
      "epoch": 290.47001620745544,
      "grad_norm": 1.4019941091537476,
      "learning_rate": 2.097852740836847e-05,
      "loss": 2.5686,
      "step": 896100
    },
    {
      "epoch": 290.5024311183144,
      "grad_norm": 1.307421088218689,
      "learning_rate": 2.097528381446643e-05,
      "loss": 2.5521,
      "step": 896200
    },
    {
      "epoch": 290.5348460291734,
      "grad_norm": 1.3107515573501587,
      "learning_rate": 2.097204022056439e-05,
      "loss": 2.5645,
      "step": 896300
    },
    {
      "epoch": 290.56726094003244,
      "grad_norm": 1.2636712789535522,
      "learning_rate": 2.0968796626662344e-05,
      "loss": 2.5784,
      "step": 896400
    },
    {
      "epoch": 290.5996758508914,
      "grad_norm": 1.3878883123397827,
      "learning_rate": 2.09655530327603e-05,
      "loss": 2.5706,
      "step": 896500
    },
    {
      "epoch": 290.6320907617504,
      "grad_norm": 1.409513235092163,
      "learning_rate": 2.0962309438858255e-05,
      "loss": 2.5616,
      "step": 896600
    },
    {
      "epoch": 290.6645056726094,
      "grad_norm": 1.4182112216949463,
      "learning_rate": 2.0959065844956214e-05,
      "loss": 2.5887,
      "step": 896700
    },
    {
      "epoch": 290.6969205834684,
      "grad_norm": 1.3994992971420288,
      "learning_rate": 2.095582225105417e-05,
      "loss": 2.5706,
      "step": 896800
    },
    {
      "epoch": 290.7293354943274,
      "grad_norm": 1.2692815065383911,
      "learning_rate": 2.0952578657152124e-05,
      "loss": 2.582,
      "step": 896900
    },
    {
      "epoch": 290.7617504051864,
      "grad_norm": 1.4774656295776367,
      "learning_rate": 2.0949335063250083e-05,
      "loss": 2.5745,
      "step": 897000
    },
    {
      "epoch": 290.7941653160454,
      "grad_norm": 1.3064377307891846,
      "learning_rate": 2.094609146934804e-05,
      "loss": 2.5835,
      "step": 897100
    },
    {
      "epoch": 290.82658022690435,
      "grad_norm": 1.3202488422393799,
      "learning_rate": 2.0942847875445994e-05,
      "loss": 2.5747,
      "step": 897200
    },
    {
      "epoch": 290.8589951377634,
      "grad_norm": 1.4524099826812744,
      "learning_rate": 2.093960428154395e-05,
      "loss": 2.5698,
      "step": 897300
    },
    {
      "epoch": 290.8914100486224,
      "grad_norm": 1.4477417469024658,
      "learning_rate": 2.0936360687641908e-05,
      "loss": 2.5784,
      "step": 897400
    },
    {
      "epoch": 290.92382495948135,
      "grad_norm": 1.2516987323760986,
      "learning_rate": 2.0933117093739867e-05,
      "loss": 2.5692,
      "step": 897500
    },
    {
      "epoch": 290.95623987034037,
      "grad_norm": 1.2354905605316162,
      "learning_rate": 2.0929873499837822e-05,
      "loss": 2.557,
      "step": 897600
    },
    {
      "epoch": 290.98865478119933,
      "grad_norm": 1.3321702480316162,
      "learning_rate": 2.0926629905935777e-05,
      "loss": 2.5878,
      "step": 897700
    },
    {
      "epoch": 291.0,
      "eval_bleu": 1.062947692618869,
      "eval_loss": 4.147227764129639,
      "eval_runtime": 4.2212,
      "eval_samples_per_second": 116.555,
      "eval_steps_per_second": 1.895,
      "step": 897735
    },
    {
      "epoch": 291.02106969205835,
      "grad_norm": 1.2763375043869019,
      "learning_rate": 2.0923386312033736e-05,
      "loss": 2.5608,
      "step": 897800
    },
    {
      "epoch": 291.05348460291737,
      "grad_norm": 1.3033846616744995,
      "learning_rate": 2.092014271813169e-05,
      "loss": 2.5752,
      "step": 897900
    },
    {
      "epoch": 291.0858995137763,
      "grad_norm": 1.4221745729446411,
      "learning_rate": 2.0916899124229647e-05,
      "loss": 2.5527,
      "step": 898000
    },
    {
      "epoch": 291.11831442463534,
      "grad_norm": 1.2450685501098633,
      "learning_rate": 2.0913655530327602e-05,
      "loss": 2.5744,
      "step": 898100
    },
    {
      "epoch": 291.1507293354943,
      "grad_norm": 1.2871648073196411,
      "learning_rate": 2.091041193642556e-05,
      "loss": 2.5654,
      "step": 898200
    },
    {
      "epoch": 291.1831442463533,
      "grad_norm": 1.4802966117858887,
      "learning_rate": 2.0907168342523516e-05,
      "loss": 2.5677,
      "step": 898300
    },
    {
      "epoch": 291.21555915721234,
      "grad_norm": 1.3108701705932617,
      "learning_rate": 2.0903924748621472e-05,
      "loss": 2.5616,
      "step": 898400
    },
    {
      "epoch": 291.2479740680713,
      "grad_norm": 1.6273375749588013,
      "learning_rate": 2.090068115471943e-05,
      "loss": 2.5689,
      "step": 898500
    },
    {
      "epoch": 291.2803889789303,
      "grad_norm": 1.0597304105758667,
      "learning_rate": 2.0897437560817386e-05,
      "loss": 2.5846,
      "step": 898600
    },
    {
      "epoch": 291.3128038897893,
      "grad_norm": 1.2653335332870483,
      "learning_rate": 2.0894193966915345e-05,
      "loss": 2.5696,
      "step": 898700
    },
    {
      "epoch": 291.3452188006483,
      "grad_norm": 1.3620622158050537,
      "learning_rate": 2.08909503730133e-05,
      "loss": 2.5697,
      "step": 898800
    },
    {
      "epoch": 291.3776337115073,
      "grad_norm": 1.354185938835144,
      "learning_rate": 2.088770677911126e-05,
      "loss": 2.5817,
      "step": 898900
    },
    {
      "epoch": 291.4100486223663,
      "grad_norm": 1.3514419794082642,
      "learning_rate": 2.0884463185209214e-05,
      "loss": 2.5598,
      "step": 899000
    },
    {
      "epoch": 291.4424635332253,
      "grad_norm": 1.3262890577316284,
      "learning_rate": 2.088121959130717e-05,
      "loss": 2.5666,
      "step": 899100
    },
    {
      "epoch": 291.47487844408425,
      "grad_norm": 1.258344292640686,
      "learning_rate": 2.0877975997405125e-05,
      "loss": 2.5812,
      "step": 899200
    },
    {
      "epoch": 291.5072933549433,
      "grad_norm": 1.3100230693817139,
      "learning_rate": 2.0874732403503083e-05,
      "loss": 2.571,
      "step": 899300
    },
    {
      "epoch": 291.5397082658023,
      "grad_norm": 1.409537434577942,
      "learning_rate": 2.087148880960104e-05,
      "loss": 2.5792,
      "step": 899400
    },
    {
      "epoch": 291.57212317666125,
      "grad_norm": 1.2163071632385254,
      "learning_rate": 2.0868245215698994e-05,
      "loss": 2.5665,
      "step": 899500
    },
    {
      "epoch": 291.60453808752027,
      "grad_norm": 1.255216360092163,
      "learning_rate": 2.086500162179695e-05,
      "loss": 2.5852,
      "step": 899600
    },
    {
      "epoch": 291.63695299837923,
      "grad_norm": 1.2994778156280518,
      "learning_rate": 2.0861758027894908e-05,
      "loss": 2.5787,
      "step": 899700
    },
    {
      "epoch": 291.66936790923825,
      "grad_norm": 1.3165173530578613,
      "learning_rate": 2.0858514433992864e-05,
      "loss": 2.572,
      "step": 899800
    },
    {
      "epoch": 291.70178282009726,
      "grad_norm": 1.546776533126831,
      "learning_rate": 2.0855270840090822e-05,
      "loss": 2.5829,
      "step": 899900
    },
    {
      "epoch": 291.7341977309562,
      "grad_norm": 1.3148163557052612,
      "learning_rate": 2.085202724618878e-05,
      "loss": 2.5617,
      "step": 900000
    },
    {
      "epoch": 291.76661264181524,
      "grad_norm": 1.2591675519943237,
      "learning_rate": 2.0848783652286736e-05,
      "loss": 2.5659,
      "step": 900100
    },
    {
      "epoch": 291.7990275526742,
      "grad_norm": 1.3127391338348389,
      "learning_rate": 2.0845540058384692e-05,
      "loss": 2.5623,
      "step": 900200
    },
    {
      "epoch": 291.8314424635332,
      "grad_norm": 1.3501043319702148,
      "learning_rate": 2.0842296464482647e-05,
      "loss": 2.5779,
      "step": 900300
    },
    {
      "epoch": 291.86385737439224,
      "grad_norm": 1.2846542596817017,
      "learning_rate": 2.0839052870580606e-05,
      "loss": 2.5807,
      "step": 900400
    },
    {
      "epoch": 291.8962722852512,
      "grad_norm": 1.3326716423034668,
      "learning_rate": 2.083580927667856e-05,
      "loss": 2.5959,
      "step": 900500
    },
    {
      "epoch": 291.9286871961102,
      "grad_norm": 1.2828559875488281,
      "learning_rate": 2.0832565682776517e-05,
      "loss": 2.5776,
      "step": 900600
    },
    {
      "epoch": 291.9611021069692,
      "grad_norm": 1.234976053237915,
      "learning_rate": 2.0829322088874472e-05,
      "loss": 2.569,
      "step": 900700
    },
    {
      "epoch": 291.9935170178282,
      "grad_norm": 1.3587173223495483,
      "learning_rate": 2.082607849497243e-05,
      "loss": 2.5714,
      "step": 900800
    },
    {
      "epoch": 292.0,
      "eval_bleu": 1.1344682593687598,
      "eval_loss": 4.150773048400879,
      "eval_runtime": 4.4533,
      "eval_samples_per_second": 110.479,
      "eval_steps_per_second": 1.796,
      "step": 900820
    },
    {
      "epoch": 292.0259319286872,
      "grad_norm": 1.5378466844558716,
      "learning_rate": 2.0822834901070386e-05,
      "loss": 2.5691,
      "step": 900900
    },
    {
      "epoch": 292.0583468395462,
      "grad_norm": 1.3445117473602295,
      "learning_rate": 2.081959130716834e-05,
      "loss": 2.5953,
      "step": 901000
    },
    {
      "epoch": 292.0907617504052,
      "grad_norm": 1.4487545490264893,
      "learning_rate": 2.08163477132663e-05,
      "loss": 2.5908,
      "step": 901100
    },
    {
      "epoch": 292.12317666126415,
      "grad_norm": 1.2362841367721558,
      "learning_rate": 2.0813136555303278e-05,
      "loss": 2.5685,
      "step": 901200
    },
    {
      "epoch": 292.15559157212317,
      "grad_norm": 1.1804401874542236,
      "learning_rate": 2.0809892961401233e-05,
      "loss": 2.5513,
      "step": 901300
    },
    {
      "epoch": 292.1880064829822,
      "grad_norm": 1.3073530197143555,
      "learning_rate": 2.080664936749919e-05,
      "loss": 2.5518,
      "step": 901400
    },
    {
      "epoch": 292.22042139384115,
      "grad_norm": 1.4624584913253784,
      "learning_rate": 2.0803405773597144e-05,
      "loss": 2.557,
      "step": 901500
    },
    {
      "epoch": 292.25283630470017,
      "grad_norm": 1.2383203506469727,
      "learning_rate": 2.0800162179695103e-05,
      "loss": 2.5671,
      "step": 901600
    },
    {
      "epoch": 292.2852512155592,
      "grad_norm": 1.4276611804962158,
      "learning_rate": 2.0796918585793058e-05,
      "loss": 2.5683,
      "step": 901700
    },
    {
      "epoch": 292.31766612641815,
      "grad_norm": 1.2493999004364014,
      "learning_rate": 2.0793674991891017e-05,
      "loss": 2.5856,
      "step": 901800
    },
    {
      "epoch": 292.35008103727716,
      "grad_norm": 1.4126230478286743,
      "learning_rate": 2.0790431397988972e-05,
      "loss": 2.5778,
      "step": 901900
    },
    {
      "epoch": 292.3824959481361,
      "grad_norm": 1.2320690155029297,
      "learning_rate": 2.078718780408693e-05,
      "loss": 2.5657,
      "step": 902000
    },
    {
      "epoch": 292.41491085899514,
      "grad_norm": 1.4247677326202393,
      "learning_rate": 2.0783944210184886e-05,
      "loss": 2.5901,
      "step": 902100
    },
    {
      "epoch": 292.44732576985416,
      "grad_norm": 1.3448963165283203,
      "learning_rate": 2.0780700616282842e-05,
      "loss": 2.5507,
      "step": 902200
    },
    {
      "epoch": 292.4797406807131,
      "grad_norm": 1.4663699865341187,
      "learning_rate": 2.07774570223808e-05,
      "loss": 2.5865,
      "step": 902300
    },
    {
      "epoch": 292.51215559157214,
      "grad_norm": 1.9971036911010742,
      "learning_rate": 2.0774213428478756e-05,
      "loss": 2.5607,
      "step": 902400
    },
    {
      "epoch": 292.5445705024311,
      "grad_norm": 1.3593201637268066,
      "learning_rate": 2.077096983457671e-05,
      "loss": 2.5664,
      "step": 902500
    },
    {
      "epoch": 292.5769854132901,
      "grad_norm": 1.348190426826477,
      "learning_rate": 2.0767726240674667e-05,
      "loss": 2.5583,
      "step": 902600
    },
    {
      "epoch": 292.60940032414914,
      "grad_norm": 1.147711157798767,
      "learning_rate": 2.0764482646772625e-05,
      "loss": 2.5715,
      "step": 902700
    },
    {
      "epoch": 292.6418152350081,
      "grad_norm": 1.242457628250122,
      "learning_rate": 2.076123905287058e-05,
      "loss": 2.5683,
      "step": 902800
    },
    {
      "epoch": 292.6742301458671,
      "grad_norm": 1.127286434173584,
      "learning_rate": 2.075799545896854e-05,
      "loss": 2.5588,
      "step": 902900
    },
    {
      "epoch": 292.7066450567261,
      "grad_norm": 1.1553775072097778,
      "learning_rate": 2.0754751865066495e-05,
      "loss": 2.5787,
      "step": 903000
    },
    {
      "epoch": 292.7390599675851,
      "grad_norm": 1.4190982580184937,
      "learning_rate": 2.0751508271164453e-05,
      "loss": 2.5621,
      "step": 903100
    },
    {
      "epoch": 292.7714748784441,
      "grad_norm": 1.4806643724441528,
      "learning_rate": 2.0748297113201428e-05,
      "loss": 2.5768,
      "step": 903200
    },
    {
      "epoch": 292.80388978930307,
      "grad_norm": 1.3489567041397095,
      "learning_rate": 2.0745053519299383e-05,
      "loss": 2.5834,
      "step": 903300
    },
    {
      "epoch": 292.8363047001621,
      "grad_norm": 1.2875194549560547,
      "learning_rate": 2.074180992539734e-05,
      "loss": 2.5639,
      "step": 903400
    },
    {
      "epoch": 292.86871961102105,
      "grad_norm": 1.2719706296920776,
      "learning_rate": 2.0738566331495297e-05,
      "loss": 2.5862,
      "step": 903500
    },
    {
      "epoch": 292.90113452188007,
      "grad_norm": 1.1851739883422852,
      "learning_rate": 2.0735322737593256e-05,
      "loss": 2.564,
      "step": 903600
    },
    {
      "epoch": 292.9335494327391,
      "grad_norm": 1.2622263431549072,
      "learning_rate": 2.073211157963023e-05,
      "loss": 2.5594,
      "step": 903700
    },
    {
      "epoch": 292.96596434359805,
      "grad_norm": 1.1815470457077026,
      "learning_rate": 2.0728867985728186e-05,
      "loss": 2.5933,
      "step": 903800
    },
    {
      "epoch": 292.99837925445706,
      "grad_norm": 1.3291741609573364,
      "learning_rate": 2.0725624391826145e-05,
      "loss": 2.5879,
      "step": 903900
    },
    {
      "epoch": 293.0,
      "eval_bleu": 1.1983308031853948,
      "eval_loss": 4.149401664733887,
      "eval_runtime": 4.3785,
      "eval_samples_per_second": 112.366,
      "eval_steps_per_second": 1.827,
      "step": 903905
    },
    {
      "epoch": 293.030794165316,
      "grad_norm": 1.2063167095184326,
      "learning_rate": 2.07223807979241e-05,
      "loss": 2.5763,
      "step": 904000
    },
    {
      "epoch": 293.06320907617504,
      "grad_norm": 1.3148494958877563,
      "learning_rate": 2.071913720402206e-05,
      "loss": 2.5641,
      "step": 904100
    },
    {
      "epoch": 293.09562398703406,
      "grad_norm": 1.5295721292495728,
      "learning_rate": 2.0715893610120014e-05,
      "loss": 2.5637,
      "step": 904200
    },
    {
      "epoch": 293.128038897893,
      "grad_norm": 1.2775393724441528,
      "learning_rate": 2.0712650016217973e-05,
      "loss": 2.5941,
      "step": 904300
    },
    {
      "epoch": 293.16045380875204,
      "grad_norm": 1.470831274986267,
      "learning_rate": 2.0709406422315928e-05,
      "loss": 2.5569,
      "step": 904400
    },
    {
      "epoch": 293.192868719611,
      "grad_norm": 1.3625531196594238,
      "learning_rate": 2.0706162828413883e-05,
      "loss": 2.5465,
      "step": 904500
    },
    {
      "epoch": 293.22528363047,
      "grad_norm": 1.5417039394378662,
      "learning_rate": 2.070291923451184e-05,
      "loss": 2.5858,
      "step": 904600
    },
    {
      "epoch": 293.25769854132903,
      "grad_norm": 1.3206051588058472,
      "learning_rate": 2.0699675640609798e-05,
      "loss": 2.5784,
      "step": 904700
    },
    {
      "epoch": 293.290113452188,
      "grad_norm": 1.281402349472046,
      "learning_rate": 2.0696432046707753e-05,
      "loss": 2.5458,
      "step": 904800
    },
    {
      "epoch": 293.322528363047,
      "grad_norm": 1.3145424127578735,
      "learning_rate": 2.0693188452805708e-05,
      "loss": 2.5599,
      "step": 904900
    },
    {
      "epoch": 293.354943273906,
      "grad_norm": 1.4713934659957886,
      "learning_rate": 2.0689944858903667e-05,
      "loss": 2.5523,
      "step": 905000
    },
    {
      "epoch": 293.387358184765,
      "grad_norm": 1.392446517944336,
      "learning_rate": 2.0686701265001622e-05,
      "loss": 2.5738,
      "step": 905100
    },
    {
      "epoch": 293.419773095624,
      "grad_norm": 1.3818327188491821,
      "learning_rate": 2.0683457671099578e-05,
      "loss": 2.5642,
      "step": 905200
    },
    {
      "epoch": 293.45218800648297,
      "grad_norm": 1.3200339078903198,
      "learning_rate": 2.0680214077197536e-05,
      "loss": 2.5493,
      "step": 905300
    },
    {
      "epoch": 293.484602917342,
      "grad_norm": 1.4136594533920288,
      "learning_rate": 2.0676970483295495e-05,
      "loss": 2.5634,
      "step": 905400
    },
    {
      "epoch": 293.51701782820095,
      "grad_norm": 1.4109952449798584,
      "learning_rate": 2.067372688939345e-05,
      "loss": 2.5542,
      "step": 905500
    },
    {
      "epoch": 293.54943273905997,
      "grad_norm": 1.4220613241195679,
      "learning_rate": 2.0670483295491406e-05,
      "loss": 2.5811,
      "step": 905600
    },
    {
      "epoch": 293.581847649919,
      "grad_norm": 1.16846764087677,
      "learning_rate": 2.066723970158936e-05,
      "loss": 2.5702,
      "step": 905700
    },
    {
      "epoch": 293.61426256077795,
      "grad_norm": 1.4133399724960327,
      "learning_rate": 2.066399610768732e-05,
      "loss": 2.543,
      "step": 905800
    },
    {
      "epoch": 293.64667747163696,
      "grad_norm": 1.3616392612457275,
      "learning_rate": 2.0660752513785275e-05,
      "loss": 2.5591,
      "step": 905900
    },
    {
      "epoch": 293.6790923824959,
      "grad_norm": 1.3739713430404663,
      "learning_rate": 2.065750891988323e-05,
      "loss": 2.5946,
      "step": 906000
    },
    {
      "epoch": 293.71150729335494,
      "grad_norm": 1.335207223892212,
      "learning_rate": 2.0654265325981186e-05,
      "loss": 2.5667,
      "step": 906100
    },
    {
      "epoch": 293.74392220421396,
      "grad_norm": 1.338578224182129,
      "learning_rate": 2.0651021732079145e-05,
      "loss": 2.5663,
      "step": 906200
    },
    {
      "epoch": 293.7763371150729,
      "grad_norm": 1.2655614614486694,
      "learning_rate": 2.06477781381771e-05,
      "loss": 2.5767,
      "step": 906300
    },
    {
      "epoch": 293.80875202593194,
      "grad_norm": 1.3816828727722168,
      "learning_rate": 2.0644534544275056e-05,
      "loss": 2.5753,
      "step": 906400
    },
    {
      "epoch": 293.8411669367909,
      "grad_norm": 1.2944144010543823,
      "learning_rate": 2.0641290950373014e-05,
      "loss": 2.5642,
      "step": 906500
    },
    {
      "epoch": 293.8735818476499,
      "grad_norm": 1.432518720626831,
      "learning_rate": 2.0638047356470973e-05,
      "loss": 2.5895,
      "step": 906600
    },
    {
      "epoch": 293.90599675850893,
      "grad_norm": 1.5207982063293457,
      "learning_rate": 2.063480376256893e-05,
      "loss": 2.5717,
      "step": 906700
    },
    {
      "epoch": 293.9384116693679,
      "grad_norm": 1.2400394678115845,
      "learning_rate": 2.0631560168666884e-05,
      "loss": 2.5915,
      "step": 906800
    },
    {
      "epoch": 293.9708265802269,
      "grad_norm": 1.6002532243728638,
      "learning_rate": 2.0628316574764842e-05,
      "loss": 2.5995,
      "step": 906900
    },
    {
      "epoch": 294.0,
      "eval_bleu": 1.0507811918763474,
      "eval_loss": 4.150907516479492,
      "eval_runtime": 4.6531,
      "eval_samples_per_second": 105.736,
      "eval_steps_per_second": 1.719,
      "step": 906990
    },
    {
      "epoch": 294.0032414910859,
      "grad_norm": 1.2749418020248413,
      "learning_rate": 2.0625072980862798e-05,
      "loss": 2.5782,
      "step": 907000
    },
    {
      "epoch": 294.0356564019449,
      "grad_norm": 1.2872079610824585,
      "learning_rate": 2.0621829386960753e-05,
      "loss": 2.5571,
      "step": 907100
    },
    {
      "epoch": 294.0680713128039,
      "grad_norm": 1.2279454469680786,
      "learning_rate": 2.061858579305871e-05,
      "loss": 2.5538,
      "step": 907200
    },
    {
      "epoch": 294.10048622366287,
      "grad_norm": 1.571915626525879,
      "learning_rate": 2.0615342199156667e-05,
      "loss": 2.5769,
      "step": 907300
    },
    {
      "epoch": 294.1329011345219,
      "grad_norm": 1.13237726688385,
      "learning_rate": 2.0612098605254623e-05,
      "loss": 2.5892,
      "step": 907400
    },
    {
      "epoch": 294.16531604538085,
      "grad_norm": 1.4713860750198364,
      "learning_rate": 2.0608855011352578e-05,
      "loss": 2.5585,
      "step": 907500
    },
    {
      "epoch": 294.19773095623987,
      "grad_norm": 1.5319645404815674,
      "learning_rate": 2.0605611417450533e-05,
      "loss": 2.5671,
      "step": 907600
    },
    {
      "epoch": 294.2301458670989,
      "grad_norm": 1.5746376514434814,
      "learning_rate": 2.0602400259487515e-05,
      "loss": 2.563,
      "step": 907700
    },
    {
      "epoch": 294.26256077795784,
      "grad_norm": 1.3043893575668335,
      "learning_rate": 2.059915666558547e-05,
      "loss": 2.5675,
      "step": 907800
    },
    {
      "epoch": 294.29497568881686,
      "grad_norm": 1.378031611442566,
      "learning_rate": 2.0595945507622448e-05,
      "loss": 2.5522,
      "step": 907900
    },
    {
      "epoch": 294.3273905996758,
      "grad_norm": 1.2267978191375732,
      "learning_rate": 2.0592701913720403e-05,
      "loss": 2.561,
      "step": 908000
    },
    {
      "epoch": 294.35980551053484,
      "grad_norm": 1.56734037399292,
      "learning_rate": 2.0589458319818362e-05,
      "loss": 2.5572,
      "step": 908100
    },
    {
      "epoch": 294.39222042139386,
      "grad_norm": 1.3879350423812866,
      "learning_rate": 2.0586214725916317e-05,
      "loss": 2.5614,
      "step": 908200
    },
    {
      "epoch": 294.4246353322528,
      "grad_norm": 1.3356767892837524,
      "learning_rate": 2.0582971132014273e-05,
      "loss": 2.5756,
      "step": 908300
    },
    {
      "epoch": 294.45705024311184,
      "grad_norm": 1.2266004085540771,
      "learning_rate": 2.0579727538112228e-05,
      "loss": 2.5625,
      "step": 908400
    },
    {
      "epoch": 294.48946515397085,
      "grad_norm": 1.3897051811218262,
      "learning_rate": 2.0576483944210187e-05,
      "loss": 2.5759,
      "step": 908500
    },
    {
      "epoch": 294.5218800648298,
      "grad_norm": 1.2060542106628418,
      "learning_rate": 2.0573240350308142e-05,
      "loss": 2.5787,
      "step": 908600
    },
    {
      "epoch": 294.55429497568883,
      "grad_norm": 1.3025705814361572,
      "learning_rate": 2.0569996756406097e-05,
      "loss": 2.5688,
      "step": 908700
    },
    {
      "epoch": 294.5867098865478,
      "grad_norm": 1.4404395818710327,
      "learning_rate": 2.0566785598443075e-05,
      "loss": 2.5669,
      "step": 908800
    },
    {
      "epoch": 294.6191247974068,
      "grad_norm": 1.5122073888778687,
      "learning_rate": 2.0563542004541034e-05,
      "loss": 2.5869,
      "step": 908900
    },
    {
      "epoch": 294.65153970826583,
      "grad_norm": 1.2893797159194946,
      "learning_rate": 2.056029841063899e-05,
      "loss": 2.5692,
      "step": 909000
    },
    {
      "epoch": 294.6839546191248,
      "grad_norm": 1.2275452613830566,
      "learning_rate": 2.0557054816736945e-05,
      "loss": 2.5855,
      "step": 909100
    },
    {
      "epoch": 294.7163695299838,
      "grad_norm": 1.2048202753067017,
      "learning_rate": 2.05538112228349e-05,
      "loss": 2.569,
      "step": 909200
    },
    {
      "epoch": 294.74878444084277,
      "grad_norm": 1.4309606552124023,
      "learning_rate": 2.055056762893286e-05,
      "loss": 2.5766,
      "step": 909300
    },
    {
      "epoch": 294.7811993517018,
      "grad_norm": 1.2348185777664185,
      "learning_rate": 2.0547324035030814e-05,
      "loss": 2.5612,
      "step": 909400
    },
    {
      "epoch": 294.8136142625608,
      "grad_norm": 1.279672622680664,
      "learning_rate": 2.0544080441128773e-05,
      "loss": 2.5773,
      "step": 909500
    },
    {
      "epoch": 294.84602917341977,
      "grad_norm": 1.3269226551055908,
      "learning_rate": 2.0540836847226728e-05,
      "loss": 2.574,
      "step": 909600
    },
    {
      "epoch": 294.8784440842788,
      "grad_norm": 1.4215307235717773,
      "learning_rate": 2.0537593253324687e-05,
      "loss": 2.5693,
      "step": 909700
    },
    {
      "epoch": 294.91085899513774,
      "grad_norm": 1.210098147392273,
      "learning_rate": 2.0534349659422642e-05,
      "loss": 2.5788,
      "step": 909800
    },
    {
      "epoch": 294.94327390599676,
      "grad_norm": 1.5481444597244263,
      "learning_rate": 2.0531106065520598e-05,
      "loss": 2.5717,
      "step": 909900
    },
    {
      "epoch": 294.9756888168558,
      "grad_norm": 1.1222221851348877,
      "learning_rate": 2.0527862471618553e-05,
      "loss": 2.569,
      "step": 910000
    },
    {
      "epoch": 295.0,
      "eval_bleu": 1.1167579156747196,
      "eval_loss": 4.151857376098633,
      "eval_runtime": 4.4453,
      "eval_samples_per_second": 110.679,
      "eval_steps_per_second": 1.8,
      "step": 910075
    },
    {
      "epoch": 295.00810372771474,
      "grad_norm": 1.2960988283157349,
      "learning_rate": 2.052461887771651e-05,
      "loss": 2.5727,
      "step": 910100
    },
    {
      "epoch": 295.04051863857376,
      "grad_norm": 1.2329407930374146,
      "learning_rate": 2.0521375283814467e-05,
      "loss": 2.5577,
      "step": 910200
    },
    {
      "epoch": 295.0729335494327,
      "grad_norm": 1.2213045358657837,
      "learning_rate": 2.0518131689912422e-05,
      "loss": 2.5542,
      "step": 910300
    },
    {
      "epoch": 295.10534846029174,
      "grad_norm": 1.2520290613174438,
      "learning_rate": 2.051488809601038e-05,
      "loss": 2.5622,
      "step": 910400
    },
    {
      "epoch": 295.13776337115075,
      "grad_norm": 1.4653093814849854,
      "learning_rate": 2.0511644502108336e-05,
      "loss": 2.5656,
      "step": 910500
    },
    {
      "epoch": 295.1701782820097,
      "grad_norm": 1.4487483501434326,
      "learning_rate": 2.0508400908206292e-05,
      "loss": 2.5502,
      "step": 910600
    },
    {
      "epoch": 295.20259319286873,
      "grad_norm": 1.297039270401001,
      "learning_rate": 2.050515731430425e-05,
      "loss": 2.5684,
      "step": 910700
    },
    {
      "epoch": 295.2350081037277,
      "grad_norm": 1.3852441310882568,
      "learning_rate": 2.050191372040221e-05,
      "loss": 2.5643,
      "step": 910800
    },
    {
      "epoch": 295.2674230145867,
      "grad_norm": 1.4613863229751587,
      "learning_rate": 2.0498670126500165e-05,
      "loss": 2.552,
      "step": 910900
    },
    {
      "epoch": 295.29983792544573,
      "grad_norm": 1.2002556324005127,
      "learning_rate": 2.049542653259812e-05,
      "loss": 2.5676,
      "step": 911000
    },
    {
      "epoch": 295.3322528363047,
      "grad_norm": 1.2977725267410278,
      "learning_rate": 2.0492182938696075e-05,
      "loss": 2.5748,
      "step": 911100
    },
    {
      "epoch": 295.3646677471637,
      "grad_norm": 1.5823637247085571,
      "learning_rate": 2.0488939344794034e-05,
      "loss": 2.5663,
      "step": 911200
    },
    {
      "epoch": 295.39708265802267,
      "grad_norm": 1.2668657302856445,
      "learning_rate": 2.048569575089199e-05,
      "loss": 2.565,
      "step": 911300
    },
    {
      "epoch": 295.4294975688817,
      "grad_norm": 1.2701958417892456,
      "learning_rate": 2.0482452156989945e-05,
      "loss": 2.5639,
      "step": 911400
    },
    {
      "epoch": 295.4619124797407,
      "grad_norm": 1.2821664810180664,
      "learning_rate": 2.04792085630879e-05,
      "loss": 2.5701,
      "step": 911500
    },
    {
      "epoch": 295.49432739059966,
      "grad_norm": 1.5642181634902954,
      "learning_rate": 2.047596496918586e-05,
      "loss": 2.5742,
      "step": 911600
    },
    {
      "epoch": 295.5267423014587,
      "grad_norm": 1.2252525091171265,
      "learning_rate": 2.0472721375283814e-05,
      "loss": 2.5597,
      "step": 911700
    },
    {
      "epoch": 295.55915721231764,
      "grad_norm": 1.8100643157958984,
      "learning_rate": 2.046947778138177e-05,
      "loss": 2.5724,
      "step": 911800
    },
    {
      "epoch": 295.59157212317666,
      "grad_norm": 1.2875081300735474,
      "learning_rate": 2.046623418747973e-05,
      "loss": 2.5856,
      "step": 911900
    },
    {
      "epoch": 295.6239870340357,
      "grad_norm": 1.5803812742233276,
      "learning_rate": 2.0462990593577687e-05,
      "loss": 2.5645,
      "step": 912000
    },
    {
      "epoch": 295.65640194489464,
      "grad_norm": 1.19295072555542,
      "learning_rate": 2.0459746999675642e-05,
      "loss": 2.5824,
      "step": 912100
    },
    {
      "epoch": 295.68881685575366,
      "grad_norm": 1.489140272140503,
      "learning_rate": 2.0456503405773598e-05,
      "loss": 2.5981,
      "step": 912200
    },
    {
      "epoch": 295.7212317666126,
      "grad_norm": 1.1917686462402344,
      "learning_rate": 2.0453259811871557e-05,
      "loss": 2.5492,
      "step": 912300
    },
    {
      "epoch": 295.75364667747164,
      "grad_norm": 1.2954775094985962,
      "learning_rate": 2.0450016217969512e-05,
      "loss": 2.5688,
      "step": 912400
    },
    {
      "epoch": 295.78606158833065,
      "grad_norm": 1.3148213624954224,
      "learning_rate": 2.0446772624067467e-05,
      "loss": 2.5472,
      "step": 912500
    },
    {
      "epoch": 295.8184764991896,
      "grad_norm": 1.3623061180114746,
      "learning_rate": 2.0443529030165423e-05,
      "loss": 2.5782,
      "step": 912600
    },
    {
      "epoch": 295.85089141004863,
      "grad_norm": 1.1625741720199585,
      "learning_rate": 2.044028543626338e-05,
      "loss": 2.5638,
      "step": 912700
    },
    {
      "epoch": 295.8833063209076,
      "grad_norm": 1.3009090423583984,
      "learning_rate": 2.0437041842361337e-05,
      "loss": 2.5689,
      "step": 912800
    },
    {
      "epoch": 295.9157212317666,
      "grad_norm": 1.3554282188415527,
      "learning_rate": 2.0433798248459292e-05,
      "loss": 2.5816,
      "step": 912900
    },
    {
      "epoch": 295.94813614262563,
      "grad_norm": 1.31361985206604,
      "learning_rate": 2.0430554654557247e-05,
      "loss": 2.5892,
      "step": 913000
    },
    {
      "epoch": 295.9805510534846,
      "grad_norm": 1.5196802616119385,
      "learning_rate": 2.0427311060655206e-05,
      "loss": 2.5665,
      "step": 913100
    },
    {
      "epoch": 296.0,
      "eval_bleu": 1.0580842652174793,
      "eval_loss": 4.146989822387695,
      "eval_runtime": 4.5143,
      "eval_samples_per_second": 108.987,
      "eval_steps_per_second": 1.772,
      "step": 913160
    },
    {
      "epoch": 296.0129659643436,
      "grad_norm": 1.2265089750289917,
      "learning_rate": 2.0424067466753165e-05,
      "loss": 2.5673,
      "step": 913200
    },
    {
      "epoch": 296.04538087520257,
      "grad_norm": 1.4953407049179077,
      "learning_rate": 2.042082387285112e-05,
      "loss": 2.5458,
      "step": 913300
    },
    {
      "epoch": 296.0777957860616,
      "grad_norm": 1.3156546354293823,
      "learning_rate": 2.041758027894908e-05,
      "loss": 2.5453,
      "step": 913400
    },
    {
      "epoch": 296.1102106969206,
      "grad_norm": 1.2753643989562988,
      "learning_rate": 2.0414336685047034e-05,
      "loss": 2.5448,
      "step": 913500
    },
    {
      "epoch": 296.14262560777956,
      "grad_norm": 1.3569854497909546,
      "learning_rate": 2.041109309114499e-05,
      "loss": 2.5871,
      "step": 913600
    },
    {
      "epoch": 296.1750405186386,
      "grad_norm": 1.4058369398117065,
      "learning_rate": 2.0407849497242945e-05,
      "loss": 2.5512,
      "step": 913700
    },
    {
      "epoch": 296.20745542949754,
      "grad_norm": 1.2996212244033813,
      "learning_rate": 2.0404605903340904e-05,
      "loss": 2.5473,
      "step": 913800
    },
    {
      "epoch": 296.23987034035656,
      "grad_norm": 1.4866814613342285,
      "learning_rate": 2.040136230943886e-05,
      "loss": 2.5612,
      "step": 913900
    },
    {
      "epoch": 296.2722852512156,
      "grad_norm": 1.5338940620422363,
      "learning_rate": 2.0398118715536815e-05,
      "loss": 2.57,
      "step": 914000
    },
    {
      "epoch": 296.30470016207454,
      "grad_norm": 1.2712024450302124,
      "learning_rate": 2.039487512163477e-05,
      "loss": 2.5775,
      "step": 914100
    },
    {
      "epoch": 296.33711507293356,
      "grad_norm": 1.3020310401916504,
      "learning_rate": 2.039163152773273e-05,
      "loss": 2.5709,
      "step": 914200
    },
    {
      "epoch": 296.3695299837925,
      "grad_norm": 1.3712620735168457,
      "learning_rate": 2.0388387933830684e-05,
      "loss": 2.559,
      "step": 914300
    },
    {
      "epoch": 296.40194489465154,
      "grad_norm": 1.2816747426986694,
      "learning_rate": 2.0385144339928643e-05,
      "loss": 2.5781,
      "step": 914400
    },
    {
      "epoch": 296.43435980551055,
      "grad_norm": 1.3131643533706665,
      "learning_rate": 2.03819007460266e-05,
      "loss": 2.557,
      "step": 914500
    },
    {
      "epoch": 296.4667747163695,
      "grad_norm": 1.431861400604248,
      "learning_rate": 2.0378657152124557e-05,
      "loss": 2.573,
      "step": 914600
    },
    {
      "epoch": 296.49918962722853,
      "grad_norm": 1.1887476444244385,
      "learning_rate": 2.0375413558222512e-05,
      "loss": 2.5756,
      "step": 914700
    },
    {
      "epoch": 296.5316045380875,
      "grad_norm": 1.34093177318573,
      "learning_rate": 2.0372202400259487e-05,
      "loss": 2.5681,
      "step": 914800
    },
    {
      "epoch": 296.5640194489465,
      "grad_norm": 1.179400086402893,
      "learning_rate": 2.0368958806357445e-05,
      "loss": 2.5807,
      "step": 914900
    },
    {
      "epoch": 296.5964343598055,
      "grad_norm": 1.432648777961731,
      "learning_rate": 2.03657152124554e-05,
      "loss": 2.5729,
      "step": 915000
    },
    {
      "epoch": 296.6288492706645,
      "grad_norm": 1.368485927581787,
      "learning_rate": 2.036247161855336e-05,
      "loss": 2.5696,
      "step": 915100
    },
    {
      "epoch": 296.6612641815235,
      "grad_norm": 1.232913851737976,
      "learning_rate": 2.0359228024651315e-05,
      "loss": 2.5752,
      "step": 915200
    },
    {
      "epoch": 296.6936790923825,
      "grad_norm": 1.4572103023529053,
      "learning_rate": 2.0355984430749274e-05,
      "loss": 2.5637,
      "step": 915300
    },
    {
      "epoch": 296.7260940032415,
      "grad_norm": 1.5542728900909424,
      "learning_rate": 2.035274083684723e-05,
      "loss": 2.587,
      "step": 915400
    },
    {
      "epoch": 296.7585089141005,
      "grad_norm": 1.3433682918548584,
      "learning_rate": 2.0349497242945184e-05,
      "loss": 2.5681,
      "step": 915500
    },
    {
      "epoch": 296.79092382495946,
      "grad_norm": 1.2304916381835938,
      "learning_rate": 2.034625364904314e-05,
      "loss": 2.5628,
      "step": 915600
    },
    {
      "epoch": 296.8233387358185,
      "grad_norm": 1.4667035341262817,
      "learning_rate": 2.03430100551411e-05,
      "loss": 2.5721,
      "step": 915700
    },
    {
      "epoch": 296.8557536466775,
      "grad_norm": 1.3187943696975708,
      "learning_rate": 2.0339766461239054e-05,
      "loss": 2.5705,
      "step": 915800
    },
    {
      "epoch": 296.88816855753646,
      "grad_norm": 1.224928855895996,
      "learning_rate": 2.033652286733701e-05,
      "loss": 2.5745,
      "step": 915900
    },
    {
      "epoch": 296.9205834683955,
      "grad_norm": 1.4058115482330322,
      "learning_rate": 2.0333279273434964e-05,
      "loss": 2.5546,
      "step": 916000
    },
    {
      "epoch": 296.95299837925444,
      "grad_norm": 1.2904772758483887,
      "learning_rate": 2.0330035679532923e-05,
      "loss": 2.5656,
      "step": 916100
    },
    {
      "epoch": 296.98541329011346,
      "grad_norm": 1.2536472082138062,
      "learning_rate": 2.0326792085630882e-05,
      "loss": 2.585,
      "step": 916200
    },
    {
      "epoch": 297.0,
      "eval_bleu": 0.9475381581864301,
      "eval_loss": 4.154513835906982,
      "eval_runtime": 4.1878,
      "eval_samples_per_second": 117.484,
      "eval_steps_per_second": 1.91,
      "step": 916245
    },
    {
      "epoch": 297.0178282009725,
      "grad_norm": 1.3668571710586548,
      "learning_rate": 2.0323580927667856e-05,
      "loss": 2.556,
      "step": 916300
    },
    {
      "epoch": 297.05024311183143,
      "grad_norm": 1.2828916311264038,
      "learning_rate": 2.032033733376581e-05,
      "loss": 2.5758,
      "step": 916400
    },
    {
      "epoch": 297.08265802269045,
      "grad_norm": 1.3233383893966675,
      "learning_rate": 2.031709373986377e-05,
      "loss": 2.5784,
      "step": 916500
    },
    {
      "epoch": 297.1150729335494,
      "grad_norm": 1.5558550357818604,
      "learning_rate": 2.0313850145961726e-05,
      "loss": 2.5548,
      "step": 916600
    },
    {
      "epoch": 297.14748784440843,
      "grad_norm": 1.3332581520080566,
      "learning_rate": 2.031060655205968e-05,
      "loss": 2.5809,
      "step": 916700
    },
    {
      "epoch": 297.17990275526745,
      "grad_norm": 1.3997405767440796,
      "learning_rate": 2.030736295815764e-05,
      "loss": 2.5758,
      "step": 916800
    },
    {
      "epoch": 297.2123176661264,
      "grad_norm": 1.3536453247070312,
      "learning_rate": 2.03041193642556e-05,
      "loss": 2.5638,
      "step": 916900
    },
    {
      "epoch": 297.2447325769854,
      "grad_norm": 1.1969773769378662,
      "learning_rate": 2.0300875770353554e-05,
      "loss": 2.545,
      "step": 917000
    },
    {
      "epoch": 297.2771474878444,
      "grad_norm": 1.377406358718872,
      "learning_rate": 2.029763217645151e-05,
      "loss": 2.54,
      "step": 917100
    },
    {
      "epoch": 297.3095623987034,
      "grad_norm": 1.3008977174758911,
      "learning_rate": 2.0294388582549465e-05,
      "loss": 2.5698,
      "step": 917200
    },
    {
      "epoch": 297.3419773095624,
      "grad_norm": 1.3474159240722656,
      "learning_rate": 2.0291144988647423e-05,
      "loss": 2.5501,
      "step": 917300
    },
    {
      "epoch": 297.3743922204214,
      "grad_norm": 1.225825548171997,
      "learning_rate": 2.028790139474538e-05,
      "loss": 2.5627,
      "step": 917400
    },
    {
      "epoch": 297.4068071312804,
      "grad_norm": 1.2254222631454468,
      "learning_rate": 2.0284657800843334e-05,
      "loss": 2.5675,
      "step": 917500
    },
    {
      "epoch": 297.43922204213936,
      "grad_norm": 1.3305721282958984,
      "learning_rate": 2.0281414206941293e-05,
      "loss": 2.5535,
      "step": 917600
    },
    {
      "epoch": 297.4716369529984,
      "grad_norm": 1.2608433961868286,
      "learning_rate": 2.0278170613039248e-05,
      "loss": 2.5854,
      "step": 917700
    },
    {
      "epoch": 297.5040518638574,
      "grad_norm": 1.5208539962768555,
      "learning_rate": 2.0274927019137204e-05,
      "loss": 2.5683,
      "step": 917800
    },
    {
      "epoch": 297.53646677471636,
      "grad_norm": 1.1304386854171753,
      "learning_rate": 2.027168342523516e-05,
      "loss": 2.5814,
      "step": 917900
    },
    {
      "epoch": 297.5688816855754,
      "grad_norm": 1.5142812728881836,
      "learning_rate": 2.0268439831333118e-05,
      "loss": 2.5744,
      "step": 918000
    },
    {
      "epoch": 297.60129659643434,
      "grad_norm": 1.4533734321594238,
      "learning_rate": 2.0265196237431076e-05,
      "loss": 2.568,
      "step": 918100
    },
    {
      "epoch": 297.63371150729336,
      "grad_norm": 1.466752052307129,
      "learning_rate": 2.0261952643529032e-05,
      "loss": 2.565,
      "step": 918200
    },
    {
      "epoch": 297.6661264181524,
      "grad_norm": 1.3929523229599,
      "learning_rate": 2.0258709049626987e-05,
      "loss": 2.5626,
      "step": 918300
    },
    {
      "epoch": 297.69854132901133,
      "grad_norm": 1.3534460067749023,
      "learning_rate": 2.0255465455724946e-05,
      "loss": 2.5588,
      "step": 918400
    },
    {
      "epoch": 297.73095623987035,
      "grad_norm": 1.3173249959945679,
      "learning_rate": 2.02522218618229e-05,
      "loss": 2.5687,
      "step": 918500
    },
    {
      "epoch": 297.7633711507293,
      "grad_norm": 1.3662105798721313,
      "learning_rate": 2.024901070385988e-05,
      "loss": 2.5674,
      "step": 918600
    },
    {
      "epoch": 297.79578606158833,
      "grad_norm": 1.234963059425354,
      "learning_rate": 2.0245767109957834e-05,
      "loss": 2.554,
      "step": 918700
    },
    {
      "epoch": 297.82820097244735,
      "grad_norm": 1.2653273344039917,
      "learning_rate": 2.0242523516055793e-05,
      "loss": 2.5585,
      "step": 918800
    },
    {
      "epoch": 297.8606158833063,
      "grad_norm": 1.3414043188095093,
      "learning_rate": 2.023927992215375e-05,
      "loss": 2.5801,
      "step": 918900
    },
    {
      "epoch": 297.8930307941653,
      "grad_norm": 1.1903427839279175,
      "learning_rate": 2.0236036328251704e-05,
      "loss": 2.5583,
      "step": 919000
    },
    {
      "epoch": 297.9254457050243,
      "grad_norm": 1.318324327468872,
      "learning_rate": 2.023279273434966e-05,
      "loss": 2.5611,
      "step": 919100
    },
    {
      "epoch": 297.9578606158833,
      "grad_norm": 1.2581497430801392,
      "learning_rate": 2.0229549140447618e-05,
      "loss": 2.5721,
      "step": 919200
    },
    {
      "epoch": 297.9902755267423,
      "grad_norm": 1.3940563201904297,
      "learning_rate": 2.0226305546545573e-05,
      "loss": 2.5777,
      "step": 919300
    },
    {
      "epoch": 298.0,
      "eval_bleu": 0.9537245060600384,
      "eval_loss": 4.156601905822754,
      "eval_runtime": 4.4142,
      "eval_samples_per_second": 111.458,
      "eval_steps_per_second": 1.812,
      "step": 919330
    },
    {
      "epoch": 298.0226904376013,
      "grad_norm": 1.176676630973816,
      "learning_rate": 2.022306195264353e-05,
      "loss": 2.5356,
      "step": 919400
    },
    {
      "epoch": 298.0551053484603,
      "grad_norm": 1.2232959270477295,
      "learning_rate": 2.0219818358741484e-05,
      "loss": 2.5695,
      "step": 919500
    },
    {
      "epoch": 298.08752025931926,
      "grad_norm": 1.4500881433486938,
      "learning_rate": 2.0216574764839443e-05,
      "loss": 2.5687,
      "step": 919600
    },
    {
      "epoch": 298.1199351701783,
      "grad_norm": 1.4675859212875366,
      "learning_rate": 2.0213331170937398e-05,
      "loss": 2.5617,
      "step": 919700
    },
    {
      "epoch": 298.1523500810373,
      "grad_norm": 1.2806710004806519,
      "learning_rate": 2.0210087577035357e-05,
      "loss": 2.5431,
      "step": 919800
    },
    {
      "epoch": 298.18476499189626,
      "grad_norm": 1.4812908172607422,
      "learning_rate": 2.0206843983133316e-05,
      "loss": 2.5623,
      "step": 919900
    },
    {
      "epoch": 298.2171799027553,
      "grad_norm": 1.2045096158981323,
      "learning_rate": 2.020360038923127e-05,
      "loss": 2.5673,
      "step": 920000
    },
    {
      "epoch": 298.24959481361424,
      "grad_norm": 1.536803960800171,
      "learning_rate": 2.0200356795329226e-05,
      "loss": 2.5577,
      "step": 920100
    },
    {
      "epoch": 298.28200972447326,
      "grad_norm": 1.1600136756896973,
      "learning_rate": 2.019711320142718e-05,
      "loss": 2.5679,
      "step": 920200
    },
    {
      "epoch": 298.3144246353323,
      "grad_norm": 1.4016023874282837,
      "learning_rate": 2.019386960752514e-05,
      "loss": 2.5639,
      "step": 920300
    },
    {
      "epoch": 298.34683954619123,
      "grad_norm": 1.3040771484375,
      "learning_rate": 2.0190626013623096e-05,
      "loss": 2.594,
      "step": 920400
    },
    {
      "epoch": 298.37925445705025,
      "grad_norm": 1.2836823463439941,
      "learning_rate": 2.018738241972105e-05,
      "loss": 2.5417,
      "step": 920500
    },
    {
      "epoch": 298.4116693679092,
      "grad_norm": 1.2389110326766968,
      "learning_rate": 2.0184138825819006e-05,
      "loss": 2.5669,
      "step": 920600
    },
    {
      "epoch": 298.44408427876823,
      "grad_norm": 1.226083517074585,
      "learning_rate": 2.0180895231916965e-05,
      "loss": 2.5738,
      "step": 920700
    },
    {
      "epoch": 298.47649918962725,
      "grad_norm": 1.2674473524093628,
      "learning_rate": 2.017765163801492e-05,
      "loss": 2.5529,
      "step": 920800
    },
    {
      "epoch": 298.5089141004862,
      "grad_norm": 1.145298719406128,
      "learning_rate": 2.0174408044112876e-05,
      "loss": 2.5744,
      "step": 920900
    },
    {
      "epoch": 298.5413290113452,
      "grad_norm": 1.2625737190246582,
      "learning_rate": 2.0171164450210835e-05,
      "loss": 2.5657,
      "step": 921000
    },
    {
      "epoch": 298.5737439222042,
      "grad_norm": 1.2605098485946655,
      "learning_rate": 2.0167920856308793e-05,
      "loss": 2.5455,
      "step": 921100
    },
    {
      "epoch": 298.6061588330632,
      "grad_norm": 1.2317562103271484,
      "learning_rate": 2.016467726240675e-05,
      "loss": 2.5753,
      "step": 921200
    },
    {
      "epoch": 298.6385737439222,
      "grad_norm": 1.4134242534637451,
      "learning_rate": 2.0161433668504704e-05,
      "loss": 2.5367,
      "step": 921300
    },
    {
      "epoch": 298.6709886547812,
      "grad_norm": 1.2969800233840942,
      "learning_rate": 2.0158190074602663e-05,
      "loss": 2.5795,
      "step": 921400
    },
    {
      "epoch": 298.7034035656402,
      "grad_norm": 1.247456669807434,
      "learning_rate": 2.0154946480700618e-05,
      "loss": 2.5586,
      "step": 921500
    },
    {
      "epoch": 298.73581847649916,
      "grad_norm": 1.379944086074829,
      "learning_rate": 2.0151702886798574e-05,
      "loss": 2.5684,
      "step": 921600
    },
    {
      "epoch": 298.7682333873582,
      "grad_norm": 1.1111700534820557,
      "learning_rate": 2.014845929289653e-05,
      "loss": 2.567,
      "step": 921700
    },
    {
      "epoch": 298.8006482982172,
      "grad_norm": 1.221230149269104,
      "learning_rate": 2.0145215698994488e-05,
      "loss": 2.5874,
      "step": 921800
    },
    {
      "epoch": 298.83306320907616,
      "grad_norm": 1.3763561248779297,
      "learning_rate": 2.0141972105092443e-05,
      "loss": 2.5858,
      "step": 921900
    },
    {
      "epoch": 298.8654781199352,
      "grad_norm": 1.4004881381988525,
      "learning_rate": 2.01387285111904e-05,
      "loss": 2.5529,
      "step": 922000
    },
    {
      "epoch": 298.8978930307942,
      "grad_norm": 1.384999394416809,
      "learning_rate": 2.0135517353227376e-05,
      "loss": 2.5794,
      "step": 922100
    },
    {
      "epoch": 298.93030794165315,
      "grad_norm": 1.5667585134506226,
      "learning_rate": 2.0132273759325335e-05,
      "loss": 2.5673,
      "step": 922200
    },
    {
      "epoch": 298.9627228525122,
      "grad_norm": 1.4971214532852173,
      "learning_rate": 2.012903016542329e-05,
      "loss": 2.5448,
      "step": 922300
    },
    {
      "epoch": 298.99513776337113,
      "grad_norm": 1.4155261516571045,
      "learning_rate": 2.0125786571521246e-05,
      "loss": 2.5699,
      "step": 922400
    },
    {
      "epoch": 299.0,
      "eval_bleu": 0.9921183283019904,
      "eval_loss": 4.158862590789795,
      "eval_runtime": 4.3932,
      "eval_samples_per_second": 111.991,
      "eval_steps_per_second": 1.821,
      "step": 922415
    },
    {
      "epoch": 299.02755267423015,
      "grad_norm": 1.2239060401916504,
      "learning_rate": 2.01225429776192e-05,
      "loss": 2.5343,
      "step": 922500
    },
    {
      "epoch": 299.05996758508917,
      "grad_norm": 1.4186722040176392,
      "learning_rate": 2.011929938371716e-05,
      "loss": 2.5535,
      "step": 922600
    },
    {
      "epoch": 299.09238249594813,
      "grad_norm": 1.3125414848327637,
      "learning_rate": 2.0116055789815115e-05,
      "loss": 2.5427,
      "step": 922700
    },
    {
      "epoch": 299.12479740680715,
      "grad_norm": 1.3337361812591553,
      "learning_rate": 2.0112812195913074e-05,
      "loss": 2.5794,
      "step": 922800
    },
    {
      "epoch": 299.1572123176661,
      "grad_norm": 1.2104811668395996,
      "learning_rate": 2.010956860201103e-05,
      "loss": 2.549,
      "step": 922900
    },
    {
      "epoch": 299.1896272285251,
      "grad_norm": 1.2411407232284546,
      "learning_rate": 2.0106325008108988e-05,
      "loss": 2.5559,
      "step": 923000
    },
    {
      "epoch": 299.22204213938414,
      "grad_norm": 1.4935566186904907,
      "learning_rate": 2.0103081414206943e-05,
      "loss": 2.5518,
      "step": 923100
    },
    {
      "epoch": 299.2544570502431,
      "grad_norm": 1.2099945545196533,
      "learning_rate": 2.00998378203049e-05,
      "loss": 2.5606,
      "step": 923200
    },
    {
      "epoch": 299.2868719611021,
      "grad_norm": 1.3980011940002441,
      "learning_rate": 2.0096594226402854e-05,
      "loss": 2.5514,
      "step": 923300
    },
    {
      "epoch": 299.3192868719611,
      "grad_norm": 1.2031288146972656,
      "learning_rate": 2.0093350632500813e-05,
      "loss": 2.5573,
      "step": 923400
    },
    {
      "epoch": 299.3517017828201,
      "grad_norm": 1.281166434288025,
      "learning_rate": 2.0090107038598768e-05,
      "loss": 2.5619,
      "step": 923500
    },
    {
      "epoch": 299.3841166936791,
      "grad_norm": 1.209745168685913,
      "learning_rate": 2.0086863444696723e-05,
      "loss": 2.568,
      "step": 923600
    },
    {
      "epoch": 299.4165316045381,
      "grad_norm": 1.4577267169952393,
      "learning_rate": 2.0083619850794682e-05,
      "loss": 2.57,
      "step": 923700
    },
    {
      "epoch": 299.4489465153971,
      "grad_norm": 1.3798483610153198,
      "learning_rate": 2.0080376256892638e-05,
      "loss": 2.5667,
      "step": 923800
    },
    {
      "epoch": 299.48136142625606,
      "grad_norm": 1.543210506439209,
      "learning_rate": 2.0077132662990593e-05,
      "loss": 2.5739,
      "step": 923900
    },
    {
      "epoch": 299.5137763371151,
      "grad_norm": 1.4025895595550537,
      "learning_rate": 2.007388906908855e-05,
      "loss": 2.5497,
      "step": 924000
    },
    {
      "epoch": 299.5461912479741,
      "grad_norm": 1.3631060123443604,
      "learning_rate": 2.0070645475186507e-05,
      "loss": 2.5653,
      "step": 924100
    },
    {
      "epoch": 299.57860615883305,
      "grad_norm": 1.5112229585647583,
      "learning_rate": 2.0067401881284466e-05,
      "loss": 2.5764,
      "step": 924200
    },
    {
      "epoch": 299.6110210696921,
      "grad_norm": 1.2248104810714722,
      "learning_rate": 2.006415828738242e-05,
      "loss": 2.5755,
      "step": 924300
    },
    {
      "epoch": 299.64343598055103,
      "grad_norm": 1.2783910036087036,
      "learning_rate": 2.0060914693480376e-05,
      "loss": 2.5652,
      "step": 924400
    },
    {
      "epoch": 299.67585089141005,
      "grad_norm": 1.181252360343933,
      "learning_rate": 2.0057671099578335e-05,
      "loss": 2.5384,
      "step": 924500
    },
    {
      "epoch": 299.70826580226907,
      "grad_norm": 1.3080965280532837,
      "learning_rate": 2.005442750567629e-05,
      "loss": 2.5907,
      "step": 924600
    },
    {
      "epoch": 299.74068071312803,
      "grad_norm": 1.5114942789077759,
      "learning_rate": 2.0051183911774246e-05,
      "loss": 2.5651,
      "step": 924700
    },
    {
      "epoch": 299.77309562398705,
      "grad_norm": 1.4929609298706055,
      "learning_rate": 2.0047940317872205e-05,
      "loss": 2.5539,
      "step": 924800
    },
    {
      "epoch": 299.805510534846,
      "grad_norm": 1.2809962034225464,
      "learning_rate": 2.004469672397016e-05,
      "loss": 2.5645,
      "step": 924900
    },
    {
      "epoch": 299.837925445705,
      "grad_norm": 1.546106219291687,
      "learning_rate": 2.0041453130068115e-05,
      "loss": 2.568,
      "step": 925000
    },
    {
      "epoch": 299.87034035656404,
      "grad_norm": 1.3091195821762085,
      "learning_rate": 2.003820953616607e-05,
      "loss": 2.578,
      "step": 925100
    },
    {
      "epoch": 299.902755267423,
      "grad_norm": 1.300657868385315,
      "learning_rate": 2.003496594226403e-05,
      "loss": 2.5802,
      "step": 925200
    },
    {
      "epoch": 299.935170178282,
      "grad_norm": 1.0723754167556763,
      "learning_rate": 2.0031722348361985e-05,
      "loss": 2.5718,
      "step": 925300
    },
    {
      "epoch": 299.967585089141,
      "grad_norm": 1.6014125347137451,
      "learning_rate": 2.0028478754459944e-05,
      "loss": 2.5568,
      "step": 925400
    },
    {
      "epoch": 300.0,
      "grad_norm": 1.5119946002960205,
      "learning_rate": 2.00252351605579e-05,
      "loss": 2.577,
      "step": 925500
    },
    {
      "epoch": 300.0,
      "eval_bleu": 1.0151289447201133,
      "eval_loss": 4.156699180603027,
      "eval_runtime": 4.6414,
      "eval_samples_per_second": 106.001,
      "eval_steps_per_second": 1.724,
      "step": 925500
    },
    {
      "epoch": 300.032414910859,
      "grad_norm": 1.5306706428527832,
      "learning_rate": 2.0021991566655858e-05,
      "loss": 2.5737,
      "step": 925600
    },
    {
      "epoch": 300.064829821718,
      "grad_norm": 1.375889778137207,
      "learning_rate": 2.0018747972753813e-05,
      "loss": 2.5464,
      "step": 925700
    },
    {
      "epoch": 300.097244732577,
      "grad_norm": 1.6863001585006714,
      "learning_rate": 2.001550437885177e-05,
      "loss": 2.5727,
      "step": 925800
    },
    {
      "epoch": 300.12965964343596,
      "grad_norm": 1.1621336936950684,
      "learning_rate": 2.0012260784949724e-05,
      "loss": 2.5601,
      "step": 925900
    },
    {
      "epoch": 300.162074554295,
      "grad_norm": 1.4959847927093506,
      "learning_rate": 2.0009017191047682e-05,
      "loss": 2.544,
      "step": 926000
    },
    {
      "epoch": 300.194489465154,
      "grad_norm": 1.2847079038619995,
      "learning_rate": 2.000580603308466e-05,
      "loss": 2.5528,
      "step": 926100
    },
    {
      "epoch": 300.22690437601295,
      "grad_norm": 1.389050841331482,
      "learning_rate": 2.0002562439182616e-05,
      "loss": 2.5694,
      "step": 926200
    },
    {
      "epoch": 300.25931928687197,
      "grad_norm": 1.27890145778656,
      "learning_rate": 1.999931884528057e-05,
      "loss": 2.5579,
      "step": 926300
    },
    {
      "epoch": 300.29173419773093,
      "grad_norm": 1.635498046875,
      "learning_rate": 1.999607525137853e-05,
      "loss": 2.5788,
      "step": 926400
    },
    {
      "epoch": 300.32414910858995,
      "grad_norm": 1.1832424402236938,
      "learning_rate": 1.9992831657476485e-05,
      "loss": 2.5543,
      "step": 926500
    },
    {
      "epoch": 300.35656401944897,
      "grad_norm": 1.3785178661346436,
      "learning_rate": 1.998958806357444e-05,
      "loss": 2.5918,
      "step": 926600
    },
    {
      "epoch": 300.3889789303079,
      "grad_norm": 1.3600126504898071,
      "learning_rate": 1.9986344469672396e-05,
      "loss": 2.5659,
      "step": 926700
    },
    {
      "epoch": 300.42139384116695,
      "grad_norm": 1.2461329698562622,
      "learning_rate": 1.9983100875770354e-05,
      "loss": 2.5756,
      "step": 926800
    },
    {
      "epoch": 300.4538087520259,
      "grad_norm": 1.2699145078659058,
      "learning_rate": 1.997985728186831e-05,
      "loss": 2.5424,
      "step": 926900
    },
    {
      "epoch": 300.4862236628849,
      "grad_norm": 1.2933025360107422,
      "learning_rate": 1.9976613687966265e-05,
      "loss": 2.5611,
      "step": 927000
    },
    {
      "epoch": 300.51863857374394,
      "grad_norm": 1.4431008100509644,
      "learning_rate": 1.9973370094064224e-05,
      "loss": 2.5594,
      "step": 927100
    },
    {
      "epoch": 300.5510534846029,
      "grad_norm": 1.2489336729049683,
      "learning_rate": 1.9970126500162183e-05,
      "loss": 2.5596,
      "step": 927200
    },
    {
      "epoch": 300.5834683954619,
      "grad_norm": 1.3141428232192993,
      "learning_rate": 1.9966882906260138e-05,
      "loss": 2.5839,
      "step": 927300
    },
    {
      "epoch": 300.6158833063209,
      "grad_norm": 1.4434458017349243,
      "learning_rate": 1.9963639312358093e-05,
      "loss": 2.5684,
      "step": 927400
    },
    {
      "epoch": 300.6482982171799,
      "grad_norm": 1.2789649963378906,
      "learning_rate": 1.9960395718456052e-05,
      "loss": 2.5734,
      "step": 927500
    },
    {
      "epoch": 300.6807131280389,
      "grad_norm": 1.5383222103118896,
      "learning_rate": 1.9957152124554007e-05,
      "loss": 2.5638,
      "step": 927600
    },
    {
      "epoch": 300.7131280388979,
      "grad_norm": 1.2893424034118652,
      "learning_rate": 1.9953908530651963e-05,
      "loss": 2.5493,
      "step": 927700
    },
    {
      "epoch": 300.7455429497569,
      "grad_norm": 1.3258280754089355,
      "learning_rate": 1.9950664936749918e-05,
      "loss": 2.559,
      "step": 927800
    },
    {
      "epoch": 300.77795786061586,
      "grad_norm": 1.3097560405731201,
      "learning_rate": 1.9947421342847877e-05,
      "loss": 2.5615,
      "step": 927900
    },
    {
      "epoch": 300.8103727714749,
      "grad_norm": 1.4461873769760132,
      "learning_rate": 1.9944177748945832e-05,
      "loss": 2.5583,
      "step": 928000
    },
    {
      "epoch": 300.8427876823339,
      "grad_norm": 1.3528391122817993,
      "learning_rate": 1.994096659098281e-05,
      "loss": 2.5481,
      "step": 928100
    },
    {
      "epoch": 300.87520259319285,
      "grad_norm": 1.3474338054656982,
      "learning_rate": 1.9937722997080765e-05,
      "loss": 2.5663,
      "step": 928200
    },
    {
      "epoch": 300.90761750405187,
      "grad_norm": 1.3868674039840698,
      "learning_rate": 1.9934479403178724e-05,
      "loss": 2.5685,
      "step": 928300
    },
    {
      "epoch": 300.94003241491083,
      "grad_norm": 1.1517884731292725,
      "learning_rate": 1.993123580927668e-05,
      "loss": 2.5573,
      "step": 928400
    },
    {
      "epoch": 300.97244732576985,
      "grad_norm": 1.2091331481933594,
      "learning_rate": 1.9927992215374635e-05,
      "loss": 2.5661,
      "step": 928500
    },
    {
      "epoch": 301.0,
      "eval_bleu": 0.8914431321970308,
      "eval_loss": 4.155838489532471,
      "eval_runtime": 4.5757,
      "eval_samples_per_second": 107.525,
      "eval_steps_per_second": 1.748,
      "step": 928585
    },
    {
      "epoch": 301.00486223662887,
      "grad_norm": 1.164854645729065,
      "learning_rate": 1.992474862147259e-05,
      "loss": 2.5607,
      "step": 928600
    },
    {
      "epoch": 301.0372771474878,
      "grad_norm": 1.2146373987197876,
      "learning_rate": 1.992150502757055e-05,
      "loss": 2.5613,
      "step": 928700
    },
    {
      "epoch": 301.06969205834685,
      "grad_norm": 1.7341383695602417,
      "learning_rate": 1.9918261433668504e-05,
      "loss": 2.5492,
      "step": 928800
    },
    {
      "epoch": 301.1021069692058,
      "grad_norm": 1.4302562475204468,
      "learning_rate": 1.9915017839766463e-05,
      "loss": 2.5588,
      "step": 928900
    },
    {
      "epoch": 301.1345218800648,
      "grad_norm": 1.3261642456054688,
      "learning_rate": 1.991177424586442e-05,
      "loss": 2.5524,
      "step": 929000
    },
    {
      "epoch": 301.16693679092384,
      "grad_norm": 1.3464173078536987,
      "learning_rate": 1.9908530651962377e-05,
      "loss": 2.5577,
      "step": 929100
    },
    {
      "epoch": 301.1993517017828,
      "grad_norm": 1.319919228553772,
      "learning_rate": 1.9905287058060333e-05,
      "loss": 2.545,
      "step": 929200
    },
    {
      "epoch": 301.2317666126418,
      "grad_norm": 1.3223704099655151,
      "learning_rate": 1.9902043464158288e-05,
      "loss": 2.5508,
      "step": 929300
    },
    {
      "epoch": 301.26418152350084,
      "grad_norm": 1.2302438020706177,
      "learning_rate": 1.9898799870256247e-05,
      "loss": 2.5617,
      "step": 929400
    },
    {
      "epoch": 301.2965964343598,
      "grad_norm": 1.326869249343872,
      "learning_rate": 1.9895556276354202e-05,
      "loss": 2.5672,
      "step": 929500
    },
    {
      "epoch": 301.3290113452188,
      "grad_norm": 1.4170349836349487,
      "learning_rate": 1.9892312682452157e-05,
      "loss": 2.5441,
      "step": 929600
    },
    {
      "epoch": 301.3614262560778,
      "grad_norm": 1.3187837600708008,
      "learning_rate": 1.9889069088550113e-05,
      "loss": 2.5551,
      "step": 929700
    },
    {
      "epoch": 301.3938411669368,
      "grad_norm": 1.4199671745300293,
      "learning_rate": 1.988582549464807e-05,
      "loss": 2.5734,
      "step": 929800
    },
    {
      "epoch": 301.4262560777958,
      "grad_norm": 1.5595731735229492,
      "learning_rate": 1.9882581900746027e-05,
      "loss": 2.5589,
      "step": 929900
    },
    {
      "epoch": 301.4586709886548,
      "grad_norm": 1.327828288078308,
      "learning_rate": 1.9879338306843982e-05,
      "loss": 2.5501,
      "step": 930000
    },
    {
      "epoch": 301.4910858995138,
      "grad_norm": 1.2605175971984863,
      "learning_rate": 1.987612714888096e-05,
      "loss": 2.5822,
      "step": 930100
    },
    {
      "epoch": 301.52350081037275,
      "grad_norm": 1.36233389377594,
      "learning_rate": 1.987288355497892e-05,
      "loss": 2.5863,
      "step": 930200
    },
    {
      "epoch": 301.55591572123177,
      "grad_norm": 1.235284447669983,
      "learning_rate": 1.9869639961076874e-05,
      "loss": 2.58,
      "step": 930300
    },
    {
      "epoch": 301.5883306320908,
      "grad_norm": 1.3515409231185913,
      "learning_rate": 1.986639636717483e-05,
      "loss": 2.5725,
      "step": 930400
    },
    {
      "epoch": 301.62074554294975,
      "grad_norm": 1.1599535942077637,
      "learning_rate": 1.9863152773272785e-05,
      "loss": 2.5665,
      "step": 930500
    },
    {
      "epoch": 301.65316045380877,
      "grad_norm": 1.4419575929641724,
      "learning_rate": 1.9859909179370744e-05,
      "loss": 2.5752,
      "step": 930600
    },
    {
      "epoch": 301.6855753646677,
      "grad_norm": 1.3067972660064697,
      "learning_rate": 1.9856665585468702e-05,
      "loss": 2.543,
      "step": 930700
    },
    {
      "epoch": 301.71799027552674,
      "grad_norm": 1.3734549283981323,
      "learning_rate": 1.9853421991566658e-05,
      "loss": 2.5783,
      "step": 930800
    },
    {
      "epoch": 301.75040518638576,
      "grad_norm": 1.2589236497879028,
      "learning_rate": 1.9850178397664613e-05,
      "loss": 2.5673,
      "step": 930900
    },
    {
      "epoch": 301.7828200972447,
      "grad_norm": 1.3473554849624634,
      "learning_rate": 1.9846934803762572e-05,
      "loss": 2.5313,
      "step": 931000
    },
    {
      "epoch": 301.81523500810374,
      "grad_norm": 1.2834402322769165,
      "learning_rate": 1.9843691209860527e-05,
      "loss": 2.562,
      "step": 931100
    },
    {
      "epoch": 301.8476499189627,
      "grad_norm": 1.5401347875595093,
      "learning_rate": 1.9840447615958482e-05,
      "loss": 2.5298,
      "step": 931200
    },
    {
      "epoch": 301.8800648298217,
      "grad_norm": 1.376279354095459,
      "learning_rate": 1.9837204022056438e-05,
      "loss": 2.5577,
      "step": 931300
    },
    {
      "epoch": 301.91247974068074,
      "grad_norm": 1.3461315631866455,
      "learning_rate": 1.9833960428154397e-05,
      "loss": 2.5524,
      "step": 931400
    },
    {
      "epoch": 301.9448946515397,
      "grad_norm": 1.3878669738769531,
      "learning_rate": 1.9830716834252352e-05,
      "loss": 2.5853,
      "step": 931500
    },
    {
      "epoch": 301.9773095623987,
      "grad_norm": 1.2601298093795776,
      "learning_rate": 1.9827473240350307e-05,
      "loss": 2.5706,
      "step": 931600
    },
    {
      "epoch": 302.0,
      "eval_bleu": 0.9662734651022834,
      "eval_loss": 4.162966251373291,
      "eval_runtime": 4.5678,
      "eval_samples_per_second": 107.712,
      "eval_steps_per_second": 1.751,
      "step": 931670
    },
    {
      "epoch": 302.0097244732577,
      "grad_norm": 1.374343991279602,
      "learning_rate": 1.9824229646448266e-05,
      "loss": 2.5498,
      "step": 931700
    },
    {
      "epoch": 302.0421393841167,
      "grad_norm": 1.5494440793991089,
      "learning_rate": 1.982098605254622e-05,
      "loss": 2.5607,
      "step": 931800
    },
    {
      "epoch": 302.0745542949757,
      "grad_norm": 1.3750910758972168,
      "learning_rate": 1.981774245864418e-05,
      "loss": 2.5758,
      "step": 931900
    },
    {
      "epoch": 302.1069692058347,
      "grad_norm": 1.358635663986206,
      "learning_rate": 1.9814498864742135e-05,
      "loss": 2.555,
      "step": 932000
    },
    {
      "epoch": 302.1393841166937,
      "grad_norm": 1.4314035177230835,
      "learning_rate": 1.981128770677911e-05,
      "loss": 2.5647,
      "step": 932100
    },
    {
      "epoch": 302.17179902755265,
      "grad_norm": 1.2829893827438354,
      "learning_rate": 1.980804411287707e-05,
      "loss": 2.5398,
      "step": 932200
    },
    {
      "epoch": 302.20421393841167,
      "grad_norm": 1.4103882312774658,
      "learning_rate": 1.9804800518975024e-05,
      "loss": 2.5448,
      "step": 932300
    },
    {
      "epoch": 302.2366288492707,
      "grad_norm": 1.3508756160736084,
      "learning_rate": 1.9801556925072983e-05,
      "loss": 2.5543,
      "step": 932400
    },
    {
      "epoch": 302.26904376012965,
      "grad_norm": 1.3376811742782593,
      "learning_rate": 1.9798313331170938e-05,
      "loss": 2.5628,
      "step": 932500
    },
    {
      "epoch": 302.30145867098867,
      "grad_norm": 1.3449500799179077,
      "learning_rate": 1.9795069737268897e-05,
      "loss": 2.5522,
      "step": 932600
    },
    {
      "epoch": 302.3338735818476,
      "grad_norm": 1.337629795074463,
      "learning_rate": 1.9791826143366852e-05,
      "loss": 2.5569,
      "step": 932700
    },
    {
      "epoch": 302.36628849270664,
      "grad_norm": 1.5267175436019897,
      "learning_rate": 1.9788582549464807e-05,
      "loss": 2.5636,
      "step": 932800
    },
    {
      "epoch": 302.39870340356566,
      "grad_norm": 1.391061544418335,
      "learning_rate": 1.9785338955562766e-05,
      "loss": 2.5579,
      "step": 932900
    },
    {
      "epoch": 302.4311183144246,
      "grad_norm": 1.606056809425354,
      "learning_rate": 1.978209536166072e-05,
      "loss": 2.5817,
      "step": 933000
    },
    {
      "epoch": 302.46353322528364,
      "grad_norm": 1.429504156112671,
      "learning_rate": 1.9778851767758677e-05,
      "loss": 2.5422,
      "step": 933100
    },
    {
      "epoch": 302.4959481361426,
      "grad_norm": 1.268748164176941,
      "learning_rate": 1.9775608173856632e-05,
      "loss": 2.5618,
      "step": 933200
    },
    {
      "epoch": 302.5283630470016,
      "grad_norm": 1.4135620594024658,
      "learning_rate": 1.977236457995459e-05,
      "loss": 2.5759,
      "step": 933300
    },
    {
      "epoch": 302.56077795786064,
      "grad_norm": 1.4713529348373413,
      "learning_rate": 1.9769120986052546e-05,
      "loss": 2.5767,
      "step": 933400
    },
    {
      "epoch": 302.5931928687196,
      "grad_norm": 1.3576511144638062,
      "learning_rate": 1.9765877392150502e-05,
      "loss": 2.574,
      "step": 933500
    },
    {
      "epoch": 302.6256077795786,
      "grad_norm": 1.3839293718338013,
      "learning_rate": 1.976263379824846e-05,
      "loss": 2.5393,
      "step": 933600
    },
    {
      "epoch": 302.6580226904376,
      "grad_norm": 1.4013015031814575,
      "learning_rate": 1.9759390204346416e-05,
      "loss": 2.5728,
      "step": 933700
    },
    {
      "epoch": 302.6904376012966,
      "grad_norm": 1.1424312591552734,
      "learning_rate": 1.9756146610444375e-05,
      "loss": 2.5607,
      "step": 933800
    },
    {
      "epoch": 302.7228525121556,
      "grad_norm": 1.2083953619003296,
      "learning_rate": 1.975290301654233e-05,
      "loss": 2.5517,
      "step": 933900
    },
    {
      "epoch": 302.7552674230146,
      "grad_norm": 1.2931891679763794,
      "learning_rate": 1.974965942264029e-05,
      "loss": 2.5398,
      "step": 934000
    },
    {
      "epoch": 302.7876823338736,
      "grad_norm": 1.3585366010665894,
      "learning_rate": 1.9746448264677263e-05,
      "loss": 2.5583,
      "step": 934100
    },
    {
      "epoch": 302.82009724473255,
      "grad_norm": 1.352582335472107,
      "learning_rate": 1.974320467077522e-05,
      "loss": 2.5691,
      "step": 934200
    },
    {
      "epoch": 302.85251215559157,
      "grad_norm": 1.4074184894561768,
      "learning_rate": 1.9739961076873177e-05,
      "loss": 2.5563,
      "step": 934300
    },
    {
      "epoch": 302.8849270664506,
      "grad_norm": 1.4450852870941162,
      "learning_rate": 1.9736717482971136e-05,
      "loss": 2.5634,
      "step": 934400
    },
    {
      "epoch": 302.91734197730955,
      "grad_norm": 1.39240562915802,
      "learning_rate": 1.973347388906909e-05,
      "loss": 2.553,
      "step": 934500
    },
    {
      "epoch": 302.94975688816857,
      "grad_norm": 1.1735180616378784,
      "learning_rate": 1.9730230295167047e-05,
      "loss": 2.5609,
      "step": 934600
    },
    {
      "epoch": 302.9821717990275,
      "grad_norm": 1.1927475929260254,
      "learning_rate": 1.9726986701265002e-05,
      "loss": 2.5715,
      "step": 934700
    },
    {
      "epoch": 303.0,
      "eval_bleu": 0.9232029862635768,
      "eval_loss": 4.164459228515625,
      "eval_runtime": 4.7873,
      "eval_samples_per_second": 102.772,
      "eval_steps_per_second": 1.671,
      "step": 934755
    },
    {
      "epoch": 303.01458670988654,
      "grad_norm": 1.335972785949707,
      "learning_rate": 1.972374310736296e-05,
      "loss": 2.5423,
      "step": 934800
    },
    {
      "epoch": 303.04700162074556,
      "grad_norm": 1.4011292457580566,
      "learning_rate": 1.9720499513460916e-05,
      "loss": 2.5654,
      "step": 934900
    },
    {
      "epoch": 303.0794165316045,
      "grad_norm": 1.127166509628296,
      "learning_rate": 1.971725591955887e-05,
      "loss": 2.5581,
      "step": 935000
    },
    {
      "epoch": 303.11183144246354,
      "grad_norm": 1.401405692100525,
      "learning_rate": 1.9714012325656827e-05,
      "loss": 2.5321,
      "step": 935100
    },
    {
      "epoch": 303.1442463533225,
      "grad_norm": 1.3556861877441406,
      "learning_rate": 1.9710768731754786e-05,
      "loss": 2.5556,
      "step": 935200
    },
    {
      "epoch": 303.1766612641815,
      "grad_norm": 1.5234951972961426,
      "learning_rate": 1.970752513785274e-05,
      "loss": 2.5624,
      "step": 935300
    },
    {
      "epoch": 303.20907617504054,
      "grad_norm": 1.3623768091201782,
      "learning_rate": 1.9704281543950696e-05,
      "loss": 2.5685,
      "step": 935400
    },
    {
      "epoch": 303.2414910858995,
      "grad_norm": 1.2229036092758179,
      "learning_rate": 1.9701037950048655e-05,
      "loss": 2.5583,
      "step": 935500
    },
    {
      "epoch": 303.2739059967585,
      "grad_norm": 1.5070862770080566,
      "learning_rate": 1.9697794356146614e-05,
      "loss": 2.5482,
      "step": 935600
    },
    {
      "epoch": 303.3063209076175,
      "grad_norm": 1.279253363609314,
      "learning_rate": 1.969455076224457e-05,
      "loss": 2.5391,
      "step": 935700
    },
    {
      "epoch": 303.3387358184765,
      "grad_norm": 1.22842538356781,
      "learning_rate": 1.9691307168342524e-05,
      "loss": 2.5558,
      "step": 935800
    },
    {
      "epoch": 303.3711507293355,
      "grad_norm": 1.3492478132247925,
      "learning_rate": 1.9688063574440483e-05,
      "loss": 2.5444,
      "step": 935900
    },
    {
      "epoch": 303.4035656401945,
      "grad_norm": 1.2860558032989502,
      "learning_rate": 1.968481998053844e-05,
      "loss": 2.5494,
      "step": 936000
    },
    {
      "epoch": 303.4359805510535,
      "grad_norm": 1.1892611980438232,
      "learning_rate": 1.9681608822575416e-05,
      "loss": 2.5585,
      "step": 936100
    },
    {
      "epoch": 303.4683954619125,
      "grad_norm": 1.288953185081482,
      "learning_rate": 1.9678365228673372e-05,
      "loss": 2.561,
      "step": 936200
    },
    {
      "epoch": 303.50081037277147,
      "grad_norm": 1.320395588874817,
      "learning_rate": 1.9675121634771327e-05,
      "loss": 2.5478,
      "step": 936300
    },
    {
      "epoch": 303.5332252836305,
      "grad_norm": 1.2731719017028809,
      "learning_rate": 1.9671878040869286e-05,
      "loss": 2.5571,
      "step": 936400
    },
    {
      "epoch": 303.56564019448945,
      "grad_norm": 1.3435362577438354,
      "learning_rate": 1.966863444696724e-05,
      "loss": 2.5656,
      "step": 936500
    },
    {
      "epoch": 303.59805510534846,
      "grad_norm": 1.3146986961364746,
      "learning_rate": 1.9665390853065197e-05,
      "loss": 2.5438,
      "step": 936600
    },
    {
      "epoch": 303.6304700162075,
      "grad_norm": 1.3489296436309814,
      "learning_rate": 1.9662147259163155e-05,
      "loss": 2.5588,
      "step": 936700
    },
    {
      "epoch": 303.66288492706644,
      "grad_norm": 1.4214153289794922,
      "learning_rate": 1.965890366526111e-05,
      "loss": 2.5848,
      "step": 936800
    },
    {
      "epoch": 303.69529983792546,
      "grad_norm": 1.3175987005233765,
      "learning_rate": 1.9655660071359066e-05,
      "loss": 2.5688,
      "step": 936900
    },
    {
      "epoch": 303.7277147487844,
      "grad_norm": 1.2965967655181885,
      "learning_rate": 1.965241647745702e-05,
      "loss": 2.543,
      "step": 937000
    },
    {
      "epoch": 303.76012965964344,
      "grad_norm": 1.2468955516815186,
      "learning_rate": 1.964917288355498e-05,
      "loss": 2.5551,
      "step": 937100
    },
    {
      "epoch": 303.79254457050246,
      "grad_norm": 1.2081209421157837,
      "learning_rate": 1.9645929289652935e-05,
      "loss": 2.5664,
      "step": 937200
    },
    {
      "epoch": 303.8249594813614,
      "grad_norm": 1.3998572826385498,
      "learning_rate": 1.9642685695750894e-05,
      "loss": 2.5583,
      "step": 937300
    },
    {
      "epoch": 303.85737439222044,
      "grad_norm": 1.372497797012329,
      "learning_rate": 1.963944210184885e-05,
      "loss": 2.5642,
      "step": 937400
    },
    {
      "epoch": 303.8897893030794,
      "grad_norm": 1.4011057615280151,
      "learning_rate": 1.9636198507946808e-05,
      "loss": 2.5798,
      "step": 937500
    },
    {
      "epoch": 303.9222042139384,
      "grad_norm": 1.2103337049484253,
      "learning_rate": 1.9632954914044764e-05,
      "loss": 2.5656,
      "step": 937600
    },
    {
      "epoch": 303.95461912479743,
      "grad_norm": 1.30152428150177,
      "learning_rate": 1.962971132014272e-05,
      "loss": 2.5556,
      "step": 937700
    },
    {
      "epoch": 303.9870340356564,
      "grad_norm": 1.477461814880371,
      "learning_rate": 1.9626467726240674e-05,
      "loss": 2.5744,
      "step": 937800
    },
    {
      "epoch": 304.0,
      "eval_bleu": 0.9857884845962731,
      "eval_loss": 4.165118217468262,
      "eval_runtime": 4.4995,
      "eval_samples_per_second": 109.346,
      "eval_steps_per_second": 1.778,
      "step": 937840
    },
    {
      "epoch": 304.0194489465154,
      "grad_norm": 1.3805440664291382,
      "learning_rate": 1.9623224132338633e-05,
      "loss": 2.5679,
      "step": 937900
    },
    {
      "epoch": 304.05186385737437,
      "grad_norm": 1.2269628047943115,
      "learning_rate": 1.961998053843659e-05,
      "loss": 2.5506,
      "step": 938000
    },
    {
      "epoch": 304.0842787682334,
      "grad_norm": 1.2667789459228516,
      "learning_rate": 1.9616769380473566e-05,
      "loss": 2.5519,
      "step": 938100
    },
    {
      "epoch": 304.1166936790924,
      "grad_norm": 1.3607814311981201,
      "learning_rate": 1.961352578657152e-05,
      "loss": 2.5598,
      "step": 938200
    },
    {
      "epoch": 304.14910858995137,
      "grad_norm": 1.3624694347381592,
      "learning_rate": 1.961028219266948e-05,
      "loss": 2.5384,
      "step": 938300
    },
    {
      "epoch": 304.1815235008104,
      "grad_norm": 1.3508988618850708,
      "learning_rate": 1.9607038598767436e-05,
      "loss": 2.5546,
      "step": 938400
    },
    {
      "epoch": 304.21393841166935,
      "grad_norm": 1.336345911026001,
      "learning_rate": 1.960379500486539e-05,
      "loss": 2.5721,
      "step": 938500
    },
    {
      "epoch": 304.24635332252836,
      "grad_norm": 1.2418752908706665,
      "learning_rate": 1.9600551410963346e-05,
      "loss": 2.5572,
      "step": 938600
    },
    {
      "epoch": 304.2787682333874,
      "grad_norm": 1.4027684926986694,
      "learning_rate": 1.9597307817061305e-05,
      "loss": 2.5497,
      "step": 938700
    },
    {
      "epoch": 304.31118314424634,
      "grad_norm": 1.3236359357833862,
      "learning_rate": 1.959406422315926e-05,
      "loss": 2.5681,
      "step": 938800
    },
    {
      "epoch": 304.34359805510536,
      "grad_norm": 1.3379004001617432,
      "learning_rate": 1.9590820629257216e-05,
      "loss": 2.5545,
      "step": 938900
    },
    {
      "epoch": 304.3760129659643,
      "grad_norm": 1.288712739944458,
      "learning_rate": 1.9587577035355175e-05,
      "loss": 2.5552,
      "step": 939000
    },
    {
      "epoch": 304.40842787682334,
      "grad_norm": 1.221505045890808,
      "learning_rate": 1.958433344145313e-05,
      "loss": 2.5662,
      "step": 939100
    },
    {
      "epoch": 304.44084278768236,
      "grad_norm": 1.5850136280059814,
      "learning_rate": 1.958108984755109e-05,
      "loss": 2.5465,
      "step": 939200
    },
    {
      "epoch": 304.4732576985413,
      "grad_norm": 1.251212239265442,
      "learning_rate": 1.9577846253649044e-05,
      "loss": 2.5655,
      "step": 939300
    },
    {
      "epoch": 304.50567260940034,
      "grad_norm": 1.3017265796661377,
      "learning_rate": 1.9574602659747003e-05,
      "loss": 2.5525,
      "step": 939400
    },
    {
      "epoch": 304.5380875202593,
      "grad_norm": 1.3199608325958252,
      "learning_rate": 1.9571359065844958e-05,
      "loss": 2.5665,
      "step": 939500
    },
    {
      "epoch": 304.5705024311183,
      "grad_norm": 1.3163304328918457,
      "learning_rate": 1.9568115471942913e-05,
      "loss": 2.5685,
      "step": 939600
    },
    {
      "epoch": 304.60291734197733,
      "grad_norm": 1.4030557870864868,
      "learning_rate": 1.956487187804087e-05,
      "loss": 2.572,
      "step": 939700
    },
    {
      "epoch": 304.6353322528363,
      "grad_norm": 1.1605710983276367,
      "learning_rate": 1.9561628284138828e-05,
      "loss": 2.5626,
      "step": 939800
    },
    {
      "epoch": 304.6677471636953,
      "grad_norm": 1.4928094148635864,
      "learning_rate": 1.9558417126175805e-05,
      "loss": 2.5425,
      "step": 939900
    },
    {
      "epoch": 304.70016207455427,
      "grad_norm": 1.4082560539245605,
      "learning_rate": 1.955517353227376e-05,
      "loss": 2.5639,
      "step": 940000
    },
    {
      "epoch": 304.7325769854133,
      "grad_norm": 1.2751264572143555,
      "learning_rate": 1.9551929938371716e-05,
      "loss": 2.5577,
      "step": 940100
    },
    {
      "epoch": 304.7649918962723,
      "grad_norm": 1.361268401145935,
      "learning_rate": 1.9548686344469675e-05,
      "loss": 2.5427,
      "step": 940200
    },
    {
      "epoch": 304.79740680713127,
      "grad_norm": 1.3799079656600952,
      "learning_rate": 1.954544275056763e-05,
      "loss": 2.5672,
      "step": 940300
    },
    {
      "epoch": 304.8298217179903,
      "grad_norm": 1.4528483152389526,
      "learning_rate": 1.9542199156665586e-05,
      "loss": 2.5743,
      "step": 940400
    },
    {
      "epoch": 304.86223662884925,
      "grad_norm": 1.5560173988342285,
      "learning_rate": 1.953895556276354e-05,
      "loss": 2.5606,
      "step": 940500
    },
    {
      "epoch": 304.89465153970826,
      "grad_norm": 1.2495770454406738,
      "learning_rate": 1.95357119688615e-05,
      "loss": 2.5646,
      "step": 940600
    },
    {
      "epoch": 304.9270664505673,
      "grad_norm": 1.3100559711456299,
      "learning_rate": 1.9532468374959455e-05,
      "loss": 2.5404,
      "step": 940700
    },
    {
      "epoch": 304.95948136142624,
      "grad_norm": 1.3649992942810059,
      "learning_rate": 1.952922478105741e-05,
      "loss": 2.5594,
      "step": 940800
    },
    {
      "epoch": 304.99189627228526,
      "grad_norm": 1.4238532781600952,
      "learning_rate": 1.952598118715537e-05,
      "loss": 2.5498,
      "step": 940900
    },
    {
      "epoch": 305.0,
      "eval_bleu": 0.9198408100901921,
      "eval_loss": 4.165264129638672,
      "eval_runtime": 4.5825,
      "eval_samples_per_second": 107.365,
      "eval_steps_per_second": 1.746,
      "step": 940925
    },
    {
      "epoch": 305.0243111831442,
      "grad_norm": 1.3160014152526855,
      "learning_rate": 1.9522737593253328e-05,
      "loss": 2.5656,
      "step": 941000
    },
    {
      "epoch": 305.05672609400324,
      "grad_norm": 1.522495985031128,
      "learning_rate": 1.9519493999351283e-05,
      "loss": 2.5455,
      "step": 941100
    },
    {
      "epoch": 305.08914100486226,
      "grad_norm": 1.4309523105621338,
      "learning_rate": 1.951625040544924e-05,
      "loss": 2.5657,
      "step": 941200
    },
    {
      "epoch": 305.1215559157212,
      "grad_norm": 1.3489059209823608,
      "learning_rate": 1.9513006811547197e-05,
      "loss": 2.5547,
      "step": 941300
    },
    {
      "epoch": 305.15397082658023,
      "grad_norm": 1.1544557809829712,
      "learning_rate": 1.9509763217645153e-05,
      "loss": 2.5453,
      "step": 941400
    },
    {
      "epoch": 305.1863857374392,
      "grad_norm": 1.2768160104751587,
      "learning_rate": 1.9506519623743108e-05,
      "loss": 2.5463,
      "step": 941500
    },
    {
      "epoch": 305.2188006482982,
      "grad_norm": 1.5585509538650513,
      "learning_rate": 1.9503276029841063e-05,
      "loss": 2.5195,
      "step": 941600
    },
    {
      "epoch": 305.25121555915723,
      "grad_norm": 1.6025763750076294,
      "learning_rate": 1.9500032435939022e-05,
      "loss": 2.5636,
      "step": 941700
    },
    {
      "epoch": 305.2836304700162,
      "grad_norm": 1.3365111351013184,
      "learning_rate": 1.9496788842036977e-05,
      "loss": 2.5639,
      "step": 941800
    },
    {
      "epoch": 305.3160453808752,
      "grad_norm": 1.3001431226730347,
      "learning_rate": 1.9493545248134933e-05,
      "loss": 2.5431,
      "step": 941900
    },
    {
      "epoch": 305.34846029173417,
      "grad_norm": 1.393449068069458,
      "learning_rate": 1.9490301654232888e-05,
      "loss": 2.5767,
      "step": 942000
    },
    {
      "epoch": 305.3808752025932,
      "grad_norm": 1.39655339717865,
      "learning_rate": 1.9487058060330847e-05,
      "loss": 2.5547,
      "step": 942100
    },
    {
      "epoch": 305.4132901134522,
      "grad_norm": 1.2365210056304932,
      "learning_rate": 1.9483814466428806e-05,
      "loss": 2.5578,
      "step": 942200
    },
    {
      "epoch": 305.44570502431117,
      "grad_norm": 1.485342264175415,
      "learning_rate": 1.948060330846578e-05,
      "loss": 2.5427,
      "step": 942300
    },
    {
      "epoch": 305.4781199351702,
      "grad_norm": 1.222318172454834,
      "learning_rate": 1.9477359714563735e-05,
      "loss": 2.5748,
      "step": 942400
    },
    {
      "epoch": 305.51053484602915,
      "grad_norm": 1.4161850214004517,
      "learning_rate": 1.9474116120661694e-05,
      "loss": 2.561,
      "step": 942500
    },
    {
      "epoch": 305.54294975688816,
      "grad_norm": 1.24573814868927,
      "learning_rate": 1.947087252675965e-05,
      "loss": 2.5646,
      "step": 942600
    },
    {
      "epoch": 305.5753646677472,
      "grad_norm": 1.4356293678283691,
      "learning_rate": 1.9467628932857608e-05,
      "loss": 2.546,
      "step": 942700
    },
    {
      "epoch": 305.60777957860614,
      "grad_norm": 1.400601863861084,
      "learning_rate": 1.9464385338955564e-05,
      "loss": 2.5675,
      "step": 942800
    },
    {
      "epoch": 305.64019448946516,
      "grad_norm": 1.2934826612472534,
      "learning_rate": 1.9461141745053522e-05,
      "loss": 2.5674,
      "step": 942900
    },
    {
      "epoch": 305.6726094003242,
      "grad_norm": 1.4097466468811035,
      "learning_rate": 1.9457898151151478e-05,
      "loss": 2.5654,
      "step": 943000
    },
    {
      "epoch": 305.70502431118314,
      "grad_norm": 1.3392603397369385,
      "learning_rate": 1.9454654557249433e-05,
      "loss": 2.558,
      "step": 943100
    },
    {
      "epoch": 305.73743922204216,
      "grad_norm": 1.328917384147644,
      "learning_rate": 1.9451410963347392e-05,
      "loss": 2.5369,
      "step": 943200
    },
    {
      "epoch": 305.7698541329011,
      "grad_norm": 1.4717233180999756,
      "learning_rate": 1.9448167369445347e-05,
      "loss": 2.5687,
      "step": 943300
    },
    {
      "epoch": 305.80226904376013,
      "grad_norm": 1.415967583656311,
      "learning_rate": 1.9444923775543303e-05,
      "loss": 2.5584,
      "step": 943400
    },
    {
      "epoch": 305.83468395461915,
      "grad_norm": 1.2215029001235962,
      "learning_rate": 1.9441680181641258e-05,
      "loss": 2.5494,
      "step": 943500
    },
    {
      "epoch": 305.8670988654781,
      "grad_norm": 1.5314075946807861,
      "learning_rate": 1.9438436587739217e-05,
      "loss": 2.5523,
      "step": 943600
    },
    {
      "epoch": 305.89951377633713,
      "grad_norm": 1.2933510541915894,
      "learning_rate": 1.9435192993837172e-05,
      "loss": 2.5682,
      "step": 943700
    },
    {
      "epoch": 305.9319286871961,
      "grad_norm": 1.3409533500671387,
      "learning_rate": 1.9431949399935127e-05,
      "loss": 2.5619,
      "step": 943800
    },
    {
      "epoch": 305.9643435980551,
      "grad_norm": 1.2359594106674194,
      "learning_rate": 1.9428705806033086e-05,
      "loss": 2.583,
      "step": 943900
    },
    {
      "epoch": 305.9967585089141,
      "grad_norm": 1.3043347597122192,
      "learning_rate": 1.9425462212131045e-05,
      "loss": 2.5657,
      "step": 944000
    },
    {
      "epoch": 306.0,
      "eval_bleu": 0.794471679175129,
      "eval_loss": 4.1646270751953125,
      "eval_runtime": 4.4966,
      "eval_samples_per_second": 109.415,
      "eval_steps_per_second": 1.779,
      "step": 944010
    },
    {
      "epoch": 306.0291734197731,
      "grad_norm": 1.4910277128219604,
      "learning_rate": 1.9422218618229e-05,
      "loss": 2.537,
      "step": 944100
    },
    {
      "epoch": 306.0615883306321,
      "grad_norm": 1.4584912061691284,
      "learning_rate": 1.9418975024326956e-05,
      "loss": 2.5517,
      "step": 944200
    },
    {
      "epoch": 306.09400324149107,
      "grad_norm": 1.2846242189407349,
      "learning_rate": 1.941573143042491e-05,
      "loss": 2.5574,
      "step": 944300
    },
    {
      "epoch": 306.1264181523501,
      "grad_norm": 1.3085286617279053,
      "learning_rate": 1.941248783652287e-05,
      "loss": 2.5456,
      "step": 944400
    },
    {
      "epoch": 306.1588330632091,
      "grad_norm": 1.6471387147903442,
      "learning_rate": 1.9409244242620825e-05,
      "loss": 2.5434,
      "step": 944500
    },
    {
      "epoch": 306.19124797406806,
      "grad_norm": 1.2524561882019043,
      "learning_rate": 1.940600064871878e-05,
      "loss": 2.5631,
      "step": 944600
    },
    {
      "epoch": 306.2236628849271,
      "grad_norm": 1.4604164361953735,
      "learning_rate": 1.940275705481674e-05,
      "loss": 2.5495,
      "step": 944700
    },
    {
      "epoch": 306.25607779578604,
      "grad_norm": 1.303799033164978,
      "learning_rate": 1.9399545896853717e-05,
      "loss": 2.5479,
      "step": 944800
    },
    {
      "epoch": 306.28849270664506,
      "grad_norm": 1.4637027978897095,
      "learning_rate": 1.9396302302951672e-05,
      "loss": 2.5562,
      "step": 944900
    },
    {
      "epoch": 306.3209076175041,
      "grad_norm": 1.3593577146530151,
      "learning_rate": 1.9393058709049628e-05,
      "loss": 2.5554,
      "step": 945000
    },
    {
      "epoch": 306.35332252836304,
      "grad_norm": 1.5097699165344238,
      "learning_rate": 1.9389815115147583e-05,
      "loss": 2.5451,
      "step": 945100
    },
    {
      "epoch": 306.38573743922205,
      "grad_norm": 1.3263368606567383,
      "learning_rate": 1.938657152124554e-05,
      "loss": 2.5509,
      "step": 945200
    },
    {
      "epoch": 306.418152350081,
      "grad_norm": 1.4334567785263062,
      "learning_rate": 1.9383327927343497e-05,
      "loss": 2.5558,
      "step": 945300
    },
    {
      "epoch": 306.45056726094003,
      "grad_norm": 1.3272857666015625,
      "learning_rate": 1.9380084333441452e-05,
      "loss": 2.5607,
      "step": 945400
    },
    {
      "epoch": 306.48298217179905,
      "grad_norm": 1.430041790008545,
      "learning_rate": 1.937684073953941e-05,
      "loss": 2.5563,
      "step": 945500
    },
    {
      "epoch": 306.515397082658,
      "grad_norm": 1.20265531539917,
      "learning_rate": 1.9373597145637366e-05,
      "loss": 2.5573,
      "step": 945600
    },
    {
      "epoch": 306.54781199351703,
      "grad_norm": 1.3423904180526733,
      "learning_rate": 1.9370353551735322e-05,
      "loss": 2.5541,
      "step": 945700
    },
    {
      "epoch": 306.580226904376,
      "grad_norm": 1.5338127613067627,
      "learning_rate": 1.936710995783328e-05,
      "loss": 2.5693,
      "step": 945800
    },
    {
      "epoch": 306.612641815235,
      "grad_norm": 1.3450106382369995,
      "learning_rate": 1.936386636393124e-05,
      "loss": 2.5788,
      "step": 945900
    },
    {
      "epoch": 306.645056726094,
      "grad_norm": 1.3980408906936646,
      "learning_rate": 1.9360622770029195e-05,
      "loss": 2.5528,
      "step": 946000
    },
    {
      "epoch": 306.677471636953,
      "grad_norm": 1.7271628379821777,
      "learning_rate": 1.935737917612715e-05,
      "loss": 2.5392,
      "step": 946100
    },
    {
      "epoch": 306.709886547812,
      "grad_norm": 1.673696756362915,
      "learning_rate": 1.9354135582225105e-05,
      "loss": 2.5602,
      "step": 946200
    },
    {
      "epoch": 306.74230145867097,
      "grad_norm": 1.2517404556274414,
      "learning_rate": 1.9350891988323064e-05,
      "loss": 2.5643,
      "step": 946300
    },
    {
      "epoch": 306.77471636953,
      "grad_norm": 1.316522479057312,
      "learning_rate": 1.934764839442102e-05,
      "loss": 2.5458,
      "step": 946400
    },
    {
      "epoch": 306.807131280389,
      "grad_norm": 1.4217109680175781,
      "learning_rate": 1.9344404800518975e-05,
      "loss": 2.5598,
      "step": 946500
    },
    {
      "epoch": 306.83954619124796,
      "grad_norm": 1.5548598766326904,
      "learning_rate": 1.934116120661693e-05,
      "loss": 2.5553,
      "step": 946600
    },
    {
      "epoch": 306.871961102107,
      "grad_norm": 1.2023619413375854,
      "learning_rate": 1.933791761271489e-05,
      "loss": 2.5577,
      "step": 946700
    },
    {
      "epoch": 306.90437601296594,
      "grad_norm": 1.374923825263977,
      "learning_rate": 1.9334674018812844e-05,
      "loss": 2.5602,
      "step": 946800
    },
    {
      "epoch": 306.93679092382496,
      "grad_norm": 1.2921040058135986,
      "learning_rate": 1.9331430424910803e-05,
      "loss": 2.5697,
      "step": 946900
    },
    {
      "epoch": 306.969205834684,
      "grad_norm": 1.2187058925628662,
      "learning_rate": 1.932818683100876e-05,
      "loss": 2.5612,
      "step": 947000
    },
    {
      "epoch": 307.0,
      "eval_bleu": 0.9270212003439753,
      "eval_loss": 4.167197227478027,
      "eval_runtime": 4.4027,
      "eval_samples_per_second": 111.75,
      "eval_steps_per_second": 1.817,
      "step": 947095
    },
    {
      "epoch": 307.00162074554294,
      "grad_norm": 1.292553186416626,
      "learning_rate": 1.9324943237106717e-05,
      "loss": 2.5737,
      "step": 947100
    },
    {
      "epoch": 307.03403565640195,
      "grad_norm": 1.609908103942871,
      "learning_rate": 1.9321699643204672e-05,
      "loss": 2.5506,
      "step": 947200
    },
    {
      "epoch": 307.0664505672609,
      "grad_norm": 1.3687728643417358,
      "learning_rate": 1.9318456049302628e-05,
      "loss": 2.5316,
      "step": 947300
    },
    {
      "epoch": 307.09886547811993,
      "grad_norm": 1.3721592426300049,
      "learning_rate": 1.9315212455400587e-05,
      "loss": 2.543,
      "step": 947400
    },
    {
      "epoch": 307.13128038897895,
      "grad_norm": 1.209234356880188,
      "learning_rate": 1.9311968861498542e-05,
      "loss": 2.5443,
      "step": 947500
    },
    {
      "epoch": 307.1636952998379,
      "grad_norm": 1.4046294689178467,
      "learning_rate": 1.9308725267596497e-05,
      "loss": 2.5555,
      "step": 947600
    },
    {
      "epoch": 307.19611021069693,
      "grad_norm": 1.4333020448684692,
      "learning_rate": 1.9305481673694453e-05,
      "loss": 2.563,
      "step": 947700
    },
    {
      "epoch": 307.2285251215559,
      "grad_norm": 1.244379997253418,
      "learning_rate": 1.930223807979241e-05,
      "loss": 2.5675,
      "step": 947800
    },
    {
      "epoch": 307.2609400324149,
      "grad_norm": 1.3164082765579224,
      "learning_rate": 1.9298994485890367e-05,
      "loss": 2.5577,
      "step": 947900
    },
    {
      "epoch": 307.2933549432739,
      "grad_norm": 1.3027938604354858,
      "learning_rate": 1.9295783327927345e-05,
      "loss": 2.5679,
      "step": 948000
    },
    {
      "epoch": 307.3257698541329,
      "grad_norm": 1.344833254814148,
      "learning_rate": 1.92925397340253e-05,
      "loss": 2.5467,
      "step": 948100
    },
    {
      "epoch": 307.3581847649919,
      "grad_norm": 1.567539930343628,
      "learning_rate": 1.928929614012326e-05,
      "loss": 2.5422,
      "step": 948200
    },
    {
      "epoch": 307.39059967585086,
      "grad_norm": 1.2583411931991577,
      "learning_rate": 1.9286052546221214e-05,
      "loss": 2.5588,
      "step": 948300
    },
    {
      "epoch": 307.4230145867099,
      "grad_norm": 1.3813111782073975,
      "learning_rate": 1.928280895231917e-05,
      "loss": 2.5318,
      "step": 948400
    },
    {
      "epoch": 307.4554294975689,
      "grad_norm": 1.2265512943267822,
      "learning_rate": 1.9279565358417125e-05,
      "loss": 2.5664,
      "step": 948500
    },
    {
      "epoch": 307.48784440842786,
      "grad_norm": 1.3125165700912476,
      "learning_rate": 1.9276321764515083e-05,
      "loss": 2.5637,
      "step": 948600
    },
    {
      "epoch": 307.5202593192869,
      "grad_norm": 1.3118706941604614,
      "learning_rate": 1.927307817061304e-05,
      "loss": 2.5107,
      "step": 948700
    },
    {
      "epoch": 307.55267423014584,
      "grad_norm": 1.4273051023483276,
      "learning_rate": 1.9269834576710998e-05,
      "loss": 2.5666,
      "step": 948800
    },
    {
      "epoch": 307.58508914100486,
      "grad_norm": 1.323727011680603,
      "learning_rate": 1.9266590982808953e-05,
      "loss": 2.5573,
      "step": 948900
    },
    {
      "epoch": 307.6175040518639,
      "grad_norm": 1.329368233680725,
      "learning_rate": 1.926337982484593e-05,
      "loss": 2.5689,
      "step": 949000
    },
    {
      "epoch": 307.64991896272284,
      "grad_norm": 1.4385097026824951,
      "learning_rate": 1.9260136230943886e-05,
      "loss": 2.5457,
      "step": 949100
    },
    {
      "epoch": 307.68233387358185,
      "grad_norm": 1.3703033924102783,
      "learning_rate": 1.925689263704184e-05,
      "loss": 2.559,
      "step": 949200
    },
    {
      "epoch": 307.7147487844408,
      "grad_norm": 1.2765421867370605,
      "learning_rate": 1.92536490431398e-05,
      "loss": 2.549,
      "step": 949300
    },
    {
      "epoch": 307.74716369529983,
      "grad_norm": 1.3513286113739014,
      "learning_rate": 1.925040544923776e-05,
      "loss": 2.5773,
      "step": 949400
    },
    {
      "epoch": 307.77957860615885,
      "grad_norm": 1.4647431373596191,
      "learning_rate": 1.9247161855335714e-05,
      "loss": 2.5663,
      "step": 949500
    },
    {
      "epoch": 307.8119935170178,
      "grad_norm": 1.276749610900879,
      "learning_rate": 1.924391826143367e-05,
      "loss": 2.5565,
      "step": 949600
    },
    {
      "epoch": 307.84440842787683,
      "grad_norm": 1.4514330625534058,
      "learning_rate": 1.9240674667531625e-05,
      "loss": 2.5635,
      "step": 949700
    },
    {
      "epoch": 307.87682333873585,
      "grad_norm": 1.4760042428970337,
      "learning_rate": 1.9237431073629584e-05,
      "loss": 2.5482,
      "step": 949800
    },
    {
      "epoch": 307.9092382495948,
      "grad_norm": 1.3040592670440674,
      "learning_rate": 1.923418747972754e-05,
      "loss": 2.573,
      "step": 949900
    },
    {
      "epoch": 307.9416531604538,
      "grad_norm": 1.2946349382400513,
      "learning_rate": 1.9230943885825494e-05,
      "loss": 2.5693,
      "step": 950000
    },
    {
      "epoch": 307.9740680713128,
      "grad_norm": 1.3403228521347046,
      "learning_rate": 1.9227700291923453e-05,
      "loss": 2.5676,
      "step": 950100
    },
    {
      "epoch": 308.0,
      "eval_bleu": 1.0546915053120727,
      "eval_loss": 4.167435169219971,
      "eval_runtime": 4.3487,
      "eval_samples_per_second": 113.138,
      "eval_steps_per_second": 1.84,
      "step": 950180
    },
    {
      "epoch": 308.0064829821718,
      "grad_norm": 1.5406097173690796,
      "learning_rate": 1.922445669802141e-05,
      "loss": 2.564,
      "step": 950200
    },
    {
      "epoch": 308.0388978930308,
      "grad_norm": 1.420035481452942,
      "learning_rate": 1.9221213104119364e-05,
      "loss": 2.5442,
      "step": 950300
    },
    {
      "epoch": 308.0713128038898,
      "grad_norm": 1.473892092704773,
      "learning_rate": 1.921796951021732e-05,
      "loss": 2.5486,
      "step": 950400
    },
    {
      "epoch": 308.1037277147488,
      "grad_norm": 1.4405806064605713,
      "learning_rate": 1.9214725916315278e-05,
      "loss": 2.5508,
      "step": 950500
    },
    {
      "epoch": 308.13614262560776,
      "grad_norm": 1.4192142486572266,
      "learning_rate": 1.9211482322413237e-05,
      "loss": 2.5376,
      "step": 950600
    },
    {
      "epoch": 308.1685575364668,
      "grad_norm": 1.3801618814468384,
      "learning_rate": 1.9208238728511192e-05,
      "loss": 2.5529,
      "step": 950700
    },
    {
      "epoch": 308.2009724473258,
      "grad_norm": 1.349817156791687,
      "learning_rate": 1.9204995134609147e-05,
      "loss": 2.5494,
      "step": 950800
    },
    {
      "epoch": 308.23338735818476,
      "grad_norm": 1.2730696201324463,
      "learning_rate": 1.9201751540707106e-05,
      "loss": 2.5141,
      "step": 950900
    },
    {
      "epoch": 308.2658022690438,
      "grad_norm": 1.1504137516021729,
      "learning_rate": 1.919850794680506e-05,
      "loss": 2.5496,
      "step": 951000
    },
    {
      "epoch": 308.29821717990274,
      "grad_norm": 1.3161568641662598,
      "learning_rate": 1.9195264352903017e-05,
      "loss": 2.5388,
      "step": 951100
    },
    {
      "epoch": 308.33063209076175,
      "grad_norm": 1.274518370628357,
      "learning_rate": 1.9192020759000972e-05,
      "loss": 2.5784,
      "step": 951200
    },
    {
      "epoch": 308.36304700162077,
      "grad_norm": 1.5013537406921387,
      "learning_rate": 1.918877716509893e-05,
      "loss": 2.5582,
      "step": 951300
    },
    {
      "epoch": 308.39546191247973,
      "grad_norm": 1.3827683925628662,
      "learning_rate": 1.9185533571196886e-05,
      "loss": 2.5521,
      "step": 951400
    },
    {
      "epoch": 308.42787682333875,
      "grad_norm": 1.2325246334075928,
      "learning_rate": 1.918228997729484e-05,
      "loss": 2.5633,
      "step": 951500
    },
    {
      "epoch": 308.4602917341977,
      "grad_norm": 1.4431885480880737,
      "learning_rate": 1.91790463833928e-05,
      "loss": 2.5584,
      "step": 951600
    },
    {
      "epoch": 308.4927066450567,
      "grad_norm": 1.306160807609558,
      "learning_rate": 1.9175802789490756e-05,
      "loss": 2.5497,
      "step": 951700
    },
    {
      "epoch": 308.52512155591575,
      "grad_norm": 1.466349720954895,
      "learning_rate": 1.9172559195588715e-05,
      "loss": 2.5564,
      "step": 951800
    },
    {
      "epoch": 308.5575364667747,
      "grad_norm": 1.259723424911499,
      "learning_rate": 1.916931560168667e-05,
      "loss": 2.5484,
      "step": 951900
    },
    {
      "epoch": 308.5899513776337,
      "grad_norm": 1.5189948081970215,
      "learning_rate": 1.916607200778463e-05,
      "loss": 2.559,
      "step": 952000
    },
    {
      "epoch": 308.6223662884927,
      "grad_norm": 1.1976053714752197,
      "learning_rate": 1.9162828413882584e-05,
      "loss": 2.5464,
      "step": 952100
    },
    {
      "epoch": 308.6547811993517,
      "grad_norm": 1.3625574111938477,
      "learning_rate": 1.915958481998054e-05,
      "loss": 2.571,
      "step": 952200
    },
    {
      "epoch": 308.6871961102107,
      "grad_norm": 1.4026261568069458,
      "learning_rate": 1.9156341226078495e-05,
      "loss": 2.5594,
      "step": 952300
    },
    {
      "epoch": 308.7196110210697,
      "grad_norm": 1.4900158643722534,
      "learning_rate": 1.9153130068115472e-05,
      "loss": 2.5624,
      "step": 952400
    },
    {
      "epoch": 308.7520259319287,
      "grad_norm": 1.4846594333648682,
      "learning_rate": 1.914988647421343e-05,
      "loss": 2.5541,
      "step": 952500
    },
    {
      "epoch": 308.78444084278766,
      "grad_norm": 1.235947847366333,
      "learning_rate": 1.9146642880311387e-05,
      "loss": 2.5542,
      "step": 952600
    },
    {
      "epoch": 308.8168557536467,
      "grad_norm": 1.515861988067627,
      "learning_rate": 1.9143399286409342e-05,
      "loss": 2.5406,
      "step": 952700
    },
    {
      "epoch": 308.8492706645057,
      "grad_norm": 1.236026644706726,
      "learning_rate": 1.91401556925073e-05,
      "loss": 2.5642,
      "step": 952800
    },
    {
      "epoch": 308.88168557536466,
      "grad_norm": 1.2306557893753052,
      "learning_rate": 1.9136912098605256e-05,
      "loss": 2.559,
      "step": 952900
    },
    {
      "epoch": 308.9141004862237,
      "grad_norm": 1.3596464395523071,
      "learning_rate": 1.913366850470321e-05,
      "loss": 2.5382,
      "step": 953000
    },
    {
      "epoch": 308.94651539708263,
      "grad_norm": 1.4692274332046509,
      "learning_rate": 1.9130424910801167e-05,
      "loss": 2.5639,
      "step": 953100
    },
    {
      "epoch": 308.97893030794165,
      "grad_norm": 1.4555858373641968,
      "learning_rate": 1.9127181316899125e-05,
      "loss": 2.5695,
      "step": 953200
    },
    {
      "epoch": 309.0,
      "eval_bleu": 0.7298191462185559,
      "eval_loss": 4.164255142211914,
      "eval_runtime": 4.2855,
      "eval_samples_per_second": 114.805,
      "eval_steps_per_second": 1.867,
      "step": 953265
    },
    {
      "epoch": 309.01134521880067,
      "grad_norm": 1.1984177827835083,
      "learning_rate": 1.912393772299708e-05,
      "loss": 2.5704,
      "step": 953300
    },
    {
      "epoch": 309.04376012965963,
      "grad_norm": 1.594562292098999,
      "learning_rate": 1.9120694129095036e-05,
      "loss": 2.538,
      "step": 953400
    },
    {
      "epoch": 309.07617504051865,
      "grad_norm": 1.3925220966339111,
      "learning_rate": 1.9117450535192995e-05,
      "loss": 2.544,
      "step": 953500
    },
    {
      "epoch": 309.1085899513776,
      "grad_norm": 1.5851224660873413,
      "learning_rate": 1.911420694129095e-05,
      "loss": 2.5169,
      "step": 953600
    },
    {
      "epoch": 309.1410048622366,
      "grad_norm": 1.330796718597412,
      "learning_rate": 1.911096334738891e-05,
      "loss": 2.54,
      "step": 953700
    },
    {
      "epoch": 309.17341977309565,
      "grad_norm": 1.4534915685653687,
      "learning_rate": 1.9107719753486864e-05,
      "loss": 2.5594,
      "step": 953800
    },
    {
      "epoch": 309.2058346839546,
      "grad_norm": 1.3719868659973145,
      "learning_rate": 1.9104476159584823e-05,
      "loss": 2.5634,
      "step": 953900
    },
    {
      "epoch": 309.2382495948136,
      "grad_norm": 1.3144316673278809,
      "learning_rate": 1.910123256568278e-05,
      "loss": 2.5539,
      "step": 954000
    },
    {
      "epoch": 309.2706645056726,
      "grad_norm": 1.3143092393875122,
      "learning_rate": 1.9097988971780734e-05,
      "loss": 2.565,
      "step": 954100
    },
    {
      "epoch": 309.3030794165316,
      "grad_norm": 1.3534705638885498,
      "learning_rate": 1.909474537787869e-05,
      "loss": 2.5389,
      "step": 954200
    },
    {
      "epoch": 309.3354943273906,
      "grad_norm": 1.2893240451812744,
      "learning_rate": 1.9091501783976648e-05,
      "loss": 2.5617,
      "step": 954300
    },
    {
      "epoch": 309.3679092382496,
      "grad_norm": 1.368272304534912,
      "learning_rate": 1.9088258190074603e-05,
      "loss": 2.5517,
      "step": 954400
    },
    {
      "epoch": 309.4003241491086,
      "grad_norm": 1.3523576259613037,
      "learning_rate": 1.908501459617256e-05,
      "loss": 2.5627,
      "step": 954500
    },
    {
      "epoch": 309.43273905996756,
      "grad_norm": 1.3372483253479004,
      "learning_rate": 1.9081771002270514e-05,
      "loss": 2.5397,
      "step": 954600
    },
    {
      "epoch": 309.4651539708266,
      "grad_norm": 1.3516137599945068,
      "learning_rate": 1.9078527408368473e-05,
      "loss": 2.5445,
      "step": 954700
    },
    {
      "epoch": 309.4975688816856,
      "grad_norm": 1.2024909257888794,
      "learning_rate": 1.9075283814466428e-05,
      "loss": 2.559,
      "step": 954800
    },
    {
      "epoch": 309.52998379254456,
      "grad_norm": 1.3235583305358887,
      "learning_rate": 1.9072040220564387e-05,
      "loss": 2.5397,
      "step": 954900
    },
    {
      "epoch": 309.5623987034036,
      "grad_norm": 1.4093538522720337,
      "learning_rate": 1.9068796626662346e-05,
      "loss": 2.5561,
      "step": 955000
    },
    {
      "epoch": 309.59481361426253,
      "grad_norm": 1.1902185678482056,
      "learning_rate": 1.90655530327603e-05,
      "loss": 2.5633,
      "step": 955100
    },
    {
      "epoch": 309.62722852512155,
      "grad_norm": 1.413598656654358,
      "learning_rate": 1.9062309438858256e-05,
      "loss": 2.5388,
      "step": 955200
    },
    {
      "epoch": 309.65964343598057,
      "grad_norm": 1.204489827156067,
      "learning_rate": 1.905906584495621e-05,
      "loss": 2.5649,
      "step": 955300
    },
    {
      "epoch": 309.69205834683953,
      "grad_norm": 1.2877116203308105,
      "learning_rate": 1.905582225105417e-05,
      "loss": 2.5723,
      "step": 955400
    },
    {
      "epoch": 309.72447325769855,
      "grad_norm": 1.3698376417160034,
      "learning_rate": 1.9052578657152126e-05,
      "loss": 2.5233,
      "step": 955500
    },
    {
      "epoch": 309.7568881685575,
      "grad_norm": 1.4182991981506348,
      "learning_rate": 1.904933506325008e-05,
      "loss": 2.5557,
      "step": 955600
    },
    {
      "epoch": 309.7893030794165,
      "grad_norm": 1.2245146036148071,
      "learning_rate": 1.9046091469348036e-05,
      "loss": 2.558,
      "step": 955700
    },
    {
      "epoch": 309.82171799027554,
      "grad_norm": 1.5227481126785278,
      "learning_rate": 1.9042847875445995e-05,
      "loss": 2.5438,
      "step": 955800
    },
    {
      "epoch": 309.8541329011345,
      "grad_norm": 1.3962336778640747,
      "learning_rate": 1.903960428154395e-05,
      "loss": 2.5549,
      "step": 955900
    },
    {
      "epoch": 309.8865478119935,
      "grad_norm": 1.2757325172424316,
      "learning_rate": 1.903636068764191e-05,
      "loss": 2.5648,
      "step": 956000
    },
    {
      "epoch": 309.9189627228525,
      "grad_norm": 1.464930534362793,
      "learning_rate": 1.9033117093739865e-05,
      "loss": 2.5534,
      "step": 956100
    },
    {
      "epoch": 309.9513776337115,
      "grad_norm": 1.3709043264389038,
      "learning_rate": 1.9029873499837823e-05,
      "loss": 2.5436,
      "step": 956200
    },
    {
      "epoch": 309.9837925445705,
      "grad_norm": 1.4580436944961548,
      "learning_rate": 1.902662990593578e-05,
      "loss": 2.5699,
      "step": 956300
    },
    {
      "epoch": 310.0,
      "eval_bleu": 1.156047867046932,
      "eval_loss": 4.17353630065918,
      "eval_runtime": 4.4049,
      "eval_samples_per_second": 111.695,
      "eval_steps_per_second": 1.816,
      "step": 956350
    },
    {
      "epoch": 310.0162074554295,
      "grad_norm": 1.2170542478561401,
      "learning_rate": 1.9023386312033734e-05,
      "loss": 2.5533,
      "step": 956400
    },
    {
      "epoch": 310.0486223662885,
      "grad_norm": 1.352644920349121,
      "learning_rate": 1.9020142718131693e-05,
      "loss": 2.5726,
      "step": 956500
    },
    {
      "epoch": 310.0810372771475,
      "grad_norm": 1.4879271984100342,
      "learning_rate": 1.9016899124229648e-05,
      "loss": 2.538,
      "step": 956600
    },
    {
      "epoch": 310.1134521880065,
      "grad_norm": 1.3561129570007324,
      "learning_rate": 1.9013655530327604e-05,
      "loss": 2.5598,
      "step": 956700
    },
    {
      "epoch": 310.1458670988655,
      "grad_norm": 1.2643991708755493,
      "learning_rate": 1.901041193642556e-05,
      "loss": 2.5208,
      "step": 956800
    },
    {
      "epoch": 310.17828200972446,
      "grad_norm": 1.4412487745285034,
      "learning_rate": 1.9007168342523518e-05,
      "loss": 2.5489,
      "step": 956900
    },
    {
      "epoch": 310.2106969205835,
      "grad_norm": 1.30475652217865,
      "learning_rate": 1.9003924748621473e-05,
      "loss": 2.5446,
      "step": 957000
    },
    {
      "epoch": 310.2431118314425,
      "grad_norm": 1.3951730728149414,
      "learning_rate": 1.900068115471943e-05,
      "loss": 2.5526,
      "step": 957100
    },
    {
      "epoch": 310.27552674230145,
      "grad_norm": 1.2966574430465698,
      "learning_rate": 1.8997437560817387e-05,
      "loss": 2.5314,
      "step": 957200
    },
    {
      "epoch": 310.30794165316047,
      "grad_norm": 1.6072055101394653,
      "learning_rate": 1.8994193966915342e-05,
      "loss": 2.5573,
      "step": 957300
    },
    {
      "epoch": 310.34035656401943,
      "grad_norm": 1.5823159217834473,
      "learning_rate": 1.89909503730133e-05,
      "loss": 2.5367,
      "step": 957400
    },
    {
      "epoch": 310.37277147487845,
      "grad_norm": 1.3698883056640625,
      "learning_rate": 1.8987706779111257e-05,
      "loss": 2.5677,
      "step": 957500
    },
    {
      "epoch": 310.40518638573747,
      "grad_norm": 1.335689663887024,
      "learning_rate": 1.8984463185209212e-05,
      "loss": 2.5582,
      "step": 957600
    },
    {
      "epoch": 310.4376012965964,
      "grad_norm": 1.2759475708007812,
      "learning_rate": 1.898121959130717e-05,
      "loss": 2.5552,
      "step": 957700
    },
    {
      "epoch": 310.47001620745544,
      "grad_norm": 1.3542706966400146,
      "learning_rate": 1.8977975997405126e-05,
      "loss": 2.5216,
      "step": 957800
    },
    {
      "epoch": 310.5024311183144,
      "grad_norm": 1.2392544746398926,
      "learning_rate": 1.897473240350308e-05,
      "loss": 2.5358,
      "step": 957900
    },
    {
      "epoch": 310.5348460291734,
      "grad_norm": 1.369702935218811,
      "learning_rate": 1.897148880960104e-05,
      "loss": 2.5541,
      "step": 958000
    },
    {
      "epoch": 310.56726094003244,
      "grad_norm": 1.351635456085205,
      "learning_rate": 1.8968245215698995e-05,
      "loss": 2.5484,
      "step": 958100
    },
    {
      "epoch": 310.5996758508914,
      "grad_norm": 1.7582330703735352,
      "learning_rate": 1.896500162179695e-05,
      "loss": 2.5588,
      "step": 958200
    },
    {
      "epoch": 310.6320907617504,
      "grad_norm": 1.4639776945114136,
      "learning_rate": 1.8961758027894906e-05,
      "loss": 2.5701,
      "step": 958300
    },
    {
      "epoch": 310.6645056726094,
      "grad_norm": 1.2223680019378662,
      "learning_rate": 1.8958546869931884e-05,
      "loss": 2.5671,
      "step": 958400
    },
    {
      "epoch": 310.6969205834684,
      "grad_norm": 1.2597321271896362,
      "learning_rate": 1.8955303276029843e-05,
      "loss": 2.562,
      "step": 958500
    },
    {
      "epoch": 310.7293354943274,
      "grad_norm": 1.535586953163147,
      "learning_rate": 1.8952059682127798e-05,
      "loss": 2.5595,
      "step": 958600
    },
    {
      "epoch": 310.7617504051864,
      "grad_norm": 1.197633981704712,
      "learning_rate": 1.8948816088225753e-05,
      "loss": 2.5512,
      "step": 958700
    },
    {
      "epoch": 310.7941653160454,
      "grad_norm": 1.2486413717269897,
      "learning_rate": 1.8945572494323712e-05,
      "loss": 2.5372,
      "step": 958800
    },
    {
      "epoch": 310.82658022690435,
      "grad_norm": 1.323081374168396,
      "learning_rate": 1.8942328900421668e-05,
      "loss": 2.5544,
      "step": 958900
    },
    {
      "epoch": 310.8589951377634,
      "grad_norm": 1.2412046194076538,
      "learning_rate": 1.8939085306519623e-05,
      "loss": 2.5507,
      "step": 959000
    },
    {
      "epoch": 310.8914100486224,
      "grad_norm": 1.5059834718704224,
      "learning_rate": 1.893584171261758e-05,
      "loss": 2.5841,
      "step": 959100
    },
    {
      "epoch": 310.92382495948135,
      "grad_norm": 1.2031142711639404,
      "learning_rate": 1.893259811871554e-05,
      "loss": 2.5623,
      "step": 959200
    },
    {
      "epoch": 310.95623987034037,
      "grad_norm": 1.2161849737167358,
      "learning_rate": 1.8929354524813496e-05,
      "loss": 2.5469,
      "step": 959300
    },
    {
      "epoch": 310.98865478119933,
      "grad_norm": 1.3275185823440552,
      "learning_rate": 1.892611093091145e-05,
      "loss": 2.552,
      "step": 959400
    },
    {
      "epoch": 311.0,
      "eval_bleu": 0.9677782771402522,
      "eval_loss": 4.172275543212891,
      "eval_runtime": 4.6244,
      "eval_samples_per_second": 106.391,
      "eval_steps_per_second": 1.73,
      "step": 959435
    },
    {
      "epoch": 311.02106969205835,
      "grad_norm": 1.5674400329589844,
      "learning_rate": 1.8922867337009406e-05,
      "loss": 2.5249,
      "step": 959500
    },
    {
      "epoch": 311.05348460291737,
      "grad_norm": 1.3930398225784302,
      "learning_rate": 1.8919623743107365e-05,
      "loss": 2.5355,
      "step": 959600
    },
    {
      "epoch": 311.0858995137763,
      "grad_norm": 1.4635300636291504,
      "learning_rate": 1.891638014920532e-05,
      "loss": 2.5543,
      "step": 959700
    },
    {
      "epoch": 311.11831442463534,
      "grad_norm": 1.4403713941574097,
      "learning_rate": 1.8913136555303276e-05,
      "loss": 2.5733,
      "step": 959800
    },
    {
      "epoch": 311.1507293354943,
      "grad_norm": 1.4304243326187134,
      "learning_rate": 1.890989296140123e-05,
      "loss": 2.5592,
      "step": 959900
    },
    {
      "epoch": 311.1831442463533,
      "grad_norm": 1.2914589643478394,
      "learning_rate": 1.890664936749919e-05,
      "loss": 2.5508,
      "step": 960000
    },
    {
      "epoch": 311.21555915721234,
      "grad_norm": 1.228782296180725,
      "learning_rate": 1.8903405773597145e-05,
      "loss": 2.5465,
      "step": 960100
    },
    {
      "epoch": 311.2479740680713,
      "grad_norm": 1.4074902534484863,
      "learning_rate": 1.89001621796951e-05,
      "loss": 2.544,
      "step": 960200
    },
    {
      "epoch": 311.2803889789303,
      "grad_norm": 1.391638994216919,
      "learning_rate": 1.889691858579306e-05,
      "loss": 2.5508,
      "step": 960300
    },
    {
      "epoch": 311.3128038897893,
      "grad_norm": 1.4417355060577393,
      "learning_rate": 1.8893707427830037e-05,
      "loss": 2.5375,
      "step": 960400
    },
    {
      "epoch": 311.3452188006483,
      "grad_norm": 1.5886332988739014,
      "learning_rate": 1.8890463833927993e-05,
      "loss": 2.5428,
      "step": 960500
    },
    {
      "epoch": 311.3776337115073,
      "grad_norm": 1.4647680521011353,
      "learning_rate": 1.8887220240025948e-05,
      "loss": 2.5185,
      "step": 960600
    },
    {
      "epoch": 311.4100486223663,
      "grad_norm": 1.2620054483413696,
      "learning_rate": 1.8883976646123903e-05,
      "loss": 2.5468,
      "step": 960700
    },
    {
      "epoch": 311.4424635332253,
      "grad_norm": 1.3834083080291748,
      "learning_rate": 1.8880733052221862e-05,
      "loss": 2.5689,
      "step": 960800
    },
    {
      "epoch": 311.47487844408425,
      "grad_norm": 1.4064383506774902,
      "learning_rate": 1.887752189425884e-05,
      "loss": 2.554,
      "step": 960900
    },
    {
      "epoch": 311.5072933549433,
      "grad_norm": 1.2917007207870483,
      "learning_rate": 1.8874278300356795e-05,
      "loss": 2.5396,
      "step": 961000
    },
    {
      "epoch": 311.5397082658023,
      "grad_norm": 1.3178322315216064,
      "learning_rate": 1.887103470645475e-05,
      "loss": 2.5352,
      "step": 961100
    },
    {
      "epoch": 311.57212317666125,
      "grad_norm": 1.2069059610366821,
      "learning_rate": 1.886779111255271e-05,
      "loss": 2.5648,
      "step": 961200
    },
    {
      "epoch": 311.60453808752027,
      "grad_norm": 1.612484097480774,
      "learning_rate": 1.8864547518650665e-05,
      "loss": 2.5513,
      "step": 961300
    },
    {
      "epoch": 311.63695299837923,
      "grad_norm": 1.390564203262329,
      "learning_rate": 1.8861303924748623e-05,
      "loss": 2.5688,
      "step": 961400
    },
    {
      "epoch": 311.66936790923825,
      "grad_norm": 1.3346834182739258,
      "learning_rate": 1.885806033084658e-05,
      "loss": 2.5602,
      "step": 961500
    },
    {
      "epoch": 311.70178282009726,
      "grad_norm": 1.2929389476776123,
      "learning_rate": 1.8854816736944537e-05,
      "loss": 2.5676,
      "step": 961600
    },
    {
      "epoch": 311.7341977309562,
      "grad_norm": 1.2833225727081299,
      "learning_rate": 1.8851573143042493e-05,
      "loss": 2.5711,
      "step": 961700
    },
    {
      "epoch": 311.76661264181524,
      "grad_norm": 1.3114488124847412,
      "learning_rate": 1.8848329549140448e-05,
      "loss": 2.5477,
      "step": 961800
    },
    {
      "epoch": 311.7990275526742,
      "grad_norm": 1.3073079586029053,
      "learning_rate": 1.8845085955238407e-05,
      "loss": 2.5558,
      "step": 961900
    },
    {
      "epoch": 311.8314424635332,
      "grad_norm": 1.247878909111023,
      "learning_rate": 1.8841842361336362e-05,
      "loss": 2.5515,
      "step": 962000
    },
    {
      "epoch": 311.86385737439224,
      "grad_norm": 1.3809841871261597,
      "learning_rate": 1.8838598767434318e-05,
      "loss": 2.5613,
      "step": 962100
    },
    {
      "epoch": 311.8962722852512,
      "grad_norm": 1.2335253953933716,
      "learning_rate": 1.8835355173532273e-05,
      "loss": 2.555,
      "step": 962200
    },
    {
      "epoch": 311.9286871961102,
      "grad_norm": 1.414832353591919,
      "learning_rate": 1.8832111579630232e-05,
      "loss": 2.5536,
      "step": 962300
    },
    {
      "epoch": 311.9611021069692,
      "grad_norm": 1.398709774017334,
      "learning_rate": 1.8828867985728187e-05,
      "loss": 2.5475,
      "step": 962400
    },
    {
      "epoch": 311.9935170178282,
      "grad_norm": 1.4814311265945435,
      "learning_rate": 1.8825624391826142e-05,
      "loss": 2.5466,
      "step": 962500
    },
    {
      "epoch": 312.0,
      "eval_bleu": 1.020862871495917,
      "eval_loss": 4.173105239868164,
      "eval_runtime": 4.9773,
      "eval_samples_per_second": 98.85,
      "eval_steps_per_second": 1.607,
      "step": 962520
    },
    {
      "epoch": 312.0259319286872,
      "grad_norm": 1.5478806495666504,
      "learning_rate": 1.88223807979241e-05,
      "loss": 2.5469,
      "step": 962600
    },
    {
      "epoch": 312.0583468395462,
      "grad_norm": 1.4507330656051636,
      "learning_rate": 1.8819137204022057e-05,
      "loss": 2.528,
      "step": 962700
    },
    {
      "epoch": 312.0907617504052,
      "grad_norm": 1.1956477165222168,
      "learning_rate": 1.8815893610120015e-05,
      "loss": 2.5712,
      "step": 962800
    },
    {
      "epoch": 312.12317666126415,
      "grad_norm": 1.4308011531829834,
      "learning_rate": 1.881265001621797e-05,
      "loss": 2.567,
      "step": 962900
    },
    {
      "epoch": 312.15559157212317,
      "grad_norm": 1.3043628931045532,
      "learning_rate": 1.880940642231593e-05,
      "loss": 2.5693,
      "step": 963000
    },
    {
      "epoch": 312.1880064829822,
      "grad_norm": 1.3128435611724854,
      "learning_rate": 1.8806162828413885e-05,
      "loss": 2.5452,
      "step": 963100
    },
    {
      "epoch": 312.22042139384115,
      "grad_norm": 1.7763478755950928,
      "learning_rate": 1.880291923451184e-05,
      "loss": 2.5391,
      "step": 963200
    },
    {
      "epoch": 312.25283630470017,
      "grad_norm": 1.3549258708953857,
      "learning_rate": 1.8799675640609795e-05,
      "loss": 2.5708,
      "step": 963300
    },
    {
      "epoch": 312.2852512155592,
      "grad_norm": 1.2832211256027222,
      "learning_rate": 1.8796432046707754e-05,
      "loss": 2.555,
      "step": 963400
    },
    {
      "epoch": 312.31766612641815,
      "grad_norm": 1.2618385553359985,
      "learning_rate": 1.879318845280571e-05,
      "loss": 2.5354,
      "step": 963500
    },
    {
      "epoch": 312.35008103727716,
      "grad_norm": 1.3056044578552246,
      "learning_rate": 1.8789944858903665e-05,
      "loss": 2.5397,
      "step": 963600
    },
    {
      "epoch": 312.3824959481361,
      "grad_norm": 1.34105384349823,
      "learning_rate": 1.878670126500162e-05,
      "loss": 2.5477,
      "step": 963700
    },
    {
      "epoch": 312.41491085899514,
      "grad_norm": 1.2413835525512695,
      "learning_rate": 1.878345767109958e-05,
      "loss": 2.556,
      "step": 963800
    },
    {
      "epoch": 312.44732576985416,
      "grad_norm": 1.5407159328460693,
      "learning_rate": 1.8780214077197534e-05,
      "loss": 2.5516,
      "step": 963900
    },
    {
      "epoch": 312.4797406807131,
      "grad_norm": 1.2965360879898071,
      "learning_rate": 1.8776970483295493e-05,
      "loss": 2.5355,
      "step": 964000
    },
    {
      "epoch": 312.51215559157214,
      "grad_norm": 1.432367205619812,
      "learning_rate": 1.877372688939345e-05,
      "loss": 2.5636,
      "step": 964100
    },
    {
      "epoch": 312.5445705024311,
      "grad_norm": 1.3139196634292603,
      "learning_rate": 1.8770483295491407e-05,
      "loss": 2.5363,
      "step": 964200
    },
    {
      "epoch": 312.5769854132901,
      "grad_norm": 1.2530382871627808,
      "learning_rate": 1.8767239701589363e-05,
      "loss": 2.5567,
      "step": 964300
    },
    {
      "epoch": 312.60940032414914,
      "grad_norm": 1.6854674816131592,
      "learning_rate": 1.8763996107687318e-05,
      "loss": 2.5407,
      "step": 964400
    },
    {
      "epoch": 312.6418152350081,
      "grad_norm": 1.5588371753692627,
      "learning_rate": 1.8760752513785277e-05,
      "loss": 2.532,
      "step": 964500
    },
    {
      "epoch": 312.6742301458671,
      "grad_norm": 1.2629456520080566,
      "learning_rate": 1.8757508919883232e-05,
      "loss": 2.5383,
      "step": 964600
    },
    {
      "epoch": 312.7066450567261,
      "grad_norm": 1.2938766479492188,
      "learning_rate": 1.8754265325981187e-05,
      "loss": 2.5562,
      "step": 964700
    },
    {
      "epoch": 312.7390599675851,
      "grad_norm": 1.3648310899734497,
      "learning_rate": 1.8751021732079143e-05,
      "loss": 2.5255,
      "step": 964800
    },
    {
      "epoch": 312.7714748784441,
      "grad_norm": 1.3804001808166504,
      "learning_rate": 1.874781057411612e-05,
      "loss": 2.5573,
      "step": 964900
    },
    {
      "epoch": 312.80388978930307,
      "grad_norm": 1.7510908842086792,
      "learning_rate": 1.874456698021408e-05,
      "loss": 2.5395,
      "step": 965000
    },
    {
      "epoch": 312.8363047001621,
      "grad_norm": 1.230669379234314,
      "learning_rate": 1.8741323386312035e-05,
      "loss": 2.5647,
      "step": 965100
    },
    {
      "epoch": 312.86871961102105,
      "grad_norm": 1.5718286037445068,
      "learning_rate": 1.873807979240999e-05,
      "loss": 2.5539,
      "step": 965200
    },
    {
      "epoch": 312.90113452188007,
      "grad_norm": 1.2227472066879272,
      "learning_rate": 1.873483619850795e-05,
      "loss": 2.5596,
      "step": 965300
    },
    {
      "epoch": 312.9335494327391,
      "grad_norm": 1.3360283374786377,
      "learning_rate": 1.8731592604605904e-05,
      "loss": 2.5411,
      "step": 965400
    },
    {
      "epoch": 312.96596434359805,
      "grad_norm": 1.2450945377349854,
      "learning_rate": 1.872834901070386e-05,
      "loss": 2.5429,
      "step": 965500
    },
    {
      "epoch": 312.99837925445706,
      "grad_norm": 1.4470438957214355,
      "learning_rate": 1.8725105416801815e-05,
      "loss": 2.5702,
      "step": 965600
    },
    {
      "epoch": 313.0,
      "eval_bleu": 1.0505780027752634,
      "eval_loss": 4.178104877471924,
      "eval_runtime": 4.7503,
      "eval_samples_per_second": 103.572,
      "eval_steps_per_second": 1.684,
      "step": 965605
    },
    {
      "epoch": 313.030794165316,
      "grad_norm": 1.4019582271575928,
      "learning_rate": 1.8721861822899774e-05,
      "loss": 2.5591,
      "step": 965700
    },
    {
      "epoch": 313.06320907617504,
      "grad_norm": 1.532325267791748,
      "learning_rate": 1.8718618228997732e-05,
      "loss": 2.5369,
      "step": 965800
    },
    {
      "epoch": 313.09562398703406,
      "grad_norm": 1.307376503944397,
      "learning_rate": 1.8715374635095688e-05,
      "loss": 2.5514,
      "step": 965900
    },
    {
      "epoch": 313.128038897893,
      "grad_norm": 1.3773658275604248,
      "learning_rate": 1.8712131041193643e-05,
      "loss": 2.5565,
      "step": 966000
    },
    {
      "epoch": 313.16045380875204,
      "grad_norm": 1.445585012435913,
      "learning_rate": 1.8708887447291602e-05,
      "loss": 2.5486,
      "step": 966100
    },
    {
      "epoch": 313.192868719611,
      "grad_norm": 1.2865052223205566,
      "learning_rate": 1.8705643853389557e-05,
      "loss": 2.5524,
      "step": 966200
    },
    {
      "epoch": 313.22528363047,
      "grad_norm": 1.2952845096588135,
      "learning_rate": 1.8702400259487512e-05,
      "loss": 2.5299,
      "step": 966300
    },
    {
      "epoch": 313.25769854132903,
      "grad_norm": 1.2355653047561646,
      "learning_rate": 1.8699156665585468e-05,
      "loss": 2.551,
      "step": 966400
    },
    {
      "epoch": 313.290113452188,
      "grad_norm": 1.1320679187774658,
      "learning_rate": 1.8695913071683427e-05,
      "loss": 2.5152,
      "step": 966500
    },
    {
      "epoch": 313.322528363047,
      "grad_norm": 1.5396560430526733,
      "learning_rate": 1.8692669477781382e-05,
      "loss": 2.5555,
      "step": 966600
    },
    {
      "epoch": 313.354943273906,
      "grad_norm": 1.5110862255096436,
      "learning_rate": 1.8689425883879337e-05,
      "loss": 2.5625,
      "step": 966700
    },
    {
      "epoch": 313.387358184765,
      "grad_norm": 1.3183917999267578,
      "learning_rate": 1.8686182289977296e-05,
      "loss": 2.5422,
      "step": 966800
    },
    {
      "epoch": 313.419773095624,
      "grad_norm": 1.7477351427078247,
      "learning_rate": 1.8682971132014274e-05,
      "loss": 2.5665,
      "step": 966900
    },
    {
      "epoch": 313.45218800648297,
      "grad_norm": 1.411655068397522,
      "learning_rate": 1.867972753811223e-05,
      "loss": 2.5418,
      "step": 967000
    },
    {
      "epoch": 313.484602917342,
      "grad_norm": 1.552634835243225,
      "learning_rate": 1.8676483944210184e-05,
      "loss": 2.5378,
      "step": 967100
    },
    {
      "epoch": 313.51701782820095,
      "grad_norm": 1.3959012031555176,
      "learning_rate": 1.867324035030814e-05,
      "loss": 2.5639,
      "step": 967200
    },
    {
      "epoch": 313.54943273905997,
      "grad_norm": 1.210586428642273,
      "learning_rate": 1.867002919234512e-05,
      "loss": 2.5343,
      "step": 967300
    },
    {
      "epoch": 313.581847649919,
      "grad_norm": 1.2605196237564087,
      "learning_rate": 1.8666785598443076e-05,
      "loss": 2.5607,
      "step": 967400
    },
    {
      "epoch": 313.61426256077795,
      "grad_norm": 1.5269392728805542,
      "learning_rate": 1.8663542004541032e-05,
      "loss": 2.5565,
      "step": 967500
    },
    {
      "epoch": 313.64667747163696,
      "grad_norm": 1.3342070579528809,
      "learning_rate": 1.8660298410638987e-05,
      "loss": 2.5533,
      "step": 967600
    },
    {
      "epoch": 313.6790923824959,
      "grad_norm": 1.618792176246643,
      "learning_rate": 1.8657054816736946e-05,
      "loss": 2.5633,
      "step": 967700
    },
    {
      "epoch": 313.71150729335494,
      "grad_norm": 1.345177173614502,
      "learning_rate": 1.86538112228349e-05,
      "loss": 2.5556,
      "step": 967800
    },
    {
      "epoch": 313.74392220421396,
      "grad_norm": 1.200049638748169,
      "learning_rate": 1.8650567628932857e-05,
      "loss": 2.5322,
      "step": 967900
    },
    {
      "epoch": 313.7763371150729,
      "grad_norm": 1.2835862636566162,
      "learning_rate": 1.8647324035030815e-05,
      "loss": 2.5733,
      "step": 968000
    },
    {
      "epoch": 313.80875202593194,
      "grad_norm": 1.4777430295944214,
      "learning_rate": 1.864408044112877e-05,
      "loss": 2.5404,
      "step": 968100
    },
    {
      "epoch": 313.8411669367909,
      "grad_norm": 1.3585354089736938,
      "learning_rate": 1.864083684722673e-05,
      "loss": 2.5219,
      "step": 968200
    },
    {
      "epoch": 313.8735818476499,
      "grad_norm": 1.289323329925537,
      "learning_rate": 1.8637593253324685e-05,
      "loss": 2.5741,
      "step": 968300
    },
    {
      "epoch": 313.90599675850893,
      "grad_norm": 1.3126683235168457,
      "learning_rate": 1.8634349659422643e-05,
      "loss": 2.5385,
      "step": 968400
    },
    {
      "epoch": 313.9384116693679,
      "grad_norm": 1.5330514907836914,
      "learning_rate": 1.86311060655206e-05,
      "loss": 2.5433,
      "step": 968500
    },
    {
      "epoch": 313.9708265802269,
      "grad_norm": 1.3133835792541504,
      "learning_rate": 1.8627862471618554e-05,
      "loss": 2.5482,
      "step": 968600
    },
    {
      "epoch": 314.0,
      "eval_bleu": 0.9259779778048519,
      "eval_loss": 4.1744866371154785,
      "eval_runtime": 4.9714,
      "eval_samples_per_second": 98.966,
      "eval_steps_per_second": 1.609,
      "step": 968690
    },
    {
      "epoch": 314.0032414910859,
      "grad_norm": 1.2012059688568115,
      "learning_rate": 1.862461887771651e-05,
      "loss": 2.572,
      "step": 968700
    },
    {
      "epoch": 314.0356564019449,
      "grad_norm": 1.4924527406692505,
      "learning_rate": 1.8621375283814468e-05,
      "loss": 2.5385,
      "step": 968800
    },
    {
      "epoch": 314.0680713128039,
      "grad_norm": 1.266176700592041,
      "learning_rate": 1.8618131689912424e-05,
      "loss": 2.5475,
      "step": 968900
    },
    {
      "epoch": 314.10048622366287,
      "grad_norm": 1.2844488620758057,
      "learning_rate": 1.861488809601038e-05,
      "loss": 2.552,
      "step": 969000
    },
    {
      "epoch": 314.1329011345219,
      "grad_norm": 1.2899352312088013,
      "learning_rate": 1.8611644502108334e-05,
      "loss": 2.5285,
      "step": 969100
    },
    {
      "epoch": 314.16531604538085,
      "grad_norm": 1.3690422773361206,
      "learning_rate": 1.8608400908206293e-05,
      "loss": 2.5489,
      "step": 969200
    },
    {
      "epoch": 314.19773095623987,
      "grad_norm": 1.59219229221344,
      "learning_rate": 1.860515731430425e-05,
      "loss": 2.566,
      "step": 969300
    },
    {
      "epoch": 314.2301458670989,
      "grad_norm": 1.3784840106964111,
      "learning_rate": 1.8601913720402207e-05,
      "loss": 2.5339,
      "step": 969400
    },
    {
      "epoch": 314.26256077795784,
      "grad_norm": 1.3030990362167358,
      "learning_rate": 1.8598670126500163e-05,
      "loss": 2.5418,
      "step": 969500
    },
    {
      "epoch": 314.29497568881686,
      "grad_norm": 1.317325234413147,
      "learning_rate": 1.859542653259812e-05,
      "loss": 2.5458,
      "step": 969600
    },
    {
      "epoch": 314.3273905996758,
      "grad_norm": 1.652432918548584,
      "learning_rate": 1.8592182938696077e-05,
      "loss": 2.5399,
      "step": 969700
    },
    {
      "epoch": 314.35980551053484,
      "grad_norm": 1.4120357036590576,
      "learning_rate": 1.8588939344794032e-05,
      "loss": 2.5488,
      "step": 969800
    },
    {
      "epoch": 314.39222042139386,
      "grad_norm": 1.459660530090332,
      "learning_rate": 1.858569575089199e-05,
      "loss": 2.5381,
      "step": 969900
    },
    {
      "epoch": 314.4246353322528,
      "grad_norm": 1.2879048585891724,
      "learning_rate": 1.8582452156989946e-05,
      "loss": 2.5559,
      "step": 970000
    },
    {
      "epoch": 314.45705024311184,
      "grad_norm": 1.2185496091842651,
      "learning_rate": 1.85792085630879e-05,
      "loss": 2.5585,
      "step": 970100
    },
    {
      "epoch": 314.48946515397085,
      "grad_norm": 1.388945460319519,
      "learning_rate": 1.8575964969185857e-05,
      "loss": 2.5416,
      "step": 970200
    },
    {
      "epoch": 314.5218800648298,
      "grad_norm": 1.7177845239639282,
      "learning_rate": 1.8572721375283816e-05,
      "loss": 2.537,
      "step": 970300
    },
    {
      "epoch": 314.55429497568883,
      "grad_norm": 1.2795006036758423,
      "learning_rate": 1.856947778138177e-05,
      "loss": 2.5647,
      "step": 970400
    },
    {
      "epoch": 314.5867098865478,
      "grad_norm": 1.385829210281372,
      "learning_rate": 1.856623418747973e-05,
      "loss": 2.5279,
      "step": 970500
    },
    {
      "epoch": 314.6191247974068,
      "grad_norm": 1.5056190490722656,
      "learning_rate": 1.8562990593577685e-05,
      "loss": 2.5262,
      "step": 970600
    },
    {
      "epoch": 314.65153970826583,
      "grad_norm": 1.5868078470230103,
      "learning_rate": 1.8559746999675644e-05,
      "loss": 2.5554,
      "step": 970700
    },
    {
      "epoch": 314.6839546191248,
      "grad_norm": 1.191835880279541,
      "learning_rate": 1.85565034057736e-05,
      "loss": 2.5371,
      "step": 970800
    },
    {
      "epoch": 314.7163695299838,
      "grad_norm": 1.4135783910751343,
      "learning_rate": 1.8553259811871554e-05,
      "loss": 2.5522,
      "step": 970900
    },
    {
      "epoch": 314.74878444084277,
      "grad_norm": 1.42196786403656,
      "learning_rate": 1.855004865390853e-05,
      "loss": 2.5382,
      "step": 971000
    },
    {
      "epoch": 314.7811993517018,
      "grad_norm": 1.2648589611053467,
      "learning_rate": 1.8546805060006488e-05,
      "loss": 2.5549,
      "step": 971100
    },
    {
      "epoch": 314.8136142625608,
      "grad_norm": 1.1765769720077515,
      "learning_rate": 1.8543561466104446e-05,
      "loss": 2.5258,
      "step": 971200
    },
    {
      "epoch": 314.84602917341977,
      "grad_norm": 1.3328622579574585,
      "learning_rate": 1.8540317872202402e-05,
      "loss": 2.5624,
      "step": 971300
    },
    {
      "epoch": 314.8784440842788,
      "grad_norm": 1.3255791664123535,
      "learning_rate": 1.8537074278300357e-05,
      "loss": 2.5636,
      "step": 971400
    },
    {
      "epoch": 314.91085899513774,
      "grad_norm": 1.4216351509094238,
      "learning_rate": 1.8533830684398316e-05,
      "loss": 2.5837,
      "step": 971500
    },
    {
      "epoch": 314.94327390599676,
      "grad_norm": 1.2461307048797607,
      "learning_rate": 1.853058709049627e-05,
      "loss": 2.5501,
      "step": 971600
    },
    {
      "epoch": 314.9756888168558,
      "grad_norm": 1.3113963603973389,
      "learning_rate": 1.8527343496594227e-05,
      "loss": 2.5549,
      "step": 971700
    },
    {
      "epoch": 315.0,
      "eval_bleu": 1.0036071874659072,
      "eval_loss": 4.1826653480529785,
      "eval_runtime": 4.7303,
      "eval_samples_per_second": 104.011,
      "eval_steps_per_second": 1.691,
      "step": 971775
    },
    {
      "epoch": 315.00810372771474,
      "grad_norm": 1.4876313209533691,
      "learning_rate": 1.8524099902692185e-05,
      "loss": 2.5394,
      "step": 971800
    },
    {
      "epoch": 315.04051863857376,
      "grad_norm": 1.1532390117645264,
      "learning_rate": 1.852085630879014e-05,
      "loss": 2.5579,
      "step": 971900
    },
    {
      "epoch": 315.0729335494327,
      "grad_norm": 1.3312923908233643,
      "learning_rate": 1.8517612714888096e-05,
      "loss": 2.538,
      "step": 972000
    },
    {
      "epoch": 315.10534846029174,
      "grad_norm": 1.381470799446106,
      "learning_rate": 1.851436912098605e-05,
      "loss": 2.5649,
      "step": 972100
    },
    {
      "epoch": 315.13776337115075,
      "grad_norm": 1.2364250421524048,
      "learning_rate": 1.851112552708401e-05,
      "loss": 2.5644,
      "step": 972200
    },
    {
      "epoch": 315.1701782820097,
      "grad_norm": 1.5567127466201782,
      "learning_rate": 1.8507881933181965e-05,
      "loss": 2.5455,
      "step": 972300
    },
    {
      "epoch": 315.20259319286873,
      "grad_norm": 1.3269520998001099,
      "learning_rate": 1.8504638339279924e-05,
      "loss": 2.5437,
      "step": 972400
    },
    {
      "epoch": 315.2350081037277,
      "grad_norm": 1.2231894731521606,
      "learning_rate": 1.850139474537788e-05,
      "loss": 2.5605,
      "step": 972500
    },
    {
      "epoch": 315.2674230145867,
      "grad_norm": 1.3183530569076538,
      "learning_rate": 1.8498151151475838e-05,
      "loss": 2.5385,
      "step": 972600
    },
    {
      "epoch": 315.29983792544573,
      "grad_norm": 1.4112285375595093,
      "learning_rate": 1.8494907557573794e-05,
      "loss": 2.531,
      "step": 972700
    },
    {
      "epoch": 315.3322528363047,
      "grad_norm": 1.2780810594558716,
      "learning_rate": 1.849166396367175e-05,
      "loss": 2.5545,
      "step": 972800
    },
    {
      "epoch": 315.3646677471637,
      "grad_norm": 1.4446502923965454,
      "learning_rate": 1.8488420369769704e-05,
      "loss": 2.5685,
      "step": 972900
    },
    {
      "epoch": 315.39708265802267,
      "grad_norm": 1.5776501893997192,
      "learning_rate": 1.8485176775867663e-05,
      "loss": 2.5393,
      "step": 973000
    },
    {
      "epoch": 315.4294975688817,
      "grad_norm": 1.2620227336883545,
      "learning_rate": 1.848193318196562e-05,
      "loss": 2.5588,
      "step": 973100
    },
    {
      "epoch": 315.4619124797407,
      "grad_norm": 1.1923081874847412,
      "learning_rate": 1.8478689588063574e-05,
      "loss": 2.559,
      "step": 973200
    },
    {
      "epoch": 315.49432739059966,
      "grad_norm": 1.3294196128845215,
      "learning_rate": 1.8475445994161533e-05,
      "loss": 2.5509,
      "step": 973300
    },
    {
      "epoch": 315.5267423014587,
      "grad_norm": 1.44478178024292,
      "learning_rate": 1.8472202400259488e-05,
      "loss": 2.5331,
      "step": 973400
    },
    {
      "epoch": 315.55915721231764,
      "grad_norm": 1.5095375776290894,
      "learning_rate": 1.8468958806357443e-05,
      "loss": 2.5421,
      "step": 973500
    },
    {
      "epoch": 315.59157212317666,
      "grad_norm": 1.2739202976226807,
      "learning_rate": 1.8465715212455402e-05,
      "loss": 2.5222,
      "step": 973600
    },
    {
      "epoch": 315.6239870340357,
      "grad_norm": 1.5297341346740723,
      "learning_rate": 1.846247161855336e-05,
      "loss": 2.5684,
      "step": 973700
    },
    {
      "epoch": 315.65640194489464,
      "grad_norm": 1.2569750547409058,
      "learning_rate": 1.8459228024651316e-05,
      "loss": 2.5418,
      "step": 973800
    },
    {
      "epoch": 315.68881685575366,
      "grad_norm": 1.4830304384231567,
      "learning_rate": 1.845598443074927e-05,
      "loss": 2.5348,
      "step": 973900
    },
    {
      "epoch": 315.7212317666126,
      "grad_norm": 1.2837988138198853,
      "learning_rate": 1.8452740836847227e-05,
      "loss": 2.5404,
      "step": 974000
    },
    {
      "epoch": 315.75364667747164,
      "grad_norm": 1.441190242767334,
      "learning_rate": 1.8449497242945186e-05,
      "loss": 2.5531,
      "step": 974100
    },
    {
      "epoch": 315.78606158833065,
      "grad_norm": 1.4405442476272583,
      "learning_rate": 1.844625364904314e-05,
      "loss": 2.5449,
      "step": 974200
    },
    {
      "epoch": 315.8184764991896,
      "grad_norm": 1.441739559173584,
      "learning_rate": 1.8443010055141096e-05,
      "loss": 2.5403,
      "step": 974300
    },
    {
      "epoch": 315.85089141004863,
      "grad_norm": 1.269133448600769,
      "learning_rate": 1.843976646123905e-05,
      "loss": 2.5496,
      "step": 974400
    },
    {
      "epoch": 315.8833063209076,
      "grad_norm": 1.117005467414856,
      "learning_rate": 1.843652286733701e-05,
      "loss": 2.549,
      "step": 974500
    },
    {
      "epoch": 315.9157212317666,
      "grad_norm": 1.4674752950668335,
      "learning_rate": 1.8433279273434966e-05,
      "loss": 2.5496,
      "step": 974600
    },
    {
      "epoch": 315.94813614262563,
      "grad_norm": 1.1963911056518555,
      "learning_rate": 1.843003567953292e-05,
      "loss": 2.5402,
      "step": 974700
    },
    {
      "epoch": 315.9805510534846,
      "grad_norm": 1.5289318561553955,
      "learning_rate": 1.842679208563088e-05,
      "loss": 2.5246,
      "step": 974800
    },
    {
      "epoch": 316.0,
      "eval_bleu": 1.0009264841551955,
      "eval_loss": 4.175643444061279,
      "eval_runtime": 4.9118,
      "eval_samples_per_second": 100.166,
      "eval_steps_per_second": 1.629,
      "step": 974860
    },
    {
      "epoch": 316.0129659643436,
      "grad_norm": 1.283963918685913,
      "learning_rate": 1.842354849172884e-05,
      "loss": 2.5277,
      "step": 974900
    },
    {
      "epoch": 316.04538087520257,
      "grad_norm": 1.406928539276123,
      "learning_rate": 1.8420337333765813e-05,
      "loss": 2.5539,
      "step": 975000
    },
    {
      "epoch": 316.0777957860616,
      "grad_norm": 1.2149162292480469,
      "learning_rate": 1.8417093739863768e-05,
      "loss": 2.5516,
      "step": 975100
    },
    {
      "epoch": 316.1102106969206,
      "grad_norm": 1.3051029443740845,
      "learning_rate": 1.8413850145961724e-05,
      "loss": 2.5637,
      "step": 975200
    },
    {
      "epoch": 316.14262560777956,
      "grad_norm": 1.358449935913086,
      "learning_rate": 1.8410606552059682e-05,
      "loss": 2.5492,
      "step": 975300
    },
    {
      "epoch": 316.1750405186386,
      "grad_norm": 1.320054531097412,
      "learning_rate": 1.840739539409666e-05,
      "loss": 2.5518,
      "step": 975400
    },
    {
      "epoch": 316.20745542949754,
      "grad_norm": 1.3540719747543335,
      "learning_rate": 1.8404151800194616e-05,
      "loss": 2.5345,
      "step": 975500
    },
    {
      "epoch": 316.23987034035656,
      "grad_norm": 1.2674919366836548,
      "learning_rate": 1.840090820629257e-05,
      "loss": 2.5512,
      "step": 975600
    },
    {
      "epoch": 316.2722852512156,
      "grad_norm": 1.2287187576293945,
      "learning_rate": 1.839766461239053e-05,
      "loss": 2.5409,
      "step": 975700
    },
    {
      "epoch": 316.30470016207454,
      "grad_norm": 1.518653154373169,
      "learning_rate": 1.8394421018488485e-05,
      "loss": 2.5383,
      "step": 975800
    },
    {
      "epoch": 316.33711507293356,
      "grad_norm": 1.3506457805633545,
      "learning_rate": 1.8391177424586444e-05,
      "loss": 2.5355,
      "step": 975900
    },
    {
      "epoch": 316.3695299837925,
      "grad_norm": 1.4415864944458008,
      "learning_rate": 1.83879338306844e-05,
      "loss": 2.5501,
      "step": 976000
    },
    {
      "epoch": 316.40194489465154,
      "grad_norm": 1.2467684745788574,
      "learning_rate": 1.8384690236782358e-05,
      "loss": 2.5563,
      "step": 976100
    },
    {
      "epoch": 316.43435980551055,
      "grad_norm": 1.423264503479004,
      "learning_rate": 1.8381446642880313e-05,
      "loss": 2.5601,
      "step": 976200
    },
    {
      "epoch": 316.4667747163695,
      "grad_norm": 1.3215185403823853,
      "learning_rate": 1.837820304897827e-05,
      "loss": 2.5309,
      "step": 976300
    },
    {
      "epoch": 316.49918962722853,
      "grad_norm": 1.487824559211731,
      "learning_rate": 1.8374959455076227e-05,
      "loss": 2.5538,
      "step": 976400
    },
    {
      "epoch": 316.5316045380875,
      "grad_norm": 1.3149425983428955,
      "learning_rate": 1.8371715861174183e-05,
      "loss": 2.5396,
      "step": 976500
    },
    {
      "epoch": 316.5640194489465,
      "grad_norm": 1.3654534816741943,
      "learning_rate": 1.8368472267272138e-05,
      "loss": 2.531,
      "step": 976600
    },
    {
      "epoch": 316.5964343598055,
      "grad_norm": 1.3570245504379272,
      "learning_rate": 1.8365228673370093e-05,
      "loss": 2.5553,
      "step": 976700
    },
    {
      "epoch": 316.6288492706645,
      "grad_norm": 1.3520578145980835,
      "learning_rate": 1.8361985079468052e-05,
      "loss": 2.5602,
      "step": 976800
    },
    {
      "epoch": 316.6612641815235,
      "grad_norm": 1.4635074138641357,
      "learning_rate": 1.8358741485566007e-05,
      "loss": 2.5443,
      "step": 976900
    },
    {
      "epoch": 316.6936790923825,
      "grad_norm": 1.37403404712677,
      "learning_rate": 1.8355497891663963e-05,
      "loss": 2.5444,
      "step": 977000
    },
    {
      "epoch": 316.7260940032415,
      "grad_norm": 1.4320200681686401,
      "learning_rate": 1.835225429776192e-05,
      "loss": 2.541,
      "step": 977100
    },
    {
      "epoch": 316.7585089141005,
      "grad_norm": 1.253191590309143,
      "learning_rate": 1.8349010703859877e-05,
      "loss": 2.5324,
      "step": 977200
    },
    {
      "epoch": 316.79092382495946,
      "grad_norm": 1.396998643875122,
      "learning_rate": 1.8345767109957836e-05,
      "loss": 2.5439,
      "step": 977300
    },
    {
      "epoch": 316.8233387358185,
      "grad_norm": 1.405869722366333,
      "learning_rate": 1.834252351605579e-05,
      "loss": 2.5382,
      "step": 977400
    },
    {
      "epoch": 316.8557536466775,
      "grad_norm": 1.5908793210983276,
      "learning_rate": 1.8339279922153746e-05,
      "loss": 2.5617,
      "step": 977500
    },
    {
      "epoch": 316.88816855753646,
      "grad_norm": 1.3701677322387695,
      "learning_rate": 1.8336036328251705e-05,
      "loss": 2.5474,
      "step": 977600
    },
    {
      "epoch": 316.9205834683955,
      "grad_norm": 1.2853723764419556,
      "learning_rate": 1.833279273434966e-05,
      "loss": 2.5351,
      "step": 977700
    },
    {
      "epoch": 316.95299837925444,
      "grad_norm": 1.3306021690368652,
      "learning_rate": 1.8329549140447616e-05,
      "loss": 2.5159,
      "step": 977800
    },
    {
      "epoch": 316.98541329011346,
      "grad_norm": 1.20084810256958,
      "learning_rate": 1.8326305546545575e-05,
      "loss": 2.5408,
      "step": 977900
    },
    {
      "epoch": 317.0,
      "eval_bleu": 1.2022283452014073,
      "eval_loss": 4.182836532592773,
      "eval_runtime": 4.5705,
      "eval_samples_per_second": 107.647,
      "eval_steps_per_second": 1.75,
      "step": 977945
    },
    {
      "epoch": 317.0178282009725,
      "grad_norm": 1.529228687286377,
      "learning_rate": 1.832306195264353e-05,
      "loss": 2.547,
      "step": 978000
    },
    {
      "epoch": 317.05024311183143,
      "grad_norm": 1.3590058088302612,
      "learning_rate": 1.8319818358741485e-05,
      "loss": 2.5593,
      "step": 978100
    },
    {
      "epoch": 317.08265802269045,
      "grad_norm": 1.301513910293579,
      "learning_rate": 1.831657476483944e-05,
      "loss": 2.5376,
      "step": 978200
    },
    {
      "epoch": 317.1150729335494,
      "grad_norm": 1.402637004852295,
      "learning_rate": 1.83133311709374e-05,
      "loss": 2.5314,
      "step": 978300
    },
    {
      "epoch": 317.14748784440843,
      "grad_norm": 1.3048052787780762,
      "learning_rate": 1.8310087577035355e-05,
      "loss": 2.5446,
      "step": 978400
    },
    {
      "epoch": 317.17990275526745,
      "grad_norm": 1.3856751918792725,
      "learning_rate": 1.8306843983133313e-05,
      "loss": 2.5637,
      "step": 978500
    },
    {
      "epoch": 317.2123176661264,
      "grad_norm": 1.4234483242034912,
      "learning_rate": 1.830360038923127e-05,
      "loss": 2.541,
      "step": 978600
    },
    {
      "epoch": 317.2447325769854,
      "grad_norm": 1.35899817943573,
      "learning_rate": 1.8300356795329228e-05,
      "loss": 2.5442,
      "step": 978700
    },
    {
      "epoch": 317.2771474878444,
      "grad_norm": 1.2924798727035522,
      "learning_rate": 1.8297113201427183e-05,
      "loss": 2.539,
      "step": 978800
    },
    {
      "epoch": 317.3095623987034,
      "grad_norm": 1.3572138547897339,
      "learning_rate": 1.8293869607525138e-05,
      "loss": 2.5416,
      "step": 978900
    },
    {
      "epoch": 317.3419773095624,
      "grad_norm": 1.298473834991455,
      "learning_rate": 1.8290626013623094e-05,
      "loss": 2.5454,
      "step": 979000
    },
    {
      "epoch": 317.3743922204214,
      "grad_norm": 1.422610878944397,
      "learning_rate": 1.8287382419721052e-05,
      "loss": 2.539,
      "step": 979100
    },
    {
      "epoch": 317.4068071312804,
      "grad_norm": 1.1231269836425781,
      "learning_rate": 1.8284138825819008e-05,
      "loss": 2.5648,
      "step": 979200
    },
    {
      "epoch": 317.43922204213936,
      "grad_norm": 1.3894281387329102,
      "learning_rate": 1.8280895231916963e-05,
      "loss": 2.5401,
      "step": 979300
    },
    {
      "epoch": 317.4716369529984,
      "grad_norm": 1.51462984085083,
      "learning_rate": 1.827768407395394e-05,
      "loss": 2.5288,
      "step": 979400
    },
    {
      "epoch": 317.5040518638574,
      "grad_norm": 1.4757506847381592,
      "learning_rate": 1.827447291599092e-05,
      "loss": 2.5322,
      "step": 979500
    },
    {
      "epoch": 317.53646677471636,
      "grad_norm": 1.3951411247253418,
      "learning_rate": 1.8271229322088877e-05,
      "loss": 2.5522,
      "step": 979600
    },
    {
      "epoch": 317.5688816855754,
      "grad_norm": 1.401667594909668,
      "learning_rate": 1.8267985728186833e-05,
      "loss": 2.5367,
      "step": 979700
    },
    {
      "epoch": 317.60129659643434,
      "grad_norm": 1.3152761459350586,
      "learning_rate": 1.8264742134284788e-05,
      "loss": 2.526,
      "step": 979800
    },
    {
      "epoch": 317.63371150729336,
      "grad_norm": 1.4201912879943848,
      "learning_rate": 1.8261498540382747e-05,
      "loss": 2.5654,
      "step": 979900
    },
    {
      "epoch": 317.6661264181524,
      "grad_norm": 1.3945562839508057,
      "learning_rate": 1.8258254946480702e-05,
      "loss": 2.5399,
      "step": 980000
    },
    {
      "epoch": 317.69854132901133,
      "grad_norm": 1.3772578239440918,
      "learning_rate": 1.8255011352578658e-05,
      "loss": 2.5487,
      "step": 980100
    },
    {
      "epoch": 317.73095623987035,
      "grad_norm": 1.31277334690094,
      "learning_rate": 1.8251767758676613e-05,
      "loss": 2.5461,
      "step": 980200
    },
    {
      "epoch": 317.7633711507293,
      "grad_norm": 1.23329496383667,
      "learning_rate": 1.824852416477457e-05,
      "loss": 2.5462,
      "step": 980300
    },
    {
      "epoch": 317.79578606158833,
      "grad_norm": 1.4154051542282104,
      "learning_rate": 1.8245280570872527e-05,
      "loss": 2.5385,
      "step": 980400
    },
    {
      "epoch": 317.82820097244735,
      "grad_norm": 1.3346418142318726,
      "learning_rate": 1.8242036976970482e-05,
      "loss": 2.5441,
      "step": 980500
    },
    {
      "epoch": 317.8606158833063,
      "grad_norm": 1.413711428642273,
      "learning_rate": 1.8238793383068438e-05,
      "loss": 2.5582,
      "step": 980600
    },
    {
      "epoch": 317.8930307941653,
      "grad_norm": 1.3520658016204834,
      "learning_rate": 1.8235549789166396e-05,
      "loss": 2.5549,
      "step": 980700
    },
    {
      "epoch": 317.9254457050243,
      "grad_norm": 1.2590444087982178,
      "learning_rate": 1.8232306195264355e-05,
      "loss": 2.5326,
      "step": 980800
    },
    {
      "epoch": 317.9578606158833,
      "grad_norm": 1.5802907943725586,
      "learning_rate": 1.822906260136231e-05,
      "loss": 2.5341,
      "step": 980900
    },
    {
      "epoch": 317.9902755267423,
      "grad_norm": 1.416207194328308,
      "learning_rate": 1.822581900746027e-05,
      "loss": 2.5596,
      "step": 981000
    },
    {
      "epoch": 318.0,
      "eval_bleu": 1.106904749370297,
      "eval_loss": 4.178314208984375,
      "eval_runtime": 4.5482,
      "eval_samples_per_second": 108.174,
      "eval_steps_per_second": 1.759,
      "step": 981030
    },
    {
      "epoch": 318.0226904376013,
      "grad_norm": 1.3369718790054321,
      "learning_rate": 1.8222575413558225e-05,
      "loss": 2.5308,
      "step": 981100
    },
    {
      "epoch": 318.0551053484603,
      "grad_norm": 1.2953391075134277,
      "learning_rate": 1.821933181965618e-05,
      "loss": 2.523,
      "step": 981200
    },
    {
      "epoch": 318.08752025931926,
      "grad_norm": 1.5186398029327393,
      "learning_rate": 1.8216088225754135e-05,
      "loss": 2.5362,
      "step": 981300
    },
    {
      "epoch": 318.1199351701783,
      "grad_norm": 1.3242911100387573,
      "learning_rate": 1.8212844631852094e-05,
      "loss": 2.5562,
      "step": 981400
    },
    {
      "epoch": 318.1523500810373,
      "grad_norm": 1.3633712530136108,
      "learning_rate": 1.820960103795005e-05,
      "loss": 2.5558,
      "step": 981500
    },
    {
      "epoch": 318.18476499189626,
      "grad_norm": 1.3761107921600342,
      "learning_rate": 1.8206357444048005e-05,
      "loss": 2.5417,
      "step": 981600
    },
    {
      "epoch": 318.2171799027553,
      "grad_norm": 1.4700909852981567,
      "learning_rate": 1.820311385014596e-05,
      "loss": 2.558,
      "step": 981700
    },
    {
      "epoch": 318.24959481361424,
      "grad_norm": 1.289268970489502,
      "learning_rate": 1.819987025624392e-05,
      "loss": 2.5324,
      "step": 981800
    },
    {
      "epoch": 318.28200972447326,
      "grad_norm": 1.2974863052368164,
      "learning_rate": 1.8196626662341874e-05,
      "loss": 2.5369,
      "step": 981900
    },
    {
      "epoch": 318.3144246353323,
      "grad_norm": 1.381018042564392,
      "learning_rate": 1.8193383068439833e-05,
      "loss": 2.547,
      "step": 982000
    },
    {
      "epoch": 318.34683954619123,
      "grad_norm": 1.522092580795288,
      "learning_rate": 1.8190139474537792e-05,
      "loss": 2.5345,
      "step": 982100
    },
    {
      "epoch": 318.37925445705025,
      "grad_norm": 1.322302222251892,
      "learning_rate": 1.8186895880635747e-05,
      "loss": 2.5344,
      "step": 982200
    },
    {
      "epoch": 318.4116693679092,
      "grad_norm": 1.4821288585662842,
      "learning_rate": 1.8183652286733702e-05,
      "loss": 2.5553,
      "step": 982300
    },
    {
      "epoch": 318.44408427876823,
      "grad_norm": 1.2566208839416504,
      "learning_rate": 1.8180408692831658e-05,
      "loss": 2.53,
      "step": 982400
    },
    {
      "epoch": 318.47649918962725,
      "grad_norm": 1.2920994758605957,
      "learning_rate": 1.8177165098929617e-05,
      "loss": 2.5372,
      "step": 982500
    },
    {
      "epoch": 318.5089141004862,
      "grad_norm": 1.2650033235549927,
      "learning_rate": 1.8173921505027572e-05,
      "loss": 2.5229,
      "step": 982600
    },
    {
      "epoch": 318.5413290113452,
      "grad_norm": 1.2347403764724731,
      "learning_rate": 1.8170677911125527e-05,
      "loss": 2.5388,
      "step": 982700
    },
    {
      "epoch": 318.5737439222042,
      "grad_norm": 1.1827659606933594,
      "learning_rate": 1.8167434317223483e-05,
      "loss": 2.5396,
      "step": 982800
    },
    {
      "epoch": 318.6061588330632,
      "grad_norm": 1.2072292566299438,
      "learning_rate": 1.816419072332144e-05,
      "loss": 2.5591,
      "step": 982900
    },
    {
      "epoch": 318.6385737439222,
      "grad_norm": 1.381081223487854,
      "learning_rate": 1.8160947129419397e-05,
      "loss": 2.5442,
      "step": 983000
    },
    {
      "epoch": 318.6709886547812,
      "grad_norm": 1.358972191810608,
      "learning_rate": 1.8157735971456375e-05,
      "loss": 2.5515,
      "step": 983100
    },
    {
      "epoch": 318.7034035656402,
      "grad_norm": 1.354101300239563,
      "learning_rate": 1.815449237755433e-05,
      "loss": 2.534,
      "step": 983200
    },
    {
      "epoch": 318.73581847649916,
      "grad_norm": 1.3707753419876099,
      "learning_rate": 1.815124878365229e-05,
      "loss": 2.5539,
      "step": 983300
    },
    {
      "epoch": 318.7682333873582,
      "grad_norm": 1.500185489654541,
      "learning_rate": 1.8148005189750244e-05,
      "loss": 2.5568,
      "step": 983400
    },
    {
      "epoch": 318.8006482982172,
      "grad_norm": 1.7343910932540894,
      "learning_rate": 1.81447615958482e-05,
      "loss": 2.5748,
      "step": 983500
    },
    {
      "epoch": 318.83306320907616,
      "grad_norm": 1.2770698070526123,
      "learning_rate": 1.8141518001946155e-05,
      "loss": 2.5309,
      "step": 983600
    },
    {
      "epoch": 318.8654781199352,
      "grad_norm": 1.3333088159561157,
      "learning_rate": 1.8138306843983136e-05,
      "loss": 2.5388,
      "step": 983700
    },
    {
      "epoch": 318.8978930307942,
      "grad_norm": 1.4677098989486694,
      "learning_rate": 1.813506325008109e-05,
      "loss": 2.5667,
      "step": 983800
    },
    {
      "epoch": 318.93030794165315,
      "grad_norm": 1.3388915061950684,
      "learning_rate": 1.8131819656179047e-05,
      "loss": 2.552,
      "step": 983900
    },
    {
      "epoch": 318.9627228525122,
      "grad_norm": 1.3266109228134155,
      "learning_rate": 1.8128576062277002e-05,
      "loss": 2.5393,
      "step": 984000
    },
    {
      "epoch": 318.99513776337113,
      "grad_norm": 1.4767779111862183,
      "learning_rate": 1.812533246837496e-05,
      "loss": 2.5543,
      "step": 984100
    },
    {
      "epoch": 319.0,
      "eval_bleu": 1.0179320187888454,
      "eval_loss": 4.179269790649414,
      "eval_runtime": 4.5245,
      "eval_samples_per_second": 108.74,
      "eval_steps_per_second": 1.768,
      "step": 984115
    },
    {
      "epoch": 319.02755267423015,
      "grad_norm": 1.2035263776779175,
      "learning_rate": 1.8122088874472916e-05,
      "loss": 2.5433,
      "step": 984200
    },
    {
      "epoch": 319.05996758508917,
      "grad_norm": 1.3127362728118896,
      "learning_rate": 1.811884528057087e-05,
      "loss": 2.5527,
      "step": 984300
    },
    {
      "epoch": 319.09238249594813,
      "grad_norm": 1.3536847829818726,
      "learning_rate": 1.811560168666883e-05,
      "loss": 2.5544,
      "step": 984400
    },
    {
      "epoch": 319.12479740680715,
      "grad_norm": 1.4675066471099854,
      "learning_rate": 1.811235809276679e-05,
      "loss": 2.5332,
      "step": 984500
    },
    {
      "epoch": 319.1572123176661,
      "grad_norm": 1.3137998580932617,
      "learning_rate": 1.8109114498864744e-05,
      "loss": 2.5286,
      "step": 984600
    },
    {
      "epoch": 319.1896272285251,
      "grad_norm": 1.5313349962234497,
      "learning_rate": 1.81058709049627e-05,
      "loss": 2.5427,
      "step": 984700
    },
    {
      "epoch": 319.22204213938414,
      "grad_norm": 1.1958520412445068,
      "learning_rate": 1.8102627311060655e-05,
      "loss": 2.5264,
      "step": 984800
    },
    {
      "epoch": 319.2544570502431,
      "grad_norm": 1.336695909500122,
      "learning_rate": 1.8099383717158614e-05,
      "loss": 2.5379,
      "step": 984900
    },
    {
      "epoch": 319.2868719611021,
      "grad_norm": 1.415232539176941,
      "learning_rate": 1.809614012325657e-05,
      "loss": 2.5476,
      "step": 985000
    },
    {
      "epoch": 319.3192868719611,
      "grad_norm": 1.3062312602996826,
      "learning_rate": 1.8092896529354524e-05,
      "loss": 2.5454,
      "step": 985100
    },
    {
      "epoch": 319.3517017828201,
      "grad_norm": 1.266814947128296,
      "learning_rate": 1.8089652935452483e-05,
      "loss": 2.5361,
      "step": 985200
    },
    {
      "epoch": 319.3841166936791,
      "grad_norm": 1.4225380420684814,
      "learning_rate": 1.808640934155044e-05,
      "loss": 2.5341,
      "step": 985300
    },
    {
      "epoch": 319.4165316045381,
      "grad_norm": 1.2739273309707642,
      "learning_rate": 1.8083165747648394e-05,
      "loss": 2.5441,
      "step": 985400
    },
    {
      "epoch": 319.4489465153971,
      "grad_norm": 1.2853333950042725,
      "learning_rate": 1.807992215374635e-05,
      "loss": 2.5373,
      "step": 985500
    },
    {
      "epoch": 319.48136142625606,
      "grad_norm": 1.4444010257720947,
      "learning_rate": 1.8076678559844308e-05,
      "loss": 2.547,
      "step": 985600
    },
    {
      "epoch": 319.5137763371151,
      "grad_norm": 1.549599051475525,
      "learning_rate": 1.8073434965942267e-05,
      "loss": 2.551,
      "step": 985700
    },
    {
      "epoch": 319.5461912479741,
      "grad_norm": 1.2422094345092773,
      "learning_rate": 1.8070191372040222e-05,
      "loss": 2.5511,
      "step": 985800
    },
    {
      "epoch": 319.57860615883305,
      "grad_norm": 1.7034597396850586,
      "learning_rate": 1.8066947778138177e-05,
      "loss": 2.5413,
      "step": 985900
    },
    {
      "epoch": 319.6110210696921,
      "grad_norm": 1.5176739692687988,
      "learning_rate": 1.8063704184236136e-05,
      "loss": 2.5318,
      "step": 986000
    },
    {
      "epoch": 319.64343598055103,
      "grad_norm": 1.4076899290084839,
      "learning_rate": 1.806046059033409e-05,
      "loss": 2.529,
      "step": 986100
    },
    {
      "epoch": 319.67585089141005,
      "grad_norm": 1.2465375661849976,
      "learning_rate": 1.8057216996432047e-05,
      "loss": 2.545,
      "step": 986200
    },
    {
      "epoch": 319.70826580226907,
      "grad_norm": 1.3916980028152466,
      "learning_rate": 1.8053973402530002e-05,
      "loss": 2.5579,
      "step": 986300
    },
    {
      "epoch": 319.74068071312803,
      "grad_norm": 1.7354308366775513,
      "learning_rate": 1.805072980862796e-05,
      "loss": 2.5327,
      "step": 986400
    },
    {
      "epoch": 319.77309562398705,
      "grad_norm": 1.577783465385437,
      "learning_rate": 1.8047486214725916e-05,
      "loss": 2.5479,
      "step": 986500
    },
    {
      "epoch": 319.805510534846,
      "grad_norm": 1.336361050605774,
      "learning_rate": 1.804424262082387e-05,
      "loss": 2.5612,
      "step": 986600
    },
    {
      "epoch": 319.837925445705,
      "grad_norm": 1.428612470626831,
      "learning_rate": 1.804099902692183e-05,
      "loss": 2.543,
      "step": 986700
    },
    {
      "epoch": 319.87034035656404,
      "grad_norm": 1.7768007516860962,
      "learning_rate": 1.8037755433019786e-05,
      "loss": 2.5401,
      "step": 986800
    },
    {
      "epoch": 319.902755267423,
      "grad_norm": 1.4078946113586426,
      "learning_rate": 1.8034511839117745e-05,
      "loss": 2.5405,
      "step": 986900
    },
    {
      "epoch": 319.935170178282,
      "grad_norm": 1.2512305974960327,
      "learning_rate": 1.80312682452157e-05,
      "loss": 2.5583,
      "step": 987000
    },
    {
      "epoch": 319.967585089141,
      "grad_norm": 1.2218172550201416,
      "learning_rate": 1.802802465131366e-05,
      "loss": 2.542,
      "step": 987100
    },
    {
      "epoch": 320.0,
      "grad_norm": 1.4291510581970215,
      "learning_rate": 1.8024781057411614e-05,
      "loss": 2.544,
      "step": 987200
    },
    {
      "epoch": 320.0,
      "eval_bleu": 1.059475501425284,
      "eval_loss": 4.181571960449219,
      "eval_runtime": 4.9444,
      "eval_samples_per_second": 99.507,
      "eval_steps_per_second": 1.618,
      "step": 987200
    },
    {
      "epoch": 320.032414910859,
      "grad_norm": 1.3047528266906738,
      "learning_rate": 1.802153746350957e-05,
      "loss": 2.5233,
      "step": 987300
    },
    {
      "epoch": 320.064829821718,
      "grad_norm": 1.424042820930481,
      "learning_rate": 1.8018293869607525e-05,
      "loss": 2.5689,
      "step": 987400
    },
    {
      "epoch": 320.097244732577,
      "grad_norm": 1.4254591464996338,
      "learning_rate": 1.8015050275705483e-05,
      "loss": 2.5261,
      "step": 987500
    },
    {
      "epoch": 320.12965964343596,
      "grad_norm": 1.3687268495559692,
      "learning_rate": 1.801180668180344e-05,
      "loss": 2.5246,
      "step": 987600
    },
    {
      "epoch": 320.162074554295,
      "grad_norm": 1.2316275835037231,
      "learning_rate": 1.8008563087901394e-05,
      "loss": 2.5262,
      "step": 987700
    },
    {
      "epoch": 320.194489465154,
      "grad_norm": 1.2640421390533447,
      "learning_rate": 1.800531949399935e-05,
      "loss": 2.5407,
      "step": 987800
    },
    {
      "epoch": 320.22690437601295,
      "grad_norm": 1.335171103477478,
      "learning_rate": 1.8002075900097308e-05,
      "loss": 2.5326,
      "step": 987900
    },
    {
      "epoch": 320.25931928687197,
      "grad_norm": 1.306966781616211,
      "learning_rate": 1.7998832306195264e-05,
      "loss": 2.5725,
      "step": 988000
    },
    {
      "epoch": 320.29173419773093,
      "grad_norm": 1.2707935571670532,
      "learning_rate": 1.7995588712293222e-05,
      "loss": 2.5507,
      "step": 988100
    },
    {
      "epoch": 320.32414910858995,
      "grad_norm": 1.2199229001998901,
      "learning_rate": 1.799234511839118e-05,
      "loss": 2.5467,
      "step": 988200
    },
    {
      "epoch": 320.35656401944897,
      "grad_norm": 1.2152279615402222,
      "learning_rate": 1.7989101524489136e-05,
      "loss": 2.5542,
      "step": 988300
    },
    {
      "epoch": 320.3889789303079,
      "grad_norm": 1.319863200187683,
      "learning_rate": 1.798589036652611e-05,
      "loss": 2.5427,
      "step": 988400
    },
    {
      "epoch": 320.42139384116695,
      "grad_norm": 1.426952600479126,
      "learning_rate": 1.7982646772624066e-05,
      "loss": 2.5455,
      "step": 988500
    },
    {
      "epoch": 320.4538087520259,
      "grad_norm": 1.3288952112197876,
      "learning_rate": 1.7979403178722025e-05,
      "loss": 2.5342,
      "step": 988600
    },
    {
      "epoch": 320.4862236628849,
      "grad_norm": 1.3919854164123535,
      "learning_rate": 1.7976159584819984e-05,
      "loss": 2.5485,
      "step": 988700
    },
    {
      "epoch": 320.51863857374394,
      "grad_norm": 1.3133769035339355,
      "learning_rate": 1.797291599091794e-05,
      "loss": 2.5281,
      "step": 988800
    },
    {
      "epoch": 320.5510534846029,
      "grad_norm": 1.287251353263855,
      "learning_rate": 1.7969672397015894e-05,
      "loss": 2.5285,
      "step": 988900
    },
    {
      "epoch": 320.5834683954619,
      "grad_norm": 1.3282394409179688,
      "learning_rate": 1.7966428803113853e-05,
      "loss": 2.527,
      "step": 989000
    },
    {
      "epoch": 320.6158833063209,
      "grad_norm": 1.2834408283233643,
      "learning_rate": 1.796318520921181e-05,
      "loss": 2.5481,
      "step": 989100
    },
    {
      "epoch": 320.6482982171799,
      "grad_norm": 1.4780749082565308,
      "learning_rate": 1.7959941615309764e-05,
      "loss": 2.5498,
      "step": 989200
    },
    {
      "epoch": 320.6807131280389,
      "grad_norm": 1.1384034156799316,
      "learning_rate": 1.795669802140772e-05,
      "loss": 2.5501,
      "step": 989300
    },
    {
      "epoch": 320.7131280388979,
      "grad_norm": 1.2490172386169434,
      "learning_rate": 1.7953454427505678e-05,
      "loss": 2.5386,
      "step": 989400
    },
    {
      "epoch": 320.7455429497569,
      "grad_norm": 1.5028584003448486,
      "learning_rate": 1.7950210833603633e-05,
      "loss": 2.5252,
      "step": 989500
    },
    {
      "epoch": 320.77795786061586,
      "grad_norm": 1.470320701599121,
      "learning_rate": 1.794696723970159e-05,
      "loss": 2.5436,
      "step": 989600
    },
    {
      "epoch": 320.8103727714749,
      "grad_norm": 1.4083824157714844,
      "learning_rate": 1.7943723645799544e-05,
      "loss": 2.5301,
      "step": 989700
    },
    {
      "epoch": 320.8427876823339,
      "grad_norm": 1.4534735679626465,
      "learning_rate": 1.7940480051897503e-05,
      "loss": 2.5641,
      "step": 989800
    },
    {
      "epoch": 320.87520259319285,
      "grad_norm": 1.3756353855133057,
      "learning_rate": 1.793723645799546e-05,
      "loss": 2.5489,
      "step": 989900
    },
    {
      "epoch": 320.90761750405187,
      "grad_norm": 1.353867530822754,
      "learning_rate": 1.7933992864093417e-05,
      "loss": 2.5339,
      "step": 990000
    },
    {
      "epoch": 320.94003241491083,
      "grad_norm": 1.2813371419906616,
      "learning_rate": 1.7930749270191372e-05,
      "loss": 2.5447,
      "step": 990100
    },
    {
      "epoch": 320.97244732576985,
      "grad_norm": 1.495436668395996,
      "learning_rate": 1.792750567628933e-05,
      "loss": 2.5619,
      "step": 990200
    },
    {
      "epoch": 321.0,
      "eval_bleu": 1.0515162814041747,
      "eval_loss": 4.184642314910889,
      "eval_runtime": 4.7408,
      "eval_samples_per_second": 103.78,
      "eval_steps_per_second": 1.687,
      "step": 990285
    },
    {
      "epoch": 321.00486223662887,
      "grad_norm": 1.4523483514785767,
      "learning_rate": 1.7924262082387286e-05,
      "loss": 2.5205,
      "step": 990300
    },
    {
      "epoch": 321.0372771474878,
      "grad_norm": 1.4870342016220093,
      "learning_rate": 1.792101848848524e-05,
      "loss": 2.5284,
      "step": 990400
    },
    {
      "epoch": 321.06969205834685,
      "grad_norm": 1.329959511756897,
      "learning_rate": 1.79177748945832e-05,
      "loss": 2.5393,
      "step": 990500
    },
    {
      "epoch": 321.1021069692058,
      "grad_norm": 1.2574950456619263,
      "learning_rate": 1.7914531300681156e-05,
      "loss": 2.5338,
      "step": 990600
    },
    {
      "epoch": 321.1345218800648,
      "grad_norm": 1.4177019596099854,
      "learning_rate": 1.791128770677911e-05,
      "loss": 2.5424,
      "step": 990700
    },
    {
      "epoch": 321.16693679092384,
      "grad_norm": 1.4701154232025146,
      "learning_rate": 1.7908044112877066e-05,
      "loss": 2.5367,
      "step": 990800
    },
    {
      "epoch": 321.1993517017828,
      "grad_norm": 1.3089128732681274,
      "learning_rate": 1.7904800518975025e-05,
      "loss": 2.5203,
      "step": 990900
    },
    {
      "epoch": 321.2317666126418,
      "grad_norm": 1.4043912887573242,
      "learning_rate": 1.790155692507298e-05,
      "loss": 2.5401,
      "step": 991000
    },
    {
      "epoch": 321.26418152350084,
      "grad_norm": 1.1841158866882324,
      "learning_rate": 1.789831333117094e-05,
      "loss": 2.5363,
      "step": 991100
    },
    {
      "epoch": 321.2965964343598,
      "grad_norm": 1.3323924541473389,
      "learning_rate": 1.7895069737268895e-05,
      "loss": 2.5211,
      "step": 991200
    },
    {
      "epoch": 321.3290113452188,
      "grad_norm": 1.4037350416183472,
      "learning_rate": 1.7891826143366853e-05,
      "loss": 2.5265,
      "step": 991300
    },
    {
      "epoch": 321.3614262560778,
      "grad_norm": 1.5298829078674316,
      "learning_rate": 1.788858254946481e-05,
      "loss": 2.5351,
      "step": 991400
    },
    {
      "epoch": 321.3938411669368,
      "grad_norm": 1.2386122941970825,
      "learning_rate": 1.7885338955562764e-05,
      "loss": 2.547,
      "step": 991500
    },
    {
      "epoch": 321.4262560777958,
      "grad_norm": 1.419700026512146,
      "learning_rate": 1.7882095361660723e-05,
      "loss": 2.5532,
      "step": 991600
    },
    {
      "epoch": 321.4586709886548,
      "grad_norm": 1.2719606161117554,
      "learning_rate": 1.7878851767758678e-05,
      "loss": 2.5428,
      "step": 991700
    },
    {
      "epoch": 321.4910858995138,
      "grad_norm": 1.2524468898773193,
      "learning_rate": 1.7875608173856634e-05,
      "loss": 2.5424,
      "step": 991800
    },
    {
      "epoch": 321.52350081037275,
      "grad_norm": 1.2222180366516113,
      "learning_rate": 1.787236457995459e-05,
      "loss": 2.5165,
      "step": 991900
    },
    {
      "epoch": 321.55591572123177,
      "grad_norm": 1.5213828086853027,
      "learning_rate": 1.7869153421991567e-05,
      "loss": 2.546,
      "step": 992000
    },
    {
      "epoch": 321.5883306320908,
      "grad_norm": 1.3494393825531006,
      "learning_rate": 1.7865909828089525e-05,
      "loss": 2.5586,
      "step": 992100
    },
    {
      "epoch": 321.62074554294975,
      "grad_norm": 1.1806937456130981,
      "learning_rate": 1.786266623418748e-05,
      "loss": 2.547,
      "step": 992200
    },
    {
      "epoch": 321.65316045380877,
      "grad_norm": 1.321994423866272,
      "learning_rate": 1.7859422640285436e-05,
      "loss": 2.5113,
      "step": 992300
    },
    {
      "epoch": 321.6855753646677,
      "grad_norm": 1.3348904848098755,
      "learning_rate": 1.7856179046383395e-05,
      "loss": 2.5404,
      "step": 992400
    },
    {
      "epoch": 321.71799027552674,
      "grad_norm": 1.2737520933151245,
      "learning_rate": 1.785293545248135e-05,
      "loss": 2.5686,
      "step": 992500
    },
    {
      "epoch": 321.75040518638576,
      "grad_norm": 1.2548704147338867,
      "learning_rate": 1.7849691858579306e-05,
      "loss": 2.5395,
      "step": 992600
    },
    {
      "epoch": 321.7828200972447,
      "grad_norm": 1.3587760925292969,
      "learning_rate": 1.784644826467726e-05,
      "loss": 2.5352,
      "step": 992700
    },
    {
      "epoch": 321.81523500810374,
      "grad_norm": 1.591206431388855,
      "learning_rate": 1.784320467077522e-05,
      "loss": 2.5809,
      "step": 992800
    },
    {
      "epoch": 321.8476499189627,
      "grad_norm": 1.5442824363708496,
      "learning_rate": 1.783996107687318e-05,
      "loss": 2.5492,
      "step": 992900
    },
    {
      "epoch": 321.8800648298217,
      "grad_norm": 1.4735783338546753,
      "learning_rate": 1.7836717482971134e-05,
      "loss": 2.5364,
      "step": 993000
    },
    {
      "epoch": 321.91247974068074,
      "grad_norm": 1.4092237949371338,
      "learning_rate": 1.783347388906909e-05,
      "loss": 2.5577,
      "step": 993100
    },
    {
      "epoch": 321.9448946515397,
      "grad_norm": 1.3452788591384888,
      "learning_rate": 1.7830230295167048e-05,
      "loss": 2.5339,
      "step": 993200
    },
    {
      "epoch": 321.9773095623987,
      "grad_norm": 1.3690218925476074,
      "learning_rate": 1.7826986701265003e-05,
      "loss": 2.5394,
      "step": 993300
    },
    {
      "epoch": 322.0,
      "eval_bleu": 1.1018780878241634,
      "eval_loss": 4.181273460388184,
      "eval_runtime": 5.1027,
      "eval_samples_per_second": 96.42,
      "eval_steps_per_second": 1.568,
      "step": 993370
    },
    {
      "epoch": 322.0097244732577,
      "grad_norm": 1.2363108396530151,
      "learning_rate": 1.782374310736296e-05,
      "loss": 2.5356,
      "step": 993400
    },
    {
      "epoch": 322.0421393841167,
      "grad_norm": 1.324547290802002,
      "learning_rate": 1.7820499513460914e-05,
      "loss": 2.5346,
      "step": 993500
    },
    {
      "epoch": 322.0745542949757,
      "grad_norm": 1.3820520639419556,
      "learning_rate": 1.7817255919558873e-05,
      "loss": 2.5413,
      "step": 993600
    },
    {
      "epoch": 322.1069692058347,
      "grad_norm": 1.5446211099624634,
      "learning_rate": 1.7814012325656828e-05,
      "loss": 2.5288,
      "step": 993700
    },
    {
      "epoch": 322.1393841166937,
      "grad_norm": 1.5667608976364136,
      "learning_rate": 1.7810768731754783e-05,
      "loss": 2.5647,
      "step": 993800
    },
    {
      "epoch": 322.17179902755265,
      "grad_norm": 1.4535819292068481,
      "learning_rate": 1.7807525137852742e-05,
      "loss": 2.5507,
      "step": 993900
    },
    {
      "epoch": 322.20421393841167,
      "grad_norm": 1.4328206777572632,
      "learning_rate": 1.7804281543950698e-05,
      "loss": 2.535,
      "step": 994000
    },
    {
      "epoch": 322.2366288492707,
      "grad_norm": 1.353489875793457,
      "learning_rate": 1.7801037950048656e-05,
      "loss": 2.5185,
      "step": 994100
    },
    {
      "epoch": 322.26904376012965,
      "grad_norm": 1.193428874015808,
      "learning_rate": 1.779779435614661e-05,
      "loss": 2.5604,
      "step": 994200
    },
    {
      "epoch": 322.30145867098867,
      "grad_norm": 1.4796069860458374,
      "learning_rate": 1.779455076224457e-05,
      "loss": 2.5227,
      "step": 994300
    },
    {
      "epoch": 322.3338735818476,
      "grad_norm": 1.4256377220153809,
      "learning_rate": 1.7791307168342526e-05,
      "loss": 2.5218,
      "step": 994400
    },
    {
      "epoch": 322.36628849270664,
      "grad_norm": 1.2940547466278076,
      "learning_rate": 1.778806357444048e-05,
      "loss": 2.5487,
      "step": 994500
    },
    {
      "epoch": 322.39870340356566,
      "grad_norm": 1.778679370880127,
      "learning_rate": 1.7784819980538436e-05,
      "loss": 2.5635,
      "step": 994600
    },
    {
      "epoch": 322.4311183144246,
      "grad_norm": 1.6555876731872559,
      "learning_rate": 1.7781576386636395e-05,
      "loss": 2.5402,
      "step": 994700
    },
    {
      "epoch": 322.46353322528364,
      "grad_norm": 1.2773668766021729,
      "learning_rate": 1.777833279273435e-05,
      "loss": 2.5455,
      "step": 994800
    },
    {
      "epoch": 322.4959481361426,
      "grad_norm": 1.3182315826416016,
      "learning_rate": 1.7775089198832306e-05,
      "loss": 2.5394,
      "step": 994900
    },
    {
      "epoch": 322.5283630470016,
      "grad_norm": 1.2217975854873657,
      "learning_rate": 1.777184560493026e-05,
      "loss": 2.5428,
      "step": 995000
    },
    {
      "epoch": 322.56077795786064,
      "grad_norm": 1.3757284879684448,
      "learning_rate": 1.7768634446967242e-05,
      "loss": 2.5513,
      "step": 995100
    },
    {
      "epoch": 322.5931928687196,
      "grad_norm": 1.2546827793121338,
      "learning_rate": 1.7765423289004217e-05,
      "loss": 2.5315,
      "step": 995200
    },
    {
      "epoch": 322.6256077795786,
      "grad_norm": 1.4048553705215454,
      "learning_rate": 1.7762179695102176e-05,
      "loss": 2.5266,
      "step": 995300
    },
    {
      "epoch": 322.6580226904376,
      "grad_norm": 1.476945161819458,
      "learning_rate": 1.775893610120013e-05,
      "loss": 2.5247,
      "step": 995400
    },
    {
      "epoch": 322.6904376012966,
      "grad_norm": 1.2949799299240112,
      "learning_rate": 1.775569250729809e-05,
      "loss": 2.5434,
      "step": 995500
    },
    {
      "epoch": 322.7228525121556,
      "grad_norm": 1.3441848754882812,
      "learning_rate": 1.7752448913396045e-05,
      "loss": 2.536,
      "step": 995600
    },
    {
      "epoch": 322.7552674230146,
      "grad_norm": 1.3904846906661987,
      "learning_rate": 1.7749205319494e-05,
      "loss": 2.5332,
      "step": 995700
    },
    {
      "epoch": 322.7876823338736,
      "grad_norm": 1.3969316482543945,
      "learning_rate": 1.7745961725591956e-05,
      "loss": 2.5195,
      "step": 995800
    },
    {
      "epoch": 322.82009724473255,
      "grad_norm": 1.2545325756072998,
      "learning_rate": 1.7742718131689914e-05,
      "loss": 2.5661,
      "step": 995900
    },
    {
      "epoch": 322.85251215559157,
      "grad_norm": 1.625584363937378,
      "learning_rate": 1.773947453778787e-05,
      "loss": 2.5324,
      "step": 996000
    },
    {
      "epoch": 322.8849270664506,
      "grad_norm": 1.1789357662200928,
      "learning_rate": 1.7736230943885825e-05,
      "loss": 2.5658,
      "step": 996100
    },
    {
      "epoch": 322.91734197730955,
      "grad_norm": 1.463965892791748,
      "learning_rate": 1.773298734998378e-05,
      "loss": 2.5345,
      "step": 996200
    },
    {
      "epoch": 322.94975688816857,
      "grad_norm": 1.3887553215026855,
      "learning_rate": 1.772974375608174e-05,
      "loss": 2.5532,
      "step": 996300
    },
    {
      "epoch": 322.9821717990275,
      "grad_norm": 1.3908560276031494,
      "learning_rate": 1.7726500162179695e-05,
      "loss": 2.5488,
      "step": 996400
    },
    {
      "epoch": 323.0,
      "eval_bleu": 1.003199824796191,
      "eval_loss": 4.184145927429199,
      "eval_runtime": 4.4471,
      "eval_samples_per_second": 110.635,
      "eval_steps_per_second": 1.799,
      "step": 996455
    },
    {
      "epoch": 323.01458670988654,
      "grad_norm": 1.4150347709655762,
      "learning_rate": 1.7723256568277653e-05,
      "loss": 2.5166,
      "step": 996500
    },
    {
      "epoch": 323.04700162074556,
      "grad_norm": 1.271424412727356,
      "learning_rate": 1.772001297437561e-05,
      "loss": 2.5288,
      "step": 996600
    },
    {
      "epoch": 323.0794165316045,
      "grad_norm": 1.3123387098312378,
      "learning_rate": 1.7716769380473567e-05,
      "loss": 2.5486,
      "step": 996700
    },
    {
      "epoch": 323.11183144246354,
      "grad_norm": 1.3115419149398804,
      "learning_rate": 1.7713525786571523e-05,
      "loss": 2.5453,
      "step": 996800
    },
    {
      "epoch": 323.1442463533225,
      "grad_norm": 1.3612208366394043,
      "learning_rate": 1.7710282192669478e-05,
      "loss": 2.5246,
      "step": 996900
    },
    {
      "epoch": 323.1766612641815,
      "grad_norm": 1.4733763933181763,
      "learning_rate": 1.7707038598767437e-05,
      "loss": 2.5682,
      "step": 997000
    },
    {
      "epoch": 323.20907617504054,
      "grad_norm": 1.2163664102554321,
      "learning_rate": 1.7703795004865392e-05,
      "loss": 2.5152,
      "step": 997100
    },
    {
      "epoch": 323.2414910858995,
      "grad_norm": 1.5956581830978394,
      "learning_rate": 1.7700551410963348e-05,
      "loss": 2.5526,
      "step": 997200
    },
    {
      "epoch": 323.2739059967585,
      "grad_norm": 1.4385114908218384,
      "learning_rate": 1.7697307817061303e-05,
      "loss": 2.5201,
      "step": 997300
    },
    {
      "epoch": 323.3063209076175,
      "grad_norm": 1.4798493385314941,
      "learning_rate": 1.7694064223159262e-05,
      "loss": 2.5464,
      "step": 997400
    },
    {
      "epoch": 323.3387358184765,
      "grad_norm": 1.2893351316452026,
      "learning_rate": 1.7690820629257217e-05,
      "loss": 2.5477,
      "step": 997500
    },
    {
      "epoch": 323.3711507293355,
      "grad_norm": 1.3941692113876343,
      "learning_rate": 1.7687577035355172e-05,
      "loss": 2.5282,
      "step": 997600
    },
    {
      "epoch": 323.4035656401945,
      "grad_norm": 1.5189518928527832,
      "learning_rate": 1.768433344145313e-05,
      "loss": 2.5487,
      "step": 997700
    },
    {
      "epoch": 323.4359805510535,
      "grad_norm": 1.4648282527923584,
      "learning_rate": 1.768108984755109e-05,
      "loss": 2.5466,
      "step": 997800
    },
    {
      "epoch": 323.4683954619125,
      "grad_norm": 1.4773160219192505,
      "learning_rate": 1.7677846253649045e-05,
      "loss": 2.5547,
      "step": 997900
    },
    {
      "epoch": 323.50081037277147,
      "grad_norm": 1.2943116426467896,
      "learning_rate": 1.7674602659747e-05,
      "loss": 2.5338,
      "step": 998000
    },
    {
      "epoch": 323.5332252836305,
      "grad_norm": 1.4593334197998047,
      "learning_rate": 1.7671359065844956e-05,
      "loss": 2.5366,
      "step": 998100
    },
    {
      "epoch": 323.56564019448945,
      "grad_norm": 1.621471643447876,
      "learning_rate": 1.7668115471942915e-05,
      "loss": 2.5471,
      "step": 998200
    },
    {
      "epoch": 323.59805510534846,
      "grad_norm": 1.3240439891815186,
      "learning_rate": 1.766487187804087e-05,
      "loss": 2.5429,
      "step": 998300
    },
    {
      "epoch": 323.6304700162075,
      "grad_norm": 1.26373291015625,
      "learning_rate": 1.7661628284138825e-05,
      "loss": 2.5195,
      "step": 998400
    },
    {
      "epoch": 323.66288492706644,
      "grad_norm": 1.319321632385254,
      "learning_rate": 1.7658384690236784e-05,
      "loss": 2.4971,
      "step": 998500
    },
    {
      "epoch": 323.69529983792546,
      "grad_norm": 1.3708245754241943,
      "learning_rate": 1.765514109633474e-05,
      "loss": 2.5582,
      "step": 998600
    },
    {
      "epoch": 323.7277147487844,
      "grad_norm": 1.2141144275665283,
      "learning_rate": 1.7651897502432695e-05,
      "loss": 2.536,
      "step": 998700
    },
    {
      "epoch": 323.76012965964344,
      "grad_norm": 1.1567027568817139,
      "learning_rate": 1.764865390853065e-05,
      "loss": 2.5622,
      "step": 998800
    },
    {
      "epoch": 323.79254457050246,
      "grad_norm": 1.439353585243225,
      "learning_rate": 1.764541031462861e-05,
      "loss": 2.5229,
      "step": 998900
    },
    {
      "epoch": 323.8249594813614,
      "grad_norm": 1.49040949344635,
      "learning_rate": 1.7642166720726568e-05,
      "loss": 2.5492,
      "step": 999000
    },
    {
      "epoch": 323.85737439222044,
      "grad_norm": 1.532618522644043,
      "learning_rate": 1.7638923126824523e-05,
      "loss": 2.5364,
      "step": 999100
    },
    {
      "epoch": 323.8897893030794,
      "grad_norm": 1.213704228401184,
      "learning_rate": 1.763567953292248e-05,
      "loss": 2.5367,
      "step": 999200
    },
    {
      "epoch": 323.9222042139384,
      "grad_norm": 1.1727415323257446,
      "learning_rate": 1.7632435939020437e-05,
      "loss": 2.5572,
      "step": 999300
    },
    {
      "epoch": 323.95461912479743,
      "grad_norm": 1.5858293771743774,
      "learning_rate": 1.7629192345118393e-05,
      "loss": 2.5442,
      "step": 999400
    },
    {
      "epoch": 323.9870340356564,
      "grad_norm": 1.1749215126037598,
      "learning_rate": 1.7625948751216348e-05,
      "loss": 2.5264,
      "step": 999500
    },
    {
      "epoch": 324.0,
      "eval_bleu": 0.9942342077526678,
      "eval_loss": 4.188729286193848,
      "eval_runtime": 4.4787,
      "eval_samples_per_second": 109.853,
      "eval_steps_per_second": 1.786,
      "step": 999540
    },
    {
      "epoch": 324.0194489465154,
      "grad_norm": 1.2988708019256592,
      "learning_rate": 1.7622705157314303e-05,
      "loss": 2.5301,
      "step": 999600
    },
    {
      "epoch": 324.05186385737437,
      "grad_norm": 1.388995885848999,
      "learning_rate": 1.7619461563412262e-05,
      "loss": 2.5431,
      "step": 999700
    },
    {
      "epoch": 324.0842787682334,
      "grad_norm": 1.2512696981430054,
      "learning_rate": 1.7616217969510217e-05,
      "loss": 2.5126,
      "step": 999800
    },
    {
      "epoch": 324.1166936790924,
      "grad_norm": 1.5026094913482666,
      "learning_rate": 1.7612974375608173e-05,
      "loss": 2.512,
      "step": 999900
    },
    {
      "epoch": 324.14910858995137,
      "grad_norm": 1.3206443786621094,
      "learning_rate": 1.760973078170613e-05,
      "loss": 2.529,
      "step": 1000000
    },
    {
      "epoch": 324.1815235008104,
      "grad_norm": 1.452899694442749,
      "learning_rate": 1.7606487187804087e-05,
      "loss": 2.5284,
      "step": 1000100
    },
    {
      "epoch": 324.21393841166935,
      "grad_norm": 1.3090919256210327,
      "learning_rate": 1.7603243593902046e-05,
      "loss": 2.5105,
      "step": 1000200
    },
    {
      "epoch": 324.24635332252836,
      "grad_norm": 1.3406965732574463,
      "learning_rate": 1.76e-05,
      "loss": 2.5339,
      "step": 1000300
    },
    {
      "epoch": 324.2787682333874,
      "grad_norm": 1.3286939859390259,
      "learning_rate": 1.759675640609796e-05,
      "loss": 2.5661,
      "step": 1000400
    },
    {
      "epoch": 324.31118314424634,
      "grad_norm": 1.3481234312057495,
      "learning_rate": 1.7593512812195915e-05,
      "loss": 2.5468,
      "step": 1000500
    },
    {
      "epoch": 324.34359805510536,
      "grad_norm": 1.2576004266738892,
      "learning_rate": 1.759026921829387e-05,
      "loss": 2.5426,
      "step": 1000600
    },
    {
      "epoch": 324.3760129659643,
      "grad_norm": 1.2474381923675537,
      "learning_rate": 1.7587025624391826e-05,
      "loss": 2.534,
      "step": 1000700
    },
    {
      "epoch": 324.40842787682334,
      "grad_norm": 1.2779240608215332,
      "learning_rate": 1.7583782030489784e-05,
      "loss": 2.5174,
      "step": 1000800
    },
    {
      "epoch": 324.44084278768236,
      "grad_norm": 1.3721342086791992,
      "learning_rate": 1.758053843658774e-05,
      "loss": 2.5527,
      "step": 1000900
    },
    {
      "epoch": 324.4732576985413,
      "grad_norm": 1.2213807106018066,
      "learning_rate": 1.7577294842685695e-05,
      "loss": 2.5322,
      "step": 1001000
    },
    {
      "epoch": 324.50567260940034,
      "grad_norm": 1.261239767074585,
      "learning_rate": 1.7574051248783654e-05,
      "loss": 2.5509,
      "step": 1001100
    },
    {
      "epoch": 324.5380875202593,
      "grad_norm": 1.2338171005249023,
      "learning_rate": 1.7570840090820632e-05,
      "loss": 2.5277,
      "step": 1001200
    },
    {
      "epoch": 324.5705024311183,
      "grad_norm": 1.338863730430603,
      "learning_rate": 1.7567596496918587e-05,
      "loss": 2.5378,
      "step": 1001300
    },
    {
      "epoch": 324.60291734197733,
      "grad_norm": 1.2689313888549805,
      "learning_rate": 1.7564352903016542e-05,
      "loss": 2.5612,
      "step": 1001400
    },
    {
      "epoch": 324.6353322528363,
      "grad_norm": 1.246906042098999,
      "learning_rate": 1.7561109309114498e-05,
      "loss": 2.5572,
      "step": 1001500
    },
    {
      "epoch": 324.6677471636953,
      "grad_norm": 1.2149332761764526,
      "learning_rate": 1.7557865715212457e-05,
      "loss": 2.5537,
      "step": 1001600
    },
    {
      "epoch": 324.70016207455427,
      "grad_norm": 1.403326392173767,
      "learning_rate": 1.7554622121310412e-05,
      "loss": 2.5152,
      "step": 1001700
    },
    {
      "epoch": 324.7325769854133,
      "grad_norm": 1.361973524093628,
      "learning_rate": 1.7551378527408367e-05,
      "loss": 2.5407,
      "step": 1001800
    },
    {
      "epoch": 324.7649918962723,
      "grad_norm": 1.3829724788665771,
      "learning_rate": 1.7548134933506326e-05,
      "loss": 2.5532,
      "step": 1001900
    },
    {
      "epoch": 324.79740680713127,
      "grad_norm": 1.25624418258667,
      "learning_rate": 1.754489133960428e-05,
      "loss": 2.5372,
      "step": 1002000
    },
    {
      "epoch": 324.8298217179903,
      "grad_norm": 1.364764928817749,
      "learning_rate": 1.754164774570224e-05,
      "loss": 2.5502,
      "step": 1002100
    },
    {
      "epoch": 324.86223662884925,
      "grad_norm": 1.318536400794983,
      "learning_rate": 1.7538404151800195e-05,
      "loss": 2.5384,
      "step": 1002200
    },
    {
      "epoch": 324.89465153970826,
      "grad_norm": 1.3558238744735718,
      "learning_rate": 1.7535160557898154e-05,
      "loss": 2.5458,
      "step": 1002300
    },
    {
      "epoch": 324.9270664505673,
      "grad_norm": 1.260030746459961,
      "learning_rate": 1.753191696399611e-05,
      "loss": 2.5315,
      "step": 1002400
    },
    {
      "epoch": 324.95948136142624,
      "grad_norm": 1.3392084836959839,
      "learning_rate": 1.7528673370094065e-05,
      "loss": 2.5196,
      "step": 1002500
    },
    {
      "epoch": 324.99189627228526,
      "grad_norm": 1.4799500703811646,
      "learning_rate": 1.752542977619202e-05,
      "loss": 2.5477,
      "step": 1002600
    },
    {
      "epoch": 325.0,
      "eval_bleu": 1.0433560986496346,
      "eval_loss": 4.18937873840332,
      "eval_runtime": 4.6456,
      "eval_samples_per_second": 105.906,
      "eval_steps_per_second": 1.722,
      "step": 1002625
    },
    {
      "epoch": 325.0243111831442,
      "grad_norm": 1.2261600494384766,
      "learning_rate": 1.752218618228998e-05,
      "loss": 2.5404,
      "step": 1002700
    },
    {
      "epoch": 325.05672609400324,
      "grad_norm": 1.5221763849258423,
      "learning_rate": 1.7518975024326957e-05,
      "loss": 2.5411,
      "step": 1002800
    },
    {
      "epoch": 325.08914100486226,
      "grad_norm": 1.3018739223480225,
      "learning_rate": 1.7515731430424912e-05,
      "loss": 2.5262,
      "step": 1002900
    },
    {
      "epoch": 325.1215559157212,
      "grad_norm": 1.3034862279891968,
      "learning_rate": 1.7512520272461887e-05,
      "loss": 2.534,
      "step": 1003000
    },
    {
      "epoch": 325.15397082658023,
      "grad_norm": 1.2423717975616455,
      "learning_rate": 1.7509276678559845e-05,
      "loss": 2.5535,
      "step": 1003100
    },
    {
      "epoch": 325.1863857374392,
      "grad_norm": 1.4900175333023071,
      "learning_rate": 1.7506033084657804e-05,
      "loss": 2.5497,
      "step": 1003200
    },
    {
      "epoch": 325.2188006482982,
      "grad_norm": 1.375504732131958,
      "learning_rate": 1.750278949075576e-05,
      "loss": 2.5412,
      "step": 1003300
    },
    {
      "epoch": 325.25121555915723,
      "grad_norm": 1.3304868936538696,
      "learning_rate": 1.7499545896853715e-05,
      "loss": 2.5339,
      "step": 1003400
    },
    {
      "epoch": 325.2836304700162,
      "grad_norm": 1.2754182815551758,
      "learning_rate": 1.7496302302951673e-05,
      "loss": 2.5247,
      "step": 1003500
    },
    {
      "epoch": 325.3160453808752,
      "grad_norm": 1.2304612398147583,
      "learning_rate": 1.749305870904963e-05,
      "loss": 2.5374,
      "step": 1003600
    },
    {
      "epoch": 325.34846029173417,
      "grad_norm": 1.4043292999267578,
      "learning_rate": 1.7489815115147584e-05,
      "loss": 2.5273,
      "step": 1003700
    },
    {
      "epoch": 325.3808752025932,
      "grad_norm": 1.5433744192123413,
      "learning_rate": 1.748657152124554e-05,
      "loss": 2.5155,
      "step": 1003800
    },
    {
      "epoch": 325.4132901134522,
      "grad_norm": 1.229269027709961,
      "learning_rate": 1.7483327927343498e-05,
      "loss": 2.5411,
      "step": 1003900
    },
    {
      "epoch": 325.44570502431117,
      "grad_norm": 1.448857069015503,
      "learning_rate": 1.7480084333441454e-05,
      "loss": 2.5352,
      "step": 1004000
    },
    {
      "epoch": 325.4781199351702,
      "grad_norm": 1.5552167892456055,
      "learning_rate": 1.747684073953941e-05,
      "loss": 2.5253,
      "step": 1004100
    },
    {
      "epoch": 325.51053484602915,
      "grad_norm": 1.3673994541168213,
      "learning_rate": 1.7473597145637364e-05,
      "loss": 2.5546,
      "step": 1004200
    },
    {
      "epoch": 325.54294975688816,
      "grad_norm": 1.2224323749542236,
      "learning_rate": 1.7470353551735323e-05,
      "loss": 2.5382,
      "step": 1004300
    },
    {
      "epoch": 325.5753646677472,
      "grad_norm": 1.2015489339828491,
      "learning_rate": 1.7467109957833282e-05,
      "loss": 2.5357,
      "step": 1004400
    },
    {
      "epoch": 325.60777957860614,
      "grad_norm": 1.5781205892562866,
      "learning_rate": 1.7463866363931237e-05,
      "loss": 2.536,
      "step": 1004500
    },
    {
      "epoch": 325.64019448946516,
      "grad_norm": 1.3130367994308472,
      "learning_rate": 1.7460622770029193e-05,
      "loss": 2.5286,
      "step": 1004600
    },
    {
      "epoch": 325.6726094003242,
      "grad_norm": 1.3766146898269653,
      "learning_rate": 1.745737917612715e-05,
      "loss": 2.5268,
      "step": 1004700
    },
    {
      "epoch": 325.70502431118314,
      "grad_norm": 1.3220505714416504,
      "learning_rate": 1.7454135582225107e-05,
      "loss": 2.5329,
      "step": 1004800
    },
    {
      "epoch": 325.73743922204216,
      "grad_norm": 1.3561819791793823,
      "learning_rate": 1.7450891988323062e-05,
      "loss": 2.5371,
      "step": 1004900
    },
    {
      "epoch": 325.7698541329011,
      "grad_norm": 1.5372133255004883,
      "learning_rate": 1.744764839442102e-05,
      "loss": 2.536,
      "step": 1005000
    },
    {
      "epoch": 325.80226904376013,
      "grad_norm": 1.4720953702926636,
      "learning_rate": 1.7444404800518976e-05,
      "loss": 2.5466,
      "step": 1005100
    },
    {
      "epoch": 325.83468395461915,
      "grad_norm": 1.4040719270706177,
      "learning_rate": 1.744116120661693e-05,
      "loss": 2.557,
      "step": 1005200
    },
    {
      "epoch": 325.8670988654781,
      "grad_norm": 1.3123944997787476,
      "learning_rate": 1.7437917612714887e-05,
      "loss": 2.536,
      "step": 1005300
    },
    {
      "epoch": 325.89951377633713,
      "grad_norm": 1.2116948366165161,
      "learning_rate": 1.7434674018812846e-05,
      "loss": 2.5483,
      "step": 1005400
    },
    {
      "epoch": 325.9319286871961,
      "grad_norm": 1.7561105489730835,
      "learning_rate": 1.74314304249108e-05,
      "loss": 2.5346,
      "step": 1005500
    },
    {
      "epoch": 325.9643435980551,
      "grad_norm": 1.2432315349578857,
      "learning_rate": 1.742818683100876e-05,
      "loss": 2.5369,
      "step": 1005600
    },
    {
      "epoch": 325.9967585089141,
      "grad_norm": 1.4991543292999268,
      "learning_rate": 1.7424943237106715e-05,
      "loss": 2.5409,
      "step": 1005700
    },
    {
      "epoch": 326.0,
      "eval_bleu": 1.0196963023753671,
      "eval_loss": 4.195099830627441,
      "eval_runtime": 4.5182,
      "eval_samples_per_second": 108.892,
      "eval_steps_per_second": 1.771,
      "step": 1005710
    },
    {
      "epoch": 326.0291734197731,
      "grad_norm": 1.1893409490585327,
      "learning_rate": 1.7421699643204674e-05,
      "loss": 2.5252,
      "step": 1005800
    },
    {
      "epoch": 326.0615883306321,
      "grad_norm": 1.4962092638015747,
      "learning_rate": 1.741845604930263e-05,
      "loss": 2.5155,
      "step": 1005900
    },
    {
      "epoch": 326.09400324149107,
      "grad_norm": 1.5236505270004272,
      "learning_rate": 1.7415212455400584e-05,
      "loss": 2.5617,
      "step": 1006000
    },
    {
      "epoch": 326.1264181523501,
      "grad_norm": 1.2889944314956665,
      "learning_rate": 1.741196886149854e-05,
      "loss": 2.5346,
      "step": 1006100
    },
    {
      "epoch": 326.1588330632091,
      "grad_norm": 1.2633286714553833,
      "learning_rate": 1.74087252675965e-05,
      "loss": 2.5414,
      "step": 1006200
    },
    {
      "epoch": 326.19124797406806,
      "grad_norm": 1.4904133081436157,
      "learning_rate": 1.7405481673694454e-05,
      "loss": 2.5532,
      "step": 1006300
    },
    {
      "epoch": 326.2236628849271,
      "grad_norm": 1.532817006111145,
      "learning_rate": 1.740223807979241e-05,
      "loss": 2.5314,
      "step": 1006400
    },
    {
      "epoch": 326.25607779578604,
      "grad_norm": 1.379650592803955,
      "learning_rate": 1.7398994485890368e-05,
      "loss": 2.5415,
      "step": 1006500
    },
    {
      "epoch": 326.28849270664506,
      "grad_norm": 1.3931517601013184,
      "learning_rate": 1.7395750891988323e-05,
      "loss": 2.5299,
      "step": 1006600
    },
    {
      "epoch": 326.3209076175041,
      "grad_norm": 1.3396872282028198,
      "learning_rate": 1.739250729808628e-05,
      "loss": 2.5271,
      "step": 1006700
    },
    {
      "epoch": 326.35332252836304,
      "grad_norm": 1.4671335220336914,
      "learning_rate": 1.7389263704184237e-05,
      "loss": 2.5416,
      "step": 1006800
    },
    {
      "epoch": 326.38573743922205,
      "grad_norm": 1.4955132007598877,
      "learning_rate": 1.7386020110282196e-05,
      "loss": 2.5004,
      "step": 1006900
    },
    {
      "epoch": 326.418152350081,
      "grad_norm": 1.387202262878418,
      "learning_rate": 1.738277651638015e-05,
      "loss": 2.5517,
      "step": 1007000
    },
    {
      "epoch": 326.45056726094003,
      "grad_norm": 1.4576141834259033,
      "learning_rate": 1.7379532922478107e-05,
      "loss": 2.533,
      "step": 1007100
    },
    {
      "epoch": 326.48298217179905,
      "grad_norm": 1.4612340927124023,
      "learning_rate": 1.7376289328576062e-05,
      "loss": 2.5188,
      "step": 1007200
    },
    {
      "epoch": 326.515397082658,
      "grad_norm": 1.3522920608520508,
      "learning_rate": 1.737304573467402e-05,
      "loss": 2.5449,
      "step": 1007300
    },
    {
      "epoch": 326.54781199351703,
      "grad_norm": 1.3932666778564453,
      "learning_rate": 1.7369802140771976e-05,
      "loss": 2.5323,
      "step": 1007400
    },
    {
      "epoch": 326.580226904376,
      "grad_norm": 1.5128133296966553,
      "learning_rate": 1.7366558546869932e-05,
      "loss": 2.5364,
      "step": 1007500
    },
    {
      "epoch": 326.612641815235,
      "grad_norm": 1.3946102857589722,
      "learning_rate": 1.7363314952967887e-05,
      "loss": 2.5374,
      "step": 1007600
    },
    {
      "epoch": 326.645056726094,
      "grad_norm": 1.4504132270812988,
      "learning_rate": 1.7360071359065846e-05,
      "loss": 2.5446,
      "step": 1007700
    },
    {
      "epoch": 326.677471636953,
      "grad_norm": 1.597091555595398,
      "learning_rate": 1.73568277651638e-05,
      "loss": 2.5382,
      "step": 1007800
    },
    {
      "epoch": 326.709886547812,
      "grad_norm": 1.3003735542297363,
      "learning_rate": 1.7353584171261757e-05,
      "loss": 2.544,
      "step": 1007900
    },
    {
      "epoch": 326.74230145867097,
      "grad_norm": 1.3920990228652954,
      "learning_rate": 1.7350340577359715e-05,
      "loss": 2.5566,
      "step": 1008000
    },
    {
      "epoch": 326.77471636953,
      "grad_norm": 1.3440321683883667,
      "learning_rate": 1.7347096983457674e-05,
      "loss": 2.5191,
      "step": 1008100
    },
    {
      "epoch": 326.807131280389,
      "grad_norm": 1.2193059921264648,
      "learning_rate": 1.734385338955563e-05,
      "loss": 2.5451,
      "step": 1008200
    },
    {
      "epoch": 326.83954619124796,
      "grad_norm": 1.5382906198501587,
      "learning_rate": 1.7340609795653585e-05,
      "loss": 2.5449,
      "step": 1008300
    },
    {
      "epoch": 326.871961102107,
      "grad_norm": 1.6493703126907349,
      "learning_rate": 1.7337366201751543e-05,
      "loss": 2.505,
      "step": 1008400
    },
    {
      "epoch": 326.90437601296594,
      "grad_norm": 1.4192665815353394,
      "learning_rate": 1.73341226078495e-05,
      "loss": 2.5419,
      "step": 1008500
    },
    {
      "epoch": 326.93679092382496,
      "grad_norm": 1.292191505432129,
      "learning_rate": 1.7330879013947454e-05,
      "loss": 2.5362,
      "step": 1008600
    },
    {
      "epoch": 326.969205834684,
      "grad_norm": 1.4528483152389526,
      "learning_rate": 1.732763542004541e-05,
      "loss": 2.5519,
      "step": 1008700
    },
    {
      "epoch": 327.0,
      "eval_bleu": 1.007483433931061,
      "eval_loss": 4.192409515380859,
      "eval_runtime": 4.3151,
      "eval_samples_per_second": 114.019,
      "eval_steps_per_second": 1.854,
      "step": 1008795
    },
    {
      "epoch": 327.00162074554294,
      "grad_norm": 1.4483593702316284,
      "learning_rate": 1.7324391826143368e-05,
      "loss": 2.5302,
      "step": 1008800
    },
    {
      "epoch": 327.03403565640195,
      "grad_norm": 1.2924541234970093,
      "learning_rate": 1.7321148232241324e-05,
      "loss": 2.5377,
      "step": 1008900
    },
    {
      "epoch": 327.0664505672609,
      "grad_norm": 1.4940437078475952,
      "learning_rate": 1.73179370742783e-05,
      "loss": 2.5028,
      "step": 1009000
    },
    {
      "epoch": 327.09886547811993,
      "grad_norm": 1.2944676876068115,
      "learning_rate": 1.7314693480376257e-05,
      "loss": 2.5295,
      "step": 1009100
    },
    {
      "epoch": 327.13128038897895,
      "grad_norm": 1.5895066261291504,
      "learning_rate": 1.7311449886474216e-05,
      "loss": 2.5208,
      "step": 1009200
    },
    {
      "epoch": 327.1636952998379,
      "grad_norm": 1.3156461715698242,
      "learning_rate": 1.730820629257217e-05,
      "loss": 2.5595,
      "step": 1009300
    },
    {
      "epoch": 327.19611021069693,
      "grad_norm": 1.5189824104309082,
      "learning_rate": 1.7304962698670126e-05,
      "loss": 2.5499,
      "step": 1009400
    },
    {
      "epoch": 327.2285251215559,
      "grad_norm": 1.2484573125839233,
      "learning_rate": 1.730171910476808e-05,
      "loss": 2.5395,
      "step": 1009500
    },
    {
      "epoch": 327.2609400324149,
      "grad_norm": 1.2817977666854858,
      "learning_rate": 1.7298507946805063e-05,
      "loss": 2.5107,
      "step": 1009600
    },
    {
      "epoch": 327.2933549432739,
      "grad_norm": 1.3558577299118042,
      "learning_rate": 1.7295264352903018e-05,
      "loss": 2.5306,
      "step": 1009700
    },
    {
      "epoch": 327.3257698541329,
      "grad_norm": 1.4559202194213867,
      "learning_rate": 1.7292020759000973e-05,
      "loss": 2.5288,
      "step": 1009800
    },
    {
      "epoch": 327.3581847649919,
      "grad_norm": 1.5944077968597412,
      "learning_rate": 1.728877716509893e-05,
      "loss": 2.5498,
      "step": 1009900
    },
    {
      "epoch": 327.39059967585086,
      "grad_norm": 1.369166612625122,
      "learning_rate": 1.7285533571196888e-05,
      "loss": 2.5293,
      "step": 1010000
    },
    {
      "epoch": 327.4230145867099,
      "grad_norm": 1.4246379137039185,
      "learning_rate": 1.7282289977294843e-05,
      "loss": 2.538,
      "step": 1010100
    },
    {
      "epoch": 327.4554294975689,
      "grad_norm": 1.2215075492858887,
      "learning_rate": 1.7279046383392798e-05,
      "loss": 2.5447,
      "step": 1010200
    },
    {
      "epoch": 327.48784440842786,
      "grad_norm": 1.543028473854065,
      "learning_rate": 1.7275802789490757e-05,
      "loss": 2.5177,
      "step": 1010300
    },
    {
      "epoch": 327.5202593192869,
      "grad_norm": 1.532792091369629,
      "learning_rate": 1.7272559195588712e-05,
      "loss": 2.5452,
      "step": 1010400
    },
    {
      "epoch": 327.55267423014584,
      "grad_norm": 1.2989716529846191,
      "learning_rate": 1.726931560168667e-05,
      "loss": 2.5454,
      "step": 1010500
    },
    {
      "epoch": 327.58508914100486,
      "grad_norm": 1.4174491167068481,
      "learning_rate": 1.7266072007784626e-05,
      "loss": 2.5339,
      "step": 1010600
    },
    {
      "epoch": 327.6175040518639,
      "grad_norm": 1.3053438663482666,
      "learning_rate": 1.7262828413882585e-05,
      "loss": 2.5066,
      "step": 1010700
    },
    {
      "epoch": 327.64991896272284,
      "grad_norm": 1.3821563720703125,
      "learning_rate": 1.725958481998054e-05,
      "loss": 2.5499,
      "step": 1010800
    },
    {
      "epoch": 327.68233387358185,
      "grad_norm": 1.4464383125305176,
      "learning_rate": 1.7256341226078496e-05,
      "loss": 2.5561,
      "step": 1010900
    },
    {
      "epoch": 327.7147487844408,
      "grad_norm": 1.424453854560852,
      "learning_rate": 1.725309763217645e-05,
      "loss": 2.55,
      "step": 1011000
    },
    {
      "epoch": 327.74716369529983,
      "grad_norm": 1.4304007291793823,
      "learning_rate": 1.724985403827441e-05,
      "loss": 2.5226,
      "step": 1011100
    },
    {
      "epoch": 327.77957860615885,
      "grad_norm": 1.5719038248062134,
      "learning_rate": 1.7246610444372365e-05,
      "loss": 2.5567,
      "step": 1011200
    },
    {
      "epoch": 327.8119935170178,
      "grad_norm": 1.422061562538147,
      "learning_rate": 1.724336685047032e-05,
      "loss": 2.5216,
      "step": 1011300
    },
    {
      "epoch": 327.84440842787683,
      "grad_norm": 1.4103386402130127,
      "learning_rate": 1.7240123256568276e-05,
      "loss": 2.5084,
      "step": 1011400
    },
    {
      "epoch": 327.87682333873585,
      "grad_norm": 1.4608325958251953,
      "learning_rate": 1.7236879662666235e-05,
      "loss": 2.5296,
      "step": 1011500
    },
    {
      "epoch": 327.9092382495948,
      "grad_norm": 1.4831339120864868,
      "learning_rate": 1.723363606876419e-05,
      "loss": 2.5405,
      "step": 1011600
    },
    {
      "epoch": 327.9416531604538,
      "grad_norm": 1.3100879192352295,
      "learning_rate": 1.723039247486215e-05,
      "loss": 2.5359,
      "step": 1011700
    },
    {
      "epoch": 327.9740680713128,
      "grad_norm": 1.459317922592163,
      "learning_rate": 1.7227148880960104e-05,
      "loss": 2.5331,
      "step": 1011800
    },
    {
      "epoch": 328.0,
      "eval_bleu": 1.1039048528651854,
      "eval_loss": 4.192901611328125,
      "eval_runtime": 4.6729,
      "eval_samples_per_second": 105.288,
      "eval_steps_per_second": 1.712,
      "step": 1011880
    },
    {
      "epoch": 328.0064829821718,
      "grad_norm": 1.4023152589797974,
      "learning_rate": 1.7223905287058063e-05,
      "loss": 2.5258,
      "step": 1011900
    },
    {
      "epoch": 328.0388978930308,
      "grad_norm": 1.5219240188598633,
      "learning_rate": 1.722066169315602e-05,
      "loss": 2.5106,
      "step": 1012000
    },
    {
      "epoch": 328.0713128038898,
      "grad_norm": 1.3676741123199463,
      "learning_rate": 1.7217418099253974e-05,
      "loss": 2.5242,
      "step": 1012100
    },
    {
      "epoch": 328.1037277147488,
      "grad_norm": 1.291271448135376,
      "learning_rate": 1.7214174505351932e-05,
      "loss": 2.5418,
      "step": 1012200
    },
    {
      "epoch": 328.13614262560776,
      "grad_norm": 1.207783818244934,
      "learning_rate": 1.7210930911449888e-05,
      "loss": 2.5491,
      "step": 1012300
    },
    {
      "epoch": 328.1685575364668,
      "grad_norm": 1.5418386459350586,
      "learning_rate": 1.7207719753486866e-05,
      "loss": 2.555,
      "step": 1012400
    },
    {
      "epoch": 328.2009724473258,
      "grad_norm": 1.1973762512207031,
      "learning_rate": 1.720447615958482e-05,
      "loss": 2.5529,
      "step": 1012500
    },
    {
      "epoch": 328.23338735818476,
      "grad_norm": 1.3727426528930664,
      "learning_rate": 1.7201232565682776e-05,
      "loss": 2.5581,
      "step": 1012600
    },
    {
      "epoch": 328.2658022690438,
      "grad_norm": 1.6030553579330444,
      "learning_rate": 1.7197988971780735e-05,
      "loss": 2.5182,
      "step": 1012700
    },
    {
      "epoch": 328.29821717990274,
      "grad_norm": 1.1653144359588623,
      "learning_rate": 1.719474537787869e-05,
      "loss": 2.5304,
      "step": 1012800
    },
    {
      "epoch": 328.33063209076175,
      "grad_norm": 1.5982853174209595,
      "learning_rate": 1.7191501783976646e-05,
      "loss": 2.5085,
      "step": 1012900
    },
    {
      "epoch": 328.36304700162077,
      "grad_norm": 1.3327929973602295,
      "learning_rate": 1.7188258190074605e-05,
      "loss": 2.5455,
      "step": 1013000
    },
    {
      "epoch": 328.39546191247973,
      "grad_norm": 1.3313943147659302,
      "learning_rate": 1.718501459617256e-05,
      "loss": 2.5245,
      "step": 1013100
    },
    {
      "epoch": 328.42787682333875,
      "grad_norm": 1.4393421411514282,
      "learning_rate": 1.7181771002270515e-05,
      "loss": 2.5383,
      "step": 1013200
    },
    {
      "epoch": 328.4602917341977,
      "grad_norm": 1.1581114530563354,
      "learning_rate": 1.717852740836847e-05,
      "loss": 2.5258,
      "step": 1013300
    },
    {
      "epoch": 328.4927066450567,
      "grad_norm": 1.3959344625473022,
      "learning_rate": 1.717528381446643e-05,
      "loss": 2.5249,
      "step": 1013400
    },
    {
      "epoch": 328.52512155591575,
      "grad_norm": 1.599879264831543,
      "learning_rate": 1.7172040220564388e-05,
      "loss": 2.5396,
      "step": 1013500
    },
    {
      "epoch": 328.5575364667747,
      "grad_norm": 1.2814772129058838,
      "learning_rate": 1.7168796626662343e-05,
      "loss": 2.5275,
      "step": 1013600
    },
    {
      "epoch": 328.5899513776337,
      "grad_norm": 1.4481104612350464,
      "learning_rate": 1.71655530327603e-05,
      "loss": 2.5338,
      "step": 1013700
    },
    {
      "epoch": 328.6223662884927,
      "grad_norm": 1.2579262256622314,
      "learning_rate": 1.7162309438858258e-05,
      "loss": 2.5337,
      "step": 1013800
    },
    {
      "epoch": 328.6547811993517,
      "grad_norm": 1.2445191144943237,
      "learning_rate": 1.7159065844956213e-05,
      "loss": 2.5114,
      "step": 1013900
    },
    {
      "epoch": 328.6871961102107,
      "grad_norm": 1.2415355443954468,
      "learning_rate": 1.7155822251054168e-05,
      "loss": 2.5339,
      "step": 1014000
    },
    {
      "epoch": 328.7196110210697,
      "grad_norm": 1.2250351905822754,
      "learning_rate": 1.7152578657152124e-05,
      "loss": 2.5147,
      "step": 1014100
    },
    {
      "epoch": 328.7520259319287,
      "grad_norm": 1.2285841703414917,
      "learning_rate": 1.7149335063250082e-05,
      "loss": 2.5339,
      "step": 1014200
    },
    {
      "epoch": 328.78444084278766,
      "grad_norm": 1.3224339485168457,
      "learning_rate": 1.7146091469348038e-05,
      "loss": 2.5497,
      "step": 1014300
    },
    {
      "epoch": 328.8168557536467,
      "grad_norm": 1.2343156337738037,
      "learning_rate": 1.7142847875445993e-05,
      "loss": 2.5494,
      "step": 1014400
    },
    {
      "epoch": 328.8492706645057,
      "grad_norm": 1.5141111612319946,
      "learning_rate": 1.7139604281543952e-05,
      "loss": 2.5292,
      "step": 1014500
    },
    {
      "epoch": 328.88168557536466,
      "grad_norm": 1.3343654870986938,
      "learning_rate": 1.7136360687641907e-05,
      "loss": 2.5291,
      "step": 1014600
    },
    {
      "epoch": 328.9141004862237,
      "grad_norm": 1.5717339515686035,
      "learning_rate": 1.7133117093739866e-05,
      "loss": 2.545,
      "step": 1014700
    },
    {
      "epoch": 328.94651539708263,
      "grad_norm": 1.2404361963272095,
      "learning_rate": 1.712987349983782e-05,
      "loss": 2.537,
      "step": 1014800
    },
    {
      "epoch": 328.97893030794165,
      "grad_norm": 1.249196171760559,
      "learning_rate": 1.712662990593578e-05,
      "loss": 2.5285,
      "step": 1014900
    },
    {
      "epoch": 329.0,
      "eval_bleu": 1.053562179646466,
      "eval_loss": 4.194114685058594,
      "eval_runtime": 4.5099,
      "eval_samples_per_second": 109.094,
      "eval_steps_per_second": 1.774,
      "step": 1014965
    },
    {
      "epoch": 329.01134521880067,
      "grad_norm": 1.2504180669784546,
      "learning_rate": 1.7123386312033735e-05,
      "loss": 2.5354,
      "step": 1015000
    },
    {
      "epoch": 329.04376012965963,
      "grad_norm": 1.4550588130950928,
      "learning_rate": 1.712014271813169e-05,
      "loss": 2.5177,
      "step": 1015100
    },
    {
      "epoch": 329.07617504051865,
      "grad_norm": 1.509508490562439,
      "learning_rate": 1.7116899124229646e-05,
      "loss": 2.5157,
      "step": 1015200
    },
    {
      "epoch": 329.1085899513776,
      "grad_norm": 1.2985310554504395,
      "learning_rate": 1.7113655530327605e-05,
      "loss": 2.5324,
      "step": 1015300
    },
    {
      "epoch": 329.1410048622366,
      "grad_norm": 1.3710707426071167,
      "learning_rate": 1.711041193642556e-05,
      "loss": 2.521,
      "step": 1015400
    },
    {
      "epoch": 329.17341977309565,
      "grad_norm": 1.2479740381240845,
      "learning_rate": 1.7107168342523516e-05,
      "loss": 2.5451,
      "step": 1015500
    },
    {
      "epoch": 329.2058346839546,
      "grad_norm": 1.5617879629135132,
      "learning_rate": 1.710392474862147e-05,
      "loss": 2.5161,
      "step": 1015600
    },
    {
      "epoch": 329.2382495948136,
      "grad_norm": 1.4512696266174316,
      "learning_rate": 1.710068115471943e-05,
      "loss": 2.5099,
      "step": 1015700
    },
    {
      "epoch": 329.2706645056726,
      "grad_norm": 1.5685105323791504,
      "learning_rate": 1.7097437560817385e-05,
      "loss": 2.5257,
      "step": 1015800
    },
    {
      "epoch": 329.3030794165316,
      "grad_norm": 1.409949541091919,
      "learning_rate": 1.7094193966915344e-05,
      "loss": 2.5196,
      "step": 1015900
    },
    {
      "epoch": 329.3354943273906,
      "grad_norm": 1.3876550197601318,
      "learning_rate": 1.7090950373013302e-05,
      "loss": 2.5483,
      "step": 1016000
    },
    {
      "epoch": 329.3679092382496,
      "grad_norm": 1.3961598873138428,
      "learning_rate": 1.7087706779111258e-05,
      "loss": 2.5374,
      "step": 1016100
    },
    {
      "epoch": 329.4003241491086,
      "grad_norm": 1.3810585737228394,
      "learning_rate": 1.7084463185209213e-05,
      "loss": 2.5205,
      "step": 1016200
    },
    {
      "epoch": 329.43273905996756,
      "grad_norm": 1.5681413412094116,
      "learning_rate": 1.7081252027246188e-05,
      "loss": 2.5474,
      "step": 1016300
    },
    {
      "epoch": 329.4651539708266,
      "grad_norm": 1.2505098581314087,
      "learning_rate": 1.7078008433344146e-05,
      "loss": 2.556,
      "step": 1016400
    },
    {
      "epoch": 329.4975688816856,
      "grad_norm": 1.3350074291229248,
      "learning_rate": 1.7074764839442105e-05,
      "loss": 2.5364,
      "step": 1016500
    },
    {
      "epoch": 329.52998379254456,
      "grad_norm": 1.5149085521697998,
      "learning_rate": 1.707152124554006e-05,
      "loss": 2.5381,
      "step": 1016600
    },
    {
      "epoch": 329.5623987034036,
      "grad_norm": 1.4927009344100952,
      "learning_rate": 1.7068277651638016e-05,
      "loss": 2.5148,
      "step": 1016700
    },
    {
      "epoch": 329.59481361426253,
      "grad_norm": 1.248683214187622,
      "learning_rate": 1.7065034057735975e-05,
      "loss": 2.5443,
      "step": 1016800
    },
    {
      "epoch": 329.62722852512155,
      "grad_norm": 1.7868319749832153,
      "learning_rate": 1.706179046383393e-05,
      "loss": 2.5226,
      "step": 1016900
    },
    {
      "epoch": 329.65964343598057,
      "grad_norm": 1.2964016199111938,
      "learning_rate": 1.7058546869931885e-05,
      "loss": 2.5468,
      "step": 1017000
    },
    {
      "epoch": 329.69205834683953,
      "grad_norm": 1.3062517642974854,
      "learning_rate": 1.705530327602984e-05,
      "loss": 2.5098,
      "step": 1017100
    },
    {
      "epoch": 329.72447325769855,
      "grad_norm": 1.2914901971817017,
      "learning_rate": 1.70520596821278e-05,
      "loss": 2.5508,
      "step": 1017200
    },
    {
      "epoch": 329.7568881685575,
      "grad_norm": 1.4409455060958862,
      "learning_rate": 1.7048816088225755e-05,
      "loss": 2.5611,
      "step": 1017300
    },
    {
      "epoch": 329.7893030794165,
      "grad_norm": 1.366979956626892,
      "learning_rate": 1.704557249432371e-05,
      "loss": 2.5311,
      "step": 1017400
    },
    {
      "epoch": 329.82171799027554,
      "grad_norm": 1.276179552078247,
      "learning_rate": 1.7042328900421665e-05,
      "loss": 2.5319,
      "step": 1017500
    },
    {
      "epoch": 329.8541329011345,
      "grad_norm": 1.2696549892425537,
      "learning_rate": 1.7039085306519624e-05,
      "loss": 2.5218,
      "step": 1017600
    },
    {
      "epoch": 329.8865478119935,
      "grad_norm": 1.482657551765442,
      "learning_rate": 1.7035841712617583e-05,
      "loss": 2.5459,
      "step": 1017700
    },
    {
      "epoch": 329.9189627228525,
      "grad_norm": 1.2649431228637695,
      "learning_rate": 1.7032598118715538e-05,
      "loss": 2.5501,
      "step": 1017800
    },
    {
      "epoch": 329.9513776337115,
      "grad_norm": 1.3280342817306519,
      "learning_rate": 1.7029354524813494e-05,
      "loss": 2.5334,
      "step": 1017900
    },
    {
      "epoch": 329.9837925445705,
      "grad_norm": 1.276819109916687,
      "learning_rate": 1.7026110930911452e-05,
      "loss": 2.5578,
      "step": 1018000
    },
    {
      "epoch": 330.0,
      "eval_bleu": 1.05047205773672,
      "eval_loss": 4.192701816558838,
      "eval_runtime": 4.7453,
      "eval_samples_per_second": 103.682,
      "eval_steps_per_second": 1.686,
      "step": 1018050
    },
    {
      "epoch": 330.0162074554295,
      "grad_norm": 1.2496426105499268,
      "learning_rate": 1.7022867337009408e-05,
      "loss": 2.5301,
      "step": 1018100
    },
    {
      "epoch": 330.0486223662885,
      "grad_norm": 1.2475506067276,
      "learning_rate": 1.7019623743107363e-05,
      "loss": 2.5353,
      "step": 1018200
    },
    {
      "epoch": 330.0810372771475,
      "grad_norm": 1.410385251045227,
      "learning_rate": 1.7016380149205322e-05,
      "loss": 2.5251,
      "step": 1018300
    },
    {
      "epoch": 330.1134521880065,
      "grad_norm": 1.4456872940063477,
      "learning_rate": 1.7013136555303277e-05,
      "loss": 2.5393,
      "step": 1018400
    },
    {
      "epoch": 330.1458670988655,
      "grad_norm": 1.4076780080795288,
      "learning_rate": 1.7009925397340255e-05,
      "loss": 2.5339,
      "step": 1018500
    },
    {
      "epoch": 330.17828200972446,
      "grad_norm": 1.243082046508789,
      "learning_rate": 1.700668180343821e-05,
      "loss": 2.5333,
      "step": 1018600
    },
    {
      "epoch": 330.2106969205835,
      "grad_norm": 1.530919075012207,
      "learning_rate": 1.7003438209536166e-05,
      "loss": 2.5434,
      "step": 1018700
    },
    {
      "epoch": 330.2431118314425,
      "grad_norm": 1.3345181941986084,
      "learning_rate": 1.7000194615634124e-05,
      "loss": 2.5594,
      "step": 1018800
    },
    {
      "epoch": 330.27552674230145,
      "grad_norm": 1.4467614889144897,
      "learning_rate": 1.699695102173208e-05,
      "loss": 2.5054,
      "step": 1018900
    },
    {
      "epoch": 330.30794165316047,
      "grad_norm": 1.3107349872589111,
      "learning_rate": 1.6993707427830035e-05,
      "loss": 2.5189,
      "step": 1019000
    },
    {
      "epoch": 330.34035656401943,
      "grad_norm": 1.4621402025222778,
      "learning_rate": 1.6990463833927994e-05,
      "loss": 2.5192,
      "step": 1019100
    },
    {
      "epoch": 330.37277147487845,
      "grad_norm": 1.298270583152771,
      "learning_rate": 1.698722024002595e-05,
      "loss": 2.5397,
      "step": 1019200
    },
    {
      "epoch": 330.40518638573747,
      "grad_norm": 1.5852625370025635,
      "learning_rate": 1.6983976646123905e-05,
      "loss": 2.511,
      "step": 1019300
    },
    {
      "epoch": 330.4376012965964,
      "grad_norm": 1.5513213872909546,
      "learning_rate": 1.6980733052221863e-05,
      "loss": 2.5448,
      "step": 1019400
    },
    {
      "epoch": 330.47001620745544,
      "grad_norm": 1.4138190746307373,
      "learning_rate": 1.697748945831982e-05,
      "loss": 2.5273,
      "step": 1019500
    },
    {
      "epoch": 330.5024311183144,
      "grad_norm": 1.3502931594848633,
      "learning_rate": 1.6974245864417777e-05,
      "loss": 2.5083,
      "step": 1019600
    },
    {
      "epoch": 330.5348460291734,
      "grad_norm": 1.2174763679504395,
      "learning_rate": 1.6971002270515733e-05,
      "loss": 2.5217,
      "step": 1019700
    },
    {
      "epoch": 330.56726094003244,
      "grad_norm": 1.4099994897842407,
      "learning_rate": 1.6967758676613688e-05,
      "loss": 2.5331,
      "step": 1019800
    },
    {
      "epoch": 330.5996758508914,
      "grad_norm": 1.3383640050888062,
      "learning_rate": 1.6964515082711647e-05,
      "loss": 2.5267,
      "step": 1019900
    },
    {
      "epoch": 330.6320907617504,
      "grad_norm": 1.291214942932129,
      "learning_rate": 1.6961271488809602e-05,
      "loss": 2.5492,
      "step": 1020000
    },
    {
      "epoch": 330.6645056726094,
      "grad_norm": 1.4398338794708252,
      "learning_rate": 1.6958027894907558e-05,
      "loss": 2.5335,
      "step": 1020100
    },
    {
      "epoch": 330.6969205834684,
      "grad_norm": 1.4110016822814941,
      "learning_rate": 1.6954784301005516e-05,
      "loss": 2.5462,
      "step": 1020200
    },
    {
      "epoch": 330.7293354943274,
      "grad_norm": 1.317865014076233,
      "learning_rate": 1.695154070710347e-05,
      "loss": 2.5188,
      "step": 1020300
    },
    {
      "epoch": 330.7617504051864,
      "grad_norm": 1.5142885446548462,
      "learning_rate": 1.6948297113201427e-05,
      "loss": 2.5358,
      "step": 1020400
    },
    {
      "epoch": 330.7941653160454,
      "grad_norm": 1.2900668382644653,
      "learning_rate": 1.6945053519299382e-05,
      "loss": 2.5432,
      "step": 1020500
    },
    {
      "epoch": 330.82658022690435,
      "grad_norm": 1.324771523475647,
      "learning_rate": 1.694180992539734e-05,
      "loss": 2.5383,
      "step": 1020600
    },
    {
      "epoch": 330.8589951377634,
      "grad_norm": 1.35735285282135,
      "learning_rate": 1.6938566331495296e-05,
      "loss": 2.5363,
      "step": 1020700
    },
    {
      "epoch": 330.8914100486224,
      "grad_norm": 1.3822826147079468,
      "learning_rate": 1.6935322737593255e-05,
      "loss": 2.5258,
      "step": 1020800
    },
    {
      "epoch": 330.92382495948135,
      "grad_norm": 1.331955909729004,
      "learning_rate": 1.693207914369121e-05,
      "loss": 2.5326,
      "step": 1020900
    },
    {
      "epoch": 330.95623987034037,
      "grad_norm": 1.4586505889892578,
      "learning_rate": 1.692883554978917e-05,
      "loss": 2.5111,
      "step": 1021000
    },
    {
      "epoch": 330.98865478119933,
      "grad_norm": 1.4405035972595215,
      "learning_rate": 1.6925591955887125e-05,
      "loss": 2.5549,
      "step": 1021100
    },
    {
      "epoch": 331.0,
      "eval_bleu": 0.8324822350340868,
      "eval_loss": 4.199822902679443,
      "eval_runtime": 4.3626,
      "eval_samples_per_second": 112.777,
      "eval_steps_per_second": 1.834,
      "step": 1021135
    },
    {
      "epoch": 331.02106969205835,
      "grad_norm": 1.243528962135315,
      "learning_rate": 1.692234836198508e-05,
      "loss": 2.5204,
      "step": 1021200
    },
    {
      "epoch": 331.05348460291737,
      "grad_norm": 1.3882298469543457,
      "learning_rate": 1.6919104768083035e-05,
      "loss": 2.5192,
      "step": 1021300
    },
    {
      "epoch": 331.0858995137763,
      "grad_norm": 1.3613406419754028,
      "learning_rate": 1.6915861174180994e-05,
      "loss": 2.5267,
      "step": 1021400
    },
    {
      "epoch": 331.11831442463534,
      "grad_norm": 1.329707384109497,
      "learning_rate": 1.691261758027895e-05,
      "loss": 2.5329,
      "step": 1021500
    },
    {
      "epoch": 331.1507293354943,
      "grad_norm": 1.7870218753814697,
      "learning_rate": 1.6909373986376905e-05,
      "loss": 2.5392,
      "step": 1021600
    },
    {
      "epoch": 331.1831442463533,
      "grad_norm": 1.2564042806625366,
      "learning_rate": 1.6906130392474864e-05,
      "loss": 2.5306,
      "step": 1021700
    },
    {
      "epoch": 331.21555915721234,
      "grad_norm": 1.3171011209487915,
      "learning_rate": 1.690288679857282e-05,
      "loss": 2.5268,
      "step": 1021800
    },
    {
      "epoch": 331.2479740680713,
      "grad_norm": NaN,
      "learning_rate": 1.6899643204670774e-05,
      "loss": 2.5275,
      "step": 1021900
    },
    {
      "epoch": 331.2803889789303,
      "grad_norm": 1.4283186197280884,
      "learning_rate": 1.6896432046707752e-05,
      "loss": 2.5068,
      "step": 1022000
    },
    {
      "epoch": 331.3128038897893,
      "grad_norm": 1.3581904172897339,
      "learning_rate": 1.6893188452805707e-05,
      "loss": 2.5418,
      "step": 1022100
    },
    {
      "epoch": 331.3452188006483,
      "grad_norm": 1.3119194507598877,
      "learning_rate": 1.6889944858903666e-05,
      "loss": 2.5328,
      "step": 1022200
    },
    {
      "epoch": 331.3776337115073,
      "grad_norm": 1.4663500785827637,
      "learning_rate": 1.688670126500162e-05,
      "loss": 2.5417,
      "step": 1022300
    },
    {
      "epoch": 331.4100486223663,
      "grad_norm": 1.341180443763733,
      "learning_rate": 1.6883457671099577e-05,
      "loss": 2.5159,
      "step": 1022400
    },
    {
      "epoch": 331.4424635332253,
      "grad_norm": 1.608386754989624,
      "learning_rate": 1.6880214077197536e-05,
      "loss": 2.5323,
      "step": 1022500
    },
    {
      "epoch": 331.47487844408425,
      "grad_norm": 1.2415521144866943,
      "learning_rate": 1.6876970483295494e-05,
      "loss": 2.5069,
      "step": 1022600
    },
    {
      "epoch": 331.5072933549433,
      "grad_norm": 1.5288721323013306,
      "learning_rate": 1.687372688939345e-05,
      "loss": 2.52,
      "step": 1022700
    },
    {
      "epoch": 331.5397082658023,
      "grad_norm": 1.3733223676681519,
      "learning_rate": 1.6870483295491405e-05,
      "loss": 2.5479,
      "step": 1022800
    },
    {
      "epoch": 331.57212317666125,
      "grad_norm": 1.4057176113128662,
      "learning_rate": 1.6867239701589364e-05,
      "loss": 2.5367,
      "step": 1022900
    },
    {
      "epoch": 331.60453808752027,
      "grad_norm": 1.3112484216690063,
      "learning_rate": 1.686399610768732e-05,
      "loss": 2.521,
      "step": 1023000
    },
    {
      "epoch": 331.63695299837923,
      "grad_norm": 1.5416091680526733,
      "learning_rate": 1.6860752513785275e-05,
      "loss": 2.542,
      "step": 1023100
    },
    {
      "epoch": 331.66936790923825,
      "grad_norm": 1.4451435804367065,
      "learning_rate": 1.685750891988323e-05,
      "loss": 2.5215,
      "step": 1023200
    },
    {
      "epoch": 331.70178282009726,
      "grad_norm": 1.3654396533966064,
      "learning_rate": 1.685426532598119e-05,
      "loss": 2.5327,
      "step": 1023300
    },
    {
      "epoch": 331.7341977309562,
      "grad_norm": 1.2156970500946045,
      "learning_rate": 1.6851021732079144e-05,
      "loss": 2.5375,
      "step": 1023400
    },
    {
      "epoch": 331.76661264181524,
      "grad_norm": 1.2453012466430664,
      "learning_rate": 1.68477781381771e-05,
      "loss": 2.5307,
      "step": 1023500
    },
    {
      "epoch": 331.7990275526742,
      "grad_norm": 1.4233940839767456,
      "learning_rate": 1.6844534544275055e-05,
      "loss": 2.5518,
      "step": 1023600
    },
    {
      "epoch": 331.8314424635332,
      "grad_norm": 1.427165150642395,
      "learning_rate": 1.6841290950373013e-05,
      "loss": 2.5216,
      "step": 1023700
    },
    {
      "epoch": 331.86385737439224,
      "grad_norm": 1.3189263343811035,
      "learning_rate": 1.6838047356470972e-05,
      "loss": 2.5247,
      "step": 1023800
    },
    {
      "epoch": 331.8962722852512,
      "grad_norm": 1.2319508790969849,
      "learning_rate": 1.6834803762568928e-05,
      "loss": 2.5425,
      "step": 1023900
    },
    {
      "epoch": 331.9286871961102,
      "grad_norm": 1.7928684949874878,
      "learning_rate": 1.6831560168666886e-05,
      "loss": 2.5502,
      "step": 1024000
    },
    {
      "epoch": 331.9611021069692,
      "grad_norm": 1.2635669708251953,
      "learning_rate": 1.682831657476484e-05,
      "loss": 2.5272,
      "step": 1024100
    },
    {
      "epoch": 331.9935170178282,
      "grad_norm": 1.3496190309524536,
      "learning_rate": 1.6825072980862797e-05,
      "loss": 2.5338,
      "step": 1024200
    },
    {
      "epoch": 332.0,
      "eval_bleu": 0.9260119276215032,
      "eval_loss": 4.192892074584961,
      "eval_runtime": 4.2595,
      "eval_samples_per_second": 115.508,
      "eval_steps_per_second": 1.878,
      "step": 1024220
    },
    {
      "epoch": 332.0259319286872,
      "grad_norm": 1.387647032737732,
      "learning_rate": 1.6821829386960752e-05,
      "loss": 2.5345,
      "step": 1024300
    },
    {
      "epoch": 332.0583468395462,
      "grad_norm": 1.2656757831573486,
      "learning_rate": 1.681858579305871e-05,
      "loss": 2.5295,
      "step": 1024400
    },
    {
      "epoch": 332.0907617504052,
      "grad_norm": 1.5149970054626465,
      "learning_rate": 1.6815342199156666e-05,
      "loss": 2.5359,
      "step": 1024500
    },
    {
      "epoch": 332.12317666126415,
      "grad_norm": 1.3362951278686523,
      "learning_rate": 1.6812098605254622e-05,
      "loss": 2.5218,
      "step": 1024600
    },
    {
      "epoch": 332.15559157212317,
      "grad_norm": 1.1937741041183472,
      "learning_rate": 1.68088874472916e-05,
      "loss": 2.53,
      "step": 1024700
    },
    {
      "epoch": 332.1880064829822,
      "grad_norm": 1.498831033706665,
      "learning_rate": 1.680564385338956e-05,
      "loss": 2.5366,
      "step": 1024800
    },
    {
      "epoch": 332.22042139384115,
      "grad_norm": 1.4497113227844238,
      "learning_rate": 1.6802400259487514e-05,
      "loss": 2.5175,
      "step": 1024900
    },
    {
      "epoch": 332.25283630470017,
      "grad_norm": 1.4552206993103027,
      "learning_rate": 1.679915666558547e-05,
      "loss": 2.5287,
      "step": 1025000
    },
    {
      "epoch": 332.2852512155592,
      "grad_norm": 1.3739622831344604,
      "learning_rate": 1.6795913071683424e-05,
      "loss": 2.5112,
      "step": 1025100
    },
    {
      "epoch": 332.31766612641815,
      "grad_norm": 1.3391339778900146,
      "learning_rate": 1.6792669477781383e-05,
      "loss": 2.5456,
      "step": 1025200
    },
    {
      "epoch": 332.35008103727716,
      "grad_norm": 1.2468273639678955,
      "learning_rate": 1.678942588387934e-05,
      "loss": 2.5417,
      "step": 1025300
    },
    {
      "epoch": 332.3824959481361,
      "grad_norm": 1.3489079475402832,
      "learning_rate": 1.6786182289977294e-05,
      "loss": 2.5052,
      "step": 1025400
    },
    {
      "epoch": 332.41491085899514,
      "grad_norm": 1.5633031129837036,
      "learning_rate": 1.6782938696075253e-05,
      "loss": 2.5265,
      "step": 1025500
    },
    {
      "epoch": 332.44732576985416,
      "grad_norm": 1.4316543340682983,
      "learning_rate": 1.6779695102173208e-05,
      "loss": 2.548,
      "step": 1025600
    },
    {
      "epoch": 332.4797406807131,
      "grad_norm": 1.3719009160995483,
      "learning_rate": 1.6776451508271167e-05,
      "loss": 2.5441,
      "step": 1025700
    },
    {
      "epoch": 332.51215559157214,
      "grad_norm": 1.3617769479751587,
      "learning_rate": 1.6773207914369122e-05,
      "loss": 2.5219,
      "step": 1025800
    },
    {
      "epoch": 332.5445705024311,
      "grad_norm": 1.3289653062820435,
      "learning_rate": 1.6769964320467077e-05,
      "loss": 2.5351,
      "step": 1025900
    },
    {
      "epoch": 332.5769854132901,
      "grad_norm": 1.3094298839569092,
      "learning_rate": 1.6766720726565036e-05,
      "loss": 2.5402,
      "step": 1026000
    },
    {
      "epoch": 332.60940032414914,
      "grad_norm": 1.3046730756759644,
      "learning_rate": 1.676347713266299e-05,
      "loss": 2.5041,
      "step": 1026100
    },
    {
      "epoch": 332.6418152350081,
      "grad_norm": 1.5048081874847412,
      "learning_rate": 1.6760233538760947e-05,
      "loss": 2.5169,
      "step": 1026200
    },
    {
      "epoch": 332.6742301458671,
      "grad_norm": 1.7416589260101318,
      "learning_rate": 1.6756989944858906e-05,
      "loss": 2.5235,
      "step": 1026300
    },
    {
      "epoch": 332.7066450567261,
      "grad_norm": 1.2563071250915527,
      "learning_rate": 1.675374635095686e-05,
      "loss": 2.533,
      "step": 1026400
    },
    {
      "epoch": 332.7390599675851,
      "grad_norm": 1.3020268678665161,
      "learning_rate": 1.6750502757054816e-05,
      "loss": 2.5208,
      "step": 1026500
    },
    {
      "epoch": 332.7714748784441,
      "grad_norm": 1.2291982173919678,
      "learning_rate": 1.674725916315277e-05,
      "loss": 2.5288,
      "step": 1026600
    },
    {
      "epoch": 332.80388978930307,
      "grad_norm": 1.5023903846740723,
      "learning_rate": 1.674401556925073e-05,
      "loss": 2.5257,
      "step": 1026700
    },
    {
      "epoch": 332.8363047001621,
      "grad_norm": 1.4651315212249756,
      "learning_rate": 1.674077197534869e-05,
      "loss": 2.5398,
      "step": 1026800
    },
    {
      "epoch": 332.86871961102105,
      "grad_norm": 1.1733167171478271,
      "learning_rate": 1.6737528381446644e-05,
      "loss": 2.5626,
      "step": 1026900
    },
    {
      "epoch": 332.90113452188007,
      "grad_norm": 1.2993453741073608,
      "learning_rate": 1.67342847875446e-05,
      "loss": 2.5325,
      "step": 1027000
    },
    {
      "epoch": 332.9335494327391,
      "grad_norm": 1.3214102983474731,
      "learning_rate": 1.673104119364256e-05,
      "loss": 2.5406,
      "step": 1027100
    },
    {
      "epoch": 332.96596434359805,
      "grad_norm": 1.3340317010879517,
      "learning_rate": 1.6727797599740514e-05,
      "loss": 2.5336,
      "step": 1027200
    },
    {
      "epoch": 332.99837925445706,
      "grad_norm": 1.4223865270614624,
      "learning_rate": 1.672455400583847e-05,
      "loss": 2.5297,
      "step": 1027300
    },
    {
      "epoch": 333.0,
      "eval_bleu": 0.9954597716970325,
      "eval_loss": 4.197280406951904,
      "eval_runtime": 4.4537,
      "eval_samples_per_second": 110.469,
      "eval_steps_per_second": 1.796,
      "step": 1027305
    },
    {
      "epoch": 333.030794165316,
      "grad_norm": 1.172936201095581,
      "learning_rate": 1.6721310411936425e-05,
      "loss": 2.5424,
      "step": 1027400
    },
    {
      "epoch": 333.06320907617504,
      "grad_norm": 1.296326994895935,
      "learning_rate": 1.6718099253973406e-05,
      "loss": 2.5457,
      "step": 1027500
    },
    {
      "epoch": 333.09562398703406,
      "grad_norm": 1.3328005075454712,
      "learning_rate": 1.671485566007136e-05,
      "loss": 2.5261,
      "step": 1027600
    },
    {
      "epoch": 333.128038897893,
      "grad_norm": 1.3059356212615967,
      "learning_rate": 1.6711612066169317e-05,
      "loss": 2.5297,
      "step": 1027700
    },
    {
      "epoch": 333.16045380875204,
      "grad_norm": 1.5938830375671387,
      "learning_rate": 1.6708368472267272e-05,
      "loss": 2.5282,
      "step": 1027800
    },
    {
      "epoch": 333.192868719611,
      "grad_norm": 1.385946273803711,
      "learning_rate": 1.670512487836523e-05,
      "loss": 2.5041,
      "step": 1027900
    },
    {
      "epoch": 333.22528363047,
      "grad_norm": 1.588819146156311,
      "learning_rate": 1.6701881284463186e-05,
      "loss": 2.5101,
      "step": 1028000
    },
    {
      "epoch": 333.25769854132903,
      "grad_norm": 1.3527827262878418,
      "learning_rate": 1.669863769056114e-05,
      "loss": 2.5233,
      "step": 1028100
    },
    {
      "epoch": 333.290113452188,
      "grad_norm": 1.2229052782058716,
      "learning_rate": 1.6695394096659097e-05,
      "loss": 2.523,
      "step": 1028200
    },
    {
      "epoch": 333.322528363047,
      "grad_norm": 1.1904975175857544,
      "learning_rate": 1.6692150502757055e-05,
      "loss": 2.5219,
      "step": 1028300
    },
    {
      "epoch": 333.354943273906,
      "grad_norm": 1.2440292835235596,
      "learning_rate": 1.668890690885501e-05,
      "loss": 2.5234,
      "step": 1028400
    },
    {
      "epoch": 333.387358184765,
      "grad_norm": 1.2832250595092773,
      "learning_rate": 1.668566331495297e-05,
      "loss": 2.534,
      "step": 1028500
    },
    {
      "epoch": 333.419773095624,
      "grad_norm": 1.3411928415298462,
      "learning_rate": 1.6682419721050925e-05,
      "loss": 2.5261,
      "step": 1028600
    },
    {
      "epoch": 333.45218800648297,
      "grad_norm": 1.3615273237228394,
      "learning_rate": 1.6679176127148884e-05,
      "loss": 2.5524,
      "step": 1028700
    },
    {
      "epoch": 333.484602917342,
      "grad_norm": 1.3245575428009033,
      "learning_rate": 1.667593253324684e-05,
      "loss": 2.5361,
      "step": 1028800
    },
    {
      "epoch": 333.51701782820095,
      "grad_norm": 1.2902839183807373,
      "learning_rate": 1.6672688939344794e-05,
      "loss": 2.5442,
      "step": 1028900
    },
    {
      "epoch": 333.54943273905997,
      "grad_norm": 1.4762738943099976,
      "learning_rate": 1.6669445345442753e-05,
      "loss": 2.5263,
      "step": 1029000
    },
    {
      "epoch": 333.581847649919,
      "grad_norm": 1.2806121110916138,
      "learning_rate": 1.666620175154071e-05,
      "loss": 2.5086,
      "step": 1029100
    },
    {
      "epoch": 333.61426256077795,
      "grad_norm": 1.2243672609329224,
      "learning_rate": 1.6662958157638664e-05,
      "loss": 2.5146,
      "step": 1029200
    },
    {
      "epoch": 333.64667747163696,
      "grad_norm": 1.3735675811767578,
      "learning_rate": 1.665971456373662e-05,
      "loss": 2.5537,
      "step": 1029300
    },
    {
      "epoch": 333.6790923824959,
      "grad_norm": 1.2771891355514526,
      "learning_rate": 1.6656470969834578e-05,
      "loss": 2.5294,
      "step": 1029400
    },
    {
      "epoch": 333.71150729335494,
      "grad_norm": 1.2864112854003906,
      "learning_rate": 1.6653227375932533e-05,
      "loss": 2.5338,
      "step": 1029500
    },
    {
      "epoch": 333.74392220421396,
      "grad_norm": 1.4090312719345093,
      "learning_rate": 1.664998378203049e-05,
      "loss": 2.5293,
      "step": 1029600
    },
    {
      "epoch": 333.7763371150729,
      "grad_norm": 1.4137871265411377,
      "learning_rate": 1.6646740188128447e-05,
      "loss": 2.5083,
      "step": 1029700
    },
    {
      "epoch": 333.80875202593194,
      "grad_norm": 1.3744663000106812,
      "learning_rate": 1.6643496594226403e-05,
      "loss": 2.5277,
      "step": 1029800
    },
    {
      "epoch": 333.8411669367909,
      "grad_norm": 1.729653239250183,
      "learning_rate": 1.664025300032436e-05,
      "loss": 2.5311,
      "step": 1029900
    },
    {
      "epoch": 333.8735818476499,
      "grad_norm": 1.3881192207336426,
      "learning_rate": 1.6637009406422317e-05,
      "loss": 2.5082,
      "step": 1030000
    },
    {
      "epoch": 333.90599675850893,
      "grad_norm": 1.208968997001648,
      "learning_rate": 1.6633765812520276e-05,
      "loss": 2.5436,
      "step": 1030100
    },
    {
      "epoch": 333.9384116693679,
      "grad_norm": 1.4785152673721313,
      "learning_rate": 1.663052221861823e-05,
      "loss": 2.5397,
      "step": 1030200
    },
    {
      "epoch": 333.9708265802269,
      "grad_norm": 1.485194206237793,
      "learning_rate": 1.6627278624716186e-05,
      "loss": 2.5634,
      "step": 1030300
    },
    {
      "epoch": 334.0,
      "eval_bleu": 1.0122860672579181,
      "eval_loss": 4.195444107055664,
      "eval_runtime": 4.6814,
      "eval_samples_per_second": 105.098,
      "eval_steps_per_second": 1.709,
      "step": 1030390
    },
    {
      "epoch": 334.0032414910859,
      "grad_norm": 1.2269394397735596,
      "learning_rate": 1.662403503081414e-05,
      "loss": 2.5254,
      "step": 1030400
    },
    {
      "epoch": 334.0356564019449,
      "grad_norm": 1.3709441423416138,
      "learning_rate": 1.66207914369121e-05,
      "loss": 2.5261,
      "step": 1030500
    },
    {
      "epoch": 334.0680713128039,
      "grad_norm": 1.2552653551101685,
      "learning_rate": 1.6617547843010056e-05,
      "loss": 2.5169,
      "step": 1030600
    },
    {
      "epoch": 334.10048622366287,
      "grad_norm": 1.4214669466018677,
      "learning_rate": 1.661430424910801e-05,
      "loss": 2.517,
      "step": 1030700
    },
    {
      "epoch": 334.1329011345219,
      "grad_norm": 1.4782065153121948,
      "learning_rate": 1.6611060655205966e-05,
      "loss": 2.5296,
      "step": 1030800
    },
    {
      "epoch": 334.16531604538085,
      "grad_norm": 1.2914108037948608,
      "learning_rate": 1.6607817061303925e-05,
      "loss": 2.5242,
      "step": 1030900
    },
    {
      "epoch": 334.19773095623987,
      "grad_norm": 1.3052995204925537,
      "learning_rate": 1.660457346740188e-05,
      "loss": 2.5433,
      "step": 1031000
    },
    {
      "epoch": 334.2301458670989,
      "grad_norm": 1.4593124389648438,
      "learning_rate": 1.660132987349984e-05,
      "loss": 2.5362,
      "step": 1031100
    },
    {
      "epoch": 334.26256077795784,
      "grad_norm": 1.304099678993225,
      "learning_rate": 1.6598086279597798e-05,
      "loss": 2.4981,
      "step": 1031200
    },
    {
      "epoch": 334.29497568881686,
      "grad_norm": 1.182529330253601,
      "learning_rate": 1.6594842685695753e-05,
      "loss": 2.5355,
      "step": 1031300
    },
    {
      "epoch": 334.3273905996758,
      "grad_norm": 1.4487327337265015,
      "learning_rate": 1.659159909179371e-05,
      "loss": 2.5352,
      "step": 1031400
    },
    {
      "epoch": 334.35980551053484,
      "grad_norm": 1.3019797801971436,
      "learning_rate": 1.6588387933830683e-05,
      "loss": 2.5167,
      "step": 1031500
    },
    {
      "epoch": 334.39222042139386,
      "grad_norm": 1.4009442329406738,
      "learning_rate": 1.6585144339928642e-05,
      "loss": 2.4867,
      "step": 1031600
    },
    {
      "epoch": 334.4246353322528,
      "grad_norm": 1.3124544620513916,
      "learning_rate": 1.65819007460266e-05,
      "loss": 2.5191,
      "step": 1031700
    },
    {
      "epoch": 334.45705024311184,
      "grad_norm": 1.3123328685760498,
      "learning_rate": 1.6578657152124556e-05,
      "loss": 2.5254,
      "step": 1031800
    },
    {
      "epoch": 334.48946515397085,
      "grad_norm": 1.4515024423599243,
      "learning_rate": 1.657541355822251e-05,
      "loss": 2.5416,
      "step": 1031900
    },
    {
      "epoch": 334.5218800648298,
      "grad_norm": 1.620385766029358,
      "learning_rate": 1.657216996432047e-05,
      "loss": 2.5425,
      "step": 1032000
    },
    {
      "epoch": 334.55429497568883,
      "grad_norm": 1.4293010234832764,
      "learning_rate": 1.6568926370418425e-05,
      "loss": 2.524,
      "step": 1032100
    },
    {
      "epoch": 334.5867098865478,
      "grad_norm": 1.4322385787963867,
      "learning_rate": 1.656568277651638e-05,
      "loss": 2.5311,
      "step": 1032200
    },
    {
      "epoch": 334.6191247974068,
      "grad_norm": 1.419684648513794,
      "learning_rate": 1.6562439182614336e-05,
      "loss": 2.5498,
      "step": 1032300
    },
    {
      "epoch": 334.65153970826583,
      "grad_norm": 1.2474586963653564,
      "learning_rate": 1.6559195588712295e-05,
      "loss": 2.5479,
      "step": 1032400
    },
    {
      "epoch": 334.6839546191248,
      "grad_norm": 1.4285528659820557,
      "learning_rate": 1.655595199481025e-05,
      "loss": 2.5416,
      "step": 1032500
    },
    {
      "epoch": 334.7163695299838,
      "grad_norm": 1.2288641929626465,
      "learning_rate": 1.6552708400908206e-05,
      "loss": 2.5094,
      "step": 1032600
    },
    {
      "epoch": 334.74878444084277,
      "grad_norm": 1.43341064453125,
      "learning_rate": 1.654946480700616e-05,
      "loss": 2.5284,
      "step": 1032700
    },
    {
      "epoch": 334.7811993517018,
      "grad_norm": 1.4208277463912964,
      "learning_rate": 1.654622121310412e-05,
      "loss": 2.533,
      "step": 1032800
    },
    {
      "epoch": 334.8136142625608,
      "grad_norm": 1.2088700532913208,
      "learning_rate": 1.654297761920208e-05,
      "loss": 2.5494,
      "step": 1032900
    },
    {
      "epoch": 334.84602917341977,
      "grad_norm": 1.2363791465759277,
      "learning_rate": 1.6539734025300034e-05,
      "loss": 2.5157,
      "step": 1033000
    },
    {
      "epoch": 334.8784440842788,
      "grad_norm": 1.4694362878799438,
      "learning_rate": 1.653649043139799e-05,
      "loss": 2.5495,
      "step": 1033100
    },
    {
      "epoch": 334.91085899513774,
      "grad_norm": 1.2863069772720337,
      "learning_rate": 1.6533246837495948e-05,
      "loss": 2.5067,
      "step": 1033200
    },
    {
      "epoch": 334.94327390599676,
      "grad_norm": 1.4303086996078491,
      "learning_rate": 1.6530003243593903e-05,
      "loss": 2.5343,
      "step": 1033300
    },
    {
      "epoch": 334.9756888168558,
      "grad_norm": 1.368546724319458,
      "learning_rate": 1.652675964969186e-05,
      "loss": 2.5239,
      "step": 1033400
    },
    {
      "epoch": 335.0,
      "eval_bleu": 0.8052211950631323,
      "eval_loss": 4.191401481628418,
      "eval_runtime": 4.371,
      "eval_samples_per_second": 112.56,
      "eval_steps_per_second": 1.83,
      "step": 1033475
    },
    {
      "epoch": 335.00810372771474,
      "grad_norm": 1.184775471687317,
      "learning_rate": 1.6523548491728836e-05,
      "loss": 2.5481,
      "step": 1033500
    },
    {
      "epoch": 335.04051863857376,
      "grad_norm": 1.3597908020019531,
      "learning_rate": 1.6520304897826795e-05,
      "loss": 2.5125,
      "step": 1033600
    },
    {
      "epoch": 335.0729335494327,
      "grad_norm": 1.5916002988815308,
      "learning_rate": 1.651706130392475e-05,
      "loss": 2.5263,
      "step": 1033700
    },
    {
      "epoch": 335.10534846029174,
      "grad_norm": 1.3983497619628906,
      "learning_rate": 1.6513817710022706e-05,
      "loss": 2.5287,
      "step": 1033800
    },
    {
      "epoch": 335.13776337115075,
      "grad_norm": 1.3444857597351074,
      "learning_rate": 1.651057411612066e-05,
      "loss": 2.5365,
      "step": 1033900
    },
    {
      "epoch": 335.1701782820097,
      "grad_norm": 1.4717832803726196,
      "learning_rate": 1.650733052221862e-05,
      "loss": 2.529,
      "step": 1034000
    },
    {
      "epoch": 335.20259319286873,
      "grad_norm": 1.3709588050842285,
      "learning_rate": 1.6504086928316575e-05,
      "loss": 2.525,
      "step": 1034100
    },
    {
      "epoch": 335.2350081037277,
      "grad_norm": 1.2310378551483154,
      "learning_rate": 1.650084333441453e-05,
      "loss": 2.5439,
      "step": 1034200
    },
    {
      "epoch": 335.2674230145867,
      "grad_norm": 1.1574509143829346,
      "learning_rate": 1.649759974051249e-05,
      "loss": 2.507,
      "step": 1034300
    },
    {
      "epoch": 335.29983792544573,
      "grad_norm": 1.2330045700073242,
      "learning_rate": 1.6494356146610445e-05,
      "loss": 2.547,
      "step": 1034400
    },
    {
      "epoch": 335.3322528363047,
      "grad_norm": 1.489429235458374,
      "learning_rate": 1.64911125527084e-05,
      "loss": 2.5252,
      "step": 1034500
    },
    {
      "epoch": 335.3646677471637,
      "grad_norm": 1.3850908279418945,
      "learning_rate": 1.648786895880636e-05,
      "loss": 2.5241,
      "step": 1034600
    },
    {
      "epoch": 335.39708265802267,
      "grad_norm": 1.4570540189743042,
      "learning_rate": 1.6484625364904314e-05,
      "loss": 2.5194,
      "step": 1034700
    },
    {
      "epoch": 335.4294975688817,
      "grad_norm": 1.272742509841919,
      "learning_rate": 1.6481381771002273e-05,
      "loss": 2.5082,
      "step": 1034800
    },
    {
      "epoch": 335.4619124797407,
      "grad_norm": 1.4722003936767578,
      "learning_rate": 1.6478138177100228e-05,
      "loss": 2.5163,
      "step": 1034900
    },
    {
      "epoch": 335.49432739059966,
      "grad_norm": 1.3752166032791138,
      "learning_rate": 1.6474894583198184e-05,
      "loss": 2.5199,
      "step": 1035000
    },
    {
      "epoch": 335.5267423014587,
      "grad_norm": 1.4320509433746338,
      "learning_rate": 1.6471650989296142e-05,
      "loss": 2.5266,
      "step": 1035100
    },
    {
      "epoch": 335.55915721231764,
      "grad_norm": 1.2700974941253662,
      "learning_rate": 1.6468407395394098e-05,
      "loss": 2.5319,
      "step": 1035200
    },
    {
      "epoch": 335.59157212317666,
      "grad_norm": 1.3325968980789185,
      "learning_rate": 1.6465163801492053e-05,
      "loss": 2.5351,
      "step": 1035300
    },
    {
      "epoch": 335.6239870340357,
      "grad_norm": 1.4967604875564575,
      "learning_rate": 1.646192020759001e-05,
      "loss": 2.5169,
      "step": 1035400
    },
    {
      "epoch": 335.65640194489464,
      "grad_norm": 1.3810808658599854,
      "learning_rate": 1.645870904962699e-05,
      "loss": 2.5096,
      "step": 1035500
    },
    {
      "epoch": 335.68881685575366,
      "grad_norm": 1.517652153968811,
      "learning_rate": 1.6455465455724945e-05,
      "loss": 2.5116,
      "step": 1035600
    },
    {
      "epoch": 335.7212317666126,
      "grad_norm": 1.202013373374939,
      "learning_rate": 1.64522218618229e-05,
      "loss": 2.5409,
      "step": 1035700
    },
    {
      "epoch": 335.75364667747164,
      "grad_norm": 1.3859987258911133,
      "learning_rate": 1.6448978267920856e-05,
      "loss": 2.5325,
      "step": 1035800
    },
    {
      "epoch": 335.78606158833065,
      "grad_norm": 1.3988145589828491,
      "learning_rate": 1.6445734674018814e-05,
      "loss": 2.5338,
      "step": 1035900
    },
    {
      "epoch": 335.8184764991896,
      "grad_norm": 1.3752515316009521,
      "learning_rate": 1.644249108011677e-05,
      "loss": 2.5465,
      "step": 1036000
    },
    {
      "epoch": 335.85089141004863,
      "grad_norm": 1.475069284439087,
      "learning_rate": 1.6439247486214725e-05,
      "loss": 2.5212,
      "step": 1036100
    },
    {
      "epoch": 335.8833063209076,
      "grad_norm": 1.5182498693466187,
      "learning_rate": 1.643600389231268e-05,
      "loss": 2.5364,
      "step": 1036200
    },
    {
      "epoch": 335.9157212317666,
      "grad_norm": 1.2284311056137085,
      "learning_rate": 1.643276029841064e-05,
      "loss": 2.5322,
      "step": 1036300
    },
    {
      "epoch": 335.94813614262563,
      "grad_norm": 1.4398564100265503,
      "learning_rate": 1.6429516704508595e-05,
      "loss": 2.531,
      "step": 1036400
    },
    {
      "epoch": 335.9805510534846,
      "grad_norm": 1.3968724012374878,
      "learning_rate": 1.6426273110606553e-05,
      "loss": 2.5262,
      "step": 1036500
    },
    {
      "epoch": 336.0,
      "eval_bleu": 0.843636085506583,
      "eval_loss": 4.200130462646484,
      "eval_runtime": 4.9113,
      "eval_samples_per_second": 100.177,
      "eval_steps_per_second": 1.629,
      "step": 1036560
    },
    {
      "epoch": 336.0129659643436,
      "grad_norm": 1.2847696542739868,
      "learning_rate": 1.6423029516704512e-05,
      "loss": 2.5153,
      "step": 1036600
    },
    {
      "epoch": 336.04538087520257,
      "grad_norm": 1.4735422134399414,
      "learning_rate": 1.6419785922802467e-05,
      "loss": 2.4948,
      "step": 1036700
    },
    {
      "epoch": 336.0777957860616,
      "grad_norm": 1.4292749166488647,
      "learning_rate": 1.6416542328900423e-05,
      "loss": 2.5168,
      "step": 1036800
    },
    {
      "epoch": 336.1102106969206,
      "grad_norm": 1.4295564889907837,
      "learning_rate": 1.6413298734998378e-05,
      "loss": 2.5319,
      "step": 1036900
    },
    {
      "epoch": 336.14262560777956,
      "grad_norm": 1.2189748287200928,
      "learning_rate": 1.6410087577035356e-05,
      "loss": 2.5324,
      "step": 1037000
    },
    {
      "epoch": 336.1750405186386,
      "grad_norm": 1.3622230291366577,
      "learning_rate": 1.6406843983133315e-05,
      "loss": 2.5223,
      "step": 1037100
    },
    {
      "epoch": 336.20745542949754,
      "grad_norm": 1.4809162616729736,
      "learning_rate": 1.640360038923127e-05,
      "loss": 2.5184,
      "step": 1037200
    },
    {
      "epoch": 336.23987034035656,
      "grad_norm": 1.2906430959701538,
      "learning_rate": 1.6400356795329225e-05,
      "loss": 2.5473,
      "step": 1037300
    },
    {
      "epoch": 336.2722852512156,
      "grad_norm": 1.3384323120117188,
      "learning_rate": 1.6397113201427184e-05,
      "loss": 2.5391,
      "step": 1037400
    },
    {
      "epoch": 336.30470016207454,
      "grad_norm": 1.4197769165039062,
      "learning_rate": 1.639386960752514e-05,
      "loss": 2.5254,
      "step": 1037500
    },
    {
      "epoch": 336.33711507293356,
      "grad_norm": 1.2500720024108887,
      "learning_rate": 1.6390626013623095e-05,
      "loss": 2.5445,
      "step": 1037600
    },
    {
      "epoch": 336.3695299837925,
      "grad_norm": 1.4270988702774048,
      "learning_rate": 1.638738241972105e-05,
      "loss": 2.5278,
      "step": 1037700
    },
    {
      "epoch": 336.40194489465154,
      "grad_norm": 1.4981541633605957,
      "learning_rate": 1.638413882581901e-05,
      "loss": 2.5326,
      "step": 1037800
    },
    {
      "epoch": 336.43435980551055,
      "grad_norm": 1.4306714534759521,
      "learning_rate": 1.6380895231916964e-05,
      "loss": 2.5296,
      "step": 1037900
    },
    {
      "epoch": 336.4667747163695,
      "grad_norm": 1.358967661857605,
      "learning_rate": 1.637765163801492e-05,
      "loss": 2.5266,
      "step": 1038000
    },
    {
      "epoch": 336.49918962722853,
      "grad_norm": 1.4775817394256592,
      "learning_rate": 1.6374408044112875e-05,
      "loss": 2.5407,
      "step": 1038100
    },
    {
      "epoch": 336.5316045380875,
      "grad_norm": 1.3967374563217163,
      "learning_rate": 1.6371164450210834e-05,
      "loss": 2.5212,
      "step": 1038200
    },
    {
      "epoch": 336.5640194489465,
      "grad_norm": 1.5355569124221802,
      "learning_rate": 1.6367920856308793e-05,
      "loss": 2.5267,
      "step": 1038300
    },
    {
      "epoch": 336.5964343598055,
      "grad_norm": 1.3547110557556152,
      "learning_rate": 1.6364677262406748e-05,
      "loss": 2.5236,
      "step": 1038400
    },
    {
      "epoch": 336.6288492706645,
      "grad_norm": 1.4151761531829834,
      "learning_rate": 1.6361433668504707e-05,
      "loss": 2.5295,
      "step": 1038500
    },
    {
      "epoch": 336.6612641815235,
      "grad_norm": 1.6159647703170776,
      "learning_rate": 1.6358190074602662e-05,
      "loss": 2.5261,
      "step": 1038600
    },
    {
      "epoch": 336.6936790923825,
      "grad_norm": 1.3823444843292236,
      "learning_rate": 1.6354946480700617e-05,
      "loss": 2.5308,
      "step": 1038700
    },
    {
      "epoch": 336.7260940032415,
      "grad_norm": 1.557755708694458,
      "learning_rate": 1.6351702886798573e-05,
      "loss": 2.5333,
      "step": 1038800
    },
    {
      "epoch": 336.7585089141005,
      "grad_norm": 1.3837909698486328,
      "learning_rate": 1.634845929289653e-05,
      "loss": 2.5314,
      "step": 1038900
    },
    {
      "epoch": 336.79092382495946,
      "grad_norm": 1.4166617393493652,
      "learning_rate": 1.6345215698994487e-05,
      "loss": 2.52,
      "step": 1039000
    },
    {
      "epoch": 336.8233387358185,
      "grad_norm": 1.330114722251892,
      "learning_rate": 1.6341972105092442e-05,
      "loss": 2.5167,
      "step": 1039100
    },
    {
      "epoch": 336.8557536466775,
      "grad_norm": 1.37371826171875,
      "learning_rate": 1.6338728511190397e-05,
      "loss": 2.5209,
      "step": 1039200
    },
    {
      "epoch": 336.88816855753646,
      "grad_norm": 1.2573938369750977,
      "learning_rate": 1.6335484917288356e-05,
      "loss": 2.5375,
      "step": 1039300
    },
    {
      "epoch": 336.9205834683955,
      "grad_norm": 1.2334332466125488,
      "learning_rate": 1.633224132338631e-05,
      "loss": 2.5046,
      "step": 1039400
    },
    {
      "epoch": 336.95299837925444,
      "grad_norm": 1.2913075685501099,
      "learning_rate": 1.632903016542329e-05,
      "loss": 2.5271,
      "step": 1039500
    },
    {
      "epoch": 336.98541329011346,
      "grad_norm": 1.4935966730117798,
      "learning_rate": 1.6325786571521245e-05,
      "loss": 2.508,
      "step": 1039600
    },
    {
      "epoch": 337.0,
      "eval_bleu": 0.7990969720066693,
      "eval_loss": 4.201787948608398,
      "eval_runtime": 4.5836,
      "eval_samples_per_second": 107.34,
      "eval_steps_per_second": 1.745,
      "step": 1039645
    },
    {
      "epoch": 337.0178282009725,
      "grad_norm": 1.3828216791152954,
      "learning_rate": 1.6322542977619203e-05,
      "loss": 2.511,
      "step": 1039700
    },
    {
      "epoch": 337.05024311183143,
      "grad_norm": 1.397346019744873,
      "learning_rate": 1.631929938371716e-05,
      "loss": 2.5315,
      "step": 1039800
    },
    {
      "epoch": 337.08265802269045,
      "grad_norm": 1.228477120399475,
      "learning_rate": 1.6316055789815114e-05,
      "loss": 2.5302,
      "step": 1039900
    },
    {
      "epoch": 337.1150729335494,
      "grad_norm": 1.3048948049545288,
      "learning_rate": 1.6312812195913073e-05,
      "loss": 2.5352,
      "step": 1040000
    },
    {
      "epoch": 337.14748784440843,
      "grad_norm": 1.4707746505737305,
      "learning_rate": 1.630956860201103e-05,
      "loss": 2.5072,
      "step": 1040100
    },
    {
      "epoch": 337.17990275526745,
      "grad_norm": 1.5139890909194946,
      "learning_rate": 1.6306325008108987e-05,
      "loss": 2.5292,
      "step": 1040200
    },
    {
      "epoch": 337.2123176661264,
      "grad_norm": 1.4173012971878052,
      "learning_rate": 1.6303081414206942e-05,
      "loss": 2.5052,
      "step": 1040300
    },
    {
      "epoch": 337.2447325769854,
      "grad_norm": 1.4668705463409424,
      "learning_rate": 1.6299837820304898e-05,
      "loss": 2.5036,
      "step": 1040400
    },
    {
      "epoch": 337.2771474878444,
      "grad_norm": 1.3263659477233887,
      "learning_rate": 1.6296594226402856e-05,
      "loss": 2.5314,
      "step": 1040500
    },
    {
      "epoch": 337.3095623987034,
      "grad_norm": 1.3597036600112915,
      "learning_rate": 1.6293350632500812e-05,
      "loss": 2.5205,
      "step": 1040600
    },
    {
      "epoch": 337.3419773095624,
      "grad_norm": 1.3828673362731934,
      "learning_rate": 1.6290107038598767e-05,
      "loss": 2.5417,
      "step": 1040700
    },
    {
      "epoch": 337.3743922204214,
      "grad_norm": 1.5692057609558105,
      "learning_rate": 1.6286863444696726e-05,
      "loss": 2.5203,
      "step": 1040800
    },
    {
      "epoch": 337.4068071312804,
      "grad_norm": 1.478885293006897,
      "learning_rate": 1.628361985079468e-05,
      "loss": 2.5179,
      "step": 1040900
    },
    {
      "epoch": 337.43922204213936,
      "grad_norm": 1.190597653388977,
      "learning_rate": 1.6280376256892637e-05,
      "loss": 2.5259,
      "step": 1041000
    },
    {
      "epoch": 337.4716369529984,
      "grad_norm": 1.4803082942962646,
      "learning_rate": 1.6277132662990592e-05,
      "loss": 2.5159,
      "step": 1041100
    },
    {
      "epoch": 337.5040518638574,
      "grad_norm": 1.2697181701660156,
      "learning_rate": 1.627388906908855e-05,
      "loss": 2.5065,
      "step": 1041200
    },
    {
      "epoch": 337.53646677471636,
      "grad_norm": 1.3319875001907349,
      "learning_rate": 1.627064547518651e-05,
      "loss": 2.5245,
      "step": 1041300
    },
    {
      "epoch": 337.5688816855754,
      "grad_norm": 1.3132271766662598,
      "learning_rate": 1.6267401881284465e-05,
      "loss": 2.5454,
      "step": 1041400
    },
    {
      "epoch": 337.60129659643434,
      "grad_norm": 1.6475733518600464,
      "learning_rate": 1.626415828738242e-05,
      "loss": 2.5382,
      "step": 1041500
    },
    {
      "epoch": 337.63371150729336,
      "grad_norm": 1.3564391136169434,
      "learning_rate": 1.626091469348038e-05,
      "loss": 2.5361,
      "step": 1041600
    },
    {
      "epoch": 337.6661264181524,
      "grad_norm": 1.2469614744186401,
      "learning_rate": 1.6257671099578334e-05,
      "loss": 2.5164,
      "step": 1041700
    },
    {
      "epoch": 337.69854132901133,
      "grad_norm": 1.2517669200897217,
      "learning_rate": 1.625442750567629e-05,
      "loss": 2.5103,
      "step": 1041800
    },
    {
      "epoch": 337.73095623987035,
      "grad_norm": 1.5011181831359863,
      "learning_rate": 1.6251183911774245e-05,
      "loss": 2.5319,
      "step": 1041900
    },
    {
      "epoch": 337.7633711507293,
      "grad_norm": 1.3584129810333252,
      "learning_rate": 1.6247940317872204e-05,
      "loss": 2.5219,
      "step": 1042000
    },
    {
      "epoch": 337.79578606158833,
      "grad_norm": 1.3517506122589111,
      "learning_rate": 1.624469672397016e-05,
      "loss": 2.5294,
      "step": 1042100
    },
    {
      "epoch": 337.82820097244735,
      "grad_norm": 1.3259197473526,
      "learning_rate": 1.6241453130068114e-05,
      "loss": 2.5311,
      "step": 1042200
    },
    {
      "epoch": 337.8606158833063,
      "grad_norm": 1.34268319606781,
      "learning_rate": 1.6238209536166073e-05,
      "loss": 2.5633,
      "step": 1042300
    },
    {
      "epoch": 337.8930307941653,
      "grad_norm": 1.4119513034820557,
      "learning_rate": 1.623496594226403e-05,
      "loss": 2.5235,
      "step": 1042400
    },
    {
      "epoch": 337.9254457050243,
      "grad_norm": 1.2333202362060547,
      "learning_rate": 1.6231722348361987e-05,
      "loss": 2.5376,
      "step": 1042500
    },
    {
      "epoch": 337.9578606158833,
      "grad_norm": 1.3925225734710693,
      "learning_rate": 1.6228478754459943e-05,
      "loss": 2.5152,
      "step": 1042600
    },
    {
      "epoch": 337.9902755267423,
      "grad_norm": 1.2553346157073975,
      "learning_rate": 1.62252351605579e-05,
      "loss": 2.5375,
      "step": 1042700
    },
    {
      "epoch": 338.0,
      "eval_bleu": 0.9846657811930513,
      "eval_loss": 4.197446346282959,
      "eval_runtime": 4.6832,
      "eval_samples_per_second": 105.056,
      "eval_steps_per_second": 1.708,
      "step": 1042730
    },
    {
      "epoch": 338.0226904376013,
      "grad_norm": 1.3006867170333862,
      "learning_rate": 1.6221991566655857e-05,
      "loss": 2.5324,
      "step": 1042800
    },
    {
      "epoch": 338.0551053484603,
      "grad_norm": 1.3099007606506348,
      "learning_rate": 1.6218747972753812e-05,
      "loss": 2.5178,
      "step": 1042900
    },
    {
      "epoch": 338.08752025931926,
      "grad_norm": 1.3081552982330322,
      "learning_rate": 1.6215504378851767e-05,
      "loss": 2.5236,
      "step": 1043000
    },
    {
      "epoch": 338.1199351701783,
      "grad_norm": 1.54549241065979,
      "learning_rate": 1.6212260784949726e-05,
      "loss": 2.5074,
      "step": 1043100
    },
    {
      "epoch": 338.1523500810373,
      "grad_norm": 1.7072558403015137,
      "learning_rate": 1.620901719104768e-05,
      "loss": 2.5101,
      "step": 1043200
    },
    {
      "epoch": 338.18476499189626,
      "grad_norm": 1.3382538557052612,
      "learning_rate": 1.6205773597145637e-05,
      "loss": 2.5286,
      "step": 1043300
    },
    {
      "epoch": 338.2171799027553,
      "grad_norm": 1.3426711559295654,
      "learning_rate": 1.6202530003243592e-05,
      "loss": 2.5167,
      "step": 1043400
    },
    {
      "epoch": 338.24959481361424,
      "grad_norm": 1.253927230834961,
      "learning_rate": 1.6199318845280573e-05,
      "loss": 2.5251,
      "step": 1043500
    },
    {
      "epoch": 338.28200972447326,
      "grad_norm": 1.347826600074768,
      "learning_rate": 1.619607525137853e-05,
      "loss": 2.5289,
      "step": 1043600
    },
    {
      "epoch": 338.3144246353323,
      "grad_norm": 1.421168565750122,
      "learning_rate": 1.6192831657476484e-05,
      "loss": 2.5272,
      "step": 1043700
    },
    {
      "epoch": 338.34683954619123,
      "grad_norm": 1.3918348550796509,
      "learning_rate": 1.618958806357444e-05,
      "loss": 2.515,
      "step": 1043800
    },
    {
      "epoch": 338.37925445705025,
      "grad_norm": 1.4680235385894775,
      "learning_rate": 1.6186344469672398e-05,
      "loss": 2.5302,
      "step": 1043900
    },
    {
      "epoch": 338.4116693679092,
      "grad_norm": 1.2589454650878906,
      "learning_rate": 1.6183100875770354e-05,
      "loss": 2.5281,
      "step": 1044000
    },
    {
      "epoch": 338.44408427876823,
      "grad_norm": 1.3547145128250122,
      "learning_rate": 1.617985728186831e-05,
      "loss": 2.5133,
      "step": 1044100
    },
    {
      "epoch": 338.47649918962725,
      "grad_norm": 1.5006972551345825,
      "learning_rate": 1.6176613687966268e-05,
      "loss": 2.5311,
      "step": 1044200
    },
    {
      "epoch": 338.5089141004862,
      "grad_norm": 1.5761775970458984,
      "learning_rate": 1.6173370094064223e-05,
      "loss": 2.5128,
      "step": 1044300
    },
    {
      "epoch": 338.5413290113452,
      "grad_norm": 1.2710105180740356,
      "learning_rate": 1.6170126500162182e-05,
      "loss": 2.533,
      "step": 1044400
    },
    {
      "epoch": 338.5737439222042,
      "grad_norm": 1.3952929973602295,
      "learning_rate": 1.6166882906260137e-05,
      "loss": 2.5369,
      "step": 1044500
    },
    {
      "epoch": 338.6061588330632,
      "grad_norm": 1.3586320877075195,
      "learning_rate": 1.6163639312358096e-05,
      "loss": 2.5133,
      "step": 1044600
    },
    {
      "epoch": 338.6385737439222,
      "grad_norm": 1.2493752241134644,
      "learning_rate": 1.616039571845605e-05,
      "loss": 2.5275,
      "step": 1044700
    },
    {
      "epoch": 338.6709886547812,
      "grad_norm": 1.3281110525131226,
      "learning_rate": 1.6157152124554007e-05,
      "loss": 2.5237,
      "step": 1044800
    },
    {
      "epoch": 338.7034035656402,
      "grad_norm": 1.3483566045761108,
      "learning_rate": 1.6153908530651962e-05,
      "loss": 2.5442,
      "step": 1044900
    },
    {
      "epoch": 338.73581847649916,
      "grad_norm": 1.5065886974334717,
      "learning_rate": 1.615066493674992e-05,
      "loss": 2.5205,
      "step": 1045000
    },
    {
      "epoch": 338.7682333873582,
      "grad_norm": 1.3182858228683472,
      "learning_rate": 1.6147421342847876e-05,
      "loss": 2.5237,
      "step": 1045100
    },
    {
      "epoch": 338.8006482982172,
      "grad_norm": 1.3793479204177856,
      "learning_rate": 1.614417774894583e-05,
      "loss": 2.5374,
      "step": 1045200
    },
    {
      "epoch": 338.83306320907616,
      "grad_norm": 1.3153568506240845,
      "learning_rate": 1.6140934155043787e-05,
      "loss": 2.5248,
      "step": 1045300
    },
    {
      "epoch": 338.8654781199352,
      "grad_norm": 1.4874637126922607,
      "learning_rate": 1.6137690561141746e-05,
      "loss": 2.5289,
      "step": 1045400
    },
    {
      "epoch": 338.8978930307942,
      "grad_norm": 1.266594409942627,
      "learning_rate": 1.61344469672397e-05,
      "loss": 2.5336,
      "step": 1045500
    },
    {
      "epoch": 338.93030794165315,
      "grad_norm": 1.6812466382980347,
      "learning_rate": 1.613123580927668e-05,
      "loss": 2.5367,
      "step": 1045600
    },
    {
      "epoch": 338.9627228525122,
      "grad_norm": 1.2334227561950684,
      "learning_rate": 1.6127992215374634e-05,
      "loss": 2.5169,
      "step": 1045700
    },
    {
      "epoch": 338.99513776337113,
      "grad_norm": 1.1555992364883423,
      "learning_rate": 1.6124748621472593e-05,
      "loss": 2.5304,
      "step": 1045800
    },
    {
      "epoch": 339.0,
      "eval_bleu": 0.9012786514334432,
      "eval_loss": 4.203137397766113,
      "eval_runtime": 4.8245,
      "eval_samples_per_second": 101.98,
      "eval_steps_per_second": 1.658,
      "step": 1045815
    },
    {
      "epoch": 339.02755267423015,
      "grad_norm": 1.4035673141479492,
      "learning_rate": 1.6121505027570548e-05,
      "loss": 2.5221,
      "step": 1045900
    },
    {
      "epoch": 339.05996758508917,
      "grad_norm": 1.3902044296264648,
      "learning_rate": 1.6118261433668503e-05,
      "loss": 2.5241,
      "step": 1046000
    },
    {
      "epoch": 339.09238249594813,
      "grad_norm": 1.538838267326355,
      "learning_rate": 1.6115017839766462e-05,
      "loss": 2.5267,
      "step": 1046100
    },
    {
      "epoch": 339.12479740680715,
      "grad_norm": 1.3141688108444214,
      "learning_rate": 1.611177424586442e-05,
      "loss": 2.5221,
      "step": 1046200
    },
    {
      "epoch": 339.1572123176661,
      "grad_norm": 1.407633662223816,
      "learning_rate": 1.6108530651962376e-05,
      "loss": 2.5421,
      "step": 1046300
    },
    {
      "epoch": 339.1896272285251,
      "grad_norm": 1.3167344331741333,
      "learning_rate": 1.610528705806033e-05,
      "loss": 2.5255,
      "step": 1046400
    },
    {
      "epoch": 339.22204213938414,
      "grad_norm": 1.5637176036834717,
      "learning_rate": 1.6102043464158287e-05,
      "loss": 2.5318,
      "step": 1046500
    },
    {
      "epoch": 339.2544570502431,
      "grad_norm": 1.4419113397598267,
      "learning_rate": 1.6098799870256246e-05,
      "loss": 2.5158,
      "step": 1046600
    },
    {
      "epoch": 339.2868719611021,
      "grad_norm": 1.5176663398742676,
      "learning_rate": 1.60955562763542e-05,
      "loss": 2.5397,
      "step": 1046700
    },
    {
      "epoch": 339.3192868719611,
      "grad_norm": 1.3839329481124878,
      "learning_rate": 1.6092312682452156e-05,
      "loss": 2.5135,
      "step": 1046800
    },
    {
      "epoch": 339.3517017828201,
      "grad_norm": 1.4425101280212402,
      "learning_rate": 1.6089069088550115e-05,
      "loss": 2.5061,
      "step": 1046900
    },
    {
      "epoch": 339.3841166936791,
      "grad_norm": 1.4935107231140137,
      "learning_rate": 1.608582549464807e-05,
      "loss": 2.5203,
      "step": 1047000
    },
    {
      "epoch": 339.4165316045381,
      "grad_norm": 1.5526878833770752,
      "learning_rate": 1.6082581900746026e-05,
      "loss": 2.5166,
      "step": 1047100
    },
    {
      "epoch": 339.4489465153971,
      "grad_norm": 1.665235996246338,
      "learning_rate": 1.607933830684398e-05,
      "loss": 2.5109,
      "step": 1047200
    },
    {
      "epoch": 339.48136142625606,
      "grad_norm": 1.3272310495376587,
      "learning_rate": 1.607609471294194e-05,
      "loss": 2.5243,
      "step": 1047300
    },
    {
      "epoch": 339.5137763371151,
      "grad_norm": 1.2768498659133911,
      "learning_rate": 1.60728511190399e-05,
      "loss": 2.5025,
      "step": 1047400
    },
    {
      "epoch": 339.5461912479741,
      "grad_norm": 1.1520192623138428,
      "learning_rate": 1.6069607525137854e-05,
      "loss": 2.5163,
      "step": 1047500
    },
    {
      "epoch": 339.57860615883305,
      "grad_norm": 1.3940807580947876,
      "learning_rate": 1.606639636717483e-05,
      "loss": 2.5274,
      "step": 1047600
    },
    {
      "epoch": 339.6110210696921,
      "grad_norm": 1.3302029371261597,
      "learning_rate": 1.6063152773272787e-05,
      "loss": 2.5088,
      "step": 1047700
    },
    {
      "epoch": 339.64343598055103,
      "grad_norm": 1.5029754638671875,
      "learning_rate": 1.6059909179370743e-05,
      "loss": 2.5037,
      "step": 1047800
    },
    {
      "epoch": 339.67585089141005,
      "grad_norm": 1.3442134857177734,
      "learning_rate": 1.60566655854687e-05,
      "loss": 2.507,
      "step": 1047900
    },
    {
      "epoch": 339.70826580226907,
      "grad_norm": 1.2711282968521118,
      "learning_rate": 1.6053421991566657e-05,
      "loss": 2.5123,
      "step": 1048000
    },
    {
      "epoch": 339.74068071312803,
      "grad_norm": 1.4507770538330078,
      "learning_rate": 1.6050178397664615e-05,
      "loss": 2.5168,
      "step": 1048100
    },
    {
      "epoch": 339.77309562398705,
      "grad_norm": 1.4507784843444824,
      "learning_rate": 1.604693480376257e-05,
      "loss": 2.5298,
      "step": 1048200
    },
    {
      "epoch": 339.805510534846,
      "grad_norm": 1.4836536645889282,
      "learning_rate": 1.6043691209860526e-05,
      "loss": 2.5278,
      "step": 1048300
    },
    {
      "epoch": 339.837925445705,
      "grad_norm": 1.407150149345398,
      "learning_rate": 1.604044761595848e-05,
      "loss": 2.5396,
      "step": 1048400
    },
    {
      "epoch": 339.87034035656404,
      "grad_norm": 1.4177320003509521,
      "learning_rate": 1.603720402205644e-05,
      "loss": 2.541,
      "step": 1048500
    },
    {
      "epoch": 339.902755267423,
      "grad_norm": 1.3075907230377197,
      "learning_rate": 1.6033960428154396e-05,
      "loss": 2.5388,
      "step": 1048600
    },
    {
      "epoch": 339.935170178282,
      "grad_norm": 1.3375494480133057,
      "learning_rate": 1.603071683425235e-05,
      "loss": 2.5382,
      "step": 1048700
    },
    {
      "epoch": 339.967585089141,
      "grad_norm": 1.3645062446594238,
      "learning_rate": 1.602747324035031e-05,
      "loss": 2.53,
      "step": 1048800
    },
    {
      "epoch": 340.0,
      "grad_norm": 1.273625135421753,
      "learning_rate": 1.6024229646448265e-05,
      "loss": 2.526,
      "step": 1048900
    },
    {
      "epoch": 340.0,
      "eval_bleu": 1.0158122920918236,
      "eval_loss": 4.201458930969238,
      "eval_runtime": 4.936,
      "eval_samples_per_second": 99.677,
      "eval_steps_per_second": 1.621,
      "step": 1048900
    },
    {
      "epoch": 340.032414910859,
      "grad_norm": 1.4818551540374756,
      "learning_rate": 1.602098605254622e-05,
      "loss": 2.5285,
      "step": 1049000
    },
    {
      "epoch": 340.064829821718,
      "grad_norm": 1.2885711193084717,
      "learning_rate": 1.601774245864418e-05,
      "loss": 2.5204,
      "step": 1049100
    },
    {
      "epoch": 340.097244732577,
      "grad_norm": 1.4815380573272705,
      "learning_rate": 1.6014498864742138e-05,
      "loss": 2.5215,
      "step": 1049200
    },
    {
      "epoch": 340.12965964343596,
      "grad_norm": 1.2383064031600952,
      "learning_rate": 1.6011255270840093e-05,
      "loss": 2.5195,
      "step": 1049300
    },
    {
      "epoch": 340.162074554295,
      "grad_norm": 1.2958080768585205,
      "learning_rate": 1.600801167693805e-05,
      "loss": 2.4979,
      "step": 1049400
    },
    {
      "epoch": 340.194489465154,
      "grad_norm": 1.2751718759536743,
      "learning_rate": 1.6004768083036004e-05,
      "loss": 2.5563,
      "step": 1049500
    },
    {
      "epoch": 340.22690437601295,
      "grad_norm": 1.3097280263900757,
      "learning_rate": 1.6001556925072982e-05,
      "loss": 2.5109,
      "step": 1049600
    },
    {
      "epoch": 340.25931928687197,
      "grad_norm": 1.3692033290863037,
      "learning_rate": 1.5998313331170937e-05,
      "loss": 2.5183,
      "step": 1049700
    },
    {
      "epoch": 340.29173419773093,
      "grad_norm": 1.3419913053512573,
      "learning_rate": 1.5995069737268896e-05,
      "loss": 2.5344,
      "step": 1049800
    },
    {
      "epoch": 340.32414910858995,
      "grad_norm": 1.4566278457641602,
      "learning_rate": 1.599182614336685e-05,
      "loss": 2.5243,
      "step": 1049900
    },
    {
      "epoch": 340.35656401944897,
      "grad_norm": 1.3636326789855957,
      "learning_rate": 1.598858254946481e-05,
      "loss": 2.5115,
      "step": 1050000
    },
    {
      "epoch": 340.3889789303079,
      "grad_norm": 1.3213022947311401,
      "learning_rate": 1.5985338955562765e-05,
      "loss": 2.523,
      "step": 1050100
    },
    {
      "epoch": 340.42139384116695,
      "grad_norm": 1.6525280475616455,
      "learning_rate": 1.598209536166072e-05,
      "loss": 2.5144,
      "step": 1050200
    },
    {
      "epoch": 340.4538087520259,
      "grad_norm": 1.2950687408447266,
      "learning_rate": 1.5978851767758676e-05,
      "loss": 2.5077,
      "step": 1050300
    },
    {
      "epoch": 340.4862236628849,
      "grad_norm": 1.3610186576843262,
      "learning_rate": 1.5975608173856635e-05,
      "loss": 2.5134,
      "step": 1050400
    },
    {
      "epoch": 340.51863857374394,
      "grad_norm": 1.297867774963379,
      "learning_rate": 1.597236457995459e-05,
      "loss": 2.5137,
      "step": 1050500
    },
    {
      "epoch": 340.5510534846029,
      "grad_norm": 1.514480471611023,
      "learning_rate": 1.5969153421991568e-05,
      "loss": 2.5011,
      "step": 1050600
    },
    {
      "epoch": 340.5834683954619,
      "grad_norm": 1.6048059463500977,
      "learning_rate": 1.5965909828089523e-05,
      "loss": 2.5107,
      "step": 1050700
    },
    {
      "epoch": 340.6158833063209,
      "grad_norm": 1.3162081241607666,
      "learning_rate": 1.5962666234187482e-05,
      "loss": 2.511,
      "step": 1050800
    },
    {
      "epoch": 340.6482982171799,
      "grad_norm": 1.3332170248031616,
      "learning_rate": 1.5959422640285437e-05,
      "loss": 2.5246,
      "step": 1050900
    },
    {
      "epoch": 340.6807131280389,
      "grad_norm": 1.5073895454406738,
      "learning_rate": 1.5956179046383393e-05,
      "loss": 2.523,
      "step": 1051000
    },
    {
      "epoch": 340.7131280388979,
      "grad_norm": 1.3185899257659912,
      "learning_rate": 1.5952935452481348e-05,
      "loss": 2.5172,
      "step": 1051100
    },
    {
      "epoch": 340.7455429497569,
      "grad_norm": 1.5078845024108887,
      "learning_rate": 1.5949691858579307e-05,
      "loss": 2.5425,
      "step": 1051200
    },
    {
      "epoch": 340.77795786061586,
      "grad_norm": 1.5334255695343018,
      "learning_rate": 1.5946448264677262e-05,
      "loss": 2.5233,
      "step": 1051300
    },
    {
      "epoch": 340.8103727714749,
      "grad_norm": 1.1985085010528564,
      "learning_rate": 1.5943204670775218e-05,
      "loss": 2.5241,
      "step": 1051400
    },
    {
      "epoch": 340.8427876823339,
      "grad_norm": 1.4767749309539795,
      "learning_rate": 1.5939961076873176e-05,
      "loss": 2.523,
      "step": 1051500
    },
    {
      "epoch": 340.87520259319285,
      "grad_norm": 1.3364386558532715,
      "learning_rate": 1.5936717482971135e-05,
      "loss": 2.5403,
      "step": 1051600
    },
    {
      "epoch": 340.90761750405187,
      "grad_norm": 1.4362272024154663,
      "learning_rate": 1.593347388906909e-05,
      "loss": 2.529,
      "step": 1051700
    },
    {
      "epoch": 340.94003241491083,
      "grad_norm": 1.3169711828231812,
      "learning_rate": 1.5930230295167046e-05,
      "loss": 2.5395,
      "step": 1051800
    },
    {
      "epoch": 340.97244732576985,
      "grad_norm": 1.2915948629379272,
      "learning_rate": 1.5926986701265004e-05,
      "loss": 2.5356,
      "step": 1051900
    },
    {
      "epoch": 341.0,
      "eval_bleu": 1.1043842192715576,
      "eval_loss": 4.201521396636963,
      "eval_runtime": 4.6634,
      "eval_samples_per_second": 105.502,
      "eval_steps_per_second": 1.715,
      "step": 1051985
    },
    {
      "epoch": 341.00486223662887,
      "grad_norm": 1.497397541999817,
      "learning_rate": 1.592374310736296e-05,
      "loss": 2.5361,
      "step": 1052000
    },
    {
      "epoch": 341.0372771474878,
      "grad_norm": 1.5800989866256714,
      "learning_rate": 1.5920499513460915e-05,
      "loss": 2.4894,
      "step": 1052100
    },
    {
      "epoch": 341.06969205834685,
      "grad_norm": 1.4740482568740845,
      "learning_rate": 1.591725591955887e-05,
      "loss": 2.5174,
      "step": 1052200
    },
    {
      "epoch": 341.1021069692058,
      "grad_norm": 1.2421003580093384,
      "learning_rate": 1.591401232565683e-05,
      "loss": 2.5325,
      "step": 1052300
    },
    {
      "epoch": 341.1345218800648,
      "grad_norm": 1.338949203491211,
      "learning_rate": 1.5910768731754785e-05,
      "loss": 2.5042,
      "step": 1052400
    },
    {
      "epoch": 341.16693679092384,
      "grad_norm": 1.5793559551239014,
      "learning_rate": 1.590752513785274e-05,
      "loss": 2.4941,
      "step": 1052500
    },
    {
      "epoch": 341.1993517017828,
      "grad_norm": 1.3376693725585938,
      "learning_rate": 1.5904281543950695e-05,
      "loss": 2.514,
      "step": 1052600
    },
    {
      "epoch": 341.2317666126418,
      "grad_norm": 1.3631247282028198,
      "learning_rate": 1.5901037950048654e-05,
      "loss": 2.5305,
      "step": 1052700
    },
    {
      "epoch": 341.26418152350084,
      "grad_norm": 1.2529529333114624,
      "learning_rate": 1.5897794356146613e-05,
      "loss": 2.5181,
      "step": 1052800
    },
    {
      "epoch": 341.2965964343598,
      "grad_norm": 1.345286250114441,
      "learning_rate": 1.5894583198183587e-05,
      "loss": 2.5252,
      "step": 1052900
    },
    {
      "epoch": 341.3290113452188,
      "grad_norm": 1.5506092309951782,
      "learning_rate": 1.5891339604281543e-05,
      "loss": 2.5179,
      "step": 1053000
    },
    {
      "epoch": 341.3614262560778,
      "grad_norm": 1.2647699117660522,
      "learning_rate": 1.58880960103795e-05,
      "loss": 2.5439,
      "step": 1053100
    },
    {
      "epoch": 341.3938411669368,
      "grad_norm": 1.4339896440505981,
      "learning_rate": 1.5884852416477457e-05,
      "loss": 2.5349,
      "step": 1053200
    },
    {
      "epoch": 341.4262560777958,
      "grad_norm": 1.4323378801345825,
      "learning_rate": 1.5881608822575415e-05,
      "loss": 2.527,
      "step": 1053300
    },
    {
      "epoch": 341.4586709886548,
      "grad_norm": 1.503345012664795,
      "learning_rate": 1.587836522867337e-05,
      "loss": 2.5024,
      "step": 1053400
    },
    {
      "epoch": 341.4910858995138,
      "grad_norm": 1.1799347400665283,
      "learning_rate": 1.587512163477133e-05,
      "loss": 2.5129,
      "step": 1053500
    },
    {
      "epoch": 341.52350081037275,
      "grad_norm": 1.39472234249115,
      "learning_rate": 1.5871878040869285e-05,
      "loss": 2.5173,
      "step": 1053600
    },
    {
      "epoch": 341.55591572123177,
      "grad_norm": 1.3838685750961304,
      "learning_rate": 1.586863444696724e-05,
      "loss": 2.5143,
      "step": 1053700
    },
    {
      "epoch": 341.5883306320908,
      "grad_norm": 1.453642725944519,
      "learning_rate": 1.5865390853065196e-05,
      "loss": 2.5262,
      "step": 1053800
    },
    {
      "epoch": 341.62074554294975,
      "grad_norm": 1.3851823806762695,
      "learning_rate": 1.5862147259163154e-05,
      "loss": 2.5036,
      "step": 1053900
    },
    {
      "epoch": 341.65316045380877,
      "grad_norm": 1.1857924461364746,
      "learning_rate": 1.585890366526111e-05,
      "loss": 2.5139,
      "step": 1054000
    },
    {
      "epoch": 341.6855753646677,
      "grad_norm": 1.37643563747406,
      "learning_rate": 1.5855660071359065e-05,
      "loss": 2.5474,
      "step": 1054100
    },
    {
      "epoch": 341.71799027552674,
      "grad_norm": 1.1861788034439087,
      "learning_rate": 1.5852416477457024e-05,
      "loss": 2.5465,
      "step": 1054200
    },
    {
      "epoch": 341.75040518638576,
      "grad_norm": 1.4584673643112183,
      "learning_rate": 1.584917288355498e-05,
      "loss": 2.5254,
      "step": 1054300
    },
    {
      "epoch": 341.7828200972447,
      "grad_norm": 1.4237568378448486,
      "learning_rate": 1.5845929289652935e-05,
      "loss": 2.53,
      "step": 1054400
    },
    {
      "epoch": 341.81523500810374,
      "grad_norm": 1.3270822763442993,
      "learning_rate": 1.5842685695750893e-05,
      "loss": 2.5297,
      "step": 1054500
    },
    {
      "epoch": 341.8476499189627,
      "grad_norm": 1.3965222835540771,
      "learning_rate": 1.5839442101848852e-05,
      "loss": 2.5333,
      "step": 1054600
    },
    {
      "epoch": 341.8800648298217,
      "grad_norm": 1.5950992107391357,
      "learning_rate": 1.5836198507946807e-05,
      "loss": 2.5032,
      "step": 1054700
    },
    {
      "epoch": 341.91247974068074,
      "grad_norm": 1.2832154035568237,
      "learning_rate": 1.5832954914044763e-05,
      "loss": 2.5415,
      "step": 1054800
    },
    {
      "epoch": 341.9448946515397,
      "grad_norm": 1.4534298181533813,
      "learning_rate": 1.5829711320142718e-05,
      "loss": 2.5254,
      "step": 1054900
    },
    {
      "epoch": 341.9773095623987,
      "grad_norm": 1.3421868085861206,
      "learning_rate": 1.5826467726240677e-05,
      "loss": 2.5259,
      "step": 1055000
    },
    {
      "epoch": 342.0,
      "eval_bleu": 1.064002283059016,
      "eval_loss": 4.204148769378662,
      "eval_runtime": 4.8203,
      "eval_samples_per_second": 102.069,
      "eval_steps_per_second": 1.66,
      "step": 1055070
    },
    {
      "epoch": 342.0097244732577,
      "grad_norm": 1.356829047203064,
      "learning_rate": 1.5823224132338632e-05,
      "loss": 2.5175,
      "step": 1055100
    },
    {
      "epoch": 342.0421393841167,
      "grad_norm": 1.6453282833099365,
      "learning_rate": 1.5819980538436588e-05,
      "loss": 2.5084,
      "step": 1055200
    },
    {
      "epoch": 342.0745542949757,
      "grad_norm": 1.389143466949463,
      "learning_rate": 1.5816736944534543e-05,
      "loss": 2.5169,
      "step": 1055300
    },
    {
      "epoch": 342.1069692058347,
      "grad_norm": 1.7549160718917847,
      "learning_rate": 1.58134933506325e-05,
      "loss": 2.5262,
      "step": 1055400
    },
    {
      "epoch": 342.1393841166937,
      "grad_norm": 1.1304060220718384,
      "learning_rate": 1.5810249756730457e-05,
      "loss": 2.5279,
      "step": 1055500
    },
    {
      "epoch": 342.17179902755265,
      "grad_norm": 1.2404050827026367,
      "learning_rate": 1.5807006162828412e-05,
      "loss": 2.5276,
      "step": 1055600
    },
    {
      "epoch": 342.20421393841167,
      "grad_norm": 1.3157382011413574,
      "learning_rate": 1.580376256892637e-05,
      "loss": 2.5103,
      "step": 1055700
    },
    {
      "epoch": 342.2366288492707,
      "grad_norm": 1.3454103469848633,
      "learning_rate": 1.580051897502433e-05,
      "loss": 2.5299,
      "step": 1055800
    },
    {
      "epoch": 342.26904376012965,
      "grad_norm": 1.4466094970703125,
      "learning_rate": 1.5797275381122285e-05,
      "loss": 2.546,
      "step": 1055900
    },
    {
      "epoch": 342.30145867098867,
      "grad_norm": 1.718545913696289,
      "learning_rate": 1.579403178722024e-05,
      "loss": 2.502,
      "step": 1056000
    },
    {
      "epoch": 342.3338735818476,
      "grad_norm": 1.3784267902374268,
      "learning_rate": 1.57907881933182e-05,
      "loss": 2.5276,
      "step": 1056100
    },
    {
      "epoch": 342.36628849270664,
      "grad_norm": 1.4132875204086304,
      "learning_rate": 1.5787544599416155e-05,
      "loss": 2.5436,
      "step": 1056200
    },
    {
      "epoch": 342.39870340356566,
      "grad_norm": 1.2476115226745605,
      "learning_rate": 1.578430100551411e-05,
      "loss": 2.523,
      "step": 1056300
    },
    {
      "epoch": 342.4311183144246,
      "grad_norm": 1.333160161972046,
      "learning_rate": 1.5781057411612065e-05,
      "loss": 2.5185,
      "step": 1056400
    },
    {
      "epoch": 342.46353322528364,
      "grad_norm": 1.5927058458328247,
      "learning_rate": 1.5777813817710024e-05,
      "loss": 2.4994,
      "step": 1056500
    },
    {
      "epoch": 342.4959481361426,
      "grad_norm": 1.3628672361373901,
      "learning_rate": 1.577457022380798e-05,
      "loss": 2.5132,
      "step": 1056600
    },
    {
      "epoch": 342.5283630470016,
      "grad_norm": 1.4263250827789307,
      "learning_rate": 1.5771326629905935e-05,
      "loss": 2.5106,
      "step": 1056700
    },
    {
      "epoch": 342.56077795786064,
      "grad_norm": 1.3787227869033813,
      "learning_rate": 1.576808303600389e-05,
      "loss": 2.5255,
      "step": 1056800
    },
    {
      "epoch": 342.5931928687196,
      "grad_norm": 1.3167376518249512,
      "learning_rate": 1.576487187804087e-05,
      "loss": 2.5097,
      "step": 1056900
    },
    {
      "epoch": 342.6256077795786,
      "grad_norm": 1.4643548727035522,
      "learning_rate": 1.5761628284138827e-05,
      "loss": 2.5294,
      "step": 1057000
    },
    {
      "epoch": 342.6580226904376,
      "grad_norm": 1.1708085536956787,
      "learning_rate": 1.5758384690236782e-05,
      "loss": 2.5543,
      "step": 1057100
    },
    {
      "epoch": 342.6904376012966,
      "grad_norm": 1.6034380197525024,
      "learning_rate": 1.5755141096334737e-05,
      "loss": 2.5314,
      "step": 1057200
    },
    {
      "epoch": 342.7228525121556,
      "grad_norm": 1.5930300951004028,
      "learning_rate": 1.5751897502432696e-05,
      "loss": 2.5026,
      "step": 1057300
    },
    {
      "epoch": 342.7552674230146,
      "grad_norm": 1.2959673404693604,
      "learning_rate": 1.574865390853065e-05,
      "loss": 2.5071,
      "step": 1057400
    },
    {
      "epoch": 342.7876823338736,
      "grad_norm": 1.3833097219467163,
      "learning_rate": 1.574541031462861e-05,
      "loss": 2.5213,
      "step": 1057500
    },
    {
      "epoch": 342.82009724473255,
      "grad_norm": 1.469764232635498,
      "learning_rate": 1.5742166720726566e-05,
      "loss": 2.5348,
      "step": 1057600
    },
    {
      "epoch": 342.85251215559157,
      "grad_norm": 1.2454800605773926,
      "learning_rate": 1.5738923126824524e-05,
      "loss": 2.5172,
      "step": 1057700
    },
    {
      "epoch": 342.8849270664506,
      "grad_norm": 1.4543033838272095,
      "learning_rate": 1.573567953292248e-05,
      "loss": 2.5289,
      "step": 1057800
    },
    {
      "epoch": 342.91734197730955,
      "grad_norm": 1.3757784366607666,
      "learning_rate": 1.5732435939020435e-05,
      "loss": 2.4833,
      "step": 1057900
    },
    {
      "epoch": 342.94975688816857,
      "grad_norm": 1.4200867414474487,
      "learning_rate": 1.5729192345118394e-05,
      "loss": 2.5255,
      "step": 1058000
    },
    {
      "epoch": 342.9821717990275,
      "grad_norm": 1.4595178365707397,
      "learning_rate": 1.572594875121635e-05,
      "loss": 2.5321,
      "step": 1058100
    },
    {
      "epoch": 343.0,
      "eval_bleu": 1.1392634583683623,
      "eval_loss": 4.205386161804199,
      "eval_runtime": 4.635,
      "eval_samples_per_second": 106.149,
      "eval_steps_per_second": 1.726,
      "step": 1058155
    },
    {
      "epoch": 343.01458670988654,
      "grad_norm": 1.488417625427246,
      "learning_rate": 1.5722705157314305e-05,
      "loss": 2.5081,
      "step": 1058200
    },
    {
      "epoch": 343.04700162074556,
      "grad_norm": 1.2462024688720703,
      "learning_rate": 1.5719493999351282e-05,
      "loss": 2.4784,
      "step": 1058300
    },
    {
      "epoch": 343.0794165316045,
      "grad_norm": 1.6968889236450195,
      "learning_rate": 1.571625040544924e-05,
      "loss": 2.5254,
      "step": 1058400
    },
    {
      "epoch": 343.11183144246354,
      "grad_norm": 1.3609802722930908,
      "learning_rate": 1.5713006811547196e-05,
      "loss": 2.5371,
      "step": 1058500
    },
    {
      "epoch": 343.1442463533225,
      "grad_norm": 1.3339927196502686,
      "learning_rate": 1.5709763217645152e-05,
      "loss": 2.5205,
      "step": 1058600
    },
    {
      "epoch": 343.1766612641815,
      "grad_norm": 1.365809440612793,
      "learning_rate": 1.5706519623743107e-05,
      "loss": 2.5255,
      "step": 1058700
    },
    {
      "epoch": 343.20907617504054,
      "grad_norm": 1.7962465286254883,
      "learning_rate": 1.5703276029841066e-05,
      "loss": 2.5204,
      "step": 1058800
    },
    {
      "epoch": 343.2414910858995,
      "grad_norm": 1.4867565631866455,
      "learning_rate": 1.570003243593902e-05,
      "loss": 2.5293,
      "step": 1058900
    },
    {
      "epoch": 343.2739059967585,
      "grad_norm": 1.706265926361084,
      "learning_rate": 1.5696788842036977e-05,
      "loss": 2.5131,
      "step": 1059000
    },
    {
      "epoch": 343.3063209076175,
      "grad_norm": 1.458206295967102,
      "learning_rate": 1.5693545248134932e-05,
      "loss": 2.5239,
      "step": 1059100
    },
    {
      "epoch": 343.3387358184765,
      "grad_norm": 1.596213698387146,
      "learning_rate": 1.569030165423289e-05,
      "loss": 2.5054,
      "step": 1059200
    },
    {
      "epoch": 343.3711507293355,
      "grad_norm": 1.7381207942962646,
      "learning_rate": 1.5687058060330846e-05,
      "loss": 2.5184,
      "step": 1059300
    },
    {
      "epoch": 343.4035656401945,
      "grad_norm": 1.3411463499069214,
      "learning_rate": 1.5683814466428805e-05,
      "loss": 2.52,
      "step": 1059400
    },
    {
      "epoch": 343.4359805510535,
      "grad_norm": 1.3968205451965332,
      "learning_rate": 1.568057087252676e-05,
      "loss": 2.4975,
      "step": 1059500
    },
    {
      "epoch": 343.4683954619125,
      "grad_norm": 1.4241734743118286,
      "learning_rate": 1.567732727862472e-05,
      "loss": 2.5238,
      "step": 1059600
    },
    {
      "epoch": 343.50081037277147,
      "grad_norm": 1.4409626722335815,
      "learning_rate": 1.5674083684722674e-05,
      "loss": 2.5337,
      "step": 1059700
    },
    {
      "epoch": 343.5332252836305,
      "grad_norm": 1.6239757537841797,
      "learning_rate": 1.567084009082063e-05,
      "loss": 2.5207,
      "step": 1059800
    },
    {
      "epoch": 343.56564019448945,
      "grad_norm": 1.5024033784866333,
      "learning_rate": 1.5667596496918588e-05,
      "loss": 2.5175,
      "step": 1059900
    },
    {
      "epoch": 343.59805510534846,
      "grad_norm": 1.386299967765808,
      "learning_rate": 1.5664352903016544e-05,
      "loss": 2.5137,
      "step": 1060000
    },
    {
      "epoch": 343.6304700162075,
      "grad_norm": 1.268959879875183,
      "learning_rate": 1.56611093091145e-05,
      "loss": 2.5298,
      "step": 1060100
    },
    {
      "epoch": 343.66288492706644,
      "grad_norm": 1.5176056623458862,
      "learning_rate": 1.5657865715212454e-05,
      "loss": 2.5072,
      "step": 1060200
    },
    {
      "epoch": 343.69529983792546,
      "grad_norm": 1.3004831075668335,
      "learning_rate": 1.5654622121310413e-05,
      "loss": 2.5239,
      "step": 1060300
    },
    {
      "epoch": 343.7277147487844,
      "grad_norm": 1.2064651250839233,
      "learning_rate": 1.565137852740837e-05,
      "loss": 2.5269,
      "step": 1060400
    },
    {
      "epoch": 343.76012965964344,
      "grad_norm": 1.5245072841644287,
      "learning_rate": 1.5648134933506324e-05,
      "loss": 2.5133,
      "step": 1060500
    },
    {
      "epoch": 343.79254457050246,
      "grad_norm": 1.2386746406555176,
      "learning_rate": 1.56449237755433e-05,
      "loss": 2.5284,
      "step": 1060600
    },
    {
      "epoch": 343.8249594813614,
      "grad_norm": 1.3105515241622925,
      "learning_rate": 1.564168018164126e-05,
      "loss": 2.5396,
      "step": 1060700
    },
    {
      "epoch": 343.85737439222044,
      "grad_norm": 1.4665559530258179,
      "learning_rate": 1.5638436587739216e-05,
      "loss": 2.516,
      "step": 1060800
    },
    {
      "epoch": 343.8897893030794,
      "grad_norm": 1.326740026473999,
      "learning_rate": 1.563519299383717e-05,
      "loss": 2.5156,
      "step": 1060900
    },
    {
      "epoch": 343.9222042139384,
      "grad_norm": 1.3927046060562134,
      "learning_rate": 1.5631949399935126e-05,
      "loss": 2.5457,
      "step": 1061000
    },
    {
      "epoch": 343.95461912479743,
      "grad_norm": 1.4765466451644897,
      "learning_rate": 1.5628705806033085e-05,
      "loss": 2.5125,
      "step": 1061100
    },
    {
      "epoch": 343.9870340356564,
      "grad_norm": 1.314443588256836,
      "learning_rate": 1.5625462212131044e-05,
      "loss": 2.5166,
      "step": 1061200
    },
    {
      "epoch": 344.0,
      "eval_bleu": 1.0033639110840868,
      "eval_loss": 4.207404136657715,
      "eval_runtime": 4.4823,
      "eval_samples_per_second": 109.766,
      "eval_steps_per_second": 1.785,
      "step": 1061240
    },
    {
      "epoch": 344.0194489465154,
      "grad_norm": 1.363943338394165,
      "learning_rate": 1.5622218618229e-05,
      "loss": 2.5246,
      "step": 1061300
    },
    {
      "epoch": 344.05186385737437,
      "grad_norm": 1.4076076745986938,
      "learning_rate": 1.5618975024326955e-05,
      "loss": 2.5369,
      "step": 1061400
    },
    {
      "epoch": 344.0842787682334,
      "grad_norm": 1.4286803007125854,
      "learning_rate": 1.5615731430424913e-05,
      "loss": 2.5262,
      "step": 1061500
    },
    {
      "epoch": 344.1166936790924,
      "grad_norm": 1.6873692274093628,
      "learning_rate": 1.561248783652287e-05,
      "loss": 2.5227,
      "step": 1061600
    },
    {
      "epoch": 344.14910858995137,
      "grad_norm": 1.3699839115142822,
      "learning_rate": 1.5609244242620824e-05,
      "loss": 2.4955,
      "step": 1061700
    },
    {
      "epoch": 344.1815235008104,
      "grad_norm": 1.3738747835159302,
      "learning_rate": 1.560600064871878e-05,
      "loss": 2.5225,
      "step": 1061800
    },
    {
      "epoch": 344.21393841166935,
      "grad_norm": 1.3803844451904297,
      "learning_rate": 1.5602757054816738e-05,
      "loss": 2.5063,
      "step": 1061900
    },
    {
      "epoch": 344.24635332252836,
      "grad_norm": 1.3532190322875977,
      "learning_rate": 1.5599513460914694e-05,
      "loss": 2.5249,
      "step": 1062000
    },
    {
      "epoch": 344.2787682333874,
      "grad_norm": 1.366098403930664,
      "learning_rate": 1.559626986701265e-05,
      "loss": 2.5357,
      "step": 1062100
    },
    {
      "epoch": 344.31118314424634,
      "grad_norm": 1.3453330993652344,
      "learning_rate": 1.5593026273110608e-05,
      "loss": 2.5019,
      "step": 1062200
    },
    {
      "epoch": 344.34359805510536,
      "grad_norm": 1.384610652923584,
      "learning_rate": 1.5589782679208563e-05,
      "loss": 2.527,
      "step": 1062300
    },
    {
      "epoch": 344.3760129659643,
      "grad_norm": 1.4477145671844482,
      "learning_rate": 1.5586539085306522e-05,
      "loss": 2.497,
      "step": 1062400
    },
    {
      "epoch": 344.40842787682334,
      "grad_norm": 1.3230513334274292,
      "learning_rate": 1.5583295491404477e-05,
      "loss": 2.5134,
      "step": 1062500
    },
    {
      "epoch": 344.44084278768236,
      "grad_norm": 1.5396145582199097,
      "learning_rate": 1.5580051897502436e-05,
      "loss": 2.5213,
      "step": 1062600
    },
    {
      "epoch": 344.4732576985413,
      "grad_norm": 1.4414067268371582,
      "learning_rate": 1.557684073953941e-05,
      "loss": 2.5175,
      "step": 1062700
    },
    {
      "epoch": 344.50567260940034,
      "grad_norm": 1.2944167852401733,
      "learning_rate": 1.5573597145637366e-05,
      "loss": 2.5138,
      "step": 1062800
    },
    {
      "epoch": 344.5380875202593,
      "grad_norm": 1.4563570022583008,
      "learning_rate": 1.5570385987674343e-05,
      "loss": 2.5222,
      "step": 1062900
    },
    {
      "epoch": 344.5705024311183,
      "grad_norm": 1.2864327430725098,
      "learning_rate": 1.55671423937723e-05,
      "loss": 2.5064,
      "step": 1063000
    },
    {
      "epoch": 344.60291734197733,
      "grad_norm": 1.396948218345642,
      "learning_rate": 1.5563898799870257e-05,
      "loss": 2.5157,
      "step": 1063100
    },
    {
      "epoch": 344.6353322528363,
      "grad_norm": 1.4174445867538452,
      "learning_rate": 1.5560655205968213e-05,
      "loss": 2.5095,
      "step": 1063200
    },
    {
      "epoch": 344.6677471636953,
      "grad_norm": 1.3362778425216675,
      "learning_rate": 1.5557411612066168e-05,
      "loss": 2.5222,
      "step": 1063300
    },
    {
      "epoch": 344.70016207455427,
      "grad_norm": 1.8707435131072998,
      "learning_rate": 1.5554168018164127e-05,
      "loss": 2.5283,
      "step": 1063400
    },
    {
      "epoch": 344.7325769854133,
      "grad_norm": 1.3452541828155518,
      "learning_rate": 1.5550924424262082e-05,
      "loss": 2.5183,
      "step": 1063500
    },
    {
      "epoch": 344.7649918962723,
      "grad_norm": 1.337507963180542,
      "learning_rate": 1.554768083036004e-05,
      "loss": 2.5195,
      "step": 1063600
    },
    {
      "epoch": 344.79740680713127,
      "grad_norm": 1.4857110977172852,
      "learning_rate": 1.5544437236457996e-05,
      "loss": 2.5222,
      "step": 1063700
    },
    {
      "epoch": 344.8298217179903,
      "grad_norm": 1.3319201469421387,
      "learning_rate": 1.5541193642555955e-05,
      "loss": 2.5327,
      "step": 1063800
    },
    {
      "epoch": 344.86223662884925,
      "grad_norm": 1.369810938835144,
      "learning_rate": 1.553795004865391e-05,
      "loss": 2.5178,
      "step": 1063900
    },
    {
      "epoch": 344.89465153970826,
      "grad_norm": 1.3344035148620605,
      "learning_rate": 1.5534706454751866e-05,
      "loss": 2.5284,
      "step": 1064000
    },
    {
      "epoch": 344.9270664505673,
      "grad_norm": 1.3624294996261597,
      "learning_rate": 1.553146286084982e-05,
      "loss": 2.5313,
      "step": 1064100
    },
    {
      "epoch": 344.95948136142624,
      "grad_norm": 1.2316409349441528,
      "learning_rate": 1.552821926694778e-05,
      "loss": 2.5301,
      "step": 1064200
    },
    {
      "epoch": 344.99189627228526,
      "grad_norm": 1.4935733079910278,
      "learning_rate": 1.5524975673045735e-05,
      "loss": 2.5263,
      "step": 1064300
    },
    {
      "epoch": 345.0,
      "eval_bleu": 0.8824503889993633,
      "eval_loss": 4.207573413848877,
      "eval_runtime": 4.7488,
      "eval_samples_per_second": 103.604,
      "eval_steps_per_second": 1.685,
      "step": 1064325
    },
    {
      "epoch": 345.0243111831442,
      "grad_norm": 1.1800459623336792,
      "learning_rate": 1.552173207914369e-05,
      "loss": 2.5231,
      "step": 1064400
    },
    {
      "epoch": 345.05672609400324,
      "grad_norm": 1.3962465524673462,
      "learning_rate": 1.5518488485241646e-05,
      "loss": 2.5175,
      "step": 1064500
    },
    {
      "epoch": 345.08914100486226,
      "grad_norm": 1.5513521432876587,
      "learning_rate": 1.5515244891339605e-05,
      "loss": 2.5161,
      "step": 1064600
    },
    {
      "epoch": 345.1215559157212,
      "grad_norm": 1.3687855005264282,
      "learning_rate": 1.551200129743756e-05,
      "loss": 2.5106,
      "step": 1064700
    },
    {
      "epoch": 345.15397082658023,
      "grad_norm": 1.443638801574707,
      "learning_rate": 1.550875770353552e-05,
      "loss": 2.4988,
      "step": 1064800
    },
    {
      "epoch": 345.1863857374392,
      "grad_norm": 1.4761381149291992,
      "learning_rate": 1.5505514109633474e-05,
      "loss": 2.5136,
      "step": 1064900
    },
    {
      "epoch": 345.2188006482982,
      "grad_norm": 1.3179126977920532,
      "learning_rate": 1.5502270515731433e-05,
      "loss": 2.5325,
      "step": 1065000
    },
    {
      "epoch": 345.25121555915723,
      "grad_norm": 1.435560941696167,
      "learning_rate": 1.5499026921829388e-05,
      "loss": 2.516,
      "step": 1065100
    },
    {
      "epoch": 345.2836304700162,
      "grad_norm": 1.4353340864181519,
      "learning_rate": 1.5495783327927344e-05,
      "loss": 2.5106,
      "step": 1065200
    },
    {
      "epoch": 345.3160453808752,
      "grad_norm": 1.4827961921691895,
      "learning_rate": 1.5492539734025302e-05,
      "loss": 2.5159,
      "step": 1065300
    },
    {
      "epoch": 345.34846029173417,
      "grad_norm": 1.20797598361969,
      "learning_rate": 1.5489296140123258e-05,
      "loss": 2.5008,
      "step": 1065400
    },
    {
      "epoch": 345.3808752025932,
      "grad_norm": 1.3532423973083496,
      "learning_rate": 1.5486052546221213e-05,
      "loss": 2.5309,
      "step": 1065500
    },
    {
      "epoch": 345.4132901134522,
      "grad_norm": 1.450919270515442,
      "learning_rate": 1.548280895231917e-05,
      "loss": 2.5337,
      "step": 1065600
    },
    {
      "epoch": 345.44570502431117,
      "grad_norm": 1.3471183776855469,
      "learning_rate": 1.5479565358417127e-05,
      "loss": 2.5324,
      "step": 1065700
    },
    {
      "epoch": 345.4781199351702,
      "grad_norm": 1.3677752017974854,
      "learning_rate": 1.5476321764515083e-05,
      "loss": 2.5163,
      "step": 1065800
    },
    {
      "epoch": 345.51053484602915,
      "grad_norm": 1.305823802947998,
      "learning_rate": 1.5473078170613038e-05,
      "loss": 2.5246,
      "step": 1065900
    },
    {
      "epoch": 345.54294975688816,
      "grad_norm": 1.3900229930877686,
      "learning_rate": 1.5469834576710997e-05,
      "loss": 2.5443,
      "step": 1066000
    },
    {
      "epoch": 345.5753646677472,
      "grad_norm": 1.2777867317199707,
      "learning_rate": 1.5466590982808955e-05,
      "loss": 2.505,
      "step": 1066100
    },
    {
      "epoch": 345.60777957860614,
      "grad_norm": 1.5158559083938599,
      "learning_rate": 1.546334738890691e-05,
      "loss": 2.5288,
      "step": 1066200
    },
    {
      "epoch": 345.64019448946516,
      "grad_norm": 1.8502833843231201,
      "learning_rate": 1.5460103795004866e-05,
      "loss": 2.5042,
      "step": 1066300
    },
    {
      "epoch": 345.6726094003242,
      "grad_norm": 1.3673244714736938,
      "learning_rate": 1.545686020110282e-05,
      "loss": 2.5194,
      "step": 1066400
    },
    {
      "epoch": 345.70502431118314,
      "grad_norm": 1.2946630716323853,
      "learning_rate": 1.545361660720078e-05,
      "loss": 2.5276,
      "step": 1066500
    },
    {
      "epoch": 345.73743922204216,
      "grad_norm": 1.4051419496536255,
      "learning_rate": 1.5450373013298736e-05,
      "loss": 2.5114,
      "step": 1066600
    },
    {
      "epoch": 345.7698541329011,
      "grad_norm": 1.52126944065094,
      "learning_rate": 1.544712941939669e-05,
      "loss": 2.5004,
      "step": 1066700
    },
    {
      "epoch": 345.80226904376013,
      "grad_norm": 1.5700998306274414,
      "learning_rate": 1.544388582549465e-05,
      "loss": 2.5256,
      "step": 1066800
    },
    {
      "epoch": 345.83468395461915,
      "grad_norm": 1.3744258880615234,
      "learning_rate": 1.5440642231592605e-05,
      "loss": 2.5243,
      "step": 1066900
    },
    {
      "epoch": 345.8670988654781,
      "grad_norm": 1.6898934841156006,
      "learning_rate": 1.543739863769056e-05,
      "loss": 2.5092,
      "step": 1067000
    },
    {
      "epoch": 345.89951377633713,
      "grad_norm": 1.3992358446121216,
      "learning_rate": 1.5434155043788516e-05,
      "loss": 2.5057,
      "step": 1067100
    },
    {
      "epoch": 345.9319286871961,
      "grad_norm": 1.280826210975647,
      "learning_rate": 1.5430911449886474e-05,
      "loss": 2.519,
      "step": 1067200
    },
    {
      "epoch": 345.9643435980551,
      "grad_norm": 1.669344186782837,
      "learning_rate": 1.5427667855984433e-05,
      "loss": 2.5305,
      "step": 1067300
    },
    {
      "epoch": 345.9967585089141,
      "grad_norm": 1.5550740957260132,
      "learning_rate": 1.542442426208239e-05,
      "loss": 2.5079,
      "step": 1067400
    },
    {
      "epoch": 346.0,
      "eval_bleu": 0.9191375235135387,
      "eval_loss": 4.211782932281494,
      "eval_runtime": 4.612,
      "eval_samples_per_second": 106.677,
      "eval_steps_per_second": 1.735,
      "step": 1067410
    },
    {
      "epoch": 346.0291734197731,
      "grad_norm": 1.423208475112915,
      "learning_rate": 1.5421180668180344e-05,
      "loss": 2.5246,
      "step": 1067500
    },
    {
      "epoch": 346.0615883306321,
      "grad_norm": 1.5214389562606812,
      "learning_rate": 1.5417937074278303e-05,
      "loss": 2.4933,
      "step": 1067600
    },
    {
      "epoch": 346.09400324149107,
      "grad_norm": 1.3449287414550781,
      "learning_rate": 1.5414693480376258e-05,
      "loss": 2.5084,
      "step": 1067700
    },
    {
      "epoch": 346.1264181523501,
      "grad_norm": 1.4100667238235474,
      "learning_rate": 1.5411482322413236e-05,
      "loss": 2.5199,
      "step": 1067800
    },
    {
      "epoch": 346.1588330632091,
      "grad_norm": 1.2865381240844727,
      "learning_rate": 1.540823872851119e-05,
      "loss": 2.5093,
      "step": 1067900
    },
    {
      "epoch": 346.19124797406806,
      "grad_norm": 1.6556297540664673,
      "learning_rate": 1.540499513460915e-05,
      "loss": 2.5134,
      "step": 1068000
    },
    {
      "epoch": 346.2236628849271,
      "grad_norm": 1.3354724645614624,
      "learning_rate": 1.5401751540707105e-05,
      "loss": 2.5104,
      "step": 1068100
    },
    {
      "epoch": 346.25607779578604,
      "grad_norm": 1.6713435649871826,
      "learning_rate": 1.539850794680506e-05,
      "loss": 2.5107,
      "step": 1068200
    },
    {
      "epoch": 346.28849270664506,
      "grad_norm": 1.461735486984253,
      "learning_rate": 1.5395264352903016e-05,
      "loss": 2.5158,
      "step": 1068300
    },
    {
      "epoch": 346.3209076175041,
      "grad_norm": 1.4588247537612915,
      "learning_rate": 1.5392020759000975e-05,
      "loss": 2.4982,
      "step": 1068400
    },
    {
      "epoch": 346.35332252836304,
      "grad_norm": 1.3817651271820068,
      "learning_rate": 1.538877716509893e-05,
      "loss": 2.5056,
      "step": 1068500
    },
    {
      "epoch": 346.38573743922205,
      "grad_norm": 1.2060959339141846,
      "learning_rate": 1.5385533571196885e-05,
      "loss": 2.5152,
      "step": 1068600
    },
    {
      "epoch": 346.418152350081,
      "grad_norm": 1.3358962535858154,
      "learning_rate": 1.5382289977294844e-05,
      "loss": 2.5202,
      "step": 1068700
    },
    {
      "epoch": 346.45056726094003,
      "grad_norm": 1.3264082670211792,
      "learning_rate": 1.53790463833928e-05,
      "loss": 2.519,
      "step": 1068800
    },
    {
      "epoch": 346.48298217179905,
      "grad_norm": 1.3641589879989624,
      "learning_rate": 1.5375802789490755e-05,
      "loss": 2.5122,
      "step": 1068900
    },
    {
      "epoch": 346.515397082658,
      "grad_norm": 1.376989483833313,
      "learning_rate": 1.5372559195588714e-05,
      "loss": 2.5286,
      "step": 1069000
    },
    {
      "epoch": 346.54781199351703,
      "grad_norm": 1.4438310861587524,
      "learning_rate": 1.5369315601686672e-05,
      "loss": 2.5144,
      "step": 1069100
    },
    {
      "epoch": 346.580226904376,
      "grad_norm": 1.2713851928710938,
      "learning_rate": 1.5366072007784628e-05,
      "loss": 2.5233,
      "step": 1069200
    },
    {
      "epoch": 346.612641815235,
      "grad_norm": 1.3395435810089111,
      "learning_rate": 1.5362828413882583e-05,
      "loss": 2.5242,
      "step": 1069300
    },
    {
      "epoch": 346.645056726094,
      "grad_norm": 1.4123945236206055,
      "learning_rate": 1.535958481998054e-05,
      "loss": 2.5269,
      "step": 1069400
    },
    {
      "epoch": 346.677471636953,
      "grad_norm": 1.4763025045394897,
      "learning_rate": 1.5356341226078497e-05,
      "loss": 2.5342,
      "step": 1069500
    },
    {
      "epoch": 346.709886547812,
      "grad_norm": 1.5198161602020264,
      "learning_rate": 1.5353097632176453e-05,
      "loss": 2.5188,
      "step": 1069600
    },
    {
      "epoch": 346.74230145867097,
      "grad_norm": 1.507359266281128,
      "learning_rate": 1.534988647421343e-05,
      "loss": 2.5032,
      "step": 1069700
    },
    {
      "epoch": 346.77471636953,
      "grad_norm": 1.489227056503296,
      "learning_rate": 1.5346642880311386e-05,
      "loss": 2.5353,
      "step": 1069800
    },
    {
      "epoch": 346.807131280389,
      "grad_norm": 1.6352521181106567,
      "learning_rate": 1.5343399286409344e-05,
      "loss": 2.5068,
      "step": 1069900
    },
    {
      "epoch": 346.83954619124796,
      "grad_norm": 1.3939203023910522,
      "learning_rate": 1.53401556925073e-05,
      "loss": 2.5392,
      "step": 1070000
    },
    {
      "epoch": 346.871961102107,
      "grad_norm": 1.3600049018859863,
      "learning_rate": 1.5336912098605255e-05,
      "loss": 2.5177,
      "step": 1070100
    },
    {
      "epoch": 346.90437601296594,
      "grad_norm": 1.351790428161621,
      "learning_rate": 1.533366850470321e-05,
      "loss": 2.5449,
      "step": 1070200
    },
    {
      "epoch": 346.93679092382496,
      "grad_norm": 1.328220248222351,
      "learning_rate": 1.533042491080117e-05,
      "loss": 2.5347,
      "step": 1070300
    },
    {
      "epoch": 346.969205834684,
      "grad_norm": 1.4863231182098389,
      "learning_rate": 1.5327181316899125e-05,
      "loss": 2.5085,
      "step": 1070400
    },
    {
      "epoch": 347.0,
      "eval_bleu": 1.0613921785434004,
      "eval_loss": 4.211264610290527,
      "eval_runtime": 4.3297,
      "eval_samples_per_second": 113.635,
      "eval_steps_per_second": 1.848,
      "step": 1070495
    },
    {
      "epoch": 347.00162074554294,
      "grad_norm": 1.3518264293670654,
      "learning_rate": 1.532393772299708e-05,
      "loss": 2.5173,
      "step": 1070500
    },
    {
      "epoch": 347.03403565640195,
      "grad_norm": 1.5079503059387207,
      "learning_rate": 1.5320694129095035e-05,
      "loss": 2.5243,
      "step": 1070600
    },
    {
      "epoch": 347.0664505672609,
      "grad_norm": 1.4707670211791992,
      "learning_rate": 1.5317450535192994e-05,
      "loss": 2.5146,
      "step": 1070700
    },
    {
      "epoch": 347.09886547811993,
      "grad_norm": 1.1919121742248535,
      "learning_rate": 1.5314206941290953e-05,
      "loss": 2.5305,
      "step": 1070800
    },
    {
      "epoch": 347.13128038897895,
      "grad_norm": 1.2930395603179932,
      "learning_rate": 1.5310963347388908e-05,
      "loss": 2.5053,
      "step": 1070900
    },
    {
      "epoch": 347.1636952998379,
      "grad_norm": 1.276113748550415,
      "learning_rate": 1.5307719753486867e-05,
      "loss": 2.5126,
      "step": 1071000
    },
    {
      "epoch": 347.19611021069693,
      "grad_norm": 1.4225190877914429,
      "learning_rate": 1.5304476159584822e-05,
      "loss": 2.524,
      "step": 1071100
    },
    {
      "epoch": 347.2285251215559,
      "grad_norm": 1.2701455354690552,
      "learning_rate": 1.5301232565682778e-05,
      "loss": 2.5003,
      "step": 1071200
    },
    {
      "epoch": 347.2609400324149,
      "grad_norm": 1.586905837059021,
      "learning_rate": 1.5297988971780733e-05,
      "loss": 2.5096,
      "step": 1071300
    },
    {
      "epoch": 347.2933549432739,
      "grad_norm": 1.4570050239562988,
      "learning_rate": 1.529474537787869e-05,
      "loss": 2.5127,
      "step": 1071400
    },
    {
      "epoch": 347.3257698541329,
      "grad_norm": 1.5700174570083618,
      "learning_rate": 1.5291501783976647e-05,
      "loss": 2.5041,
      "step": 1071500
    },
    {
      "epoch": 347.3581847649919,
      "grad_norm": 1.3002523183822632,
      "learning_rate": 1.5288258190074602e-05,
      "loss": 2.524,
      "step": 1071600
    },
    {
      "epoch": 347.39059967585086,
      "grad_norm": 1.3464601039886475,
      "learning_rate": 1.5285014596172558e-05,
      "loss": 2.513,
      "step": 1071700
    },
    {
      "epoch": 347.4230145867099,
      "grad_norm": 1.2838001251220703,
      "learning_rate": 1.5281771002270516e-05,
      "loss": 2.4986,
      "step": 1071800
    },
    {
      "epoch": 347.4554294975689,
      "grad_norm": 1.3271435499191284,
      "learning_rate": 1.5278527408368472e-05,
      "loss": 2.5154,
      "step": 1071900
    },
    {
      "epoch": 347.48784440842786,
      "grad_norm": 1.2755308151245117,
      "learning_rate": 1.527528381446643e-05,
      "loss": 2.5253,
      "step": 1072000
    },
    {
      "epoch": 347.5202593192869,
      "grad_norm": 1.3577150106430054,
      "learning_rate": 1.5272040220564386e-05,
      "loss": 2.4836,
      "step": 1072100
    },
    {
      "epoch": 347.55267423014584,
      "grad_norm": 1.3067516088485718,
      "learning_rate": 1.5268796626662345e-05,
      "loss": 2.5206,
      "step": 1072200
    },
    {
      "epoch": 347.58508914100486,
      "grad_norm": 1.383567452430725,
      "learning_rate": 1.52655530327603e-05,
      "loss": 2.5082,
      "step": 1072300
    },
    {
      "epoch": 347.6175040518639,
      "grad_norm": 1.2419801950454712,
      "learning_rate": 1.5262309438858255e-05,
      "loss": 2.5177,
      "step": 1072400
    },
    {
      "epoch": 347.64991896272284,
      "grad_norm": 1.620937705039978,
      "learning_rate": 1.5259065844956214e-05,
      "loss": 2.5104,
      "step": 1072500
    },
    {
      "epoch": 347.68233387358185,
      "grad_norm": 1.4180048704147339,
      "learning_rate": 1.525582225105417e-05,
      "loss": 2.52,
      "step": 1072600
    },
    {
      "epoch": 347.7147487844408,
      "grad_norm": 1.4074058532714844,
      "learning_rate": 1.5252578657152125e-05,
      "loss": 2.5044,
      "step": 1072700
    },
    {
      "epoch": 347.74716369529983,
      "grad_norm": 1.4223384857177734,
      "learning_rate": 1.524933506325008e-05,
      "loss": 2.5305,
      "step": 1072800
    },
    {
      "epoch": 347.77957860615885,
      "grad_norm": 1.3499608039855957,
      "learning_rate": 1.5246091469348039e-05,
      "loss": 2.5358,
      "step": 1072900
    },
    {
      "epoch": 347.8119935170178,
      "grad_norm": 1.3945982456207275,
      "learning_rate": 1.5242847875445996e-05,
      "loss": 2.54,
      "step": 1073000
    },
    {
      "epoch": 347.84440842787683,
      "grad_norm": 1.5030245780944824,
      "learning_rate": 1.5239604281543951e-05,
      "loss": 2.515,
      "step": 1073100
    },
    {
      "epoch": 347.87682333873585,
      "grad_norm": 1.3939825296401978,
      "learning_rate": 1.5236360687641907e-05,
      "loss": 2.5339,
      "step": 1073200
    },
    {
      "epoch": 347.9092382495948,
      "grad_norm": 1.4025119543075562,
      "learning_rate": 1.5233117093739865e-05,
      "loss": 2.5168,
      "step": 1073300
    },
    {
      "epoch": 347.9416531604538,
      "grad_norm": 1.2573249340057373,
      "learning_rate": 1.522987349983782e-05,
      "loss": 2.5193,
      "step": 1073400
    },
    {
      "epoch": 347.9740680713128,
      "grad_norm": 1.3012139797210693,
      "learning_rate": 1.5226629905935776e-05,
      "loss": 2.5296,
      "step": 1073500
    },
    {
      "epoch": 348.0,
      "eval_bleu": 1.1814877410094013,
      "eval_loss": 4.209726810455322,
      "eval_runtime": 4.7311,
      "eval_samples_per_second": 103.992,
      "eval_steps_per_second": 1.691,
      "step": 1073580
    },
    {
      "epoch": 348.0064829821718,
      "grad_norm": 1.2854992151260376,
      "learning_rate": 1.5223386312033733e-05,
      "loss": 2.5112,
      "step": 1073600
    },
    {
      "epoch": 348.0388978930308,
      "grad_norm": 1.151088833808899,
      "learning_rate": 1.5220142718131692e-05,
      "loss": 2.5184,
      "step": 1073700
    },
    {
      "epoch": 348.0713128038898,
      "grad_norm": 1.2584151029586792,
      "learning_rate": 1.5216899124229647e-05,
      "loss": 2.5042,
      "step": 1073800
    },
    {
      "epoch": 348.1037277147488,
      "grad_norm": 1.5672770738601685,
      "learning_rate": 1.5213655530327603e-05,
      "loss": 2.5085,
      "step": 1073900
    },
    {
      "epoch": 348.13614262560776,
      "grad_norm": 1.2836660146713257,
      "learning_rate": 1.5210411936425561e-05,
      "loss": 2.5132,
      "step": 1074000
    },
    {
      "epoch": 348.1685575364668,
      "grad_norm": 1.5064162015914917,
      "learning_rate": 1.5207168342523517e-05,
      "loss": 2.5291,
      "step": 1074100
    },
    {
      "epoch": 348.2009724473258,
      "grad_norm": 1.4117902517318726,
      "learning_rate": 1.5203924748621474e-05,
      "loss": 2.5104,
      "step": 1074200
    },
    {
      "epoch": 348.23338735818476,
      "grad_norm": 1.29849374294281,
      "learning_rate": 1.520068115471943e-05,
      "loss": 2.5144,
      "step": 1074300
    },
    {
      "epoch": 348.2658022690438,
      "grad_norm": 1.594849705696106,
      "learning_rate": 1.5197437560817388e-05,
      "loss": 2.4987,
      "step": 1074400
    },
    {
      "epoch": 348.29821717990274,
      "grad_norm": 1.2656819820404053,
      "learning_rate": 1.5194193966915343e-05,
      "loss": 2.5174,
      "step": 1074500
    },
    {
      "epoch": 348.33063209076175,
      "grad_norm": 1.4136735200881958,
      "learning_rate": 1.5190950373013299e-05,
      "loss": 2.5196,
      "step": 1074600
    },
    {
      "epoch": 348.36304700162077,
      "grad_norm": 1.3588486909866333,
      "learning_rate": 1.5187706779111256e-05,
      "loss": 2.5231,
      "step": 1074700
    },
    {
      "epoch": 348.39546191247973,
      "grad_norm": 1.3869796991348267,
      "learning_rate": 1.5184463185209213e-05,
      "loss": 2.5279,
      "step": 1074800
    },
    {
      "epoch": 348.42787682333875,
      "grad_norm": 1.2925198078155518,
      "learning_rate": 1.518121959130717e-05,
      "loss": 2.5239,
      "step": 1074900
    },
    {
      "epoch": 348.4602917341977,
      "grad_norm": 1.3397204875946045,
      "learning_rate": 1.5177975997405125e-05,
      "loss": 2.5059,
      "step": 1075000
    },
    {
      "epoch": 348.4927066450567,
      "grad_norm": 1.366729974746704,
      "learning_rate": 1.517473240350308e-05,
      "loss": 2.5088,
      "step": 1075100
    },
    {
      "epoch": 348.52512155591575,
      "grad_norm": 1.2709661722183228,
      "learning_rate": 1.517148880960104e-05,
      "loss": 2.5351,
      "step": 1075200
    },
    {
      "epoch": 348.5575364667747,
      "grad_norm": 1.3277642726898193,
      "learning_rate": 1.5168245215698995e-05,
      "loss": 2.5131,
      "step": 1075300
    },
    {
      "epoch": 348.5899513776337,
      "grad_norm": 1.3361260890960693,
      "learning_rate": 1.5165001621796952e-05,
      "loss": 2.4957,
      "step": 1075400
    },
    {
      "epoch": 348.6223662884927,
      "grad_norm": 1.4228636026382446,
      "learning_rate": 1.5161758027894909e-05,
      "loss": 2.5222,
      "step": 1075500
    },
    {
      "epoch": 348.6547811993517,
      "grad_norm": 1.5310454368591309,
      "learning_rate": 1.5158514433992866e-05,
      "loss": 2.5147,
      "step": 1075600
    },
    {
      "epoch": 348.6871961102107,
      "grad_norm": 1.458532452583313,
      "learning_rate": 1.5155303276029842e-05,
      "loss": 2.5275,
      "step": 1075700
    },
    {
      "epoch": 348.7196110210697,
      "grad_norm": 1.2799848318099976,
      "learning_rate": 1.5152059682127797e-05,
      "loss": 2.5374,
      "step": 1075800
    },
    {
      "epoch": 348.7520259319287,
      "grad_norm": 1.3146741390228271,
      "learning_rate": 1.5148816088225754e-05,
      "loss": 2.5223,
      "step": 1075900
    },
    {
      "epoch": 348.78444084278766,
      "grad_norm": 1.4655932188034058,
      "learning_rate": 1.5145572494323711e-05,
      "loss": 2.5148,
      "step": 1076000
    },
    {
      "epoch": 348.8168557536467,
      "grad_norm": 1.4858078956604004,
      "learning_rate": 1.5142328900421668e-05,
      "loss": 2.5214,
      "step": 1076100
    },
    {
      "epoch": 348.8492706645057,
      "grad_norm": 1.3652650117874146,
      "learning_rate": 1.5139085306519624e-05,
      "loss": 2.5088,
      "step": 1076200
    },
    {
      "epoch": 348.88168557536466,
      "grad_norm": 1.3902908563613892,
      "learning_rate": 1.5135841712617582e-05,
      "loss": 2.5131,
      "step": 1076300
    },
    {
      "epoch": 348.9141004862237,
      "grad_norm": 1.5715323686599731,
      "learning_rate": 1.5132598118715538e-05,
      "loss": 2.5215,
      "step": 1076400
    },
    {
      "epoch": 348.94651539708263,
      "grad_norm": 1.3992199897766113,
      "learning_rate": 1.5129354524813493e-05,
      "loss": 2.5128,
      "step": 1076500
    },
    {
      "epoch": 348.97893030794165,
      "grad_norm": 1.2368180751800537,
      "learning_rate": 1.512611093091145e-05,
      "loss": 2.5116,
      "step": 1076600
    },
    {
      "epoch": 349.0,
      "eval_bleu": 1.025397775578234,
      "eval_loss": 4.214203357696533,
      "eval_runtime": 4.2623,
      "eval_samples_per_second": 115.43,
      "eval_steps_per_second": 1.877,
      "step": 1076665
    },
    {
      "epoch": 349.01134521880067,
      "grad_norm": 1.3716784715652466,
      "learning_rate": 1.5122867337009409e-05,
      "loss": 2.5126,
      "step": 1076700
    },
    {
      "epoch": 349.04376012965963,
      "grad_norm": 1.3172909021377563,
      "learning_rate": 1.5119623743107364e-05,
      "loss": 2.5133,
      "step": 1076800
    },
    {
      "epoch": 349.07617504051865,
      "grad_norm": 1.327418327331543,
      "learning_rate": 1.511638014920532e-05,
      "loss": 2.4933,
      "step": 1076900
    },
    {
      "epoch": 349.1085899513776,
      "grad_norm": 1.379921555519104,
      "learning_rate": 1.5113136555303275e-05,
      "loss": 2.4814,
      "step": 1077000
    },
    {
      "epoch": 349.1410048622366,
      "grad_norm": 1.48541259765625,
      "learning_rate": 1.5109892961401234e-05,
      "loss": 2.5321,
      "step": 1077100
    },
    {
      "epoch": 349.17341977309565,
      "grad_norm": 1.3644518852233887,
      "learning_rate": 1.5106649367499189e-05,
      "loss": 2.5227,
      "step": 1077200
    },
    {
      "epoch": 349.2058346839546,
      "grad_norm": 1.3857395648956299,
      "learning_rate": 1.5103405773597146e-05,
      "loss": 2.5194,
      "step": 1077300
    },
    {
      "epoch": 349.2382495948136,
      "grad_norm": 1.418473243713379,
      "learning_rate": 1.5100162179695105e-05,
      "loss": 2.5282,
      "step": 1077400
    },
    {
      "epoch": 349.2706645056726,
      "grad_norm": 1.2386828660964966,
      "learning_rate": 1.509691858579306e-05,
      "loss": 2.5092,
      "step": 1077500
    },
    {
      "epoch": 349.3030794165316,
      "grad_norm": 1.490956425666809,
      "learning_rate": 1.5093674991891016e-05,
      "loss": 2.5108,
      "step": 1077600
    },
    {
      "epoch": 349.3354943273906,
      "grad_norm": 1.3454047441482544,
      "learning_rate": 1.5090463833927992e-05,
      "loss": 2.4911,
      "step": 1077700
    },
    {
      "epoch": 349.3679092382496,
      "grad_norm": 1.4738103151321411,
      "learning_rate": 1.5087220240025949e-05,
      "loss": 2.4993,
      "step": 1077800
    },
    {
      "epoch": 349.4003241491086,
      "grad_norm": 1.2864599227905273,
      "learning_rate": 1.5083976646123907e-05,
      "loss": 2.5203,
      "step": 1077900
    },
    {
      "epoch": 349.43273905996756,
      "grad_norm": 1.4208893775939941,
      "learning_rate": 1.5080733052221863e-05,
      "loss": 2.5255,
      "step": 1078000
    },
    {
      "epoch": 349.4651539708266,
      "grad_norm": 1.4317598342895508,
      "learning_rate": 1.5077489458319818e-05,
      "loss": 2.5218,
      "step": 1078100
    },
    {
      "epoch": 349.4975688816856,
      "grad_norm": 1.5274579524993896,
      "learning_rate": 1.5074245864417777e-05,
      "loss": 2.5169,
      "step": 1078200
    },
    {
      "epoch": 349.52998379254456,
      "grad_norm": 1.3183387517929077,
      "learning_rate": 1.5071002270515732e-05,
      "loss": 2.4987,
      "step": 1078300
    },
    {
      "epoch": 349.5623987034036,
      "grad_norm": 1.2769532203674316,
      "learning_rate": 1.506775867661369e-05,
      "loss": 2.5105,
      "step": 1078400
    },
    {
      "epoch": 349.59481361426253,
      "grad_norm": 1.4238228797912598,
      "learning_rate": 1.5064515082711645e-05,
      "loss": 2.5259,
      "step": 1078500
    },
    {
      "epoch": 349.62722852512155,
      "grad_norm": 1.4348981380462646,
      "learning_rate": 1.5061271488809603e-05,
      "loss": 2.5264,
      "step": 1078600
    },
    {
      "epoch": 349.65964343598057,
      "grad_norm": 1.2857778072357178,
      "learning_rate": 1.5058027894907559e-05,
      "loss": 2.5357,
      "step": 1078700
    },
    {
      "epoch": 349.69205834683953,
      "grad_norm": 1.388095736503601,
      "learning_rate": 1.5054784301005514e-05,
      "loss": 2.5246,
      "step": 1078800
    },
    {
      "epoch": 349.72447325769855,
      "grad_norm": 1.4853345155715942,
      "learning_rate": 1.505154070710347e-05,
      "loss": 2.5147,
      "step": 1078900
    },
    {
      "epoch": 349.7568881685575,
      "grad_norm": 1.320541501045227,
      "learning_rate": 1.5048297113201428e-05,
      "loss": 2.5154,
      "step": 1079000
    },
    {
      "epoch": 349.7893030794165,
      "grad_norm": 1.2620301246643066,
      "learning_rate": 1.5045053519299385e-05,
      "loss": 2.5118,
      "step": 1079100
    },
    {
      "epoch": 349.82171799027554,
      "grad_norm": 1.407975673675537,
      "learning_rate": 1.504180992539734e-05,
      "loss": 2.5261,
      "step": 1079200
    },
    {
      "epoch": 349.8541329011345,
      "grad_norm": 1.404800534248352,
      "learning_rate": 1.5038566331495296e-05,
      "loss": 2.53,
      "step": 1079300
    },
    {
      "epoch": 349.8865478119935,
      "grad_norm": 1.413986086845398,
      "learning_rate": 1.5035322737593255e-05,
      "loss": 2.511,
      "step": 1079400
    },
    {
      "epoch": 349.9189627228525,
      "grad_norm": 1.3458114862442017,
      "learning_rate": 1.503207914369121e-05,
      "loss": 2.5223,
      "step": 1079500
    },
    {
      "epoch": 349.9513776337115,
      "grad_norm": 1.2808493375778198,
      "learning_rate": 1.5028835549789167e-05,
      "loss": 2.5114,
      "step": 1079600
    },
    {
      "epoch": 349.9837925445705,
      "grad_norm": 1.772900104522705,
      "learning_rate": 1.5025624391826143e-05,
      "loss": 2.5208,
      "step": 1079700
    },
    {
      "epoch": 350.0,
      "eval_bleu": 1.030699649326448,
      "eval_loss": 4.210564136505127,
      "eval_runtime": 4.8518,
      "eval_samples_per_second": 101.405,
      "eval_steps_per_second": 1.649,
      "step": 1079750
    },
    {
      "epoch": 350.0162074554295,
      "grad_norm": 1.2634512186050415,
      "learning_rate": 1.5022380797924102e-05,
      "loss": 2.5097,
      "step": 1079800
    },
    {
      "epoch": 350.0486223662885,
      "grad_norm": 1.4220077991485596,
      "learning_rate": 1.5019137204022057e-05,
      "loss": 2.4888,
      "step": 1079900
    },
    {
      "epoch": 350.0810372771475,
      "grad_norm": 1.2312968969345093,
      "learning_rate": 1.5015893610120013e-05,
      "loss": 2.5311,
      "step": 1080000
    },
    {
      "epoch": 350.1134521880065,
      "grad_norm": 1.382824420928955,
      "learning_rate": 1.501265001621797e-05,
      "loss": 2.5101,
      "step": 1080100
    },
    {
      "epoch": 350.1458670988655,
      "grad_norm": 1.3247714042663574,
      "learning_rate": 1.5009406422315927e-05,
      "loss": 2.5133,
      "step": 1080200
    },
    {
      "epoch": 350.17828200972446,
      "grad_norm": 1.2855877876281738,
      "learning_rate": 1.5006162828413884e-05,
      "loss": 2.5097,
      "step": 1080300
    },
    {
      "epoch": 350.2106969205835,
      "grad_norm": 1.5542348623275757,
      "learning_rate": 1.500291923451184e-05,
      "loss": 2.5094,
      "step": 1080400
    },
    {
      "epoch": 350.2431118314425,
      "grad_norm": 1.6627358198165894,
      "learning_rate": 1.4999675640609798e-05,
      "loss": 2.5139,
      "step": 1080500
    },
    {
      "epoch": 350.27552674230145,
      "grad_norm": 1.3866565227508545,
      "learning_rate": 1.4996432046707753e-05,
      "loss": 2.5269,
      "step": 1080600
    },
    {
      "epoch": 350.30794165316047,
      "grad_norm": 1.4340708255767822,
      "learning_rate": 1.4993188452805709e-05,
      "loss": 2.5236,
      "step": 1080700
    },
    {
      "epoch": 350.34035656401943,
      "grad_norm": 1.3004060983657837,
      "learning_rate": 1.4989944858903666e-05,
      "loss": 2.5235,
      "step": 1080800
    },
    {
      "epoch": 350.37277147487845,
      "grad_norm": 1.4881410598754883,
      "learning_rate": 1.4986701265001624e-05,
      "loss": 2.4994,
      "step": 1080900
    },
    {
      "epoch": 350.40518638573747,
      "grad_norm": 1.4362332820892334,
      "learning_rate": 1.498345767109958e-05,
      "loss": 2.5193,
      "step": 1081000
    },
    {
      "epoch": 350.4376012965964,
      "grad_norm": 1.3253982067108154,
      "learning_rate": 1.4980214077197535e-05,
      "loss": 2.5263,
      "step": 1081100
    },
    {
      "epoch": 350.47001620745544,
      "grad_norm": 1.4495927095413208,
      "learning_rate": 1.497697048329549e-05,
      "loss": 2.4963,
      "step": 1081200
    },
    {
      "epoch": 350.5024311183144,
      "grad_norm": 1.641256332397461,
      "learning_rate": 1.497372688939345e-05,
      "loss": 2.4967,
      "step": 1081300
    },
    {
      "epoch": 350.5348460291734,
      "grad_norm": 1.4741016626358032,
      "learning_rate": 1.4970483295491405e-05,
      "loss": 2.5178,
      "step": 1081400
    },
    {
      "epoch": 350.56726094003244,
      "grad_norm": 1.5638771057128906,
      "learning_rate": 1.4967239701589362e-05,
      "loss": 2.4926,
      "step": 1081500
    },
    {
      "epoch": 350.5996758508914,
      "grad_norm": 1.3855314254760742,
      "learning_rate": 1.4963996107687317e-05,
      "loss": 2.5215,
      "step": 1081600
    },
    {
      "epoch": 350.6320907617504,
      "grad_norm": 1.3438743352890015,
      "learning_rate": 1.4960784949724297e-05,
      "loss": 2.5021,
      "step": 1081700
    },
    {
      "epoch": 350.6645056726094,
      "grad_norm": 1.499058485031128,
      "learning_rate": 1.4957541355822252e-05,
      "loss": 2.5217,
      "step": 1081800
    },
    {
      "epoch": 350.6969205834684,
      "grad_norm": 1.455225944519043,
      "learning_rate": 1.4954297761920207e-05,
      "loss": 2.5055,
      "step": 1081900
    },
    {
      "epoch": 350.7293354943274,
      "grad_norm": 1.3011493682861328,
      "learning_rate": 1.4951054168018164e-05,
      "loss": 2.5268,
      "step": 1082000
    },
    {
      "epoch": 350.7617504051864,
      "grad_norm": 1.3148609399795532,
      "learning_rate": 1.4947810574116123e-05,
      "loss": 2.5238,
      "step": 1082100
    },
    {
      "epoch": 350.7941653160454,
      "grad_norm": 1.4567986726760864,
      "learning_rate": 1.4944566980214078e-05,
      "loss": 2.5343,
      "step": 1082200
    },
    {
      "epoch": 350.82658022690435,
      "grad_norm": 1.3238493204116821,
      "learning_rate": 1.4941323386312034e-05,
      "loss": 2.5141,
      "step": 1082300
    },
    {
      "epoch": 350.8589951377634,
      "grad_norm": 1.447824478149414,
      "learning_rate": 1.4938079792409989e-05,
      "loss": 2.5196,
      "step": 1082400
    },
    {
      "epoch": 350.8914100486224,
      "grad_norm": 1.5406783819198608,
      "learning_rate": 1.4934836198507948e-05,
      "loss": 2.5143,
      "step": 1082500
    },
    {
      "epoch": 350.92382495948135,
      "grad_norm": 1.6710540056228638,
      "learning_rate": 1.4931592604605905e-05,
      "loss": 2.5111,
      "step": 1082600
    },
    {
      "epoch": 350.95623987034037,
      "grad_norm": 1.1785067319869995,
      "learning_rate": 1.492834901070386e-05,
      "loss": 2.5197,
      "step": 1082700
    },
    {
      "epoch": 350.98865478119933,
      "grad_norm": 1.3812404870986938,
      "learning_rate": 1.4925105416801819e-05,
      "loss": 2.5439,
      "step": 1082800
    },
    {
      "epoch": 351.0,
      "eval_bleu": 1.1532330543512235,
      "eval_loss": 4.214252948760986,
      "eval_runtime": 4.9676,
      "eval_samples_per_second": 99.041,
      "eval_steps_per_second": 1.61,
      "step": 1082835
    },
    {
      "epoch": 351.02106969205835,
      "grad_norm": 1.2460259199142456,
      "learning_rate": 1.4921861822899774e-05,
      "loss": 2.5318,
      "step": 1082900
    },
    {
      "epoch": 351.05348460291737,
      "grad_norm": 1.2752267122268677,
      "learning_rate": 1.491861822899773e-05,
      "loss": 2.4998,
      "step": 1083000
    },
    {
      "epoch": 351.0858995137763,
      "grad_norm": 1.3915332555770874,
      "learning_rate": 1.4915374635095685e-05,
      "loss": 2.517,
      "step": 1083100
    },
    {
      "epoch": 351.11831442463534,
      "grad_norm": 1.3285870552062988,
      "learning_rate": 1.4912131041193644e-05,
      "loss": 2.5236,
      "step": 1083200
    },
    {
      "epoch": 351.1507293354943,
      "grad_norm": 1.2827904224395752,
      "learning_rate": 1.49088874472916e-05,
      "loss": 2.4959,
      "step": 1083300
    },
    {
      "epoch": 351.1831442463533,
      "grad_norm": 1.2897433042526245,
      "learning_rate": 1.4905643853389556e-05,
      "loss": 2.5135,
      "step": 1083400
    },
    {
      "epoch": 351.21555915721234,
      "grad_norm": 1.4599195718765259,
      "learning_rate": 1.4902400259487512e-05,
      "loss": 2.4857,
      "step": 1083500
    },
    {
      "epoch": 351.2479740680713,
      "grad_norm": 1.5986888408660889,
      "learning_rate": 1.489915666558547e-05,
      "loss": 2.5199,
      "step": 1083600
    },
    {
      "epoch": 351.2803889789303,
      "grad_norm": 1.5311439037322998,
      "learning_rate": 1.4895945507622446e-05,
      "loss": 2.4994,
      "step": 1083700
    },
    {
      "epoch": 351.3128038897893,
      "grad_norm": 1.4767123460769653,
      "learning_rate": 1.4892701913720403e-05,
      "loss": 2.5347,
      "step": 1083800
    },
    {
      "epoch": 351.3452188006483,
      "grad_norm": 1.3723074197769165,
      "learning_rate": 1.4889458319818359e-05,
      "loss": 2.5167,
      "step": 1083900
    },
    {
      "epoch": 351.3776337115073,
      "grad_norm": 1.5000249147415161,
      "learning_rate": 1.4886214725916318e-05,
      "loss": 2.4953,
      "step": 1084000
    },
    {
      "epoch": 351.4100486223663,
      "grad_norm": 1.4976375102996826,
      "learning_rate": 1.4882971132014273e-05,
      "loss": 2.509,
      "step": 1084100
    },
    {
      "epoch": 351.4424635332253,
      "grad_norm": 1.2425991296768188,
      "learning_rate": 1.4879727538112228e-05,
      "loss": 2.496,
      "step": 1084200
    },
    {
      "epoch": 351.47487844408425,
      "grad_norm": 1.3960050344467163,
      "learning_rate": 1.4876483944210184e-05,
      "loss": 2.5344,
      "step": 1084300
    },
    {
      "epoch": 351.5072933549433,
      "grad_norm": 1.417188048362732,
      "learning_rate": 1.4873240350308142e-05,
      "loss": 2.4912,
      "step": 1084400
    },
    {
      "epoch": 351.5397082658023,
      "grad_norm": 1.5155404806137085,
      "learning_rate": 1.48699967564061e-05,
      "loss": 2.5097,
      "step": 1084500
    },
    {
      "epoch": 351.57212317666125,
      "grad_norm": 1.389370083808899,
      "learning_rate": 1.4866753162504055e-05,
      "loss": 2.5032,
      "step": 1084600
    },
    {
      "epoch": 351.60453808752027,
      "grad_norm": 1.2894097566604614,
      "learning_rate": 1.486350956860201e-05,
      "loss": 2.5057,
      "step": 1084700
    },
    {
      "epoch": 351.63695299837923,
      "grad_norm": 1.412992238998413,
      "learning_rate": 1.4860265974699969e-05,
      "loss": 2.5056,
      "step": 1084800
    },
    {
      "epoch": 351.66936790923825,
      "grad_norm": 1.4788326025009155,
      "learning_rate": 1.4857054816736945e-05,
      "loss": 2.5117,
      "step": 1084900
    },
    {
      "epoch": 351.70178282009726,
      "grad_norm": 1.3953033685684204,
      "learning_rate": 1.4853811222834902e-05,
      "loss": 2.5353,
      "step": 1085000
    },
    {
      "epoch": 351.7341977309562,
      "grad_norm": 1.2558245658874512,
      "learning_rate": 1.4850567628932857e-05,
      "loss": 2.51,
      "step": 1085100
    },
    {
      "epoch": 351.76661264181524,
      "grad_norm": 1.4689345359802246,
      "learning_rate": 1.4847324035030816e-05,
      "loss": 2.5162,
      "step": 1085200
    },
    {
      "epoch": 351.7990275526742,
      "grad_norm": 1.6416246891021729,
      "learning_rate": 1.4844080441128771e-05,
      "loss": 2.5426,
      "step": 1085300
    },
    {
      "epoch": 351.8314424635332,
      "grad_norm": 1.4637041091918945,
      "learning_rate": 1.4840836847226727e-05,
      "loss": 2.5256,
      "step": 1085400
    },
    {
      "epoch": 351.86385737439224,
      "grad_norm": 1.341507077217102,
      "learning_rate": 1.4837593253324684e-05,
      "loss": 2.5084,
      "step": 1085500
    },
    {
      "epoch": 351.8962722852512,
      "grad_norm": 1.3912928104400635,
      "learning_rate": 1.4834349659422641e-05,
      "loss": 2.5109,
      "step": 1085600
    },
    {
      "epoch": 351.9286871961102,
      "grad_norm": 1.3891088962554932,
      "learning_rate": 1.4831106065520598e-05,
      "loss": 2.524,
      "step": 1085700
    },
    {
      "epoch": 351.9611021069692,
      "grad_norm": 1.4053549766540527,
      "learning_rate": 1.4827862471618553e-05,
      "loss": 2.5334,
      "step": 1085800
    },
    {
      "epoch": 351.9935170178282,
      "grad_norm": 1.4889057874679565,
      "learning_rate": 1.4824618877716512e-05,
      "loss": 2.5237,
      "step": 1085900
    },
    {
      "epoch": 352.0,
      "eval_bleu": 1.0548814405277371,
      "eval_loss": 4.216587066650391,
      "eval_runtime": 4.5634,
      "eval_samples_per_second": 107.814,
      "eval_steps_per_second": 1.753,
      "step": 1085920
    },
    {
      "epoch": 352.0259319286872,
      "grad_norm": 1.4733737707138062,
      "learning_rate": 1.4821375283814467e-05,
      "loss": 2.5028,
      "step": 1086000
    },
    {
      "epoch": 352.0583468395462,
      "grad_norm": 1.406622052192688,
      "learning_rate": 1.4818131689912423e-05,
      "loss": 2.5108,
      "step": 1086100
    },
    {
      "epoch": 352.0907617504052,
      "grad_norm": 1.2827188968658447,
      "learning_rate": 1.481488809601038e-05,
      "loss": 2.4951,
      "step": 1086200
    },
    {
      "epoch": 352.12317666126415,
      "grad_norm": 1.4252855777740479,
      "learning_rate": 1.4811644502108339e-05,
      "loss": 2.4869,
      "step": 1086300
    },
    {
      "epoch": 352.15559157212317,
      "grad_norm": 1.235769510269165,
      "learning_rate": 1.4808400908206294e-05,
      "loss": 2.5087,
      "step": 1086400
    },
    {
      "epoch": 352.1880064829822,
      "grad_norm": 1.3354216814041138,
      "learning_rate": 1.480515731430425e-05,
      "loss": 2.5017,
      "step": 1086500
    },
    {
      "epoch": 352.22042139384115,
      "grad_norm": 1.4834767580032349,
      "learning_rate": 1.4801913720402205e-05,
      "loss": 2.5112,
      "step": 1086600
    },
    {
      "epoch": 352.25283630470017,
      "grad_norm": 1.3227159976959229,
      "learning_rate": 1.4798670126500163e-05,
      "loss": 2.5119,
      "step": 1086700
    },
    {
      "epoch": 352.2852512155592,
      "grad_norm": 1.4920737743377686,
      "learning_rate": 1.4795426532598119e-05,
      "loss": 2.5226,
      "step": 1086800
    },
    {
      "epoch": 352.31766612641815,
      "grad_norm": 1.3107932806015015,
      "learning_rate": 1.4792182938696076e-05,
      "loss": 2.5001,
      "step": 1086900
    },
    {
      "epoch": 352.35008103727716,
      "grad_norm": 1.3368403911590576,
      "learning_rate": 1.4788939344794034e-05,
      "loss": 2.4875,
      "step": 1087000
    },
    {
      "epoch": 352.3824959481361,
      "grad_norm": 1.3157398700714111,
      "learning_rate": 1.478569575089199e-05,
      "loss": 2.5014,
      "step": 1087100
    },
    {
      "epoch": 352.41491085899514,
      "grad_norm": 1.4434456825256348,
      "learning_rate": 1.4782452156989945e-05,
      "loss": 2.5253,
      "step": 1087200
    },
    {
      "epoch": 352.44732576985416,
      "grad_norm": 1.3083972930908203,
      "learning_rate": 1.47792085630879e-05,
      "loss": 2.5374,
      "step": 1087300
    },
    {
      "epoch": 352.4797406807131,
      "grad_norm": 1.3349193334579468,
      "learning_rate": 1.477596496918586e-05,
      "loss": 2.5104,
      "step": 1087400
    },
    {
      "epoch": 352.51215559157214,
      "grad_norm": 1.5076322555541992,
      "learning_rate": 1.4772721375283816e-05,
      "loss": 2.5159,
      "step": 1087500
    },
    {
      "epoch": 352.5445705024311,
      "grad_norm": 1.4083038568496704,
      "learning_rate": 1.4769477781381772e-05,
      "loss": 2.53,
      "step": 1087600
    },
    {
      "epoch": 352.5769854132901,
      "grad_norm": 1.2417418956756592,
      "learning_rate": 1.4766234187479727e-05,
      "loss": 2.5256,
      "step": 1087700
    },
    {
      "epoch": 352.60940032414914,
      "grad_norm": 1.4683361053466797,
      "learning_rate": 1.4762990593577686e-05,
      "loss": 2.5033,
      "step": 1087800
    },
    {
      "epoch": 352.6418152350081,
      "grad_norm": 1.611145257949829,
      "learning_rate": 1.4759746999675641e-05,
      "loss": 2.5257,
      "step": 1087900
    },
    {
      "epoch": 352.6742301458671,
      "grad_norm": 1.5123807191848755,
      "learning_rate": 1.4756503405773597e-05,
      "loss": 2.4839,
      "step": 1088000
    },
    {
      "epoch": 352.7066450567261,
      "grad_norm": 1.2265450954437256,
      "learning_rate": 1.4753259811871554e-05,
      "loss": 2.4946,
      "step": 1088100
    },
    {
      "epoch": 352.7390599675851,
      "grad_norm": 1.2816511392593384,
      "learning_rate": 1.4750016217969512e-05,
      "loss": 2.5133,
      "step": 1088200
    },
    {
      "epoch": 352.7714748784441,
      "grad_norm": 1.1519253253936768,
      "learning_rate": 1.4746772624067468e-05,
      "loss": 2.5366,
      "step": 1088300
    },
    {
      "epoch": 352.80388978930307,
      "grad_norm": 1.3032495975494385,
      "learning_rate": 1.4743529030165423e-05,
      "loss": 2.5292,
      "step": 1088400
    },
    {
      "epoch": 352.8363047001621,
      "grad_norm": 1.4368408918380737,
      "learning_rate": 1.4740285436263382e-05,
      "loss": 2.5114,
      "step": 1088500
    },
    {
      "epoch": 352.86871961102105,
      "grad_norm": 1.7882837057113647,
      "learning_rate": 1.4737041842361337e-05,
      "loss": 2.5328,
      "step": 1088600
    },
    {
      "epoch": 352.90113452188007,
      "grad_norm": 1.5379489660263062,
      "learning_rate": 1.4733798248459294e-05,
      "loss": 2.5007,
      "step": 1088700
    },
    {
      "epoch": 352.9335494327391,
      "grad_norm": 1.446630597114563,
      "learning_rate": 1.473055465455725e-05,
      "loss": 2.5268,
      "step": 1088800
    },
    {
      "epoch": 352.96596434359805,
      "grad_norm": 1.3980412483215332,
      "learning_rate": 1.4727311060655208e-05,
      "loss": 2.5076,
      "step": 1088900
    },
    {
      "epoch": 352.99837925445706,
      "grad_norm": 1.577584147453308,
      "learning_rate": 1.4724099902692184e-05,
      "loss": 2.5371,
      "step": 1089000
    },
    {
      "epoch": 353.0,
      "eval_bleu": 0.9665639858585265,
      "eval_loss": 4.216468811035156,
      "eval_runtime": 4.5367,
      "eval_samples_per_second": 108.45,
      "eval_steps_per_second": 1.763,
      "step": 1089005
    },
    {
      "epoch": 353.030794165316,
      "grad_norm": 1.5831561088562012,
      "learning_rate": 1.472085630879014e-05,
      "loss": 2.514,
      "step": 1089100
    },
    {
      "epoch": 353.06320907617504,
      "grad_norm": 1.2874486446380615,
      "learning_rate": 1.4717612714888097e-05,
      "loss": 2.5128,
      "step": 1089200
    },
    {
      "epoch": 353.09562398703406,
      "grad_norm": 1.3390851020812988,
      "learning_rate": 1.4714369120986054e-05,
      "loss": 2.5221,
      "step": 1089300
    },
    {
      "epoch": 353.128038897893,
      "grad_norm": 1.395148515701294,
      "learning_rate": 1.4711125527084011e-05,
      "loss": 2.4992,
      "step": 1089400
    },
    {
      "epoch": 353.16045380875204,
      "grad_norm": 1.6314866542816162,
      "learning_rate": 1.4707881933181966e-05,
      "loss": 2.5168,
      "step": 1089500
    },
    {
      "epoch": 353.192868719611,
      "grad_norm": 1.687441349029541,
      "learning_rate": 1.4704638339279922e-05,
      "loss": 2.4912,
      "step": 1089600
    },
    {
      "epoch": 353.22528363047,
      "grad_norm": 1.3278878927230835,
      "learning_rate": 1.470139474537788e-05,
      "loss": 2.5344,
      "step": 1089700
    },
    {
      "epoch": 353.25769854132903,
      "grad_norm": 1.2634567022323608,
      "learning_rate": 1.4698151151475836e-05,
      "loss": 2.5168,
      "step": 1089800
    },
    {
      "epoch": 353.290113452188,
      "grad_norm": 1.3331642150878906,
      "learning_rate": 1.4694907557573793e-05,
      "loss": 2.52,
      "step": 1089900
    },
    {
      "epoch": 353.322528363047,
      "grad_norm": 1.3038713932037354,
      "learning_rate": 1.4691663963671748e-05,
      "loss": 2.4744,
      "step": 1090000
    },
    {
      "epoch": 353.354943273906,
      "grad_norm": 1.3412301540374756,
      "learning_rate": 1.4688420369769707e-05,
      "loss": 2.5146,
      "step": 1090100
    },
    {
      "epoch": 353.387358184765,
      "grad_norm": 1.46023428440094,
      "learning_rate": 1.4685176775867662e-05,
      "loss": 2.4959,
      "step": 1090200
    },
    {
      "epoch": 353.419773095624,
      "grad_norm": 1.7455519437789917,
      "learning_rate": 1.4681933181965618e-05,
      "loss": 2.4845,
      "step": 1090300
    },
    {
      "epoch": 353.45218800648297,
      "grad_norm": 1.2120236158370972,
      "learning_rate": 1.4678689588063575e-05,
      "loss": 2.5045,
      "step": 1090400
    },
    {
      "epoch": 353.484602917342,
      "grad_norm": 1.3436181545257568,
      "learning_rate": 1.4675445994161532e-05,
      "loss": 2.5104,
      "step": 1090500
    },
    {
      "epoch": 353.51701782820095,
      "grad_norm": 1.4047763347625732,
      "learning_rate": 1.4672202400259489e-05,
      "loss": 2.498,
      "step": 1090600
    },
    {
      "epoch": 353.54943273905997,
      "grad_norm": 1.3419525623321533,
      "learning_rate": 1.4668958806357444e-05,
      "loss": 2.5273,
      "step": 1090700
    },
    {
      "epoch": 353.581847649919,
      "grad_norm": 1.3227684497833252,
      "learning_rate": 1.4665715212455403e-05,
      "loss": 2.5049,
      "step": 1090800
    },
    {
      "epoch": 353.61426256077795,
      "grad_norm": 1.4625200033187866,
      "learning_rate": 1.4662471618553358e-05,
      "loss": 2.5131,
      "step": 1090900
    },
    {
      "epoch": 353.64667747163696,
      "grad_norm": 1.5123568773269653,
      "learning_rate": 1.4659260460590334e-05,
      "loss": 2.5303,
      "step": 1091000
    },
    {
      "epoch": 353.6790923824959,
      "grad_norm": 1.4123576879501343,
      "learning_rate": 1.4656016866688291e-05,
      "loss": 2.4898,
      "step": 1091100
    },
    {
      "epoch": 353.71150729335494,
      "grad_norm": 1.5838487148284912,
      "learning_rate": 1.4652773272786247e-05,
      "loss": 2.5334,
      "step": 1091200
    },
    {
      "epoch": 353.74392220421396,
      "grad_norm": 1.6667742729187012,
      "learning_rate": 1.4649529678884205e-05,
      "loss": 2.5265,
      "step": 1091300
    },
    {
      "epoch": 353.7763371150729,
      "grad_norm": 1.3307143449783325,
      "learning_rate": 1.464628608498216e-05,
      "loss": 2.5158,
      "step": 1091400
    },
    {
      "epoch": 353.80875202593194,
      "grad_norm": 1.24488365650177,
      "learning_rate": 1.4643042491080116e-05,
      "loss": 2.5257,
      "step": 1091500
    },
    {
      "epoch": 353.8411669367909,
      "grad_norm": 1.691627860069275,
      "learning_rate": 1.4639798897178075e-05,
      "loss": 2.5229,
      "step": 1091600
    },
    {
      "epoch": 353.8735818476499,
      "grad_norm": 1.5491665601730347,
      "learning_rate": 1.4636555303276032e-05,
      "loss": 2.5037,
      "step": 1091700
    },
    {
      "epoch": 353.90599675850893,
      "grad_norm": 1.3194798231124878,
      "learning_rate": 1.4633311709373987e-05,
      "loss": 2.5373,
      "step": 1091800
    },
    {
      "epoch": 353.9384116693679,
      "grad_norm": 1.3322765827178955,
      "learning_rate": 1.4630068115471943e-05,
      "loss": 2.5118,
      "step": 1091900
    },
    {
      "epoch": 353.9708265802269,
      "grad_norm": 1.3824700117111206,
      "learning_rate": 1.4626824521569901e-05,
      "loss": 2.5038,
      "step": 1092000
    },
    {
      "epoch": 354.0,
      "eval_bleu": 0.9378927469680015,
      "eval_loss": 4.212974548339844,
      "eval_runtime": 4.7718,
      "eval_samples_per_second": 103.106,
      "eval_steps_per_second": 1.677,
      "step": 1092090
    },
    {
      "epoch": 354.0032414910859,
      "grad_norm": 1.4634439945220947,
      "learning_rate": 1.4623580927667857e-05,
      "loss": 2.517,
      "step": 1092100
    },
    {
      "epoch": 354.0356564019449,
      "grad_norm": 1.5676196813583374,
      "learning_rate": 1.4620337333765812e-05,
      "loss": 2.5201,
      "step": 1092200
    },
    {
      "epoch": 354.0680713128039,
      "grad_norm": 1.5419336557388306,
      "learning_rate": 1.4617093739863769e-05,
      "loss": 2.497,
      "step": 1092300
    },
    {
      "epoch": 354.10048622366287,
      "grad_norm": 1.483984112739563,
      "learning_rate": 1.4613850145961728e-05,
      "loss": 2.5072,
      "step": 1092400
    },
    {
      "epoch": 354.1329011345219,
      "grad_norm": 1.5543091297149658,
      "learning_rate": 1.4610606552059683e-05,
      "loss": 2.5134,
      "step": 1092500
    },
    {
      "epoch": 354.16531604538085,
      "grad_norm": 1.3952805995941162,
      "learning_rate": 1.4607362958157639e-05,
      "loss": 2.5004,
      "step": 1092600
    },
    {
      "epoch": 354.19773095623987,
      "grad_norm": 1.3925797939300537,
      "learning_rate": 1.4604119364255594e-05,
      "loss": 2.4964,
      "step": 1092700
    },
    {
      "epoch": 354.2301458670989,
      "grad_norm": 1.3872239589691162,
      "learning_rate": 1.4600875770353553e-05,
      "loss": 2.5421,
      "step": 1092800
    },
    {
      "epoch": 354.26256077795784,
      "grad_norm": 1.4139271974563599,
      "learning_rate": 1.459763217645151e-05,
      "loss": 2.5113,
      "step": 1092900
    },
    {
      "epoch": 354.29497568881686,
      "grad_norm": 1.3757163286209106,
      "learning_rate": 1.4594421018488486e-05,
      "loss": 2.5123,
      "step": 1093000
    },
    {
      "epoch": 354.3273905996758,
      "grad_norm": 1.3482824563980103,
      "learning_rate": 1.4591177424586441e-05,
      "loss": 2.5153,
      "step": 1093100
    },
    {
      "epoch": 354.35980551053484,
      "grad_norm": 1.3424123525619507,
      "learning_rate": 1.45879338306844e-05,
      "loss": 2.5212,
      "step": 1093200
    },
    {
      "epoch": 354.39222042139386,
      "grad_norm": 1.224460482597351,
      "learning_rate": 1.4584690236782355e-05,
      "loss": 2.513,
      "step": 1093300
    },
    {
      "epoch": 354.4246353322528,
      "grad_norm": 1.3929530382156372,
      "learning_rate": 1.4581446642880312e-05,
      "loss": 2.5062,
      "step": 1093400
    },
    {
      "epoch": 354.45705024311184,
      "grad_norm": 1.2687351703643799,
      "learning_rate": 1.4578203048978268e-05,
      "loss": 2.5054,
      "step": 1093500
    },
    {
      "epoch": 354.48946515397085,
      "grad_norm": 1.3543756008148193,
      "learning_rate": 1.4574991891015247e-05,
      "loss": 2.4903,
      "step": 1093600
    },
    {
      "epoch": 354.5218800648298,
      "grad_norm": 1.506239652633667,
      "learning_rate": 1.4571748297113202e-05,
      "loss": 2.5186,
      "step": 1093700
    },
    {
      "epoch": 354.55429497568883,
      "grad_norm": 1.322461485862732,
      "learning_rate": 1.4568504703211158e-05,
      "loss": 2.5172,
      "step": 1093800
    },
    {
      "epoch": 354.5867098865478,
      "grad_norm": 1.551866054534912,
      "learning_rate": 1.4565261109309113e-05,
      "loss": 2.5186,
      "step": 1093900
    },
    {
      "epoch": 354.6191247974068,
      "grad_norm": 1.5646789073944092,
      "learning_rate": 1.4562017515407072e-05,
      "loss": 2.4913,
      "step": 1094000
    },
    {
      "epoch": 354.65153970826583,
      "grad_norm": 1.1884381771087646,
      "learning_rate": 1.4558773921505029e-05,
      "loss": 2.495,
      "step": 1094100
    },
    {
      "epoch": 354.6839546191248,
      "grad_norm": 1.4222588539123535,
      "learning_rate": 1.4555530327602984e-05,
      "loss": 2.5112,
      "step": 1094200
    },
    {
      "epoch": 354.7163695299838,
      "grad_norm": 1.4016454219818115,
      "learning_rate": 1.455228673370094e-05,
      "loss": 2.5066,
      "step": 1094300
    },
    {
      "epoch": 354.74878444084277,
      "grad_norm": 1.3502790927886963,
      "learning_rate": 1.4549043139798898e-05,
      "loss": 2.5299,
      "step": 1094400
    },
    {
      "epoch": 354.7811993517018,
      "grad_norm": 1.3903926610946655,
      "learning_rate": 1.4545799545896854e-05,
      "loss": 2.4921,
      "step": 1094500
    },
    {
      "epoch": 354.8136142625608,
      "grad_norm": 1.291369915008545,
      "learning_rate": 1.4542555951994811e-05,
      "loss": 2.5187,
      "step": 1094600
    },
    {
      "epoch": 354.84602917341977,
      "grad_norm": 1.1645909547805786,
      "learning_rate": 1.4539312358092768e-05,
      "loss": 2.5324,
      "step": 1094700
    },
    {
      "epoch": 354.8784440842788,
      "grad_norm": 1.5153675079345703,
      "learning_rate": 1.4536068764190725e-05,
      "loss": 2.5117,
      "step": 1094800
    },
    {
      "epoch": 354.91085899513774,
      "grad_norm": 1.3979854583740234,
      "learning_rate": 1.453282517028868e-05,
      "loss": 2.5179,
      "step": 1094900
    },
    {
      "epoch": 354.94327390599676,
      "grad_norm": 1.380998969078064,
      "learning_rate": 1.4529581576386636e-05,
      "loss": 2.5135,
      "step": 1095000
    },
    {
      "epoch": 354.9756888168558,
      "grad_norm": 1.3623467683792114,
      "learning_rate": 1.4526337982484594e-05,
      "loss": 2.5046,
      "step": 1095100
    },
    {
      "epoch": 355.0,
      "eval_bleu": 0.9281030524521463,
      "eval_loss": 4.218771934509277,
      "eval_runtime": 4.6352,
      "eval_samples_per_second": 106.145,
      "eval_steps_per_second": 1.726,
      "step": 1095175
    },
    {
      "epoch": 355.00810372771474,
      "grad_norm": 1.3172907829284668,
      "learning_rate": 1.452309438858255e-05,
      "loss": 2.5263,
      "step": 1095200
    },
    {
      "epoch": 355.04051863857376,
      "grad_norm": 1.3024059534072876,
      "learning_rate": 1.4519850794680507e-05,
      "loss": 2.5168,
      "step": 1095300
    },
    {
      "epoch": 355.0729335494327,
      "grad_norm": 1.4073210954666138,
      "learning_rate": 1.4516607200778462e-05,
      "loss": 2.5109,
      "step": 1095400
    },
    {
      "epoch": 355.10534846029174,
      "grad_norm": 1.3906973600387573,
      "learning_rate": 1.4513363606876421e-05,
      "loss": 2.4763,
      "step": 1095500
    },
    {
      "epoch": 355.13776337115075,
      "grad_norm": 1.4291383028030396,
      "learning_rate": 1.4510120012974376e-05,
      "loss": 2.5136,
      "step": 1095600
    },
    {
      "epoch": 355.1701782820097,
      "grad_norm": 1.4055404663085938,
      "learning_rate": 1.4506876419072332e-05,
      "loss": 2.5257,
      "step": 1095700
    },
    {
      "epoch": 355.20259319286873,
      "grad_norm": 1.4550001621246338,
      "learning_rate": 1.450363282517029e-05,
      "loss": 2.5217,
      "step": 1095800
    },
    {
      "epoch": 355.2350081037277,
      "grad_norm": 1.2863597869873047,
      "learning_rate": 1.4500389231268246e-05,
      "loss": 2.4916,
      "step": 1095900
    },
    {
      "epoch": 355.2674230145867,
      "grad_norm": 1.4192973375320435,
      "learning_rate": 1.4497178073305224e-05,
      "loss": 2.4816,
      "step": 1096000
    },
    {
      "epoch": 355.29983792544573,
      "grad_norm": 1.279025673866272,
      "learning_rate": 1.4493934479403179e-05,
      "loss": 2.5038,
      "step": 1096100
    },
    {
      "epoch": 355.3322528363047,
      "grad_norm": 1.529808759689331,
      "learning_rate": 1.4490690885501134e-05,
      "loss": 2.5153,
      "step": 1096200
    },
    {
      "epoch": 355.3646677471637,
      "grad_norm": 1.3570542335510254,
      "learning_rate": 1.4487447291599093e-05,
      "loss": 2.5135,
      "step": 1096300
    },
    {
      "epoch": 355.39708265802267,
      "grad_norm": 1.6400680541992188,
      "learning_rate": 1.4484203697697048e-05,
      "loss": 2.497,
      "step": 1096400
    },
    {
      "epoch": 355.4294975688817,
      "grad_norm": 1.3531707525253296,
      "learning_rate": 1.4480960103795005e-05,
      "loss": 2.5168,
      "step": 1096500
    },
    {
      "epoch": 355.4619124797407,
      "grad_norm": 1.401505947113037,
      "learning_rate": 1.4477716509892964e-05,
      "loss": 2.5113,
      "step": 1096600
    },
    {
      "epoch": 355.49432739059966,
      "grad_norm": 1.624526858329773,
      "learning_rate": 1.447447291599092e-05,
      "loss": 2.5045,
      "step": 1096700
    },
    {
      "epoch": 355.5267423014587,
      "grad_norm": 1.3782788515090942,
      "learning_rate": 1.4471229322088875e-05,
      "loss": 2.5107,
      "step": 1096800
    },
    {
      "epoch": 355.55915721231764,
      "grad_norm": 1.4926766157150269,
      "learning_rate": 1.446798572818683e-05,
      "loss": 2.4992,
      "step": 1096900
    },
    {
      "epoch": 355.59157212317666,
      "grad_norm": 1.4165693521499634,
      "learning_rate": 1.4464742134284789e-05,
      "loss": 2.531,
      "step": 1097000
    },
    {
      "epoch": 355.6239870340357,
      "grad_norm": 1.3221763372421265,
      "learning_rate": 1.4461498540382746e-05,
      "loss": 2.4951,
      "step": 1097100
    },
    {
      "epoch": 355.65640194489464,
      "grad_norm": 1.3366775512695312,
      "learning_rate": 1.4458254946480701e-05,
      "loss": 2.4984,
      "step": 1097200
    },
    {
      "epoch": 355.68881685575366,
      "grad_norm": 1.3476989269256592,
      "learning_rate": 1.4455011352578657e-05,
      "loss": 2.5143,
      "step": 1097300
    },
    {
      "epoch": 355.7212317666126,
      "grad_norm": 1.3224292993545532,
      "learning_rate": 1.4451767758676615e-05,
      "loss": 2.5151,
      "step": 1097400
    },
    {
      "epoch": 355.75364667747164,
      "grad_norm": 1.4598115682601929,
      "learning_rate": 1.444852416477457e-05,
      "loss": 2.5018,
      "step": 1097500
    },
    {
      "epoch": 355.78606158833065,
      "grad_norm": 1.248462438583374,
      "learning_rate": 1.4445280570872526e-05,
      "loss": 2.5135,
      "step": 1097600
    },
    {
      "epoch": 355.8184764991896,
      "grad_norm": 1.4161043167114258,
      "learning_rate": 1.4442036976970483e-05,
      "loss": 2.5122,
      "step": 1097700
    },
    {
      "epoch": 355.85089141004863,
      "grad_norm": 1.2889738082885742,
      "learning_rate": 1.4438793383068442e-05,
      "loss": 2.5198,
      "step": 1097800
    },
    {
      "epoch": 355.8833063209076,
      "grad_norm": 1.4049713611602783,
      "learning_rate": 1.4435549789166397e-05,
      "loss": 2.5286,
      "step": 1097900
    },
    {
      "epoch": 355.9157212317666,
      "grad_norm": 1.5355733633041382,
      "learning_rate": 1.4432306195264353e-05,
      "loss": 2.504,
      "step": 1098000
    },
    {
      "epoch": 355.94813614262563,
      "grad_norm": 1.5304303169250488,
      "learning_rate": 1.4429095037301329e-05,
      "loss": 2.5105,
      "step": 1098100
    },
    {
      "epoch": 355.9805510534846,
      "grad_norm": 1.4516080617904663,
      "learning_rate": 1.4425851443399287e-05,
      "loss": 2.531,
      "step": 1098200
    },
    {
      "epoch": 356.0,
      "eval_bleu": 0.9539570627211253,
      "eval_loss": 4.22499942779541,
      "eval_runtime": 4.7771,
      "eval_samples_per_second": 102.991,
      "eval_steps_per_second": 1.675,
      "step": 1098260
    },
    {
      "epoch": 356.0129659643436,
      "grad_norm": 1.3369221687316895,
      "learning_rate": 1.4422607849497245e-05,
      "loss": 2.5156,
      "step": 1098300
    },
    {
      "epoch": 356.04538087520257,
      "grad_norm": 1.3427720069885254,
      "learning_rate": 1.44193642555952e-05,
      "loss": 2.4804,
      "step": 1098400
    },
    {
      "epoch": 356.0777957860616,
      "grad_norm": 1.2651299238204956,
      "learning_rate": 1.4416120661693155e-05,
      "loss": 2.511,
      "step": 1098500
    },
    {
      "epoch": 356.1102106969206,
      "grad_norm": 1.4443514347076416,
      "learning_rate": 1.4412877067791114e-05,
      "loss": 2.4959,
      "step": 1098600
    },
    {
      "epoch": 356.14262560777956,
      "grad_norm": 1.5292192697525024,
      "learning_rate": 1.440963347388907e-05,
      "loss": 2.5278,
      "step": 1098700
    },
    {
      "epoch": 356.1750405186386,
      "grad_norm": 1.3102449178695679,
      "learning_rate": 1.4406389879987026e-05,
      "loss": 2.5143,
      "step": 1098800
    },
    {
      "epoch": 356.20745542949754,
      "grad_norm": 1.4051501750946045,
      "learning_rate": 1.4403146286084983e-05,
      "loss": 2.4864,
      "step": 1098900
    },
    {
      "epoch": 356.23987034035656,
      "grad_norm": 1.3240092992782593,
      "learning_rate": 1.439990269218294e-05,
      "loss": 2.494,
      "step": 1099000
    },
    {
      "epoch": 356.2722852512156,
      "grad_norm": 1.5233386754989624,
      "learning_rate": 1.4396659098280896e-05,
      "loss": 2.5061,
      "step": 1099100
    },
    {
      "epoch": 356.30470016207454,
      "grad_norm": 1.378072738647461,
      "learning_rate": 1.4393415504378851e-05,
      "loss": 2.4961,
      "step": 1099200
    },
    {
      "epoch": 356.33711507293356,
      "grad_norm": 1.2640970945358276,
      "learning_rate": 1.439017191047681e-05,
      "loss": 2.5361,
      "step": 1099300
    },
    {
      "epoch": 356.3695299837925,
      "grad_norm": 1.418330430984497,
      "learning_rate": 1.4386928316574765e-05,
      "loss": 2.4732,
      "step": 1099400
    },
    {
      "epoch": 356.40194489465154,
      "grad_norm": 1.232930064201355,
      "learning_rate": 1.4383684722672722e-05,
      "loss": 2.5169,
      "step": 1099500
    },
    {
      "epoch": 356.43435980551055,
      "grad_norm": 1.5618168115615845,
      "learning_rate": 1.4380441128770678e-05,
      "loss": 2.4996,
      "step": 1099600
    },
    {
      "epoch": 356.4667747163695,
      "grad_norm": 1.2875717878341675,
      "learning_rate": 1.4377197534868636e-05,
      "loss": 2.4953,
      "step": 1099700
    },
    {
      "epoch": 356.49918962722853,
      "grad_norm": 1.5310949087142944,
      "learning_rate": 1.4373953940966592e-05,
      "loss": 2.5094,
      "step": 1099800
    },
    {
      "epoch": 356.5316045380875,
      "grad_norm": 1.433908224105835,
      "learning_rate": 1.4370710347064547e-05,
      "loss": 2.5258,
      "step": 1099900
    },
    {
      "epoch": 356.5640194489465,
      "grad_norm": 1.327634334564209,
      "learning_rate": 1.4367466753162504e-05,
      "loss": 2.5057,
      "step": 1100000
    },
    {
      "epoch": 356.5964343598055,
      "grad_norm": 1.3121163845062256,
      "learning_rate": 1.4364223159260461e-05,
      "loss": 2.5157,
      "step": 1100100
    },
    {
      "epoch": 356.6288492706645,
      "grad_norm": 1.6457908153533936,
      "learning_rate": 1.4360979565358418e-05,
      "loss": 2.5257,
      "step": 1100200
    },
    {
      "epoch": 356.6612641815235,
      "grad_norm": 1.3608955144882202,
      "learning_rate": 1.4357735971456374e-05,
      "loss": 2.5307,
      "step": 1100300
    },
    {
      "epoch": 356.6936790923825,
      "grad_norm": 1.4433681964874268,
      "learning_rate": 1.4354492377554332e-05,
      "loss": 2.4993,
      "step": 1100400
    },
    {
      "epoch": 356.7260940032415,
      "grad_norm": 1.3670928478240967,
      "learning_rate": 1.4351248783652288e-05,
      "loss": 2.4965,
      "step": 1100500
    },
    {
      "epoch": 356.7585089141005,
      "grad_norm": 1.3098409175872803,
      "learning_rate": 1.4348005189750243e-05,
      "loss": 2.5236,
      "step": 1100600
    },
    {
      "epoch": 356.79092382495946,
      "grad_norm": 1.5368592739105225,
      "learning_rate": 1.43447615958482e-05,
      "loss": 2.5015,
      "step": 1100700
    },
    {
      "epoch": 356.8233387358185,
      "grad_norm": 1.360597848892212,
      "learning_rate": 1.4341518001946159e-05,
      "loss": 2.5205,
      "step": 1100800
    },
    {
      "epoch": 356.8557536466775,
      "grad_norm": 1.3138221502304077,
      "learning_rate": 1.4338274408044114e-05,
      "loss": 2.5131,
      "step": 1100900
    },
    {
      "epoch": 356.88816855753646,
      "grad_norm": 1.4018553495407104,
      "learning_rate": 1.433503081414207e-05,
      "loss": 2.5198,
      "step": 1101000
    },
    {
      "epoch": 356.9205834683955,
      "grad_norm": 1.4057563543319702,
      "learning_rate": 1.4331787220240025e-05,
      "loss": 2.506,
      "step": 1101100
    },
    {
      "epoch": 356.95299837925444,
      "grad_norm": 1.412856936454773,
      "learning_rate": 1.4328543626337984e-05,
      "loss": 2.5074,
      "step": 1101200
    },
    {
      "epoch": 356.98541329011346,
      "grad_norm": 1.526930809020996,
      "learning_rate": 1.4325300032435939e-05,
      "loss": 2.523,
      "step": 1101300
    },
    {
      "epoch": 357.0,
      "eval_bleu": 0.9968599628580181,
      "eval_loss": 4.223954677581787,
      "eval_runtime": 4.333,
      "eval_samples_per_second": 113.548,
      "eval_steps_per_second": 1.846,
      "step": 1101345
    },
    {
      "epoch": 357.0178282009725,
      "grad_norm": 1.2858631610870361,
      "learning_rate": 1.4322056438533896e-05,
      "loss": 2.4802,
      "step": 1101400
    },
    {
      "epoch": 357.05024311183143,
      "grad_norm": 1.3992092609405518,
      "learning_rate": 1.4318812844631851e-05,
      "loss": 2.4964,
      "step": 1101500
    },
    {
      "epoch": 357.08265802269045,
      "grad_norm": 1.3374804258346558,
      "learning_rate": 1.431556925072981e-05,
      "loss": 2.5118,
      "step": 1101600
    },
    {
      "epoch": 357.1150729335494,
      "grad_norm": 1.5149418115615845,
      "learning_rate": 1.4312358092766786e-05,
      "loss": 2.4936,
      "step": 1101700
    },
    {
      "epoch": 357.14748784440843,
      "grad_norm": 1.396445393562317,
      "learning_rate": 1.4309146934803762e-05,
      "loss": 2.5302,
      "step": 1101800
    },
    {
      "epoch": 357.17990275526745,
      "grad_norm": 1.3189140558242798,
      "learning_rate": 1.430590334090172e-05,
      "loss": 2.4915,
      "step": 1101900
    },
    {
      "epoch": 357.2123176661264,
      "grad_norm": 1.21745765209198,
      "learning_rate": 1.4302659746999678e-05,
      "loss": 2.5035,
      "step": 1102000
    },
    {
      "epoch": 357.2447325769854,
      "grad_norm": 1.2927393913269043,
      "learning_rate": 1.4299416153097634e-05,
      "loss": 2.506,
      "step": 1102100
    },
    {
      "epoch": 357.2771474878444,
      "grad_norm": 1.3352723121643066,
      "learning_rate": 1.4296172559195589e-05,
      "loss": 2.5209,
      "step": 1102200
    },
    {
      "epoch": 357.3095623987034,
      "grad_norm": 1.4964611530303955,
      "learning_rate": 1.4292928965293544e-05,
      "loss": 2.5111,
      "step": 1102300
    },
    {
      "epoch": 357.3419773095624,
      "grad_norm": 1.688441514968872,
      "learning_rate": 1.4289685371391503e-05,
      "loss": 2.5098,
      "step": 1102400
    },
    {
      "epoch": 357.3743922204214,
      "grad_norm": 1.2682420015335083,
      "learning_rate": 1.428644177748946e-05,
      "loss": 2.5073,
      "step": 1102500
    },
    {
      "epoch": 357.4068071312804,
      "grad_norm": 1.4066907167434692,
      "learning_rate": 1.4283198183587415e-05,
      "loss": 2.5136,
      "step": 1102600
    },
    {
      "epoch": 357.43922204213936,
      "grad_norm": 1.307388424873352,
      "learning_rate": 1.427995458968537e-05,
      "loss": 2.5063,
      "step": 1102700
    },
    {
      "epoch": 357.4716369529984,
      "grad_norm": 1.6446576118469238,
      "learning_rate": 1.427671099578333e-05,
      "loss": 2.4967,
      "step": 1102800
    },
    {
      "epoch": 357.5040518638574,
      "grad_norm": 1.3725773096084595,
      "learning_rate": 1.4273467401881285e-05,
      "loss": 2.5116,
      "step": 1102900
    },
    {
      "epoch": 357.53646677471636,
      "grad_norm": 1.5705450773239136,
      "learning_rate": 1.4270223807979242e-05,
      "loss": 2.4929,
      "step": 1103000
    },
    {
      "epoch": 357.5688816855754,
      "grad_norm": 1.603988528251648,
      "learning_rate": 1.4266980214077197e-05,
      "loss": 2.5325,
      "step": 1103100
    },
    {
      "epoch": 357.60129659643434,
      "grad_norm": 1.3539561033248901,
      "learning_rate": 1.4263736620175156e-05,
      "loss": 2.5198,
      "step": 1103200
    },
    {
      "epoch": 357.63371150729336,
      "grad_norm": 1.2930694818496704,
      "learning_rate": 1.4260493026273111e-05,
      "loss": 2.508,
      "step": 1103300
    },
    {
      "epoch": 357.6661264181524,
      "grad_norm": 1.4984245300292969,
      "learning_rate": 1.4257249432371067e-05,
      "loss": 2.5108,
      "step": 1103400
    },
    {
      "epoch": 357.69854132901133,
      "grad_norm": 1.5238322019577026,
      "learning_rate": 1.4254005838469025e-05,
      "loss": 2.5045,
      "step": 1103500
    },
    {
      "epoch": 357.73095623987035,
      "grad_norm": 1.6340854167938232,
      "learning_rate": 1.425076224456698e-05,
      "loss": 2.5129,
      "step": 1103600
    },
    {
      "epoch": 357.7633711507293,
      "grad_norm": 1.4445492029190063,
      "learning_rate": 1.4247518650664938e-05,
      "loss": 2.5112,
      "step": 1103700
    },
    {
      "epoch": 357.79578606158833,
      "grad_norm": 1.4106372594833374,
      "learning_rate": 1.4244275056762893e-05,
      "loss": 2.5064,
      "step": 1103800
    },
    {
      "epoch": 357.82820097244735,
      "grad_norm": 1.3127319812774658,
      "learning_rate": 1.4241031462860852e-05,
      "loss": 2.5182,
      "step": 1103900
    },
    {
      "epoch": 357.8606158833063,
      "grad_norm": 1.4306360483169556,
      "learning_rate": 1.4237787868958807e-05,
      "loss": 2.507,
      "step": 1104000
    },
    {
      "epoch": 357.8930307941653,
      "grad_norm": 1.3491641283035278,
      "learning_rate": 1.4234544275056763e-05,
      "loss": 2.4995,
      "step": 1104100
    },
    {
      "epoch": 357.9254457050243,
      "grad_norm": 1.529310703277588,
      "learning_rate": 1.423130068115472e-05,
      "loss": 2.5251,
      "step": 1104200
    },
    {
      "epoch": 357.9578606158833,
      "grad_norm": 1.5231038331985474,
      "learning_rate": 1.4228057087252677e-05,
      "loss": 2.4986,
      "step": 1104300
    },
    {
      "epoch": 357.9902755267423,
      "grad_norm": 1.3374311923980713,
      "learning_rate": 1.4224845929289655e-05,
      "loss": 2.5239,
      "step": 1104400
    },
    {
      "epoch": 358.0,
      "eval_bleu": 1.0219447832071324,
      "eval_loss": 4.218963146209717,
      "eval_runtime": 4.2402,
      "eval_samples_per_second": 116.034,
      "eval_steps_per_second": 1.887,
      "step": 1104430
    },
    {
      "epoch": 358.0226904376013,
      "grad_norm": 1.3257087469100952,
      "learning_rate": 1.422160233538761e-05,
      "loss": 2.5045,
      "step": 1104500
    },
    {
      "epoch": 358.0551053484603,
      "grad_norm": 1.372602939605713,
      "learning_rate": 1.4218358741485565e-05,
      "loss": 2.5049,
      "step": 1104600
    },
    {
      "epoch": 358.08752025931926,
      "grad_norm": 1.6235356330871582,
      "learning_rate": 1.4215115147583524e-05,
      "loss": 2.4854,
      "step": 1104700
    },
    {
      "epoch": 358.1199351701783,
      "grad_norm": 1.5948083400726318,
      "learning_rate": 1.421187155368148e-05,
      "loss": 2.5239,
      "step": 1104800
    },
    {
      "epoch": 358.1523500810373,
      "grad_norm": 1.5881648063659668,
      "learning_rate": 1.4208627959779436e-05,
      "loss": 2.513,
      "step": 1104900
    },
    {
      "epoch": 358.18476499189626,
      "grad_norm": 1.46698796749115,
      "learning_rate": 1.4205384365877392e-05,
      "loss": 2.4968,
      "step": 1105000
    },
    {
      "epoch": 358.2171799027553,
      "grad_norm": 1.406079649925232,
      "learning_rate": 1.420214077197535e-05,
      "loss": 2.5169,
      "step": 1105100
    },
    {
      "epoch": 358.24959481361424,
      "grad_norm": 1.5440593957901,
      "learning_rate": 1.4198897178073306e-05,
      "loss": 2.5026,
      "step": 1105200
    },
    {
      "epoch": 358.28200972447326,
      "grad_norm": 1.2880362272262573,
      "learning_rate": 1.4195653584171261e-05,
      "loss": 2.5016,
      "step": 1105300
    },
    {
      "epoch": 358.3144246353323,
      "grad_norm": 1.4995485544204712,
      "learning_rate": 1.419240999026922e-05,
      "loss": 2.4992,
      "step": 1105400
    },
    {
      "epoch": 358.34683954619123,
      "grad_norm": 1.4090064764022827,
      "learning_rate": 1.4189166396367175e-05,
      "loss": 2.4823,
      "step": 1105500
    },
    {
      "epoch": 358.37925445705025,
      "grad_norm": 1.2064464092254639,
      "learning_rate": 1.4185922802465132e-05,
      "loss": 2.5117,
      "step": 1105600
    },
    {
      "epoch": 358.4116693679092,
      "grad_norm": 1.599212884902954,
      "learning_rate": 1.4182679208563088e-05,
      "loss": 2.4919,
      "step": 1105700
    },
    {
      "epoch": 358.44408427876823,
      "grad_norm": 1.2605727910995483,
      "learning_rate": 1.4179435614661046e-05,
      "loss": 2.5048,
      "step": 1105800
    },
    {
      "epoch": 358.47649918962725,
      "grad_norm": 1.3693504333496094,
      "learning_rate": 1.4176192020759002e-05,
      "loss": 2.5078,
      "step": 1105900
    },
    {
      "epoch": 358.5089141004862,
      "grad_norm": 1.5098623037338257,
      "learning_rate": 1.4172948426856957e-05,
      "loss": 2.5136,
      "step": 1106000
    },
    {
      "epoch": 358.5413290113452,
      "grad_norm": 1.2208999395370483,
      "learning_rate": 1.4169704832954914e-05,
      "loss": 2.505,
      "step": 1106100
    },
    {
      "epoch": 358.5737439222042,
      "grad_norm": 1.3681560754776,
      "learning_rate": 1.4166493674991894e-05,
      "loss": 2.5245,
      "step": 1106200
    },
    {
      "epoch": 358.6061588330632,
      "grad_norm": 1.6175183057785034,
      "learning_rate": 1.4163250081089849e-05,
      "loss": 2.5088,
      "step": 1106300
    },
    {
      "epoch": 358.6385737439222,
      "grad_norm": 1.4043077230453491,
      "learning_rate": 1.4160006487187804e-05,
      "loss": 2.5117,
      "step": 1106400
    },
    {
      "epoch": 358.6709886547812,
      "grad_norm": 1.531860589981079,
      "learning_rate": 1.415676289328576e-05,
      "loss": 2.5029,
      "step": 1106500
    },
    {
      "epoch": 358.7034035656402,
      "grad_norm": 1.327309489250183,
      "learning_rate": 1.4153519299383719e-05,
      "loss": 2.5163,
      "step": 1106600
    },
    {
      "epoch": 358.73581847649916,
      "grad_norm": 1.4111062288284302,
      "learning_rate": 1.4150275705481676e-05,
      "loss": 2.501,
      "step": 1106700
    },
    {
      "epoch": 358.7682333873582,
      "grad_norm": 1.5728307962417603,
      "learning_rate": 1.4147032111579631e-05,
      "loss": 2.5057,
      "step": 1106800
    },
    {
      "epoch": 358.8006482982172,
      "grad_norm": 1.3603265285491943,
      "learning_rate": 1.4143788517677586e-05,
      "loss": 2.534,
      "step": 1106900
    },
    {
      "epoch": 358.83306320907616,
      "grad_norm": 1.466623067855835,
      "learning_rate": 1.4140544923775545e-05,
      "loss": 2.5438,
      "step": 1107000
    },
    {
      "epoch": 358.8654781199352,
      "grad_norm": 1.245107650756836,
      "learning_rate": 1.41373013298735e-05,
      "loss": 2.494,
      "step": 1107100
    },
    {
      "epoch": 358.8978930307942,
      "grad_norm": 1.3693320751190186,
      "learning_rate": 1.4134057735971456e-05,
      "loss": 2.5165,
      "step": 1107200
    },
    {
      "epoch": 358.93030794165315,
      "grad_norm": 1.2922769784927368,
      "learning_rate": 1.4130814142069413e-05,
      "loss": 2.5184,
      "step": 1107300
    },
    {
      "epoch": 358.9627228525122,
      "grad_norm": 1.1668068170547485,
      "learning_rate": 1.4127570548167372e-05,
      "loss": 2.5203,
      "step": 1107400
    },
    {
      "epoch": 358.99513776337113,
      "grad_norm": 1.4109065532684326,
      "learning_rate": 1.4124326954265327e-05,
      "loss": 2.5082,
      "step": 1107500
    },
    {
      "epoch": 359.0,
      "eval_bleu": 0.9776164858545432,
      "eval_loss": 4.220587730407715,
      "eval_runtime": 4.3746,
      "eval_samples_per_second": 112.467,
      "eval_steps_per_second": 1.829,
      "step": 1107515
    },
    {
      "epoch": 359.02755267423015,
      "grad_norm": 1.2882376909255981,
      "learning_rate": 1.4121083360363282e-05,
      "loss": 2.5066,
      "step": 1107600
    },
    {
      "epoch": 359.05996758508917,
      "grad_norm": 1.276586651802063,
      "learning_rate": 1.4117839766461241e-05,
      "loss": 2.5,
      "step": 1107700
    },
    {
      "epoch": 359.09238249594813,
      "grad_norm": 1.2783193588256836,
      "learning_rate": 1.4114596172559196e-05,
      "loss": 2.4896,
      "step": 1107800
    },
    {
      "epoch": 359.12479740680715,
      "grad_norm": 1.3952182531356812,
      "learning_rate": 1.4111352578657153e-05,
      "loss": 2.5141,
      "step": 1107900
    },
    {
      "epoch": 359.1572123176661,
      "grad_norm": 1.5174349546432495,
      "learning_rate": 1.4108108984755109e-05,
      "loss": 2.4999,
      "step": 1108000
    },
    {
      "epoch": 359.1896272285251,
      "grad_norm": 1.2095566987991333,
      "learning_rate": 1.4104865390853067e-05,
      "loss": 2.498,
      "step": 1108100
    },
    {
      "epoch": 359.22204213938414,
      "grad_norm": 1.5011646747589111,
      "learning_rate": 1.4101621796951023e-05,
      "loss": 2.5056,
      "step": 1108200
    },
    {
      "epoch": 359.2544570502431,
      "grad_norm": 1.2586371898651123,
      "learning_rate": 1.4098378203048978e-05,
      "loss": 2.5048,
      "step": 1108300
    },
    {
      "epoch": 359.2868719611021,
      "grad_norm": 1.6379538774490356,
      "learning_rate": 1.4095134609146934e-05,
      "loss": 2.4965,
      "step": 1108400
    },
    {
      "epoch": 359.3192868719611,
      "grad_norm": 1.433740258216858,
      "learning_rate": 1.4091891015244892e-05,
      "loss": 2.5126,
      "step": 1108500
    },
    {
      "epoch": 359.3517017828201,
      "grad_norm": 1.3079051971435547,
      "learning_rate": 1.408864742134285e-05,
      "loss": 2.5138,
      "step": 1108600
    },
    {
      "epoch": 359.3841166936791,
      "grad_norm": 1.5913670063018799,
      "learning_rate": 1.4085403827440805e-05,
      "loss": 2.5149,
      "step": 1108700
    },
    {
      "epoch": 359.4165316045381,
      "grad_norm": 1.431978464126587,
      "learning_rate": 1.408216023353876e-05,
      "loss": 2.5206,
      "step": 1108800
    },
    {
      "epoch": 359.4489465153971,
      "grad_norm": 1.536490797996521,
      "learning_rate": 1.4078916639636719e-05,
      "loss": 2.4993,
      "step": 1108900
    },
    {
      "epoch": 359.48136142625606,
      "grad_norm": 1.338592529296875,
      "learning_rate": 1.4075673045734674e-05,
      "loss": 2.5313,
      "step": 1109000
    },
    {
      "epoch": 359.5137763371151,
      "grad_norm": 1.3469483852386475,
      "learning_rate": 1.4072429451832631e-05,
      "loss": 2.5064,
      "step": 1109100
    },
    {
      "epoch": 359.5461912479741,
      "grad_norm": 1.2750790119171143,
      "learning_rate": 1.4069185857930588e-05,
      "loss": 2.5025,
      "step": 1109200
    },
    {
      "epoch": 359.57860615883305,
      "grad_norm": 1.4754252433776855,
      "learning_rate": 1.4065942264028545e-05,
      "loss": 2.4881,
      "step": 1109300
    },
    {
      "epoch": 359.6110210696921,
      "grad_norm": 1.8440847396850586,
      "learning_rate": 1.40626986701265e-05,
      "loss": 2.5009,
      "step": 1109400
    },
    {
      "epoch": 359.64343598055103,
      "grad_norm": 1.3210234642028809,
      "learning_rate": 1.4059455076224456e-05,
      "loss": 2.5065,
      "step": 1109500
    },
    {
      "epoch": 359.67585089141005,
      "grad_norm": 1.1490615606307983,
      "learning_rate": 1.4056211482322415e-05,
      "loss": 2.5149,
      "step": 1109600
    },
    {
      "epoch": 359.70826580226907,
      "grad_norm": 1.2518012523651123,
      "learning_rate": 1.405296788842037e-05,
      "loss": 2.5103,
      "step": 1109700
    },
    {
      "epoch": 359.74068071312803,
      "grad_norm": 1.7514358758926392,
      "learning_rate": 1.4049724294518327e-05,
      "loss": 2.5318,
      "step": 1109800
    },
    {
      "epoch": 359.77309562398705,
      "grad_norm": 1.3900598287582397,
      "learning_rate": 1.4046480700616283e-05,
      "loss": 2.4956,
      "step": 1109900
    },
    {
      "epoch": 359.805510534846,
      "grad_norm": 1.431640625,
      "learning_rate": 1.4043237106714241e-05,
      "loss": 2.5261,
      "step": 1110000
    },
    {
      "epoch": 359.837925445705,
      "grad_norm": 1.3648850917816162,
      "learning_rate": 1.4039993512812197e-05,
      "loss": 2.5061,
      "step": 1110100
    },
    {
      "epoch": 359.87034035656404,
      "grad_norm": 1.819242000579834,
      "learning_rate": 1.4036749918910152e-05,
      "loss": 2.5133,
      "step": 1110200
    },
    {
      "epoch": 359.902755267423,
      "grad_norm": 1.3243987560272217,
      "learning_rate": 1.4033506325008109e-05,
      "loss": 2.5051,
      "step": 1110300
    },
    {
      "epoch": 359.935170178282,
      "grad_norm": 1.465018391609192,
      "learning_rate": 1.4030262731106068e-05,
      "loss": 2.5113,
      "step": 1110400
    },
    {
      "epoch": 359.967585089141,
      "grad_norm": 1.3315479755401611,
      "learning_rate": 1.4027019137204023e-05,
      "loss": 2.4943,
      "step": 1110500
    },
    {
      "epoch": 360.0,
      "grad_norm": 1.4885822534561157,
      "learning_rate": 1.4023775543301978e-05,
      "loss": 2.5187,
      "step": 1110600
    },
    {
      "epoch": 360.0,
      "eval_bleu": 1.1020226099928232,
      "eval_loss": 4.22186279296875,
      "eval_runtime": 4.5747,
      "eval_samples_per_second": 107.549,
      "eval_steps_per_second": 1.749,
      "step": 1110600
    },
    {
      "epoch": 360.032414910859,
      "grad_norm": 1.4997143745422363,
      "learning_rate": 1.4020531949399937e-05,
      "loss": 2.4932,
      "step": 1110700
    },
    {
      "epoch": 360.064829821718,
      "grad_norm": 1.5338228940963745,
      "learning_rate": 1.4017288355497893e-05,
      "loss": 2.5027,
      "step": 1110800
    },
    {
      "epoch": 360.097244732577,
      "grad_norm": 1.364334225654602,
      "learning_rate": 1.4014044761595848e-05,
      "loss": 2.5062,
      "step": 1110900
    },
    {
      "epoch": 360.12965964343596,
      "grad_norm": 1.4257338047027588,
      "learning_rate": 1.4010801167693805e-05,
      "loss": 2.4997,
      "step": 1111000
    },
    {
      "epoch": 360.162074554295,
      "grad_norm": 1.3453354835510254,
      "learning_rate": 1.4007557573791764e-05,
      "loss": 2.5175,
      "step": 1111100
    },
    {
      "epoch": 360.194489465154,
      "grad_norm": 1.3956224918365479,
      "learning_rate": 1.4004313979889719e-05,
      "loss": 2.5099,
      "step": 1111200
    },
    {
      "epoch": 360.22690437601295,
      "grad_norm": 1.4153822660446167,
      "learning_rate": 1.4001070385987674e-05,
      "loss": 2.4843,
      "step": 1111300
    },
    {
      "epoch": 360.25931928687197,
      "grad_norm": 1.6146725416183472,
      "learning_rate": 1.399782679208563e-05,
      "loss": 2.5311,
      "step": 1111400
    },
    {
      "epoch": 360.29173419773093,
      "grad_norm": 1.3287663459777832,
      "learning_rate": 1.3994583198183589e-05,
      "loss": 2.5233,
      "step": 1111500
    },
    {
      "epoch": 360.32414910858995,
      "grad_norm": 1.4505482912063599,
      "learning_rate": 1.3991339604281546e-05,
      "loss": 2.505,
      "step": 1111600
    },
    {
      "epoch": 360.35656401944897,
      "grad_norm": 1.2558377981185913,
      "learning_rate": 1.3988096010379501e-05,
      "loss": 2.5086,
      "step": 1111700
    },
    {
      "epoch": 360.3889789303079,
      "grad_norm": 1.2489652633666992,
      "learning_rate": 1.3984852416477456e-05,
      "loss": 2.5066,
      "step": 1111800
    },
    {
      "epoch": 360.42139384116695,
      "grad_norm": 1.2957878112792969,
      "learning_rate": 1.3981608822575415e-05,
      "loss": 2.494,
      "step": 1111900
    },
    {
      "epoch": 360.4538087520259,
      "grad_norm": 1.3721181154251099,
      "learning_rate": 1.397836522867337e-05,
      "loss": 2.5101,
      "step": 1112000
    },
    {
      "epoch": 360.4862236628849,
      "grad_norm": 1.301628589630127,
      "learning_rate": 1.3975121634771326e-05,
      "loss": 2.4794,
      "step": 1112100
    },
    {
      "epoch": 360.51863857374394,
      "grad_norm": 1.2275809049606323,
      "learning_rate": 1.3971878040869284e-05,
      "loss": 2.4856,
      "step": 1112200
    },
    {
      "epoch": 360.5510534846029,
      "grad_norm": 1.5267162322998047,
      "learning_rate": 1.3968634446967242e-05,
      "loss": 2.5172,
      "step": 1112300
    },
    {
      "epoch": 360.5834683954619,
      "grad_norm": 1.5754222869873047,
      "learning_rate": 1.3965390853065197e-05,
      "loss": 2.5111,
      "step": 1112400
    },
    {
      "epoch": 360.6158833063209,
      "grad_norm": 1.3094433546066284,
      "learning_rate": 1.3962147259163152e-05,
      "loss": 2.5025,
      "step": 1112500
    },
    {
      "epoch": 360.6482982171799,
      "grad_norm": 1.3295235633850098,
      "learning_rate": 1.3958903665261111e-05,
      "loss": 2.5075,
      "step": 1112600
    },
    {
      "epoch": 360.6807131280389,
      "grad_norm": 1.5744527578353882,
      "learning_rate": 1.3955660071359066e-05,
      "loss": 2.5109,
      "step": 1112700
    },
    {
      "epoch": 360.7131280388979,
      "grad_norm": 1.3795816898345947,
      "learning_rate": 1.3952416477457023e-05,
      "loss": 2.4898,
      "step": 1112800
    },
    {
      "epoch": 360.7455429497569,
      "grad_norm": 1.3557475805282593,
      "learning_rate": 1.3949172883554979e-05,
      "loss": 2.5054,
      "step": 1112900
    },
    {
      "epoch": 360.77795786061586,
      "grad_norm": 1.6459591388702393,
      "learning_rate": 1.3945929289652937e-05,
      "loss": 2.5061,
      "step": 1113000
    },
    {
      "epoch": 360.8103727714749,
      "grad_norm": 1.301881194114685,
      "learning_rate": 1.3942685695750893e-05,
      "loss": 2.4956,
      "step": 1113100
    },
    {
      "epoch": 360.8427876823339,
      "grad_norm": 1.3250588178634644,
      "learning_rate": 1.3939442101848848e-05,
      "loss": 2.5192,
      "step": 1113200
    },
    {
      "epoch": 360.87520259319285,
      "grad_norm": 1.4423980712890625,
      "learning_rate": 1.3936198507946804e-05,
      "loss": 2.5113,
      "step": 1113300
    },
    {
      "epoch": 360.90761750405187,
      "grad_norm": 1.5785232782363892,
      "learning_rate": 1.3932954914044762e-05,
      "loss": 2.5234,
      "step": 1113400
    },
    {
      "epoch": 360.94003241491083,
      "grad_norm": 1.542318344116211,
      "learning_rate": 1.392971132014272e-05,
      "loss": 2.504,
      "step": 1113500
    },
    {
      "epoch": 360.97244732576985,
      "grad_norm": 1.2582734823226929,
      "learning_rate": 1.3926467726240675e-05,
      "loss": 2.5027,
      "step": 1113600
    },
    {
      "epoch": 361.0,
      "eval_bleu": 1.1090999970428057,
      "eval_loss": 4.224702835083008,
      "eval_runtime": 4.6364,
      "eval_samples_per_second": 106.117,
      "eval_steps_per_second": 1.725,
      "step": 1113685
    },
    {
      "epoch": 361.00486223662887,
      "grad_norm": 1.5638374090194702,
      "learning_rate": 1.3923224132338633e-05,
      "loss": 2.5226,
      "step": 1113700
    },
    {
      "epoch": 361.0372771474878,
      "grad_norm": 1.4298815727233887,
      "learning_rate": 1.3919980538436589e-05,
      "loss": 2.5005,
      "step": 1113800
    },
    {
      "epoch": 361.06969205834685,
      "grad_norm": 1.3657885789871216,
      "learning_rate": 1.3916736944534544e-05,
      "loss": 2.4928,
      "step": 1113900
    },
    {
      "epoch": 361.1021069692058,
      "grad_norm": 1.2712174654006958,
      "learning_rate": 1.3913493350632501e-05,
      "loss": 2.5066,
      "step": 1114000
    },
    {
      "epoch": 361.1345218800648,
      "grad_norm": 1.3744182586669922,
      "learning_rate": 1.3910249756730458e-05,
      "loss": 2.4914,
      "step": 1114100
    },
    {
      "epoch": 361.16693679092384,
      "grad_norm": 1.395363688468933,
      "learning_rate": 1.3907006162828415e-05,
      "loss": 2.5261,
      "step": 1114200
    },
    {
      "epoch": 361.1993517017828,
      "grad_norm": 1.3358443975448608,
      "learning_rate": 1.3903827440804412e-05,
      "loss": 2.5208,
      "step": 1114300
    },
    {
      "epoch": 361.2317666126418,
      "grad_norm": 1.2586413621902466,
      "learning_rate": 1.3900583846902367e-05,
      "loss": 2.509,
      "step": 1114400
    },
    {
      "epoch": 361.26418152350084,
      "grad_norm": 1.6770535707473755,
      "learning_rate": 1.3897340253000325e-05,
      "loss": 2.5135,
      "step": 1114500
    },
    {
      "epoch": 361.2965964343598,
      "grad_norm": 1.3638389110565186,
      "learning_rate": 1.3894096659098282e-05,
      "loss": 2.4798,
      "step": 1114600
    },
    {
      "epoch": 361.3290113452188,
      "grad_norm": 1.4304659366607666,
      "learning_rate": 1.3890853065196239e-05,
      "loss": 2.5191,
      "step": 1114700
    },
    {
      "epoch": 361.3614262560778,
      "grad_norm": 1.3061962127685547,
      "learning_rate": 1.3887609471294194e-05,
      "loss": 2.4849,
      "step": 1114800
    },
    {
      "epoch": 361.3938411669368,
      "grad_norm": 1.2986091375350952,
      "learning_rate": 1.3884365877392153e-05,
      "loss": 2.4989,
      "step": 1114900
    },
    {
      "epoch": 361.4262560777958,
      "grad_norm": 1.2654846906661987,
      "learning_rate": 1.3881122283490108e-05,
      "loss": 2.5115,
      "step": 1115000
    },
    {
      "epoch": 361.4586709886548,
      "grad_norm": 1.3412714004516602,
      "learning_rate": 1.3877878689588063e-05,
      "loss": 2.5092,
      "step": 1115100
    },
    {
      "epoch": 361.4910858995138,
      "grad_norm": 1.4963879585266113,
      "learning_rate": 1.387463509568602e-05,
      "loss": 2.513,
      "step": 1115200
    },
    {
      "epoch": 361.52350081037275,
      "grad_norm": 1.4553371667861938,
      "learning_rate": 1.387139150178398e-05,
      "loss": 2.5161,
      "step": 1115300
    },
    {
      "epoch": 361.55591572123177,
      "grad_norm": 1.376907229423523,
      "learning_rate": 1.3868147907881935e-05,
      "loss": 2.4977,
      "step": 1115400
    },
    {
      "epoch": 361.5883306320908,
      "grad_norm": 1.4369219541549683,
      "learning_rate": 1.386490431397989e-05,
      "loss": 2.5097,
      "step": 1115500
    },
    {
      "epoch": 361.62074554294975,
      "grad_norm": 1.4249482154846191,
      "learning_rate": 1.3861660720077845e-05,
      "loss": 2.4798,
      "step": 1115600
    },
    {
      "epoch": 361.65316045380877,
      "grad_norm": 1.2106382846832275,
      "learning_rate": 1.3858417126175804e-05,
      "loss": 2.5085,
      "step": 1115700
    },
    {
      "epoch": 361.6855753646677,
      "grad_norm": 1.2592144012451172,
      "learning_rate": 1.385517353227376e-05,
      "loss": 2.5102,
      "step": 1115800
    },
    {
      "epoch": 361.71799027552674,
      "grad_norm": 1.4354331493377686,
      "learning_rate": 1.3851929938371716e-05,
      "loss": 2.5215,
      "step": 1115900
    },
    {
      "epoch": 361.75040518638576,
      "grad_norm": 1.1475406885147095,
      "learning_rate": 1.3848686344469672e-05,
      "loss": 2.5011,
      "step": 1116000
    },
    {
      "epoch": 361.7828200972447,
      "grad_norm": 1.3253772258758545,
      "learning_rate": 1.384544275056763e-05,
      "loss": 2.5249,
      "step": 1116100
    },
    {
      "epoch": 361.81523500810374,
      "grad_norm": 1.748450517654419,
      "learning_rate": 1.3842199156665586e-05,
      "loss": 2.4955,
      "step": 1116200
    },
    {
      "epoch": 361.8476499189627,
      "grad_norm": 1.3948743343353271,
      "learning_rate": 1.3838955562763541e-05,
      "loss": 2.4941,
      "step": 1116300
    },
    {
      "epoch": 361.8800648298217,
      "grad_norm": 1.1639342308044434,
      "learning_rate": 1.3835744404800519e-05,
      "loss": 2.4928,
      "step": 1116400
    },
    {
      "epoch": 361.91247974068074,
      "grad_norm": 1.4463987350463867,
      "learning_rate": 1.3832500810898478e-05,
      "loss": 2.481,
      "step": 1116500
    },
    {
      "epoch": 361.9448946515397,
      "grad_norm": 1.335539698600769,
      "learning_rate": 1.3829257216996433e-05,
      "loss": 2.5071,
      "step": 1116600
    },
    {
      "epoch": 361.9773095623987,
      "grad_norm": 1.4899262189865112,
      "learning_rate": 1.3826013623094389e-05,
      "loss": 2.5121,
      "step": 1116700
    },
    {
      "epoch": 362.0,
      "eval_bleu": 1.0499568964615624,
      "eval_loss": 4.226250648498535,
      "eval_runtime": 4.7857,
      "eval_samples_per_second": 102.805,
      "eval_steps_per_second": 1.672,
      "step": 1116770
    },
    {
      "epoch": 362.0097244732577,
      "grad_norm": 1.3409299850463867,
      "learning_rate": 1.3822770029192344e-05,
      "loss": 2.5105,
      "step": 1116800
    },
    {
      "epoch": 362.0421393841167,
      "grad_norm": 1.2326406240463257,
      "learning_rate": 1.3819558871229323e-05,
      "loss": 2.5145,
      "step": 1116900
    },
    {
      "epoch": 362.0745542949757,
      "grad_norm": 1.4710665941238403,
      "learning_rate": 1.381631527732728e-05,
      "loss": 2.4988,
      "step": 1117000
    },
    {
      "epoch": 362.1069692058347,
      "grad_norm": 1.2479820251464844,
      "learning_rate": 1.3813071683425236e-05,
      "loss": 2.4885,
      "step": 1117100
    },
    {
      "epoch": 362.1393841166937,
      "grad_norm": 1.2733657360076904,
      "learning_rate": 1.3809828089523191e-05,
      "loss": 2.4747,
      "step": 1117200
    },
    {
      "epoch": 362.17179902755265,
      "grad_norm": 1.3560113906860352,
      "learning_rate": 1.380658449562115e-05,
      "loss": 2.5104,
      "step": 1117300
    },
    {
      "epoch": 362.20421393841167,
      "grad_norm": 1.299705147743225,
      "learning_rate": 1.3803340901719105e-05,
      "loss": 2.5165,
      "step": 1117400
    },
    {
      "epoch": 362.2366288492707,
      "grad_norm": 1.3552242517471313,
      "learning_rate": 1.3800097307817062e-05,
      "loss": 2.5143,
      "step": 1117500
    },
    {
      "epoch": 362.26904376012965,
      "grad_norm": 1.523856520652771,
      "learning_rate": 1.3796853713915018e-05,
      "loss": 2.5147,
      "step": 1117600
    },
    {
      "epoch": 362.30145867098867,
      "grad_norm": 1.4819728136062622,
      "learning_rate": 1.3793610120012976e-05,
      "loss": 2.5044,
      "step": 1117700
    },
    {
      "epoch": 362.3338735818476,
      "grad_norm": 1.4942785501480103,
      "learning_rate": 1.3790366526110932e-05,
      "loss": 2.5036,
      "step": 1117800
    },
    {
      "epoch": 362.36628849270664,
      "grad_norm": 1.5737462043762207,
      "learning_rate": 1.3787122932208887e-05,
      "loss": 2.5041,
      "step": 1117900
    },
    {
      "epoch": 362.39870340356566,
      "grad_norm": 1.4094101190567017,
      "learning_rate": 1.3783879338306846e-05,
      "loss": 2.4967,
      "step": 1118000
    },
    {
      "epoch": 362.4311183144246,
      "grad_norm": 1.562483549118042,
      "learning_rate": 1.3780635744404801e-05,
      "loss": 2.4956,
      "step": 1118100
    },
    {
      "epoch": 362.46353322528364,
      "grad_norm": 1.576069951057434,
      "learning_rate": 1.3777392150502758e-05,
      "loss": 2.5127,
      "step": 1118200
    },
    {
      "epoch": 362.4959481361426,
      "grad_norm": 1.2949460744857788,
      "learning_rate": 1.3774148556600714e-05,
      "loss": 2.5075,
      "step": 1118300
    },
    {
      "epoch": 362.5283630470016,
      "grad_norm": 1.390999436378479,
      "learning_rate": 1.3770904962698672e-05,
      "loss": 2.5446,
      "step": 1118400
    },
    {
      "epoch": 362.56077795786064,
      "grad_norm": 1.304750919342041,
      "learning_rate": 1.3767661368796628e-05,
      "loss": 2.4869,
      "step": 1118500
    },
    {
      "epoch": 362.5931928687196,
      "grad_norm": 1.4999735355377197,
      "learning_rate": 1.3764417774894583e-05,
      "loss": 2.4999,
      "step": 1118600
    },
    {
      "epoch": 362.6256077795786,
      "grad_norm": 1.2900041341781616,
      "learning_rate": 1.376117418099254e-05,
      "loss": 2.497,
      "step": 1118700
    },
    {
      "epoch": 362.6580226904376,
      "grad_norm": 1.2880613803863525,
      "learning_rate": 1.3757930587090497e-05,
      "loss": 2.5009,
      "step": 1118800
    },
    {
      "epoch": 362.6904376012966,
      "grad_norm": 1.3345993757247925,
      "learning_rate": 1.3754686993188454e-05,
      "loss": 2.4901,
      "step": 1118900
    },
    {
      "epoch": 362.7228525121556,
      "grad_norm": 1.3677741289138794,
      "learning_rate": 1.375144339928641e-05,
      "loss": 2.5005,
      "step": 1119000
    },
    {
      "epoch": 362.7552674230146,
      "grad_norm": 1.3683606386184692,
      "learning_rate": 1.3748199805384365e-05,
      "loss": 2.5161,
      "step": 1119100
    },
    {
      "epoch": 362.7876823338736,
      "grad_norm": 1.6876471042633057,
      "learning_rate": 1.3744956211482324e-05,
      "loss": 2.5168,
      "step": 1119200
    },
    {
      "epoch": 362.82009724473255,
      "grad_norm": 1.6810423135757446,
      "learning_rate": 1.3741712617580279e-05,
      "loss": 2.4911,
      "step": 1119300
    },
    {
      "epoch": 362.85251215559157,
      "grad_norm": 1.557325839996338,
      "learning_rate": 1.3738469023678236e-05,
      "loss": 2.501,
      "step": 1119400
    },
    {
      "epoch": 362.8849270664506,
      "grad_norm": 1.7438000440597534,
      "learning_rate": 1.3735225429776195e-05,
      "loss": 2.5152,
      "step": 1119500
    },
    {
      "epoch": 362.91734197730955,
      "grad_norm": 1.285386323928833,
      "learning_rate": 1.373198183587415e-05,
      "loss": 2.5051,
      "step": 1119600
    },
    {
      "epoch": 362.94975688816857,
      "grad_norm": 1.4866293668746948,
      "learning_rate": 1.3728738241972105e-05,
      "loss": 2.5159,
      "step": 1119700
    },
    {
      "epoch": 362.9821717990275,
      "grad_norm": 1.3011049032211304,
      "learning_rate": 1.372549464807006e-05,
      "loss": 2.5241,
      "step": 1119800
    },
    {
      "epoch": 363.0,
      "eval_bleu": 0.9190696240524799,
      "eval_loss": 4.220195770263672,
      "eval_runtime": 4.5501,
      "eval_samples_per_second": 108.13,
      "eval_steps_per_second": 1.758,
      "step": 1119855
    },
    {
      "epoch": 363.01458670988654,
      "grad_norm": 1.4428502321243286,
      "learning_rate": 1.372225105416802e-05,
      "loss": 2.4958,
      "step": 1119900
    },
    {
      "epoch": 363.04700162074556,
      "grad_norm": 1.387070655822754,
      "learning_rate": 1.3719007460265975e-05,
      "loss": 2.5101,
      "step": 1120000
    },
    {
      "epoch": 363.0794165316045,
      "grad_norm": 1.465361475944519,
      "learning_rate": 1.3715763866363932e-05,
      "loss": 2.5218,
      "step": 1120100
    },
    {
      "epoch": 363.11183144246354,
      "grad_norm": 1.4721620082855225,
      "learning_rate": 1.3712520272461887e-05,
      "loss": 2.4951,
      "step": 1120200
    },
    {
      "epoch": 363.1442463533225,
      "grad_norm": 1.35037362575531,
      "learning_rate": 1.3709276678559846e-05,
      "loss": 2.504,
      "step": 1120300
    },
    {
      "epoch": 363.1766612641815,
      "grad_norm": 1.7219085693359375,
      "learning_rate": 1.3706033084657801e-05,
      "loss": 2.4764,
      "step": 1120400
    },
    {
      "epoch": 363.20907617504054,
      "grad_norm": 1.2397310733795166,
      "learning_rate": 1.3702789490755757e-05,
      "loss": 2.5166,
      "step": 1120500
    },
    {
      "epoch": 363.2414910858995,
      "grad_norm": 1.4479267597198486,
      "learning_rate": 1.3699545896853714e-05,
      "loss": 2.5095,
      "step": 1120600
    },
    {
      "epoch": 363.2739059967585,
      "grad_norm": 1.5183194875717163,
      "learning_rate": 1.3696302302951673e-05,
      "loss": 2.5187,
      "step": 1120700
    },
    {
      "epoch": 363.3063209076175,
      "grad_norm": 1.3291782140731812,
      "learning_rate": 1.3693091144988649e-05,
      "loss": 2.5342,
      "step": 1120800
    },
    {
      "epoch": 363.3387358184765,
      "grad_norm": 1.5515981912612915,
      "learning_rate": 1.3689847551086604e-05,
      "loss": 2.4872,
      "step": 1120900
    },
    {
      "epoch": 363.3711507293355,
      "grad_norm": 1.2798457145690918,
      "learning_rate": 1.368660395718456e-05,
      "loss": 2.4784,
      "step": 1121000
    },
    {
      "epoch": 363.4035656401945,
      "grad_norm": 1.4238156080245972,
      "learning_rate": 1.3683360363282518e-05,
      "loss": 2.4959,
      "step": 1121100
    },
    {
      "epoch": 363.4359805510535,
      "grad_norm": 1.5751677751541138,
      "learning_rate": 1.3680116769380475e-05,
      "loss": 2.5007,
      "step": 1121200
    },
    {
      "epoch": 363.4683954619125,
      "grad_norm": 1.2795122861862183,
      "learning_rate": 1.367687317547843e-05,
      "loss": 2.5171,
      "step": 1121300
    },
    {
      "epoch": 363.50081037277147,
      "grad_norm": 1.3000215291976929,
      "learning_rate": 1.3673629581576386e-05,
      "loss": 2.5202,
      "step": 1121400
    },
    {
      "epoch": 363.5332252836305,
      "grad_norm": 1.2872196435928345,
      "learning_rate": 1.3670385987674345e-05,
      "loss": 2.499,
      "step": 1121500
    },
    {
      "epoch": 363.56564019448945,
      "grad_norm": 1.5303319692611694,
      "learning_rate": 1.36671423937723e-05,
      "loss": 2.5088,
      "step": 1121600
    },
    {
      "epoch": 363.59805510534846,
      "grad_norm": 1.4913450479507446,
      "learning_rate": 1.3663898799870255e-05,
      "loss": 2.4999,
      "step": 1121700
    },
    {
      "epoch": 363.6304700162075,
      "grad_norm": 1.683294653892517,
      "learning_rate": 1.3660655205968214e-05,
      "loss": 2.5195,
      "step": 1121800
    },
    {
      "epoch": 363.66288492706644,
      "grad_norm": 1.286854863166809,
      "learning_rate": 1.3657411612066171e-05,
      "loss": 2.4971,
      "step": 1121900
    },
    {
      "epoch": 363.69529983792546,
      "grad_norm": 1.3324769735336304,
      "learning_rate": 1.3654168018164126e-05,
      "loss": 2.5015,
      "step": 1122000
    },
    {
      "epoch": 363.7277147487844,
      "grad_norm": 1.7218260765075684,
      "learning_rate": 1.3650956860201103e-05,
      "loss": 2.5179,
      "step": 1122100
    },
    {
      "epoch": 363.76012965964344,
      "grad_norm": 1.4320013523101807,
      "learning_rate": 1.3647713266299058e-05,
      "loss": 2.4936,
      "step": 1122200
    },
    {
      "epoch": 363.79254457050246,
      "grad_norm": 1.6940698623657227,
      "learning_rate": 1.3644469672397017e-05,
      "loss": 2.5044,
      "step": 1122300
    },
    {
      "epoch": 363.8249594813614,
      "grad_norm": 1.5723094940185547,
      "learning_rate": 1.3641226078494974e-05,
      "loss": 2.4943,
      "step": 1122400
    },
    {
      "epoch": 363.85737439222044,
      "grad_norm": 1.6417890787124634,
      "learning_rate": 1.3637982484592929e-05,
      "loss": 2.4908,
      "step": 1122500
    },
    {
      "epoch": 363.8897893030794,
      "grad_norm": 1.710087537765503,
      "learning_rate": 1.3634738890690888e-05,
      "loss": 2.5147,
      "step": 1122600
    },
    {
      "epoch": 363.9222042139384,
      "grad_norm": 1.3037258386611938,
      "learning_rate": 1.3631495296788843e-05,
      "loss": 2.4921,
      "step": 1122700
    },
    {
      "epoch": 363.95461912479743,
      "grad_norm": 1.3857024908065796,
      "learning_rate": 1.3628251702886799e-05,
      "loss": 2.4977,
      "step": 1122800
    },
    {
      "epoch": 363.9870340356564,
      "grad_norm": 1.7228964567184448,
      "learning_rate": 1.3625008108984756e-05,
      "loss": 2.5094,
      "step": 1122900
    },
    {
      "epoch": 364.0,
      "eval_bleu": 0.92529676303447,
      "eval_loss": 4.223520278930664,
      "eval_runtime": 4.4868,
      "eval_samples_per_second": 109.656,
      "eval_steps_per_second": 1.783,
      "step": 1122940
    },
    {
      "epoch": 364.0194489465154,
      "grad_norm": 1.493309736251831,
      "learning_rate": 1.3621764515082713e-05,
      "loss": 2.4947,
      "step": 1123000
    },
    {
      "epoch": 364.05186385737437,
      "grad_norm": 1.3779947757720947,
      "learning_rate": 1.361852092118067e-05,
      "loss": 2.5072,
      "step": 1123100
    },
    {
      "epoch": 364.0842787682334,
      "grad_norm": 1.2068119049072266,
      "learning_rate": 1.3615277327278625e-05,
      "loss": 2.5154,
      "step": 1123200
    },
    {
      "epoch": 364.1166936790924,
      "grad_norm": 1.4264163970947266,
      "learning_rate": 1.361203373337658e-05,
      "loss": 2.5007,
      "step": 1123300
    },
    {
      "epoch": 364.14910858995137,
      "grad_norm": 1.2943127155303955,
      "learning_rate": 1.360879013947454e-05,
      "loss": 2.5069,
      "step": 1123400
    },
    {
      "epoch": 364.1815235008104,
      "grad_norm": 1.2650643587112427,
      "learning_rate": 1.3605546545572495e-05,
      "loss": 2.5114,
      "step": 1123500
    },
    {
      "epoch": 364.21393841166935,
      "grad_norm": 1.4056322574615479,
      "learning_rate": 1.3602302951670452e-05,
      "loss": 2.4884,
      "step": 1123600
    },
    {
      "epoch": 364.24635332252836,
      "grad_norm": 1.2260890007019043,
      "learning_rate": 1.3599059357768407e-05,
      "loss": 2.505,
      "step": 1123700
    },
    {
      "epoch": 364.2787682333874,
      "grad_norm": 1.3103893995285034,
      "learning_rate": 1.3595815763866366e-05,
      "loss": 2.4954,
      "step": 1123800
    },
    {
      "epoch": 364.31118314424634,
      "grad_norm": 1.521666407585144,
      "learning_rate": 1.3592572169964321e-05,
      "loss": 2.5158,
      "step": 1123900
    },
    {
      "epoch": 364.34359805510536,
      "grad_norm": 1.3790199756622314,
      "learning_rate": 1.3589328576062276e-05,
      "loss": 2.501,
      "step": 1124000
    },
    {
      "epoch": 364.3760129659643,
      "grad_norm": 1.469741940498352,
      "learning_rate": 1.3586117418099254e-05,
      "loss": 2.5163,
      "step": 1124100
    },
    {
      "epoch": 364.40842787682334,
      "grad_norm": 1.4135510921478271,
      "learning_rate": 1.3582873824197211e-05,
      "loss": 2.4905,
      "step": 1124200
    },
    {
      "epoch": 364.44084278768236,
      "grad_norm": 1.4258151054382324,
      "learning_rate": 1.3579630230295168e-05,
      "loss": 2.5107,
      "step": 1124300
    },
    {
      "epoch": 364.4732576985413,
      "grad_norm": 1.385129690170288,
      "learning_rate": 1.3576386636393124e-05,
      "loss": 2.4706,
      "step": 1124400
    },
    {
      "epoch": 364.50567260940034,
      "grad_norm": 1.7991870641708374,
      "learning_rate": 1.3573143042491082e-05,
      "loss": 2.4868,
      "step": 1124500
    },
    {
      "epoch": 364.5380875202593,
      "grad_norm": 1.334358811378479,
      "learning_rate": 1.3569899448589038e-05,
      "loss": 2.5032,
      "step": 1124600
    },
    {
      "epoch": 364.5705024311183,
      "grad_norm": 1.893113136291504,
      "learning_rate": 1.3566655854686993e-05,
      "loss": 2.5009,
      "step": 1124700
    },
    {
      "epoch": 364.60291734197733,
      "grad_norm": 1.4520663022994995,
      "learning_rate": 1.356341226078495e-05,
      "loss": 2.4963,
      "step": 1124800
    },
    {
      "epoch": 364.6353322528363,
      "grad_norm": 1.4703515768051147,
      "learning_rate": 1.3560168666882909e-05,
      "loss": 2.5197,
      "step": 1124900
    },
    {
      "epoch": 364.6677471636953,
      "grad_norm": 1.4522228240966797,
      "learning_rate": 1.3556925072980864e-05,
      "loss": 2.5043,
      "step": 1125000
    },
    {
      "epoch": 364.70016207455427,
      "grad_norm": 1.225175142288208,
      "learning_rate": 1.355368147907882e-05,
      "loss": 2.521,
      "step": 1125100
    },
    {
      "epoch": 364.7325769854133,
      "grad_norm": 1.631464958190918,
      "learning_rate": 1.3550437885176775e-05,
      "loss": 2.5168,
      "step": 1125200
    },
    {
      "epoch": 364.7649918962723,
      "grad_norm": 1.3370193243026733,
      "learning_rate": 1.3547194291274734e-05,
      "loss": 2.4866,
      "step": 1125300
    },
    {
      "epoch": 364.79740680713127,
      "grad_norm": 1.2835719585418701,
      "learning_rate": 1.3543950697372689e-05,
      "loss": 2.5295,
      "step": 1125400
    },
    {
      "epoch": 364.8298217179903,
      "grad_norm": 1.5199719667434692,
      "learning_rate": 1.3540707103470646e-05,
      "loss": 2.5018,
      "step": 1125500
    },
    {
      "epoch": 364.86223662884925,
      "grad_norm": 1.5343796014785767,
      "learning_rate": 1.3537463509568601e-05,
      "loss": 2.4907,
      "step": 1125600
    },
    {
      "epoch": 364.89465153970826,
      "grad_norm": 1.3710269927978516,
      "learning_rate": 1.353421991566656e-05,
      "loss": 2.5088,
      "step": 1125700
    },
    {
      "epoch": 364.9270664505673,
      "grad_norm": 1.362115502357483,
      "learning_rate": 1.3530976321764516e-05,
      "loss": 2.4999,
      "step": 1125800
    },
    {
      "epoch": 364.95948136142624,
      "grad_norm": 1.5378715991973877,
      "learning_rate": 1.3527732727862471e-05,
      "loss": 2.5085,
      "step": 1125900
    },
    {
      "epoch": 364.99189627228526,
      "grad_norm": 1.3315683603286743,
      "learning_rate": 1.352448913396043e-05,
      "loss": 2.4911,
      "step": 1126000
    },
    {
      "epoch": 365.0,
      "eval_bleu": 1.0761262879681195,
      "eval_loss": 4.225990295410156,
      "eval_runtime": 4.2287,
      "eval_samples_per_second": 116.347,
      "eval_steps_per_second": 1.892,
      "step": 1126025
    },
    {
      "epoch": 365.0243111831442,
      "grad_norm": 1.3438515663146973,
      "learning_rate": 1.3521245540058387e-05,
      "loss": 2.498,
      "step": 1126100
    },
    {
      "epoch": 365.05672609400324,
      "grad_norm": 1.2560845613479614,
      "learning_rate": 1.3518001946156342e-05,
      "loss": 2.4861,
      "step": 1126200
    },
    {
      "epoch": 365.08914100486226,
      "grad_norm": 1.5965772867202759,
      "learning_rate": 1.3514758352254297e-05,
      "loss": 2.5005,
      "step": 1126300
    },
    {
      "epoch": 365.1215559157212,
      "grad_norm": 1.4373102188110352,
      "learning_rate": 1.3511514758352256e-05,
      "loss": 2.4812,
      "step": 1126400
    },
    {
      "epoch": 365.15397082658023,
      "grad_norm": 1.356793999671936,
      "learning_rate": 1.3508271164450211e-05,
      "loss": 2.518,
      "step": 1126500
    },
    {
      "epoch": 365.1863857374392,
      "grad_norm": 1.4434963464736938,
      "learning_rate": 1.3505027570548169e-05,
      "loss": 2.4935,
      "step": 1126600
    },
    {
      "epoch": 365.2188006482982,
      "grad_norm": 1.232967734336853,
      "learning_rate": 1.3501783976646124e-05,
      "loss": 2.4911,
      "step": 1126700
    },
    {
      "epoch": 365.25121555915723,
      "grad_norm": 1.3880047798156738,
      "learning_rate": 1.3498540382744083e-05,
      "loss": 2.4873,
      "step": 1126800
    },
    {
      "epoch": 365.2836304700162,
      "grad_norm": 1.4869779348373413,
      "learning_rate": 1.3495296788842038e-05,
      "loss": 2.5129,
      "step": 1126900
    },
    {
      "epoch": 365.3160453808752,
      "grad_norm": 1.3531494140625,
      "learning_rate": 1.3492053194939993e-05,
      "loss": 2.5048,
      "step": 1127000
    },
    {
      "epoch": 365.34846029173417,
      "grad_norm": 1.3408093452453613,
      "learning_rate": 1.3488809601037949e-05,
      "loss": 2.5108,
      "step": 1127100
    },
    {
      "epoch": 365.3808752025932,
      "grad_norm": 1.6175285577774048,
      "learning_rate": 1.3485566007135907e-05,
      "loss": 2.5021,
      "step": 1127200
    },
    {
      "epoch": 365.4132901134522,
      "grad_norm": 1.554051399230957,
      "learning_rate": 1.3482322413233864e-05,
      "loss": 2.5253,
      "step": 1127300
    },
    {
      "epoch": 365.44570502431117,
      "grad_norm": 1.41157066822052,
      "learning_rate": 1.347907881933182e-05,
      "loss": 2.4988,
      "step": 1127400
    },
    {
      "epoch": 365.4781199351702,
      "grad_norm": 1.5165520906448364,
      "learning_rate": 1.3475835225429779e-05,
      "loss": 2.5184,
      "step": 1127500
    },
    {
      "epoch": 365.51053484602915,
      "grad_norm": 1.461517333984375,
      "learning_rate": 1.3472591631527734e-05,
      "loss": 2.5018,
      "step": 1127600
    },
    {
      "epoch": 365.54294975688816,
      "grad_norm": 1.355199933052063,
      "learning_rate": 1.346934803762569e-05,
      "loss": 2.4975,
      "step": 1127700
    },
    {
      "epoch": 365.5753646677472,
      "grad_norm": 1.4240885972976685,
      "learning_rate": 1.3466104443723646e-05,
      "loss": 2.5027,
      "step": 1127800
    },
    {
      "epoch": 365.60777957860614,
      "grad_norm": 1.3939038515090942,
      "learning_rate": 1.3462860849821603e-05,
      "loss": 2.4865,
      "step": 1127900
    },
    {
      "epoch": 365.64019448946516,
      "grad_norm": 1.180606484413147,
      "learning_rate": 1.345961725591956e-05,
      "loss": 2.4905,
      "step": 1128000
    },
    {
      "epoch": 365.6726094003242,
      "grad_norm": 1.3297429084777832,
      "learning_rate": 1.3456373662017516e-05,
      "loss": 2.5118,
      "step": 1128100
    },
    {
      "epoch": 365.70502431118314,
      "grad_norm": 1.3814423084259033,
      "learning_rate": 1.3453130068115471e-05,
      "loss": 2.5073,
      "step": 1128200
    },
    {
      "epoch": 365.73743922204216,
      "grad_norm": 1.4420284032821655,
      "learning_rate": 1.344988647421343e-05,
      "loss": 2.4954,
      "step": 1128300
    },
    {
      "epoch": 365.7698541329011,
      "grad_norm": 1.568786382675171,
      "learning_rate": 1.3446642880311385e-05,
      "loss": 2.5016,
      "step": 1128400
    },
    {
      "epoch": 365.80226904376013,
      "grad_norm": 1.3239277601242065,
      "learning_rate": 1.3443399286409342e-05,
      "loss": 2.4895,
      "step": 1128500
    },
    {
      "epoch": 365.83468395461915,
      "grad_norm": 1.2372244596481323,
      "learning_rate": 1.3440155692507298e-05,
      "loss": 2.5155,
      "step": 1128600
    },
    {
      "epoch": 365.8670988654781,
      "grad_norm": 1.305049180984497,
      "learning_rate": 1.3436912098605256e-05,
      "loss": 2.4995,
      "step": 1128700
    },
    {
      "epoch": 365.89951377633713,
      "grad_norm": 1.5301870107650757,
      "learning_rate": 1.3433668504703212e-05,
      "loss": 2.5005,
      "step": 1128800
    },
    {
      "epoch": 365.9319286871961,
      "grad_norm": 1.4023106098175049,
      "learning_rate": 1.3430424910801167e-05,
      "loss": 2.5245,
      "step": 1128900
    },
    {
      "epoch": 365.9643435980551,
      "grad_norm": 1.575122594833374,
      "learning_rate": 1.3427181316899126e-05,
      "loss": 2.5195,
      "step": 1129000
    },
    {
      "epoch": 365.9967585089141,
      "grad_norm": 1.5154184103012085,
      "learning_rate": 1.3423937722997081e-05,
      "loss": 2.4978,
      "step": 1129100
    },
    {
      "epoch": 366.0,
      "eval_bleu": 1.0489467749128263,
      "eval_loss": 4.232235431671143,
      "eval_runtime": 4.4994,
      "eval_samples_per_second": 109.349,
      "eval_steps_per_second": 1.778,
      "step": 1129110
    },
    {
      "epoch": 366.0291734197731,
      "grad_norm": 1.2745603322982788,
      "learning_rate": 1.3420694129095038e-05,
      "loss": 2.5115,
      "step": 1129200
    },
    {
      "epoch": 366.0615883306321,
      "grad_norm": 1.3261361122131348,
      "learning_rate": 1.3417450535192994e-05,
      "loss": 2.4923,
      "step": 1129300
    },
    {
      "epoch": 366.09400324149107,
      "grad_norm": 1.590345859527588,
      "learning_rate": 1.3414206941290952e-05,
      "loss": 2.5284,
      "step": 1129400
    },
    {
      "epoch": 366.1264181523501,
      "grad_norm": 1.6670105457305908,
      "learning_rate": 1.3410963347388908e-05,
      "loss": 2.4844,
      "step": 1129500
    },
    {
      "epoch": 366.1588330632091,
      "grad_norm": 1.4113065004348755,
      "learning_rate": 1.3407719753486863e-05,
      "loss": 2.4947,
      "step": 1129600
    },
    {
      "epoch": 366.19124797406806,
      "grad_norm": 1.4610579013824463,
      "learning_rate": 1.340447615958482e-05,
      "loss": 2.5326,
      "step": 1129700
    },
    {
      "epoch": 366.2236628849271,
      "grad_norm": 1.4256938695907593,
      "learning_rate": 1.3401232565682779e-05,
      "loss": 2.4938,
      "step": 1129800
    },
    {
      "epoch": 366.25607779578604,
      "grad_norm": 1.526155710220337,
      "learning_rate": 1.3397988971780734e-05,
      "loss": 2.4746,
      "step": 1129900
    },
    {
      "epoch": 366.28849270664506,
      "grad_norm": 1.3232365846633911,
      "learning_rate": 1.339474537787869e-05,
      "loss": 2.4962,
      "step": 1130000
    },
    {
      "epoch": 366.3209076175041,
      "grad_norm": 1.3455674648284912,
      "learning_rate": 1.3391501783976645e-05,
      "loss": 2.507,
      "step": 1130100
    },
    {
      "epoch": 366.35332252836304,
      "grad_norm": 1.3859835863113403,
      "learning_rate": 1.3388258190074604e-05,
      "loss": 2.4933,
      "step": 1130200
    },
    {
      "epoch": 366.38573743922205,
      "grad_norm": 1.591785192489624,
      "learning_rate": 1.3385014596172559e-05,
      "loss": 2.4861,
      "step": 1130300
    },
    {
      "epoch": 366.418152350081,
      "grad_norm": 1.5930434465408325,
      "learning_rate": 1.3381771002270516e-05,
      "loss": 2.4978,
      "step": 1130400
    },
    {
      "epoch": 366.45056726094003,
      "grad_norm": 1.2447065114974976,
      "learning_rate": 1.3378527408368475e-05,
      "loss": 2.5138,
      "step": 1130500
    },
    {
      "epoch": 366.48298217179905,
      "grad_norm": 1.3297902345657349,
      "learning_rate": 1.337528381446643e-05,
      "loss": 2.5015,
      "step": 1130600
    },
    {
      "epoch": 366.515397082658,
      "grad_norm": 1.4816231727600098,
      "learning_rate": 1.3372040220564386e-05,
      "loss": 2.5056,
      "step": 1130700
    },
    {
      "epoch": 366.54781199351703,
      "grad_norm": 1.2447971105575562,
      "learning_rate": 1.3368796626662341e-05,
      "loss": 2.5009,
      "step": 1130800
    },
    {
      "epoch": 366.580226904376,
      "grad_norm": 1.3861435651779175,
      "learning_rate": 1.33655530327603e-05,
      "loss": 2.5044,
      "step": 1130900
    },
    {
      "epoch": 366.612641815235,
      "grad_norm": 1.276188611984253,
      "learning_rate": 1.3362309438858257e-05,
      "loss": 2.5018,
      "step": 1131000
    },
    {
      "epoch": 366.645056726094,
      "grad_norm": 1.3868998289108276,
      "learning_rate": 1.3359065844956212e-05,
      "loss": 2.4631,
      "step": 1131100
    },
    {
      "epoch": 366.677471636953,
      "grad_norm": 1.4113110303878784,
      "learning_rate": 1.3355822251054167e-05,
      "loss": 2.4924,
      "step": 1131200
    },
    {
      "epoch": 366.709886547812,
      "grad_norm": 1.357419490814209,
      "learning_rate": 1.3352578657152126e-05,
      "loss": 2.4925,
      "step": 1131300
    },
    {
      "epoch": 366.74230145867097,
      "grad_norm": 1.483344316482544,
      "learning_rate": 1.3349335063250081e-05,
      "loss": 2.4885,
      "step": 1131400
    },
    {
      "epoch": 366.77471636953,
      "grad_norm": 1.2952138185501099,
      "learning_rate": 1.3346091469348039e-05,
      "loss": 2.504,
      "step": 1131500
    },
    {
      "epoch": 366.807131280389,
      "grad_norm": 1.3899726867675781,
      "learning_rate": 1.3342847875445994e-05,
      "loss": 2.5114,
      "step": 1131600
    },
    {
      "epoch": 366.83954619124796,
      "grad_norm": 1.6849340200424194,
      "learning_rate": 1.3339604281543953e-05,
      "loss": 2.5052,
      "step": 1131700
    },
    {
      "epoch": 366.871961102107,
      "grad_norm": 1.5404797792434692,
      "learning_rate": 1.3336360687641908e-05,
      "loss": 2.5138,
      "step": 1131800
    },
    {
      "epoch": 366.90437601296594,
      "grad_norm": 1.513311505317688,
      "learning_rate": 1.3333149529678884e-05,
      "loss": 2.5086,
      "step": 1131900
    },
    {
      "epoch": 366.93679092382496,
      "grad_norm": 1.448187232017517,
      "learning_rate": 1.332990593577684e-05,
      "loss": 2.5125,
      "step": 1132000
    },
    {
      "epoch": 366.969205834684,
      "grad_norm": 1.4360121488571167,
      "learning_rate": 1.3326662341874798e-05,
      "loss": 2.5197,
      "step": 1132100
    },
    {
      "epoch": 367.0,
      "eval_bleu": 0.9636044398840725,
      "eval_loss": 4.230635643005371,
      "eval_runtime": 4.5011,
      "eval_samples_per_second": 109.307,
      "eval_steps_per_second": 1.777,
      "step": 1132195
    },
    {
      "epoch": 367.00162074554294,
      "grad_norm": 1.3593300580978394,
      "learning_rate": 1.3323418747972755e-05,
      "loss": 2.5089,
      "step": 1132200
    },
    {
      "epoch": 367.03403565640195,
      "grad_norm": 1.4085155725479126,
      "learning_rate": 1.332017515407071e-05,
      "loss": 2.5075,
      "step": 1132300
    },
    {
      "epoch": 367.0664505672609,
      "grad_norm": 1.265593409538269,
      "learning_rate": 1.3316931560168666e-05,
      "loss": 2.4694,
      "step": 1132400
    },
    {
      "epoch": 367.09886547811993,
      "grad_norm": 1.4199719429016113,
      "learning_rate": 1.3313687966266625e-05,
      "loss": 2.4877,
      "step": 1132500
    },
    {
      "epoch": 367.13128038897895,
      "grad_norm": 1.4831918478012085,
      "learning_rate": 1.331044437236458e-05,
      "loss": 2.5021,
      "step": 1132600
    },
    {
      "epoch": 367.1636952998379,
      "grad_norm": 1.434696078300476,
      "learning_rate": 1.3307200778462537e-05,
      "loss": 2.5199,
      "step": 1132700
    },
    {
      "epoch": 367.19611021069693,
      "grad_norm": 1.3334118127822876,
      "learning_rate": 1.3303957184560494e-05,
      "loss": 2.5109,
      "step": 1132800
    },
    {
      "epoch": 367.2285251215559,
      "grad_norm": 1.3773351907730103,
      "learning_rate": 1.3300713590658451e-05,
      "loss": 2.4842,
      "step": 1132900
    },
    {
      "epoch": 367.2609400324149,
      "grad_norm": 1.3426939249038696,
      "learning_rate": 1.3297469996756407e-05,
      "loss": 2.5038,
      "step": 1133000
    },
    {
      "epoch": 367.2933549432739,
      "grad_norm": 1.3270691633224487,
      "learning_rate": 1.3294226402854362e-05,
      "loss": 2.5204,
      "step": 1133100
    },
    {
      "epoch": 367.3257698541329,
      "grad_norm": 1.4332302808761597,
      "learning_rate": 1.329098280895232e-05,
      "loss": 2.4849,
      "step": 1133200
    },
    {
      "epoch": 367.3581847649919,
      "grad_norm": 1.3503148555755615,
      "learning_rate": 1.3287739215050276e-05,
      "loss": 2.5304,
      "step": 1133300
    },
    {
      "epoch": 367.39059967585086,
      "grad_norm": 1.76139497756958,
      "learning_rate": 1.3284495621148233e-05,
      "loss": 2.5301,
      "step": 1133400
    },
    {
      "epoch": 367.4230145867099,
      "grad_norm": 1.4000364542007446,
      "learning_rate": 1.3281252027246188e-05,
      "loss": 2.4848,
      "step": 1133500
    },
    {
      "epoch": 367.4554294975689,
      "grad_norm": 1.5827224254608154,
      "learning_rate": 1.3278008433344147e-05,
      "loss": 2.4926,
      "step": 1133600
    },
    {
      "epoch": 367.48784440842786,
      "grad_norm": 1.4933000802993774,
      "learning_rate": 1.3274764839442102e-05,
      "loss": 2.5066,
      "step": 1133700
    },
    {
      "epoch": 367.5202593192869,
      "grad_norm": 1.458803415298462,
      "learning_rate": 1.3271521245540058e-05,
      "loss": 2.4952,
      "step": 1133800
    },
    {
      "epoch": 367.55267423014584,
      "grad_norm": 1.2806817293167114,
      "learning_rate": 1.3268277651638015e-05,
      "loss": 2.5034,
      "step": 1133900
    },
    {
      "epoch": 367.58508914100486,
      "grad_norm": 1.5709842443466187,
      "learning_rate": 1.3265034057735972e-05,
      "loss": 2.515,
      "step": 1134000
    },
    {
      "epoch": 367.6175040518639,
      "grad_norm": 1.5205941200256348,
      "learning_rate": 1.3261790463833929e-05,
      "loss": 2.4872,
      "step": 1134100
    },
    {
      "epoch": 367.64991896272284,
      "grad_norm": 1.460060954093933,
      "learning_rate": 1.3258546869931884e-05,
      "loss": 2.5046,
      "step": 1134200
    },
    {
      "epoch": 367.68233387358185,
      "grad_norm": 1.544503092765808,
      "learning_rate": 1.3255303276029843e-05,
      "loss": 2.4793,
      "step": 1134300
    },
    {
      "epoch": 367.7147487844408,
      "grad_norm": 1.2511110305786133,
      "learning_rate": 1.3252059682127798e-05,
      "loss": 2.522,
      "step": 1134400
    },
    {
      "epoch": 367.74716369529983,
      "grad_norm": 1.4783704280853271,
      "learning_rate": 1.3248816088225754e-05,
      "loss": 2.5182,
      "step": 1134500
    },
    {
      "epoch": 367.77957860615885,
      "grad_norm": 1.4384722709655762,
      "learning_rate": 1.324557249432371e-05,
      "loss": 2.4991,
      "step": 1134600
    },
    {
      "epoch": 367.8119935170178,
      "grad_norm": 1.2905280590057373,
      "learning_rate": 1.324232890042167e-05,
      "loss": 2.4976,
      "step": 1134700
    },
    {
      "epoch": 367.84440842787683,
      "grad_norm": 1.3486876487731934,
      "learning_rate": 1.3239117742458646e-05,
      "loss": 2.4753,
      "step": 1134800
    },
    {
      "epoch": 367.87682333873585,
      "grad_norm": 1.351538062095642,
      "learning_rate": 1.3235874148556601e-05,
      "loss": 2.5028,
      "step": 1134900
    },
    {
      "epoch": 367.9092382495948,
      "grad_norm": 1.4496794939041138,
      "learning_rate": 1.3232630554654556e-05,
      "loss": 2.4935,
      "step": 1135000
    },
    {
      "epoch": 367.9416531604538,
      "grad_norm": 1.596723198890686,
      "learning_rate": 1.3229386960752515e-05,
      "loss": 2.5025,
      "step": 1135100
    },
    {
      "epoch": 367.9740680713128,
      "grad_norm": 1.4101407527923584,
      "learning_rate": 1.3226143366850472e-05,
      "loss": 2.5049,
      "step": 1135200
    },
    {
      "epoch": 368.0,
      "eval_bleu": 1.084696108034055,
      "eval_loss": 4.2288055419921875,
      "eval_runtime": 4.6775,
      "eval_samples_per_second": 105.184,
      "eval_steps_per_second": 1.71,
      "step": 1135280
    },
    {
      "epoch": 368.0064829821718,
      "grad_norm": 1.660975694656372,
      "learning_rate": 1.3222899772948428e-05,
      "loss": 2.4896,
      "step": 1135300
    },
    {
      "epoch": 368.0388978930308,
      "grad_norm": 1.3471581935882568,
      "learning_rate": 1.3219656179046383e-05,
      "loss": 2.4985,
      "step": 1135400
    },
    {
      "epoch": 368.0713128038898,
      "grad_norm": 1.5368255376815796,
      "learning_rate": 1.3216412585144342e-05,
      "loss": 2.4874,
      "step": 1135500
    },
    {
      "epoch": 368.1037277147488,
      "grad_norm": 1.3047140836715698,
      "learning_rate": 1.3213168991242297e-05,
      "loss": 2.5154,
      "step": 1135600
    },
    {
      "epoch": 368.13614262560776,
      "grad_norm": 1.2939306497573853,
      "learning_rate": 1.3209925397340252e-05,
      "loss": 2.4963,
      "step": 1135700
    },
    {
      "epoch": 368.1685575364668,
      "grad_norm": 1.4071359634399414,
      "learning_rate": 1.320668180343821e-05,
      "loss": 2.498,
      "step": 1135800
    },
    {
      "epoch": 368.2009724473258,
      "grad_norm": 1.4161698818206787,
      "learning_rate": 1.3203438209536168e-05,
      "loss": 2.4832,
      "step": 1135900
    },
    {
      "epoch": 368.23338735818476,
      "grad_norm": 1.3410069942474365,
      "learning_rate": 1.3200194615634123e-05,
      "loss": 2.4942,
      "step": 1136000
    },
    {
      "epoch": 368.2658022690438,
      "grad_norm": 1.2941043376922607,
      "learning_rate": 1.3196951021732079e-05,
      "loss": 2.5026,
      "step": 1136100
    },
    {
      "epoch": 368.29821717990274,
      "grad_norm": 1.3608273267745972,
      "learning_rate": 1.3193707427830038e-05,
      "loss": 2.4881,
      "step": 1136200
    },
    {
      "epoch": 368.33063209076175,
      "grad_norm": 1.369206428527832,
      "learning_rate": 1.3190463833927993e-05,
      "loss": 2.5054,
      "step": 1136300
    },
    {
      "epoch": 368.36304700162077,
      "grad_norm": 1.4463863372802734,
      "learning_rate": 1.318722024002595e-05,
      "loss": 2.4863,
      "step": 1136400
    },
    {
      "epoch": 368.39546191247973,
      "grad_norm": 1.3648351430892944,
      "learning_rate": 1.3183976646123905e-05,
      "loss": 2.4843,
      "step": 1136500
    },
    {
      "epoch": 368.42787682333875,
      "grad_norm": 1.344905972480774,
      "learning_rate": 1.3180733052221864e-05,
      "loss": 2.5029,
      "step": 1136600
    },
    {
      "epoch": 368.4602917341977,
      "grad_norm": 1.382829189300537,
      "learning_rate": 1.317748945831982e-05,
      "loss": 2.4983,
      "step": 1136700
    },
    {
      "epoch": 368.4927066450567,
      "grad_norm": 1.4778345823287964,
      "learning_rate": 1.3174245864417775e-05,
      "loss": 2.4976,
      "step": 1136800
    },
    {
      "epoch": 368.52512155591575,
      "grad_norm": 1.8136574029922485,
      "learning_rate": 1.317100227051573e-05,
      "loss": 2.5149,
      "step": 1136900
    },
    {
      "epoch": 368.5575364667747,
      "grad_norm": 1.3047903776168823,
      "learning_rate": 1.3167758676613689e-05,
      "loss": 2.4765,
      "step": 1137000
    },
    {
      "epoch": 368.5899513776337,
      "grad_norm": 1.3498035669326782,
      "learning_rate": 1.3164515082711646e-05,
      "loss": 2.5164,
      "step": 1137100
    },
    {
      "epoch": 368.6223662884927,
      "grad_norm": 1.4009262323379517,
      "learning_rate": 1.3161271488809601e-05,
      "loss": 2.5108,
      "step": 1137200
    },
    {
      "epoch": 368.6547811993517,
      "grad_norm": 1.5443311929702759,
      "learning_rate": 1.3158027894907557e-05,
      "loss": 2.5035,
      "step": 1137300
    },
    {
      "epoch": 368.6871961102107,
      "grad_norm": 1.299466609954834,
      "learning_rate": 1.3154816736944536e-05,
      "loss": 2.5049,
      "step": 1137400
    },
    {
      "epoch": 368.7196110210697,
      "grad_norm": 1.4660725593566895,
      "learning_rate": 1.3151573143042492e-05,
      "loss": 2.492,
      "step": 1137500
    },
    {
      "epoch": 368.7520259319287,
      "grad_norm": 1.1908724308013916,
      "learning_rate": 1.3148329549140449e-05,
      "loss": 2.4915,
      "step": 1137600
    },
    {
      "epoch": 368.78444084278766,
      "grad_norm": 1.4136202335357666,
      "learning_rate": 1.3145085955238404e-05,
      "loss": 2.5107,
      "step": 1137700
    },
    {
      "epoch": 368.8168557536467,
      "grad_norm": 1.4503223896026611,
      "learning_rate": 1.3141842361336363e-05,
      "loss": 2.5075,
      "step": 1137800
    },
    {
      "epoch": 368.8492706645057,
      "grad_norm": 1.549443006515503,
      "learning_rate": 1.3138598767434318e-05,
      "loss": 2.5118,
      "step": 1137900
    },
    {
      "epoch": 368.88168557536466,
      "grad_norm": 1.49847412109375,
      "learning_rate": 1.3135355173532273e-05,
      "loss": 2.4946,
      "step": 1138000
    },
    {
      "epoch": 368.9141004862237,
      "grad_norm": 1.4117311239242554,
      "learning_rate": 1.313211157963023e-05,
      "loss": 2.5133,
      "step": 1138100
    },
    {
      "epoch": 368.94651539708263,
      "grad_norm": 1.8632798194885254,
      "learning_rate": 1.3128867985728187e-05,
      "loss": 2.504,
      "step": 1138200
    },
    {
      "epoch": 368.97893030794165,
      "grad_norm": 1.3889665603637695,
      "learning_rate": 1.3125624391826145e-05,
      "loss": 2.5125,
      "step": 1138300
    },
    {
      "epoch": 369.0,
      "eval_bleu": 0.9732173823269338,
      "eval_loss": 4.2274603843688965,
      "eval_runtime": 4.8807,
      "eval_samples_per_second": 100.805,
      "eval_steps_per_second": 1.639,
      "step": 1138365
    },
    {
      "epoch": 369.01134521880067,
      "grad_norm": 1.3417325019836426,
      "learning_rate": 1.31223807979241e-05,
      "loss": 2.5083,
      "step": 1138400
    },
    {
      "epoch": 369.04376012965963,
      "grad_norm": 1.6554406881332397,
      "learning_rate": 1.3119137204022059e-05,
      "loss": 2.4985,
      "step": 1138500
    },
    {
      "epoch": 369.07617504051865,
      "grad_norm": 1.648453950881958,
      "learning_rate": 1.3115893610120014e-05,
      "loss": 2.4879,
      "step": 1138600
    },
    {
      "epoch": 369.1085899513776,
      "grad_norm": 1.5694676637649536,
      "learning_rate": 1.311265001621797e-05,
      "loss": 2.5116,
      "step": 1138700
    },
    {
      "epoch": 369.1410048622366,
      "grad_norm": 1.3634326457977295,
      "learning_rate": 1.3109406422315926e-05,
      "loss": 2.5032,
      "step": 1138800
    },
    {
      "epoch": 369.17341977309565,
      "grad_norm": 1.281068205833435,
      "learning_rate": 1.3106195264352902e-05,
      "loss": 2.5001,
      "step": 1138900
    },
    {
      "epoch": 369.2058346839546,
      "grad_norm": 1.3917701244354248,
      "learning_rate": 1.3102951670450861e-05,
      "loss": 2.5042,
      "step": 1139000
    },
    {
      "epoch": 369.2382495948136,
      "grad_norm": 1.4618494510650635,
      "learning_rate": 1.3099708076548817e-05,
      "loss": 2.4608,
      "step": 1139100
    },
    {
      "epoch": 369.2706645056726,
      "grad_norm": 1.4513472318649292,
      "learning_rate": 1.3096464482646772e-05,
      "loss": 2.5132,
      "step": 1139200
    },
    {
      "epoch": 369.3030794165316,
      "grad_norm": 1.2465453147888184,
      "learning_rate": 1.309322088874473e-05,
      "loss": 2.4981,
      "step": 1139300
    },
    {
      "epoch": 369.3354943273906,
      "grad_norm": 1.352952003479004,
      "learning_rate": 1.3089977294842688e-05,
      "loss": 2.4885,
      "step": 1139400
    },
    {
      "epoch": 369.3679092382496,
      "grad_norm": 1.2891918420791626,
      "learning_rate": 1.3086733700940643e-05,
      "loss": 2.5258,
      "step": 1139500
    },
    {
      "epoch": 369.4003241491086,
      "grad_norm": 1.6438887119293213,
      "learning_rate": 1.3083490107038598e-05,
      "loss": 2.5038,
      "step": 1139600
    },
    {
      "epoch": 369.43273905996756,
      "grad_norm": 1.3088284730911255,
      "learning_rate": 1.3080246513136557e-05,
      "loss": 2.5162,
      "step": 1139700
    },
    {
      "epoch": 369.4651539708266,
      "grad_norm": 1.2876560688018799,
      "learning_rate": 1.3077002919234513e-05,
      "loss": 2.5017,
      "step": 1139800
    },
    {
      "epoch": 369.4975688816856,
      "grad_norm": 1.5633111000061035,
      "learning_rate": 1.3073759325332468e-05,
      "loss": 2.5129,
      "step": 1139900
    },
    {
      "epoch": 369.52998379254456,
      "grad_norm": 1.2613539695739746,
      "learning_rate": 1.3070515731430425e-05,
      "loss": 2.4928,
      "step": 1140000
    },
    {
      "epoch": 369.5623987034036,
      "grad_norm": 1.2735804319381714,
      "learning_rate": 1.3067272137528384e-05,
      "loss": 2.4931,
      "step": 1140100
    },
    {
      "epoch": 369.59481361426253,
      "grad_norm": 1.2371186017990112,
      "learning_rate": 1.3064028543626339e-05,
      "loss": 2.4925,
      "step": 1140200
    },
    {
      "epoch": 369.62722852512155,
      "grad_norm": 1.3501944541931152,
      "learning_rate": 1.3060784949724294e-05,
      "loss": 2.4981,
      "step": 1140300
    },
    {
      "epoch": 369.65964343598057,
      "grad_norm": 1.611627459526062,
      "learning_rate": 1.305754135582225e-05,
      "loss": 2.4792,
      "step": 1140400
    },
    {
      "epoch": 369.69205834683953,
      "grad_norm": 1.205058217048645,
      "learning_rate": 1.3054297761920208e-05,
      "loss": 2.4888,
      "step": 1140500
    },
    {
      "epoch": 369.72447325769855,
      "grad_norm": 1.2911068201065063,
      "learning_rate": 1.3051054168018166e-05,
      "loss": 2.5032,
      "step": 1140600
    },
    {
      "epoch": 369.7568881685575,
      "grad_norm": 1.5902345180511475,
      "learning_rate": 1.3047810574116121e-05,
      "loss": 2.5147,
      "step": 1140700
    },
    {
      "epoch": 369.7893030794165,
      "grad_norm": 1.610438585281372,
      "learning_rate": 1.304456698021408e-05,
      "loss": 2.4807,
      "step": 1140800
    },
    {
      "epoch": 369.82171799027554,
      "grad_norm": 1.5668716430664062,
      "learning_rate": 1.3041323386312035e-05,
      "loss": 2.4828,
      "step": 1140900
    },
    {
      "epoch": 369.8541329011345,
      "grad_norm": 1.3792705535888672,
      "learning_rate": 1.303807979240999e-05,
      "loss": 2.5179,
      "step": 1141000
    },
    {
      "epoch": 369.8865478119935,
      "grad_norm": 1.287621259689331,
      "learning_rate": 1.3034836198507946e-05,
      "loss": 2.4923,
      "step": 1141100
    },
    {
      "epoch": 369.9189627228525,
      "grad_norm": 1.2707449197769165,
      "learning_rate": 1.3031592604605904e-05,
      "loss": 2.5055,
      "step": 1141200
    },
    {
      "epoch": 369.9513776337115,
      "grad_norm": 1.390535593032837,
      "learning_rate": 1.3028349010703861e-05,
      "loss": 2.4851,
      "step": 1141300
    },
    {
      "epoch": 369.9837925445705,
      "grad_norm": 1.2460498809814453,
      "learning_rate": 1.3025105416801817e-05,
      "loss": 2.5158,
      "step": 1141400
    },
    {
      "epoch": 370.0,
      "eval_bleu": 0.9553611283164192,
      "eval_loss": 4.226383209228516,
      "eval_runtime": 4.34,
      "eval_samples_per_second": 113.363,
      "eval_steps_per_second": 1.843,
      "step": 1141450
    },
    {
      "epoch": 370.0162074554295,
      "grad_norm": 1.2446521520614624,
      "learning_rate": 1.3021861822899772e-05,
      "loss": 2.5014,
      "step": 1141500
    },
    {
      "epoch": 370.0486223662885,
      "grad_norm": 1.4682844877243042,
      "learning_rate": 1.3018618228997731e-05,
      "loss": 2.4898,
      "step": 1141600
    },
    {
      "epoch": 370.0810372771475,
      "grad_norm": 1.3116915225982666,
      "learning_rate": 1.3015374635095686e-05,
      "loss": 2.4918,
      "step": 1141700
    },
    {
      "epoch": 370.1134521880065,
      "grad_norm": 1.38320791721344,
      "learning_rate": 1.3012131041193643e-05,
      "loss": 2.4979,
      "step": 1141800
    },
    {
      "epoch": 370.1458670988655,
      "grad_norm": 1.6097896099090576,
      "learning_rate": 1.3008887447291599e-05,
      "loss": 2.4915,
      "step": 1141900
    },
    {
      "epoch": 370.17828200972446,
      "grad_norm": 1.3438044786453247,
      "learning_rate": 1.3005643853389557e-05,
      "loss": 2.4885,
      "step": 1142000
    },
    {
      "epoch": 370.2106969205835,
      "grad_norm": 1.376978874206543,
      "learning_rate": 1.3002400259487513e-05,
      "loss": 2.5031,
      "step": 1142100
    },
    {
      "epoch": 370.2431118314425,
      "grad_norm": 1.4576624631881714,
      "learning_rate": 1.2999156665585468e-05,
      "loss": 2.5069,
      "step": 1142200
    },
    {
      "epoch": 370.27552674230145,
      "grad_norm": 1.666121244430542,
      "learning_rate": 1.2995913071683427e-05,
      "loss": 2.4959,
      "step": 1142300
    },
    {
      "epoch": 370.30794165316047,
      "grad_norm": 1.2772154808044434,
      "learning_rate": 1.2992669477781382e-05,
      "loss": 2.5169,
      "step": 1142400
    },
    {
      "epoch": 370.34035656401943,
      "grad_norm": 1.509281039237976,
      "learning_rate": 1.298942588387934e-05,
      "loss": 2.498,
      "step": 1142500
    },
    {
      "epoch": 370.37277147487845,
      "grad_norm": 1.3383556604385376,
      "learning_rate": 1.2986182289977295e-05,
      "loss": 2.5116,
      "step": 1142600
    },
    {
      "epoch": 370.40518638573747,
      "grad_norm": 1.5167834758758545,
      "learning_rate": 1.2982938696075253e-05,
      "loss": 2.5079,
      "step": 1142700
    },
    {
      "epoch": 370.4376012965964,
      "grad_norm": 1.3281135559082031,
      "learning_rate": 1.2979695102173209e-05,
      "loss": 2.4938,
      "step": 1142800
    },
    {
      "epoch": 370.47001620745544,
      "grad_norm": 1.4469481706619263,
      "learning_rate": 1.2976451508271164e-05,
      "loss": 2.5182,
      "step": 1142900
    },
    {
      "epoch": 370.5024311183144,
      "grad_norm": 1.4044365882873535,
      "learning_rate": 1.2973207914369121e-05,
      "loss": 2.5082,
      "step": 1143000
    },
    {
      "epoch": 370.5348460291734,
      "grad_norm": 1.469049096107483,
      "learning_rate": 1.2969964320467078e-05,
      "loss": 2.4965,
      "step": 1143100
    },
    {
      "epoch": 370.56726094003244,
      "grad_norm": 1.331976056098938,
      "learning_rate": 1.2966720726565035e-05,
      "loss": 2.5108,
      "step": 1143200
    },
    {
      "epoch": 370.5996758508914,
      "grad_norm": 1.3838521242141724,
      "learning_rate": 1.296347713266299e-05,
      "loss": 2.4896,
      "step": 1143300
    },
    {
      "epoch": 370.6320907617504,
      "grad_norm": 1.334740400314331,
      "learning_rate": 1.2960233538760946e-05,
      "loss": 2.5062,
      "step": 1143400
    },
    {
      "epoch": 370.6645056726094,
      "grad_norm": 1.392187237739563,
      "learning_rate": 1.2956989944858905e-05,
      "loss": 2.483,
      "step": 1143500
    },
    {
      "epoch": 370.6969205834684,
      "grad_norm": 1.4081629514694214,
      "learning_rate": 1.295374635095686e-05,
      "loss": 2.5014,
      "step": 1143600
    },
    {
      "epoch": 370.7293354943274,
      "grad_norm": 1.394692301750183,
      "learning_rate": 1.2950502757054817e-05,
      "loss": 2.4705,
      "step": 1143700
    },
    {
      "epoch": 370.7617504051864,
      "grad_norm": 1.3218297958374023,
      "learning_rate": 1.2947259163152776e-05,
      "loss": 2.529,
      "step": 1143800
    },
    {
      "epoch": 370.7941653160454,
      "grad_norm": 1.325581431388855,
      "learning_rate": 1.2944015569250731e-05,
      "loss": 2.5138,
      "step": 1143900
    },
    {
      "epoch": 370.82658022690435,
      "grad_norm": 1.2174427509307861,
      "learning_rate": 1.2940771975348687e-05,
      "loss": 2.4787,
      "step": 1144000
    },
    {
      "epoch": 370.8589951377634,
      "grad_norm": 1.514360785484314,
      "learning_rate": 1.2937528381446642e-05,
      "loss": 2.5049,
      "step": 1144100
    },
    {
      "epoch": 370.8914100486224,
      "grad_norm": 1.2902555465698242,
      "learning_rate": 1.29342847875446e-05,
      "loss": 2.5034,
      "step": 1144200
    },
    {
      "epoch": 370.92382495948135,
      "grad_norm": 1.2382097244262695,
      "learning_rate": 1.2931041193642556e-05,
      "loss": 2.4802,
      "step": 1144300
    },
    {
      "epoch": 370.95623987034037,
      "grad_norm": 1.5410841703414917,
      "learning_rate": 1.2927797599740513e-05,
      "loss": 2.5147,
      "step": 1144400
    },
    {
      "epoch": 370.98865478119933,
      "grad_norm": 1.2663133144378662,
      "learning_rate": 1.2924554005838468e-05,
      "loss": 2.479,
      "step": 1144500
    },
    {
      "epoch": 371.0,
      "eval_bleu": 1.1254436173993432,
      "eval_loss": 4.229118824005127,
      "eval_runtime": 4.5179,
      "eval_samples_per_second": 108.901,
      "eval_steps_per_second": 1.771,
      "step": 1144535
    },
    {
      "epoch": 371.02106969205835,
      "grad_norm": 1.2325236797332764,
      "learning_rate": 1.2921310411936427e-05,
      "loss": 2.4928,
      "step": 1144600
    },
    {
      "epoch": 371.05348460291737,
      "grad_norm": 1.441175937652588,
      "learning_rate": 1.2918066818034382e-05,
      "loss": 2.4953,
      "step": 1144700
    },
    {
      "epoch": 371.0858995137763,
      "grad_norm": 1.547896146774292,
      "learning_rate": 1.2914823224132338e-05,
      "loss": 2.4945,
      "step": 1144800
    },
    {
      "epoch": 371.11831442463534,
      "grad_norm": 1.491058349609375,
      "learning_rate": 1.2911612066169316e-05,
      "loss": 2.515,
      "step": 1144900
    },
    {
      "epoch": 371.1507293354943,
      "grad_norm": 1.4032566547393799,
      "learning_rate": 1.2908368472267274e-05,
      "loss": 2.4955,
      "step": 1145000
    },
    {
      "epoch": 371.1831442463533,
      "grad_norm": 1.474895715713501,
      "learning_rate": 1.290512487836523e-05,
      "loss": 2.4918,
      "step": 1145100
    },
    {
      "epoch": 371.21555915721234,
      "grad_norm": 1.5010770559310913,
      "learning_rate": 1.2901881284463185e-05,
      "loss": 2.4692,
      "step": 1145200
    },
    {
      "epoch": 371.2479740680713,
      "grad_norm": 1.405971884727478,
      "learning_rate": 1.289863769056114e-05,
      "loss": 2.5147,
      "step": 1145300
    },
    {
      "epoch": 371.2803889789303,
      "grad_norm": 1.5056707859039307,
      "learning_rate": 1.28953940966591e-05,
      "loss": 2.4808,
      "step": 1145400
    },
    {
      "epoch": 371.3128038897893,
      "grad_norm": 1.5781793594360352,
      "learning_rate": 1.2892150502757056e-05,
      "loss": 2.5057,
      "step": 1145500
    },
    {
      "epoch": 371.3452188006483,
      "grad_norm": 1.4363186359405518,
      "learning_rate": 1.2888906908855012e-05,
      "loss": 2.5075,
      "step": 1145600
    },
    {
      "epoch": 371.3776337115073,
      "grad_norm": 1.4064911603927612,
      "learning_rate": 1.288566331495297e-05,
      "loss": 2.4863,
      "step": 1145700
    },
    {
      "epoch": 371.4100486223663,
      "grad_norm": 1.3027962446212769,
      "learning_rate": 1.2882419721050926e-05,
      "loss": 2.4881,
      "step": 1145800
    },
    {
      "epoch": 371.4424635332253,
      "grad_norm": 1.4612945318222046,
      "learning_rate": 1.2879176127148881e-05,
      "loss": 2.5013,
      "step": 1145900
    },
    {
      "epoch": 371.47487844408425,
      "grad_norm": 1.270675778388977,
      "learning_rate": 1.2875932533246836e-05,
      "loss": 2.4925,
      "step": 1146000
    },
    {
      "epoch": 371.5072933549433,
      "grad_norm": 1.4116050004959106,
      "learning_rate": 1.2872688939344795e-05,
      "loss": 2.5078,
      "step": 1146100
    },
    {
      "epoch": 371.5397082658023,
      "grad_norm": 1.3599765300750732,
      "learning_rate": 1.2869445345442752e-05,
      "loss": 2.4927,
      "step": 1146200
    },
    {
      "epoch": 371.57212317666125,
      "grad_norm": 1.364480972290039,
      "learning_rate": 1.2866201751540708e-05,
      "loss": 2.4844,
      "step": 1146300
    },
    {
      "epoch": 371.60453808752027,
      "grad_norm": 1.3812055587768555,
      "learning_rate": 1.2862958157638663e-05,
      "loss": 2.5035,
      "step": 1146400
    },
    {
      "epoch": 371.63695299837923,
      "grad_norm": 1.2340952157974243,
      "learning_rate": 1.2859714563736622e-05,
      "loss": 2.4719,
      "step": 1146500
    },
    {
      "epoch": 371.66936790923825,
      "grad_norm": 1.3263638019561768,
      "learning_rate": 1.2856470969834577e-05,
      "loss": 2.4985,
      "step": 1146600
    },
    {
      "epoch": 371.70178282009726,
      "grad_norm": 1.3044296503067017,
      "learning_rate": 1.2853227375932534e-05,
      "loss": 2.473,
      "step": 1146700
    },
    {
      "epoch": 371.7341977309562,
      "grad_norm": 1.5027241706848145,
      "learning_rate": 1.284998378203049e-05,
      "loss": 2.504,
      "step": 1146800
    },
    {
      "epoch": 371.76661264181524,
      "grad_norm": 1.3800235986709595,
      "learning_rate": 1.2846772624067469e-05,
      "loss": 2.487,
      "step": 1146900
    },
    {
      "epoch": 371.7990275526742,
      "grad_norm": 1.5074063539505005,
      "learning_rate": 1.2843529030165424e-05,
      "loss": 2.481,
      "step": 1147000
    },
    {
      "epoch": 371.8314424635332,
      "grad_norm": 1.4166042804718018,
      "learning_rate": 1.284028543626338e-05,
      "loss": 2.5145,
      "step": 1147100
    },
    {
      "epoch": 371.86385737439224,
      "grad_norm": 1.3234736919403076,
      "learning_rate": 1.2837041842361337e-05,
      "loss": 2.5074,
      "step": 1147200
    },
    {
      "epoch": 371.8962722852512,
      "grad_norm": 1.3037837743759155,
      "learning_rate": 1.2833798248459294e-05,
      "loss": 2.5149,
      "step": 1147300
    },
    {
      "epoch": 371.9286871961102,
      "grad_norm": 1.457711935043335,
      "learning_rate": 1.283055465455725e-05,
      "loss": 2.5228,
      "step": 1147400
    },
    {
      "epoch": 371.9611021069692,
      "grad_norm": 1.3956005573272705,
      "learning_rate": 1.2827311060655206e-05,
      "loss": 2.4944,
      "step": 1147500
    },
    {
      "epoch": 371.9935170178282,
      "grad_norm": 1.507017970085144,
      "learning_rate": 1.2824067466753161e-05,
      "loss": 2.5133,
      "step": 1147600
    },
    {
      "epoch": 372.0,
      "eval_bleu": 0.9381820461234512,
      "eval_loss": 4.234462738037109,
      "eval_runtime": 4.5757,
      "eval_samples_per_second": 107.523,
      "eval_steps_per_second": 1.748,
      "step": 1147620
    },
    {
      "epoch": 372.0259319286872,
      "grad_norm": 1.3640954494476318,
      "learning_rate": 1.282082387285112e-05,
      "loss": 2.4807,
      "step": 1147700
    },
    {
      "epoch": 372.0583468395462,
      "grad_norm": 1.4490426778793335,
      "learning_rate": 1.2817580278949076e-05,
      "loss": 2.4985,
      "step": 1147800
    },
    {
      "epoch": 372.0907617504052,
      "grad_norm": 1.3042057752609253,
      "learning_rate": 1.2814336685047033e-05,
      "loss": 2.5131,
      "step": 1147900
    },
    {
      "epoch": 372.12317666126415,
      "grad_norm": 1.3518930673599243,
      "learning_rate": 1.2811093091144991e-05,
      "loss": 2.5029,
      "step": 1148000
    },
    {
      "epoch": 372.15559157212317,
      "grad_norm": 1.3100285530090332,
      "learning_rate": 1.2807849497242947e-05,
      "loss": 2.4975,
      "step": 1148100
    },
    {
      "epoch": 372.1880064829822,
      "grad_norm": 1.4176968336105347,
      "learning_rate": 1.2804605903340902e-05,
      "loss": 2.4938,
      "step": 1148200
    },
    {
      "epoch": 372.22042139384115,
      "grad_norm": 1.4471867084503174,
      "learning_rate": 1.2801394745377878e-05,
      "loss": 2.51,
      "step": 1148300
    },
    {
      "epoch": 372.25283630470017,
      "grad_norm": 1.4738234281539917,
      "learning_rate": 1.2798151151475835e-05,
      "loss": 2.5349,
      "step": 1148400
    },
    {
      "epoch": 372.2852512155592,
      "grad_norm": 1.2897194623947144,
      "learning_rate": 1.2794907557573792e-05,
      "loss": 2.4951,
      "step": 1148500
    },
    {
      "epoch": 372.31766612641815,
      "grad_norm": 1.5298975706100464,
      "learning_rate": 1.279166396367175e-05,
      "loss": 2.4977,
      "step": 1148600
    },
    {
      "epoch": 372.35008103727716,
      "grad_norm": 1.2817853689193726,
      "learning_rate": 1.2788420369769705e-05,
      "loss": 2.5076,
      "step": 1148700
    },
    {
      "epoch": 372.3824959481361,
      "grad_norm": 1.4588145017623901,
      "learning_rate": 1.2785176775867663e-05,
      "loss": 2.5036,
      "step": 1148800
    },
    {
      "epoch": 372.41491085899514,
      "grad_norm": 1.513017177581787,
      "learning_rate": 1.2781933181965619e-05,
      "loss": 2.5112,
      "step": 1148900
    },
    {
      "epoch": 372.44732576985416,
      "grad_norm": 1.3668533563613892,
      "learning_rate": 1.2778689588063574e-05,
      "loss": 2.4907,
      "step": 1149000
    },
    {
      "epoch": 372.4797406807131,
      "grad_norm": 1.7286052703857422,
      "learning_rate": 1.2775445994161531e-05,
      "loss": 2.4979,
      "step": 1149100
    },
    {
      "epoch": 372.51215559157214,
      "grad_norm": 1.3247863054275513,
      "learning_rate": 1.277220240025949e-05,
      "loss": 2.4794,
      "step": 1149200
    },
    {
      "epoch": 372.5445705024311,
      "grad_norm": 1.4023783206939697,
      "learning_rate": 1.2768958806357445e-05,
      "loss": 2.4981,
      "step": 1149300
    },
    {
      "epoch": 372.5769854132901,
      "grad_norm": 1.5371931791305542,
      "learning_rate": 1.27657152124554e-05,
      "loss": 2.5125,
      "step": 1149400
    },
    {
      "epoch": 372.60940032414914,
      "grad_norm": 1.378836989402771,
      "learning_rate": 1.2762471618553356e-05,
      "loss": 2.4818,
      "step": 1149500
    },
    {
      "epoch": 372.6418152350081,
      "grad_norm": 1.4961342811584473,
      "learning_rate": 1.2759228024651315e-05,
      "loss": 2.4933,
      "step": 1149600
    },
    {
      "epoch": 372.6742301458671,
      "grad_norm": 1.4204572439193726,
      "learning_rate": 1.2755984430749272e-05,
      "loss": 2.4944,
      "step": 1149700
    },
    {
      "epoch": 372.7066450567261,
      "grad_norm": 1.2988040447235107,
      "learning_rate": 1.2752740836847227e-05,
      "loss": 2.4823,
      "step": 1149800
    },
    {
      "epoch": 372.7390599675851,
      "grad_norm": 1.4302120208740234,
      "learning_rate": 1.2749497242945182e-05,
      "loss": 2.5066,
      "step": 1149900
    },
    {
      "epoch": 372.7714748784441,
      "grad_norm": 1.325785756111145,
      "learning_rate": 1.2746253649043141e-05,
      "loss": 2.5043,
      "step": 1150000
    },
    {
      "epoch": 372.80388978930307,
      "grad_norm": 1.5913429260253906,
      "learning_rate": 1.2743010055141097e-05,
      "loss": 2.5102,
      "step": 1150100
    },
    {
      "epoch": 372.8363047001621,
      "grad_norm": 1.4212069511413574,
      "learning_rate": 1.2739766461239052e-05,
      "loss": 2.4995,
      "step": 1150200
    },
    {
      "epoch": 372.86871961102105,
      "grad_norm": 1.5953952074050903,
      "learning_rate": 1.273652286733701e-05,
      "loss": 2.5009,
      "step": 1150300
    },
    {
      "epoch": 372.90113452188007,
      "grad_norm": 1.5326879024505615,
      "learning_rate": 1.2733279273434968e-05,
      "loss": 2.4845,
      "step": 1150400
    },
    {
      "epoch": 372.9335494327391,
      "grad_norm": 1.467812418937683,
      "learning_rate": 1.2730035679532923e-05,
      "loss": 2.4825,
      "step": 1150500
    },
    {
      "epoch": 372.96596434359805,
      "grad_norm": 1.316355586051941,
      "learning_rate": 1.2726792085630878e-05,
      "loss": 2.4856,
      "step": 1150600
    },
    {
      "epoch": 372.99837925445706,
      "grad_norm": 1.48197603225708,
      "learning_rate": 1.2723548491728837e-05,
      "loss": 2.4835,
      "step": 1150700
    },
    {
      "epoch": 373.0,
      "eval_bleu": 0.9512254438752374,
      "eval_loss": 4.231531143188477,
      "eval_runtime": 4.4109,
      "eval_samples_per_second": 111.542,
      "eval_steps_per_second": 1.814,
      "step": 1150705
    },
    {
      "epoch": 373.030794165316,
      "grad_norm": 1.2389037609100342,
      "learning_rate": 1.2720304897826793e-05,
      "loss": 2.4853,
      "step": 1150800
    },
    {
      "epoch": 373.06320907617504,
      "grad_norm": 1.4175416231155396,
      "learning_rate": 1.271706130392475e-05,
      "loss": 2.5036,
      "step": 1150900
    },
    {
      "epoch": 373.09562398703406,
      "grad_norm": 1.445921540260315,
      "learning_rate": 1.2713817710022705e-05,
      "loss": 2.4813,
      "step": 1151000
    },
    {
      "epoch": 373.128038897893,
      "grad_norm": 1.4895539283752441,
      "learning_rate": 1.2710574116120664e-05,
      "loss": 2.4897,
      "step": 1151100
    },
    {
      "epoch": 373.16045380875204,
      "grad_norm": 1.405649185180664,
      "learning_rate": 1.2707330522218619e-05,
      "loss": 2.4839,
      "step": 1151200
    },
    {
      "epoch": 373.192868719611,
      "grad_norm": 1.4116615056991577,
      "learning_rate": 1.2704086928316574e-05,
      "loss": 2.5199,
      "step": 1151300
    },
    {
      "epoch": 373.22528363047,
      "grad_norm": 1.4619134664535522,
      "learning_rate": 1.270084333441453e-05,
      "loss": 2.4821,
      "step": 1151400
    },
    {
      "epoch": 373.25769854132903,
      "grad_norm": 1.46611487865448,
      "learning_rate": 1.2697599740512488e-05,
      "loss": 2.4941,
      "step": 1151500
    },
    {
      "epoch": 373.290113452188,
      "grad_norm": 1.4597342014312744,
      "learning_rate": 1.2694356146610446e-05,
      "loss": 2.4889,
      "step": 1151600
    },
    {
      "epoch": 373.322528363047,
      "grad_norm": 1.3243459463119507,
      "learning_rate": 1.2691112552708401e-05,
      "loss": 2.5164,
      "step": 1151700
    },
    {
      "epoch": 373.354943273906,
      "grad_norm": 1.4529712200164795,
      "learning_rate": 1.268786895880636e-05,
      "loss": 2.4767,
      "step": 1151800
    },
    {
      "epoch": 373.387358184765,
      "grad_norm": 1.624669075012207,
      "learning_rate": 1.2684625364904315e-05,
      "loss": 2.4986,
      "step": 1151900
    },
    {
      "epoch": 373.419773095624,
      "grad_norm": 1.4773138761520386,
      "learning_rate": 1.268138177100227e-05,
      "loss": 2.5001,
      "step": 1152000
    },
    {
      "epoch": 373.45218800648297,
      "grad_norm": 1.466582179069519,
      "learning_rate": 1.2678138177100227e-05,
      "loss": 2.5208,
      "step": 1152100
    },
    {
      "epoch": 373.484602917342,
      "grad_norm": 1.397205114364624,
      "learning_rate": 1.2674894583198184e-05,
      "loss": 2.5049,
      "step": 1152200
    },
    {
      "epoch": 373.51701782820095,
      "grad_norm": 1.431800365447998,
      "learning_rate": 1.2671683425235162e-05,
      "loss": 2.4994,
      "step": 1152300
    },
    {
      "epoch": 373.54943273905997,
      "grad_norm": 1.3667092323303223,
      "learning_rate": 1.2668439831333118e-05,
      "loss": 2.4898,
      "step": 1152400
    },
    {
      "epoch": 373.581847649919,
      "grad_norm": 1.349513053894043,
      "learning_rate": 1.2665196237431073e-05,
      "loss": 2.5188,
      "step": 1152500
    },
    {
      "epoch": 373.61426256077795,
      "grad_norm": 1.3581409454345703,
      "learning_rate": 1.2661952643529032e-05,
      "loss": 2.4839,
      "step": 1152600
    },
    {
      "epoch": 373.64667747163696,
      "grad_norm": 1.2802791595458984,
      "learning_rate": 1.2658709049626987e-05,
      "loss": 2.5034,
      "step": 1152700
    },
    {
      "epoch": 373.6790923824959,
      "grad_norm": 1.1964765787124634,
      "learning_rate": 1.2655465455724944e-05,
      "loss": 2.5158,
      "step": 1152800
    },
    {
      "epoch": 373.71150729335494,
      "grad_norm": 1.3110214471817017,
      "learning_rate": 1.26522218618229e-05,
      "loss": 2.5013,
      "step": 1152900
    },
    {
      "epoch": 373.74392220421396,
      "grad_norm": 1.4302091598510742,
      "learning_rate": 1.2648978267920858e-05,
      "loss": 2.4882,
      "step": 1153000
    },
    {
      "epoch": 373.7763371150729,
      "grad_norm": 1.4044365882873535,
      "learning_rate": 1.2645734674018814e-05,
      "loss": 2.5124,
      "step": 1153100
    },
    {
      "epoch": 373.80875202593194,
      "grad_norm": 1.2526047229766846,
      "learning_rate": 1.2642491080116769e-05,
      "loss": 2.4858,
      "step": 1153200
    },
    {
      "epoch": 373.8411669367909,
      "grad_norm": 1.6144853830337524,
      "learning_rate": 1.2639247486214726e-05,
      "loss": 2.4928,
      "step": 1153300
    },
    {
      "epoch": 373.8735818476499,
      "grad_norm": 1.321129322052002,
      "learning_rate": 1.2636003892312685e-05,
      "loss": 2.4809,
      "step": 1153400
    },
    {
      "epoch": 373.90599675850893,
      "grad_norm": 1.3125348091125488,
      "learning_rate": 1.263276029841064e-05,
      "loss": 2.51,
      "step": 1153500
    },
    {
      "epoch": 373.9384116693679,
      "grad_norm": 1.4808402061462402,
      "learning_rate": 1.2629516704508595e-05,
      "loss": 2.4916,
      "step": 1153600
    },
    {
      "epoch": 373.9708265802269,
      "grad_norm": 1.4857832193374634,
      "learning_rate": 1.2626273110606554e-05,
      "loss": 2.5049,
      "step": 1153700
    },
    {
      "epoch": 374.0,
      "eval_bleu": 0.9321375648916995,
      "eval_loss": 4.235669136047363,
      "eval_runtime": 4.4552,
      "eval_samples_per_second": 110.432,
      "eval_steps_per_second": 1.796,
      "step": 1153790
    },
    {
      "epoch": 374.0032414910859,
      "grad_norm": 1.68857741355896,
      "learning_rate": 1.262302951670451e-05,
      "loss": 2.4944,
      "step": 1153800
    },
    {
      "epoch": 374.0356564019449,
      "grad_norm": 1.3077980279922485,
      "learning_rate": 1.2619785922802465e-05,
      "loss": 2.5001,
      "step": 1153900
    },
    {
      "epoch": 374.0680713128039,
      "grad_norm": 1.5097440481185913,
      "learning_rate": 1.2616542328900422e-05,
      "loss": 2.5096,
      "step": 1154000
    },
    {
      "epoch": 374.10048622366287,
      "grad_norm": 1.279361605644226,
      "learning_rate": 1.261329873499838e-05,
      "loss": 2.512,
      "step": 1154100
    },
    {
      "epoch": 374.1329011345219,
      "grad_norm": 1.45192289352417,
      "learning_rate": 1.2610055141096336e-05,
      "loss": 2.4924,
      "step": 1154200
    },
    {
      "epoch": 374.16531604538085,
      "grad_norm": 1.5530526638031006,
      "learning_rate": 1.2606811547194291e-05,
      "loss": 2.5048,
      "step": 1154300
    },
    {
      "epoch": 374.19773095623987,
      "grad_norm": 1.2546123266220093,
      "learning_rate": 1.2603567953292247e-05,
      "loss": 2.4744,
      "step": 1154400
    },
    {
      "epoch": 374.2301458670989,
      "grad_norm": 1.312872290611267,
      "learning_rate": 1.2600356795329226e-05,
      "loss": 2.4921,
      "step": 1154500
    },
    {
      "epoch": 374.26256077795784,
      "grad_norm": 1.4583890438079834,
      "learning_rate": 1.2597113201427183e-05,
      "loss": 2.4956,
      "step": 1154600
    },
    {
      "epoch": 374.29497568881686,
      "grad_norm": 1.4722206592559814,
      "learning_rate": 1.2593869607525139e-05,
      "loss": 2.5078,
      "step": 1154700
    },
    {
      "epoch": 374.3273905996758,
      "grad_norm": 1.4410085678100586,
      "learning_rate": 1.2590626013623094e-05,
      "loss": 2.4895,
      "step": 1154800
    },
    {
      "epoch": 374.35980551053484,
      "grad_norm": 1.396704912185669,
      "learning_rate": 1.2587382419721053e-05,
      "loss": 2.503,
      "step": 1154900
    },
    {
      "epoch": 374.39222042139386,
      "grad_norm": 1.5482251644134521,
      "learning_rate": 1.2584138825819008e-05,
      "loss": 2.4976,
      "step": 1155000
    },
    {
      "epoch": 374.4246353322528,
      "grad_norm": 1.3957480192184448,
      "learning_rate": 1.2580895231916965e-05,
      "loss": 2.5025,
      "step": 1155100
    },
    {
      "epoch": 374.45705024311184,
      "grad_norm": 1.3762191534042358,
      "learning_rate": 1.257765163801492e-05,
      "loss": 2.4778,
      "step": 1155200
    },
    {
      "epoch": 374.48946515397085,
      "grad_norm": 1.3155046701431274,
      "learning_rate": 1.257440804411288e-05,
      "loss": 2.5074,
      "step": 1155300
    },
    {
      "epoch": 374.5218800648298,
      "grad_norm": 1.5126755237579346,
      "learning_rate": 1.2571164450210835e-05,
      "loss": 2.4913,
      "step": 1155400
    },
    {
      "epoch": 374.55429497568883,
      "grad_norm": 1.4149069786071777,
      "learning_rate": 1.256792085630879e-05,
      "loss": 2.4867,
      "step": 1155500
    },
    {
      "epoch": 374.5867098865478,
      "grad_norm": 1.5335670709609985,
      "learning_rate": 1.2564677262406745e-05,
      "loss": 2.4804,
      "step": 1155600
    },
    {
      "epoch": 374.6191247974068,
      "grad_norm": 1.5260584354400635,
      "learning_rate": 1.2561466104443725e-05,
      "loss": 2.4872,
      "step": 1155700
    },
    {
      "epoch": 374.65153970826583,
      "grad_norm": 1.748441219329834,
      "learning_rate": 1.2558222510541682e-05,
      "loss": 2.4953,
      "step": 1155800
    },
    {
      "epoch": 374.6839546191248,
      "grad_norm": 1.2844504117965698,
      "learning_rate": 1.2554978916639637e-05,
      "loss": 2.508,
      "step": 1155900
    },
    {
      "epoch": 374.7163695299838,
      "grad_norm": 1.531089186668396,
      "learning_rate": 1.2551735322737593e-05,
      "loss": 2.5032,
      "step": 1156000
    },
    {
      "epoch": 374.74878444084277,
      "grad_norm": 1.3620944023132324,
      "learning_rate": 1.2548491728835551e-05,
      "loss": 2.5005,
      "step": 1156100
    },
    {
      "epoch": 374.7811993517018,
      "grad_norm": 1.36909818649292,
      "learning_rate": 1.2545248134933507e-05,
      "loss": 2.4958,
      "step": 1156200
    },
    {
      "epoch": 374.8136142625608,
      "grad_norm": 1.4285670518875122,
      "learning_rate": 1.2542004541031464e-05,
      "loss": 2.4812,
      "step": 1156300
    },
    {
      "epoch": 374.84602917341977,
      "grad_norm": 1.553206205368042,
      "learning_rate": 1.2538760947129419e-05,
      "loss": 2.5084,
      "step": 1156400
    },
    {
      "epoch": 374.8784440842788,
      "grad_norm": 1.4749916791915894,
      "learning_rate": 1.2535517353227378e-05,
      "loss": 2.5136,
      "step": 1156500
    },
    {
      "epoch": 374.91085899513774,
      "grad_norm": 1.7455546855926514,
      "learning_rate": 1.2532273759325333e-05,
      "loss": 2.5032,
      "step": 1156600
    },
    {
      "epoch": 374.94327390599676,
      "grad_norm": 1.4165698289871216,
      "learning_rate": 1.2529030165423288e-05,
      "loss": 2.4961,
      "step": 1156700
    },
    {
      "epoch": 374.9756888168558,
      "grad_norm": 1.3427889347076416,
      "learning_rate": 1.2525786571521247e-05,
      "loss": 2.4655,
      "step": 1156800
    },
    {
      "epoch": 375.0,
      "eval_bleu": 0.9077367721423382,
      "eval_loss": 4.2407941818237305,
      "eval_runtime": 4.2502,
      "eval_samples_per_second": 115.759,
      "eval_steps_per_second": 1.882,
      "step": 1156875
    },
    {
      "epoch": 375.00810372771474,
      "grad_norm": 1.3870539665222168,
      "learning_rate": 1.2522542977619203e-05,
      "loss": 2.5039,
      "step": 1156900
    },
    {
      "epoch": 375.04051863857376,
      "grad_norm": 1.3420058488845825,
      "learning_rate": 1.251929938371716e-05,
      "loss": 2.5001,
      "step": 1157000
    },
    {
      "epoch": 375.0729335494327,
      "grad_norm": 1.4920048713684082,
      "learning_rate": 1.2516055789815115e-05,
      "loss": 2.5082,
      "step": 1157100
    },
    {
      "epoch": 375.10534846029174,
      "grad_norm": 1.307373285293579,
      "learning_rate": 1.2512812195913074e-05,
      "loss": 2.4961,
      "step": 1157200
    },
    {
      "epoch": 375.13776337115075,
      "grad_norm": 1.3547303676605225,
      "learning_rate": 1.2509568602011029e-05,
      "loss": 2.4798,
      "step": 1157300
    },
    {
      "epoch": 375.1701782820097,
      "grad_norm": 1.3370124101638794,
      "learning_rate": 1.2506325008108984e-05,
      "loss": 2.5067,
      "step": 1157400
    },
    {
      "epoch": 375.20259319286873,
      "grad_norm": 1.4206337928771973,
      "learning_rate": 1.2503081414206941e-05,
      "loss": 2.5009,
      "step": 1157500
    },
    {
      "epoch": 375.2350081037277,
      "grad_norm": 1.2843424081802368,
      "learning_rate": 1.2499837820304899e-05,
      "loss": 2.4863,
      "step": 1157600
    },
    {
      "epoch": 375.2674230145867,
      "grad_norm": 1.3320461511611938,
      "learning_rate": 1.2496594226402856e-05,
      "loss": 2.4848,
      "step": 1157700
    },
    {
      "epoch": 375.29983792544573,
      "grad_norm": 1.2561043500900269,
      "learning_rate": 1.2493350632500811e-05,
      "loss": 2.4769,
      "step": 1157800
    },
    {
      "epoch": 375.3322528363047,
      "grad_norm": 1.3601313829421997,
      "learning_rate": 1.2490107038598768e-05,
      "loss": 2.4881,
      "step": 1157900
    },
    {
      "epoch": 375.3646677471637,
      "grad_norm": 1.4005018472671509,
      "learning_rate": 1.2486863444696723e-05,
      "loss": 2.4733,
      "step": 1158000
    },
    {
      "epoch": 375.39708265802267,
      "grad_norm": 1.37065851688385,
      "learning_rate": 1.248361985079468e-05,
      "loss": 2.4979,
      "step": 1158100
    },
    {
      "epoch": 375.4294975688817,
      "grad_norm": 1.2883071899414062,
      "learning_rate": 1.2480376256892637e-05,
      "loss": 2.4875,
      "step": 1158200
    },
    {
      "epoch": 375.4619124797407,
      "grad_norm": 1.7470897436141968,
      "learning_rate": 1.2477132662990594e-05,
      "loss": 2.503,
      "step": 1158300
    },
    {
      "epoch": 375.49432739059966,
      "grad_norm": 1.3358261585235596,
      "learning_rate": 1.2473889069088552e-05,
      "loss": 2.5155,
      "step": 1158400
    },
    {
      "epoch": 375.5267423014587,
      "grad_norm": 1.3160487413406372,
      "learning_rate": 1.2470645475186507e-05,
      "loss": 2.5114,
      "step": 1158500
    },
    {
      "epoch": 375.55915721231764,
      "grad_norm": 1.4121969938278198,
      "learning_rate": 1.2467401881284464e-05,
      "loss": 2.5026,
      "step": 1158600
    },
    {
      "epoch": 375.59157212317666,
      "grad_norm": 1.3316279649734497,
      "learning_rate": 1.246415828738242e-05,
      "loss": 2.4976,
      "step": 1158700
    },
    {
      "epoch": 375.6239870340357,
      "grad_norm": 1.3509514331817627,
      "learning_rate": 1.2460914693480378e-05,
      "loss": 2.4937,
      "step": 1158800
    },
    {
      "epoch": 375.65640194489464,
      "grad_norm": 1.4168956279754639,
      "learning_rate": 1.2457671099578333e-05,
      "loss": 2.4808,
      "step": 1158900
    },
    {
      "epoch": 375.68881685575366,
      "grad_norm": 1.434044361114502,
      "learning_rate": 1.245442750567629e-05,
      "loss": 2.5072,
      "step": 1159000
    },
    {
      "epoch": 375.7212317666126,
      "grad_norm": 1.3681708574295044,
      "learning_rate": 1.2451183911774246e-05,
      "loss": 2.4931,
      "step": 1159100
    },
    {
      "epoch": 375.75364667747164,
      "grad_norm": 1.3858765363693237,
      "learning_rate": 1.2447940317872203e-05,
      "loss": 2.484,
      "step": 1159200
    },
    {
      "epoch": 375.78606158833065,
      "grad_norm": 1.389577031135559,
      "learning_rate": 1.2444696723970158e-05,
      "loss": 2.4853,
      "step": 1159300
    },
    {
      "epoch": 375.8184764991896,
      "grad_norm": 1.6086913347244263,
      "learning_rate": 1.2441453130068117e-05,
      "loss": 2.5101,
      "step": 1159400
    },
    {
      "epoch": 375.85089141004863,
      "grad_norm": 1.4058774709701538,
      "learning_rate": 1.2438209536166072e-05,
      "loss": 2.4974,
      "step": 1159500
    },
    {
      "epoch": 375.8833063209076,
      "grad_norm": 1.5200607776641846,
      "learning_rate": 1.243496594226403e-05,
      "loss": 2.4842,
      "step": 1159600
    },
    {
      "epoch": 375.9157212317666,
      "grad_norm": 1.5342605113983154,
      "learning_rate": 1.2431722348361986e-05,
      "loss": 2.5171,
      "step": 1159700
    },
    {
      "epoch": 375.94813614262563,
      "grad_norm": 1.3916913270950317,
      "learning_rate": 1.2428478754459942e-05,
      "loss": 2.4819,
      "step": 1159800
    },
    {
      "epoch": 375.9805510534846,
      "grad_norm": 1.405556559562683,
      "learning_rate": 1.2425235160557899e-05,
      "loss": 2.5028,
      "step": 1159900
    },
    {
      "epoch": 376.0,
      "eval_bleu": 1.188000080732337,
      "eval_loss": 4.233457088470459,
      "eval_runtime": 4.2099,
      "eval_samples_per_second": 116.868,
      "eval_steps_per_second": 1.9,
      "step": 1159960
    },
    {
      "epoch": 376.0129659643436,
      "grad_norm": 1.2417253255844116,
      "learning_rate": 1.2421991566655856e-05,
      "loss": 2.4734,
      "step": 1160000
    },
    {
      "epoch": 376.04538087520257,
      "grad_norm": 1.3482179641723633,
      "learning_rate": 1.2418747972753813e-05,
      "loss": 2.4766,
      "step": 1160100
    },
    {
      "epoch": 376.0777957860616,
      "grad_norm": 1.3418548107147217,
      "learning_rate": 1.2415504378851768e-05,
      "loss": 2.4876,
      "step": 1160200
    },
    {
      "epoch": 376.1102106969206,
      "grad_norm": 1.4769617319107056,
      "learning_rate": 1.2412260784949725e-05,
      "loss": 2.492,
      "step": 1160300
    },
    {
      "epoch": 376.14262560777956,
      "grad_norm": 1.4825794696807861,
      "learning_rate": 1.240901719104768e-05,
      "loss": 2.4908,
      "step": 1160400
    },
    {
      "epoch": 376.1750405186386,
      "grad_norm": 1.2558174133300781,
      "learning_rate": 1.2405773597145638e-05,
      "loss": 2.489,
      "step": 1160500
    },
    {
      "epoch": 376.20745542949754,
      "grad_norm": 1.4255784749984741,
      "learning_rate": 1.2402530003243595e-05,
      "loss": 2.5016,
      "step": 1160600
    },
    {
      "epoch": 376.23987034035656,
      "grad_norm": 1.452512264251709,
      "learning_rate": 1.2399286409341552e-05,
      "loss": 2.4909,
      "step": 1160700
    },
    {
      "epoch": 376.2722852512156,
      "grad_norm": 1.6239455938339233,
      "learning_rate": 1.2396042815439507e-05,
      "loss": 2.4998,
      "step": 1160800
    },
    {
      "epoch": 376.30470016207454,
      "grad_norm": 1.343711256980896,
      "learning_rate": 1.2392799221537464e-05,
      "loss": 2.4906,
      "step": 1160900
    },
    {
      "epoch": 376.33711507293356,
      "grad_norm": 1.6148672103881836,
      "learning_rate": 1.2389555627635421e-05,
      "loss": 2.484,
      "step": 1161000
    },
    {
      "epoch": 376.3695299837925,
      "grad_norm": 1.4241609573364258,
      "learning_rate": 1.2386312033733377e-05,
      "loss": 2.5149,
      "step": 1161100
    },
    {
      "epoch": 376.40194489465154,
      "grad_norm": 1.5610800981521606,
      "learning_rate": 1.2383068439831334e-05,
      "loss": 2.4903,
      "step": 1161200
    },
    {
      "epoch": 376.43435980551055,
      "grad_norm": 1.5292428731918335,
      "learning_rate": 1.237982484592929e-05,
      "loss": 2.4851,
      "step": 1161300
    },
    {
      "epoch": 376.4667747163695,
      "grad_norm": 1.4235864877700806,
      "learning_rate": 1.2376581252027248e-05,
      "loss": 2.5098,
      "step": 1161400
    },
    {
      "epoch": 376.49918962722853,
      "grad_norm": 1.4542717933654785,
      "learning_rate": 1.2373370094064224e-05,
      "loss": 2.5,
      "step": 1161500
    },
    {
      "epoch": 376.5316045380875,
      "grad_norm": 1.4314148426055908,
      "learning_rate": 1.237012650016218e-05,
      "loss": 2.5084,
      "step": 1161600
    },
    {
      "epoch": 376.5640194489465,
      "grad_norm": 1.4612407684326172,
      "learning_rate": 1.2366882906260136e-05,
      "loss": 2.5002,
      "step": 1161700
    },
    {
      "epoch": 376.5964343598055,
      "grad_norm": 1.5173393487930298,
      "learning_rate": 1.2363639312358093e-05,
      "loss": 2.4998,
      "step": 1161800
    },
    {
      "epoch": 376.6288492706645,
      "grad_norm": 1.5722670555114746,
      "learning_rate": 1.236039571845605e-05,
      "loss": 2.5048,
      "step": 1161900
    },
    {
      "epoch": 376.6612641815235,
      "grad_norm": 1.4333730936050415,
      "learning_rate": 1.2357152124554007e-05,
      "loss": 2.5069,
      "step": 1162000
    },
    {
      "epoch": 376.6936790923825,
      "grad_norm": 1.3353514671325684,
      "learning_rate": 1.2353908530651963e-05,
      "loss": 2.4896,
      "step": 1162100
    },
    {
      "epoch": 376.7260940032415,
      "grad_norm": 1.2400779724121094,
      "learning_rate": 1.235066493674992e-05,
      "loss": 2.4886,
      "step": 1162200
    },
    {
      "epoch": 376.7585089141005,
      "grad_norm": 1.224291205406189,
      "learning_rate": 1.2347421342847875e-05,
      "loss": 2.4983,
      "step": 1162300
    },
    {
      "epoch": 376.79092382495946,
      "grad_norm": 1.3608311414718628,
      "learning_rate": 1.2344177748945832e-05,
      "loss": 2.4832,
      "step": 1162400
    },
    {
      "epoch": 376.8233387358185,
      "grad_norm": 1.4864413738250732,
      "learning_rate": 1.234093415504379e-05,
      "loss": 2.4791,
      "step": 1162500
    },
    {
      "epoch": 376.8557536466775,
      "grad_norm": 1.2637556791305542,
      "learning_rate": 1.2337690561141746e-05,
      "loss": 2.4842,
      "step": 1162600
    },
    {
      "epoch": 376.88816855753646,
      "grad_norm": 1.1813632249832153,
      "learning_rate": 1.2334446967239702e-05,
      "loss": 2.4964,
      "step": 1162700
    },
    {
      "epoch": 376.9205834683955,
      "grad_norm": 1.3683292865753174,
      "learning_rate": 1.2331203373337659e-05,
      "loss": 2.483,
      "step": 1162800
    },
    {
      "epoch": 376.95299837925444,
      "grad_norm": 1.3870267868041992,
      "learning_rate": 1.2327959779435614e-05,
      "loss": 2.5071,
      "step": 1162900
    },
    {
      "epoch": 376.98541329011346,
      "grad_norm": 1.6103622913360596,
      "learning_rate": 1.2324716185533571e-05,
      "loss": 2.4857,
      "step": 1163000
    },
    {
      "epoch": 377.0,
      "eval_bleu": 1.1187832528842643,
      "eval_loss": 4.235666275024414,
      "eval_runtime": 4.0965,
      "eval_samples_per_second": 120.101,
      "eval_steps_per_second": 1.953,
      "step": 1163045
    },
    {
      "epoch": 377.0178282009725,
      "grad_norm": 1.1382076740264893,
      "learning_rate": 1.2321472591631528e-05,
      "loss": 2.4828,
      "step": 1163100
    },
    {
      "epoch": 377.05024311183143,
      "grad_norm": 1.3656433820724487,
      "learning_rate": 1.2318228997729485e-05,
      "loss": 2.5044,
      "step": 1163200
    },
    {
      "epoch": 377.08265802269045,
      "grad_norm": 1.5514094829559326,
      "learning_rate": 1.2314985403827442e-05,
      "loss": 2.5111,
      "step": 1163300
    },
    {
      "epoch": 377.1150729335494,
      "grad_norm": 1.2789881229400635,
      "learning_rate": 1.2311741809925398e-05,
      "loss": 2.4809,
      "step": 1163400
    },
    {
      "epoch": 377.14748784440843,
      "grad_norm": 1.3239808082580566,
      "learning_rate": 1.2308498216023355e-05,
      "loss": 2.514,
      "step": 1163500
    },
    {
      "epoch": 377.17990275526745,
      "grad_norm": 1.305944800376892,
      "learning_rate": 1.230525462212131e-05,
      "loss": 2.5057,
      "step": 1163600
    },
    {
      "epoch": 377.2123176661264,
      "grad_norm": 1.411791443824768,
      "learning_rate": 1.2302011028219269e-05,
      "loss": 2.5126,
      "step": 1163700
    },
    {
      "epoch": 377.2447325769854,
      "grad_norm": 1.3808547258377075,
      "learning_rate": 1.2298767434317224e-05,
      "loss": 2.4953,
      "step": 1163800
    },
    {
      "epoch": 377.2771474878444,
      "grad_norm": 1.5218318700790405,
      "learning_rate": 1.2295523840415181e-05,
      "loss": 2.4928,
      "step": 1163900
    },
    {
      "epoch": 377.3095623987034,
      "grad_norm": 1.4697632789611816,
      "learning_rate": 1.2292280246513137e-05,
      "loss": 2.5061,
      "step": 1164000
    },
    {
      "epoch": 377.3419773095624,
      "grad_norm": 1.536773920059204,
      "learning_rate": 1.2289036652611094e-05,
      "loss": 2.503,
      "step": 1164100
    },
    {
      "epoch": 377.3743922204214,
      "grad_norm": 1.3302277326583862,
      "learning_rate": 1.2285793058709049e-05,
      "loss": 2.4947,
      "step": 1164200
    },
    {
      "epoch": 377.4068071312804,
      "grad_norm": 1.3919185400009155,
      "learning_rate": 1.2282549464807008e-05,
      "loss": 2.4995,
      "step": 1164300
    },
    {
      "epoch": 377.43922204213936,
      "grad_norm": 1.2528467178344727,
      "learning_rate": 1.2279305870904963e-05,
      "loss": 2.4933,
      "step": 1164400
    },
    {
      "epoch": 377.4716369529984,
      "grad_norm": 1.339465856552124,
      "learning_rate": 1.227606227700292e-05,
      "loss": 2.4987,
      "step": 1164500
    },
    {
      "epoch": 377.5040518638574,
      "grad_norm": 1.5350308418273926,
      "learning_rate": 1.2272818683100875e-05,
      "loss": 2.4921,
      "step": 1164600
    },
    {
      "epoch": 377.53646677471636,
      "grad_norm": 1.4324592351913452,
      "learning_rate": 1.2269575089198832e-05,
      "loss": 2.5042,
      "step": 1164700
    },
    {
      "epoch": 377.5688816855754,
      "grad_norm": 1.3949869871139526,
      "learning_rate": 1.226633149529679e-05,
      "loss": 2.4902,
      "step": 1164800
    },
    {
      "epoch": 377.60129659643434,
      "grad_norm": 1.2709171772003174,
      "learning_rate": 1.2263087901394747e-05,
      "loss": 2.5016,
      "step": 1164900
    },
    {
      "epoch": 377.63371150729336,
      "grad_norm": 1.4415922164916992,
      "learning_rate": 1.2259844307492704e-05,
      "loss": 2.5019,
      "step": 1165000
    },
    {
      "epoch": 377.6661264181524,
      "grad_norm": 1.322003722190857,
      "learning_rate": 1.2256600713590659e-05,
      "loss": 2.4736,
      "step": 1165100
    },
    {
      "epoch": 377.69854132901133,
      "grad_norm": 1.33174729347229,
      "learning_rate": 1.2253357119688616e-05,
      "loss": 2.4949,
      "step": 1165200
    },
    {
      "epoch": 377.73095623987035,
      "grad_norm": 1.305166244506836,
      "learning_rate": 1.2250113525786571e-05,
      "loss": 2.4821,
      "step": 1165300
    },
    {
      "epoch": 377.7633711507293,
      "grad_norm": 1.4568862915039062,
      "learning_rate": 1.2246869931884528e-05,
      "loss": 2.4837,
      "step": 1165400
    },
    {
      "epoch": 377.79578606158833,
      "grad_norm": 1.2906599044799805,
      "learning_rate": 1.2243658773921506e-05,
      "loss": 2.4971,
      "step": 1165500
    },
    {
      "epoch": 377.82820097244735,
      "grad_norm": 1.4105793237686157,
      "learning_rate": 1.2240415180019463e-05,
      "loss": 2.4739,
      "step": 1165600
    },
    {
      "epoch": 377.8606158833063,
      "grad_norm": 1.4023950099945068,
      "learning_rate": 1.2237171586117419e-05,
      "loss": 2.4861,
      "step": 1165700
    },
    {
      "epoch": 377.8930307941653,
      "grad_norm": 1.4773768186569214,
      "learning_rate": 1.2233927992215376e-05,
      "loss": 2.4955,
      "step": 1165800
    },
    {
      "epoch": 377.9254457050243,
      "grad_norm": 1.2881098985671997,
      "learning_rate": 1.2230684398313331e-05,
      "loss": 2.504,
      "step": 1165900
    },
    {
      "epoch": 377.9578606158833,
      "grad_norm": 1.5058497190475464,
      "learning_rate": 1.2227440804411288e-05,
      "loss": 2.5092,
      "step": 1166000
    },
    {
      "epoch": 377.9902755267423,
      "grad_norm": 1.3873226642608643,
      "learning_rate": 1.2224197210509245e-05,
      "loss": 2.488,
      "step": 1166100
    },
    {
      "epoch": 378.0,
      "eval_bleu": 0.9932316291142571,
      "eval_loss": 4.2333550453186035,
      "eval_runtime": 4.227,
      "eval_samples_per_second": 116.395,
      "eval_steps_per_second": 1.893,
      "step": 1166130
    },
    {
      "epoch": 378.0226904376013,
      "grad_norm": 1.6154296398162842,
      "learning_rate": 1.2220953616607202e-05,
      "loss": 2.4879,
      "step": 1166200
    },
    {
      "epoch": 378.0551053484603,
      "grad_norm": 1.653730869293213,
      "learning_rate": 1.2217710022705158e-05,
      "loss": 2.4993,
      "step": 1166300
    },
    {
      "epoch": 378.08752025931926,
      "grad_norm": 1.5045469999313354,
      "learning_rate": 1.2214466428803115e-05,
      "loss": 2.4987,
      "step": 1166400
    },
    {
      "epoch": 378.1199351701783,
      "grad_norm": 1.313915729522705,
      "learning_rate": 1.221122283490107e-05,
      "loss": 2.498,
      "step": 1166500
    },
    {
      "epoch": 378.1523500810373,
      "grad_norm": 1.3955957889556885,
      "learning_rate": 1.2207979240999027e-05,
      "loss": 2.502,
      "step": 1166600
    },
    {
      "epoch": 378.18476499189626,
      "grad_norm": 1.4184166193008423,
      "learning_rate": 1.2204735647096984e-05,
      "loss": 2.5004,
      "step": 1166700
    },
    {
      "epoch": 378.2171799027553,
      "grad_norm": 1.661365270614624,
      "learning_rate": 1.2201492053194941e-05,
      "loss": 2.4867,
      "step": 1166800
    },
    {
      "epoch": 378.24959481361424,
      "grad_norm": 1.381499171257019,
      "learning_rate": 1.2198248459292898e-05,
      "loss": 2.4764,
      "step": 1166900
    },
    {
      "epoch": 378.28200972447326,
      "grad_norm": 1.2986518144607544,
      "learning_rate": 1.2195004865390853e-05,
      "loss": 2.4701,
      "step": 1167000
    },
    {
      "epoch": 378.3144246353323,
      "grad_norm": 1.450270414352417,
      "learning_rate": 1.219176127148881e-05,
      "loss": 2.5248,
      "step": 1167100
    },
    {
      "epoch": 378.34683954619123,
      "grad_norm": 1.5333491563796997,
      "learning_rate": 1.2188550113525787e-05,
      "loss": 2.4841,
      "step": 1167200
    },
    {
      "epoch": 378.37925445705025,
      "grad_norm": 1.3706222772598267,
      "learning_rate": 1.2185306519623744e-05,
      "loss": 2.4866,
      "step": 1167300
    },
    {
      "epoch": 378.4116693679092,
      "grad_norm": 1.4176760911941528,
      "learning_rate": 1.21820629257217e-05,
      "loss": 2.4777,
      "step": 1167400
    },
    {
      "epoch": 378.44408427876823,
      "grad_norm": 1.416735053062439,
      "learning_rate": 1.2178819331819656e-05,
      "loss": 2.4935,
      "step": 1167500
    },
    {
      "epoch": 378.47649918962725,
      "grad_norm": 1.477609395980835,
      "learning_rate": 1.2175575737917613e-05,
      "loss": 2.4906,
      "step": 1167600
    },
    {
      "epoch": 378.5089141004862,
      "grad_norm": 1.5797250270843506,
      "learning_rate": 1.217233214401557e-05,
      "loss": 2.486,
      "step": 1167700
    },
    {
      "epoch": 378.5413290113452,
      "grad_norm": 1.259322166442871,
      "learning_rate": 1.2169088550113526e-05,
      "loss": 2.4788,
      "step": 1167800
    },
    {
      "epoch": 378.5737439222042,
      "grad_norm": 1.4858942031860352,
      "learning_rate": 1.2165844956211484e-05,
      "loss": 2.4766,
      "step": 1167900
    },
    {
      "epoch": 378.6061588330632,
      "grad_norm": 1.2672368288040161,
      "learning_rate": 1.216260136230944e-05,
      "loss": 2.4927,
      "step": 1168000
    },
    {
      "epoch": 378.6385737439222,
      "grad_norm": 1.265184998512268,
      "learning_rate": 1.2159357768407397e-05,
      "loss": 2.4931,
      "step": 1168100
    },
    {
      "epoch": 378.6709886547812,
      "grad_norm": 1.2984275817871094,
      "learning_rate": 1.2156114174505352e-05,
      "loss": 2.4762,
      "step": 1168200
    },
    {
      "epoch": 378.7034035656402,
      "grad_norm": 1.457571029663086,
      "learning_rate": 1.2152870580603309e-05,
      "loss": 2.4904,
      "step": 1168300
    },
    {
      "epoch": 378.73581847649916,
      "grad_norm": 1.4775445461273193,
      "learning_rate": 1.2149626986701264e-05,
      "loss": 2.5231,
      "step": 1168400
    },
    {
      "epoch": 378.7682333873582,
      "grad_norm": 1.2674869298934937,
      "learning_rate": 1.2146383392799223e-05,
      "loss": 2.478,
      "step": 1168500
    },
    {
      "epoch": 378.8006482982172,
      "grad_norm": 1.2900117635726929,
      "learning_rate": 1.2143139798897179e-05,
      "loss": 2.5159,
      "step": 1168600
    },
    {
      "epoch": 378.83306320907616,
      "grad_norm": 1.3397080898284912,
      "learning_rate": 1.2139896204995136e-05,
      "loss": 2.4814,
      "step": 1168700
    },
    {
      "epoch": 378.8654781199352,
      "grad_norm": 1.3194056749343872,
      "learning_rate": 1.2136652611093091e-05,
      "loss": 2.4886,
      "step": 1168800
    },
    {
      "epoch": 378.8978930307942,
      "grad_norm": 1.446465015411377,
      "learning_rate": 1.2133409017191048e-05,
      "loss": 2.5071,
      "step": 1168900
    },
    {
      "epoch": 378.93030794165315,
      "grad_norm": 1.366111159324646,
      "learning_rate": 1.2130165423289003e-05,
      "loss": 2.5003,
      "step": 1169000
    },
    {
      "epoch": 378.9627228525122,
      "grad_norm": 1.3451321125030518,
      "learning_rate": 1.2126921829386962e-05,
      "loss": 2.5074,
      "step": 1169100
    },
    {
      "epoch": 378.99513776337113,
      "grad_norm": 1.3501710891723633,
      "learning_rate": 1.212367823548492e-05,
      "loss": 2.5014,
      "step": 1169200
    },
    {
      "epoch": 379.0,
      "eval_bleu": 1.1147476521780189,
      "eval_loss": 4.236637115478516,
      "eval_runtime": 4.2851,
      "eval_samples_per_second": 114.815,
      "eval_steps_per_second": 1.867,
      "step": 1169215
    },
    {
      "epoch": 379.02755267423015,
      "grad_norm": 1.430885672569275,
      "learning_rate": 1.2120434641582875e-05,
      "loss": 2.4829,
      "step": 1169300
    },
    {
      "epoch": 379.05996758508917,
      "grad_norm": 1.386292815208435,
      "learning_rate": 1.2117191047680832e-05,
      "loss": 2.5098,
      "step": 1169400
    },
    {
      "epoch": 379.09238249594813,
      "grad_norm": 1.2994384765625,
      "learning_rate": 1.2113947453778787e-05,
      "loss": 2.5191,
      "step": 1169500
    },
    {
      "epoch": 379.12479740680715,
      "grad_norm": 1.4690937995910645,
      "learning_rate": 1.2110703859876744e-05,
      "loss": 2.4959,
      "step": 1169600
    },
    {
      "epoch": 379.1572123176661,
      "grad_norm": 1.4963902235031128,
      "learning_rate": 1.2107492701913722e-05,
      "loss": 2.5022,
      "step": 1169700
    },
    {
      "epoch": 379.1896272285251,
      "grad_norm": 1.3875291347503662,
      "learning_rate": 1.2104249108011677e-05,
      "loss": 2.4858,
      "step": 1169800
    },
    {
      "epoch": 379.22204213938414,
      "grad_norm": 1.3074100017547607,
      "learning_rate": 1.2101005514109634e-05,
      "loss": 2.4793,
      "step": 1169900
    },
    {
      "epoch": 379.2544570502431,
      "grad_norm": 1.3916471004486084,
      "learning_rate": 1.2097761920207591e-05,
      "loss": 2.4849,
      "step": 1170000
    },
    {
      "epoch": 379.2868719611021,
      "grad_norm": 1.4773300886154175,
      "learning_rate": 1.2094518326305547e-05,
      "loss": 2.4949,
      "step": 1170100
    },
    {
      "epoch": 379.3192868719611,
      "grad_norm": 1.3150575160980225,
      "learning_rate": 1.2091274732403504e-05,
      "loss": 2.4832,
      "step": 1170200
    },
    {
      "epoch": 379.3517017828201,
      "grad_norm": 1.5085972547531128,
      "learning_rate": 1.208803113850146e-05,
      "loss": 2.4984,
      "step": 1170300
    },
    {
      "epoch": 379.3841166936791,
      "grad_norm": 1.5699387788772583,
      "learning_rate": 1.2084787544599418e-05,
      "loss": 2.4916,
      "step": 1170400
    },
    {
      "epoch": 379.4165316045381,
      "grad_norm": 1.3780710697174072,
      "learning_rate": 1.2081543950697373e-05,
      "loss": 2.4797,
      "step": 1170500
    },
    {
      "epoch": 379.4489465153971,
      "grad_norm": 1.4099193811416626,
      "learning_rate": 1.207830035679533e-05,
      "loss": 2.4843,
      "step": 1170600
    },
    {
      "epoch": 379.48136142625606,
      "grad_norm": 1.3676954507827759,
      "learning_rate": 1.2075056762893285e-05,
      "loss": 2.4747,
      "step": 1170700
    },
    {
      "epoch": 379.5137763371151,
      "grad_norm": 1.4741078615188599,
      "learning_rate": 1.2071813168991243e-05,
      "loss": 2.4981,
      "step": 1170800
    },
    {
      "epoch": 379.5461912479741,
      "grad_norm": 1.6192959547042847,
      "learning_rate": 1.20685695750892e-05,
      "loss": 2.4907,
      "step": 1170900
    },
    {
      "epoch": 379.57860615883305,
      "grad_norm": 1.4095196723937988,
      "learning_rate": 1.2065325981187157e-05,
      "loss": 2.4914,
      "step": 1171000
    },
    {
      "epoch": 379.6110210696921,
      "grad_norm": 1.3405414819717407,
      "learning_rate": 1.2062082387285112e-05,
      "loss": 2.508,
      "step": 1171100
    },
    {
      "epoch": 379.64343598055103,
      "grad_norm": 1.3133140802383423,
      "learning_rate": 1.2058838793383069e-05,
      "loss": 2.4776,
      "step": 1171200
    },
    {
      "epoch": 379.67585089141005,
      "grad_norm": 1.3323034048080444,
      "learning_rate": 1.2055595199481026e-05,
      "loss": 2.5077,
      "step": 1171300
    },
    {
      "epoch": 379.70826580226907,
      "grad_norm": 1.3642096519470215,
      "learning_rate": 1.2052351605578981e-05,
      "loss": 2.4816,
      "step": 1171400
    },
    {
      "epoch": 379.74068071312803,
      "grad_norm": 1.1926311254501343,
      "learning_rate": 1.2049108011676938e-05,
      "loss": 2.5123,
      "step": 1171500
    },
    {
      "epoch": 379.77309562398705,
      "grad_norm": 1.420977234840393,
      "learning_rate": 1.2045864417774896e-05,
      "loss": 2.4751,
      "step": 1171600
    },
    {
      "epoch": 379.805510534846,
      "grad_norm": 1.3961246013641357,
      "learning_rate": 1.2042620823872853e-05,
      "loss": 2.5087,
      "step": 1171700
    },
    {
      "epoch": 379.837925445705,
      "grad_norm": 1.3894392251968384,
      "learning_rate": 1.2039377229970808e-05,
      "loss": 2.502,
      "step": 1171800
    },
    {
      "epoch": 379.87034035656404,
      "grad_norm": 1.507568359375,
      "learning_rate": 1.2036133636068765e-05,
      "loss": 2.488,
      "step": 1171900
    },
    {
      "epoch": 379.902755267423,
      "grad_norm": 1.4047343730926514,
      "learning_rate": 1.203289004216672e-05,
      "loss": 2.4888,
      "step": 1172000
    },
    {
      "epoch": 379.935170178282,
      "grad_norm": 1.405094027519226,
      "learning_rate": 1.2029646448264677e-05,
      "loss": 2.4775,
      "step": 1172100
    },
    {
      "epoch": 379.967585089141,
      "grad_norm": 1.5220516920089722,
      "learning_rate": 1.2026402854362634e-05,
      "loss": 2.5299,
      "step": 1172200
    },
    {
      "epoch": 380.0,
      "grad_norm": 1.2283076047897339,
      "learning_rate": 1.2023159260460591e-05,
      "loss": 2.485,
      "step": 1172300
    },
    {
      "epoch": 380.0,
      "eval_bleu": 1.1413752607857734,
      "eval_loss": 4.233818054199219,
      "eval_runtime": 4.4328,
      "eval_samples_per_second": 110.991,
      "eval_steps_per_second": 1.805,
      "step": 1172300
    },
    {
      "epoch": 380.032414910859,
      "grad_norm": 1.2470396757125854,
      "learning_rate": 1.2019915666558547e-05,
      "loss": 2.4761,
      "step": 1172400
    },
    {
      "epoch": 380.064829821718,
      "grad_norm": 1.293794870376587,
      "learning_rate": 1.2016672072656504e-05,
      "loss": 2.4992,
      "step": 1172500
    },
    {
      "epoch": 380.097244732577,
      "grad_norm": 1.426914930343628,
      "learning_rate": 1.201342847875446e-05,
      "loss": 2.4669,
      "step": 1172600
    },
    {
      "epoch": 380.12965964343596,
      "grad_norm": 1.3632842302322388,
      "learning_rate": 1.2010184884852416e-05,
      "loss": 2.4894,
      "step": 1172700
    },
    {
      "epoch": 380.162074554295,
      "grad_norm": 1.3820632696151733,
      "learning_rate": 1.2006941290950375e-05,
      "loss": 2.4908,
      "step": 1172800
    },
    {
      "epoch": 380.194489465154,
      "grad_norm": 1.447690725326538,
      "learning_rate": 1.200369769704833e-05,
      "loss": 2.4761,
      "step": 1172900
    },
    {
      "epoch": 380.22690437601295,
      "grad_norm": 1.341515302658081,
      "learning_rate": 1.2000454103146287e-05,
      "loss": 2.4689,
      "step": 1173000
    },
    {
      "epoch": 380.25931928687197,
      "grad_norm": 1.3677502870559692,
      "learning_rate": 1.1997210509244243e-05,
      "loss": 2.4907,
      "step": 1173100
    },
    {
      "epoch": 380.29173419773093,
      "grad_norm": 1.2754366397857666,
      "learning_rate": 1.19939669153422e-05,
      "loss": 2.4884,
      "step": 1173200
    },
    {
      "epoch": 380.32414910858995,
      "grad_norm": 1.4504625797271729,
      "learning_rate": 1.1990723321440155e-05,
      "loss": 2.4922,
      "step": 1173300
    },
    {
      "epoch": 380.35656401944897,
      "grad_norm": 1.1860976219177246,
      "learning_rate": 1.1987479727538114e-05,
      "loss": 2.4981,
      "step": 1173400
    },
    {
      "epoch": 380.3889789303079,
      "grad_norm": 1.433156132698059,
      "learning_rate": 1.198423613363607e-05,
      "loss": 2.4904,
      "step": 1173500
    },
    {
      "epoch": 380.42139384116695,
      "grad_norm": 1.4816348552703857,
      "learning_rate": 1.1980992539734026e-05,
      "loss": 2.514,
      "step": 1173600
    },
    {
      "epoch": 380.4538087520259,
      "grad_norm": 1.4330862760543823,
      "learning_rate": 1.1977781381771002e-05,
      "loss": 2.4956,
      "step": 1173700
    },
    {
      "epoch": 380.4862236628849,
      "grad_norm": 1.424601674079895,
      "learning_rate": 1.197453778786896e-05,
      "loss": 2.4698,
      "step": 1173800
    },
    {
      "epoch": 380.51863857374394,
      "grad_norm": 1.415261149406433,
      "learning_rate": 1.1971294193966917e-05,
      "loss": 2.4819,
      "step": 1173900
    },
    {
      "epoch": 380.5510534846029,
      "grad_norm": 1.3681035041809082,
      "learning_rate": 1.1968050600064874e-05,
      "loss": 2.5089,
      "step": 1174000
    },
    {
      "epoch": 380.5834683954619,
      "grad_norm": 1.4220865964889526,
      "learning_rate": 1.1964807006162829e-05,
      "loss": 2.5068,
      "step": 1174100
    },
    {
      "epoch": 380.6158833063209,
      "grad_norm": 1.3799889087677002,
      "learning_rate": 1.1961563412260786e-05,
      "loss": 2.4875,
      "step": 1174200
    },
    {
      "epoch": 380.6482982171799,
      "grad_norm": 1.104951024055481,
      "learning_rate": 1.1958319818358741e-05,
      "loss": 2.4946,
      "step": 1174300
    },
    {
      "epoch": 380.6807131280389,
      "grad_norm": 1.2871408462524414,
      "learning_rate": 1.1955076224456698e-05,
      "loss": 2.5176,
      "step": 1174400
    },
    {
      "epoch": 380.7131280388979,
      "grad_norm": 1.4801924228668213,
      "learning_rate": 1.1951832630554655e-05,
      "loss": 2.4942,
      "step": 1174500
    },
    {
      "epoch": 380.7455429497569,
      "grad_norm": 1.4431370496749878,
      "learning_rate": 1.1948589036652612e-05,
      "loss": 2.5189,
      "step": 1174600
    },
    {
      "epoch": 380.77795786061586,
      "grad_norm": 1.5184671878814697,
      "learning_rate": 1.1945345442750568e-05,
      "loss": 2.4831,
      "step": 1174700
    },
    {
      "epoch": 380.8103727714749,
      "grad_norm": 1.4327415227890015,
      "learning_rate": 1.1942101848848525e-05,
      "loss": 2.4927,
      "step": 1174800
    },
    {
      "epoch": 380.8427876823339,
      "grad_norm": 1.2672532796859741,
      "learning_rate": 1.193885825494648e-05,
      "loss": 2.5119,
      "step": 1174900
    },
    {
      "epoch": 380.87520259319285,
      "grad_norm": 1.3573020696640015,
      "learning_rate": 1.1935614661044437e-05,
      "loss": 2.4971,
      "step": 1175000
    },
    {
      "epoch": 380.90761750405187,
      "grad_norm": 1.2609686851501465,
      "learning_rate": 1.1932371067142394e-05,
      "loss": 2.5,
      "step": 1175100
    },
    {
      "epoch": 380.94003241491083,
      "grad_norm": 1.2867711782455444,
      "learning_rate": 1.1929127473240351e-05,
      "loss": 2.4856,
      "step": 1175200
    },
    {
      "epoch": 380.97244732576985,
      "grad_norm": 1.334108591079712,
      "learning_rate": 1.1925883879338308e-05,
      "loss": 2.4968,
      "step": 1175300
    },
    {
      "epoch": 381.0,
      "eval_bleu": 1.2430119926054,
      "eval_loss": 4.240241050720215,
      "eval_runtime": 4.3759,
      "eval_samples_per_second": 112.434,
      "eval_steps_per_second": 1.828,
      "step": 1175385
    },
    {
      "epoch": 381.00486223662887,
      "grad_norm": 1.336450457572937,
      "learning_rate": 1.1922640285436264e-05,
      "loss": 2.4642,
      "step": 1175400
    },
    {
      "epoch": 381.0372771474878,
      "grad_norm": 1.3822036981582642,
      "learning_rate": 1.1919396691534221e-05,
      "loss": 2.493,
      "step": 1175500
    },
    {
      "epoch": 381.06969205834685,
      "grad_norm": 1.6004022359848022,
      "learning_rate": 1.1916185533571197e-05,
      "loss": 2.4971,
      "step": 1175600
    },
    {
      "epoch": 381.1021069692058,
      "grad_norm": 1.308908462524414,
      "learning_rate": 1.1912941939669154e-05,
      "loss": 2.5038,
      "step": 1175700
    },
    {
      "epoch": 381.1345218800648,
      "grad_norm": 1.382315754890442,
      "learning_rate": 1.1909698345767111e-05,
      "loss": 2.501,
      "step": 1175800
    },
    {
      "epoch": 381.16693679092384,
      "grad_norm": 1.3475501537322998,
      "learning_rate": 1.1906454751865068e-05,
      "loss": 2.4841,
      "step": 1175900
    },
    {
      "epoch": 381.1993517017828,
      "grad_norm": 1.522040843963623,
      "learning_rate": 1.1903211157963023e-05,
      "loss": 2.4922,
      "step": 1176000
    },
    {
      "epoch": 381.2317666126418,
      "grad_norm": 1.3892278671264648,
      "learning_rate": 1.189996756406098e-05,
      "loss": 2.4819,
      "step": 1176100
    },
    {
      "epoch": 381.26418152350084,
      "grad_norm": 1.4724050760269165,
      "learning_rate": 1.1896723970158936e-05,
      "loss": 2.4871,
      "step": 1176200
    },
    {
      "epoch": 381.2965964343598,
      "grad_norm": 1.430525302886963,
      "learning_rate": 1.1893480376256893e-05,
      "loss": 2.5088,
      "step": 1176300
    },
    {
      "epoch": 381.3290113452188,
      "grad_norm": 1.4504574537277222,
      "learning_rate": 1.189023678235485e-05,
      "loss": 2.4953,
      "step": 1176400
    },
    {
      "epoch": 381.3614262560778,
      "grad_norm": 1.418586254119873,
      "learning_rate": 1.1886993188452807e-05,
      "loss": 2.4896,
      "step": 1176500
    },
    {
      "epoch": 381.3938411669368,
      "grad_norm": 1.3480969667434692,
      "learning_rate": 1.1883749594550762e-05,
      "loss": 2.501,
      "step": 1176600
    },
    {
      "epoch": 381.4262560777958,
      "grad_norm": 1.2450028657913208,
      "learning_rate": 1.188050600064872e-05,
      "loss": 2.4762,
      "step": 1176700
    },
    {
      "epoch": 381.4586709886548,
      "grad_norm": 1.44783616065979,
      "learning_rate": 1.1877262406746675e-05,
      "loss": 2.4909,
      "step": 1176800
    },
    {
      "epoch": 381.4910858995138,
      "grad_norm": 1.3742386102676392,
      "learning_rate": 1.1874018812844632e-05,
      "loss": 2.4866,
      "step": 1176900
    },
    {
      "epoch": 381.52350081037275,
      "grad_norm": 1.5495173931121826,
      "learning_rate": 1.1870775218942589e-05,
      "loss": 2.4753,
      "step": 1177000
    },
    {
      "epoch": 381.55591572123177,
      "grad_norm": 1.613806962966919,
      "learning_rate": 1.1867531625040546e-05,
      "loss": 2.4821,
      "step": 1177100
    },
    {
      "epoch": 381.5883306320908,
      "grad_norm": 1.309032678604126,
      "learning_rate": 1.1864288031138503e-05,
      "loss": 2.479,
      "step": 1177200
    },
    {
      "epoch": 381.62074554294975,
      "grad_norm": 1.4847255945205688,
      "learning_rate": 1.1861044437236458e-05,
      "loss": 2.5068,
      "step": 1177300
    },
    {
      "epoch": 381.65316045380877,
      "grad_norm": 1.358786940574646,
      "learning_rate": 1.1857800843334415e-05,
      "loss": 2.5072,
      "step": 1177400
    },
    {
      "epoch": 381.6855753646677,
      "grad_norm": 1.4630329608917236,
      "learning_rate": 1.185455724943237e-05,
      "loss": 2.4831,
      "step": 1177500
    },
    {
      "epoch": 381.71799027552674,
      "grad_norm": 1.4832512140274048,
      "learning_rate": 1.185131365553033e-05,
      "loss": 2.4946,
      "step": 1177600
    },
    {
      "epoch": 381.75040518638576,
      "grad_norm": 1.375823736190796,
      "learning_rate": 1.1848070061628285e-05,
      "loss": 2.4949,
      "step": 1177700
    },
    {
      "epoch": 381.7828200972447,
      "grad_norm": 1.3035002946853638,
      "learning_rate": 1.1844826467726242e-05,
      "loss": 2.4896,
      "step": 1177800
    },
    {
      "epoch": 381.81523500810374,
      "grad_norm": 1.2597440481185913,
      "learning_rate": 1.1841582873824197e-05,
      "loss": 2.5031,
      "step": 1177900
    },
    {
      "epoch": 381.8476499189627,
      "grad_norm": 1.4652706384658813,
      "learning_rate": 1.1838339279922154e-05,
      "loss": 2.4842,
      "step": 1178000
    },
    {
      "epoch": 381.8800648298217,
      "grad_norm": 1.5875723361968994,
      "learning_rate": 1.183509568602011e-05,
      "loss": 2.479,
      "step": 1178100
    },
    {
      "epoch": 381.91247974068074,
      "grad_norm": 1.4771344661712646,
      "learning_rate": 1.1831852092118068e-05,
      "loss": 2.4887,
      "step": 1178200
    },
    {
      "epoch": 381.9448946515397,
      "grad_norm": 1.490896463394165,
      "learning_rate": 1.1828608498216024e-05,
      "loss": 2.4856,
      "step": 1178300
    },
    {
      "epoch": 381.9773095623987,
      "grad_norm": 1.3928823471069336,
      "learning_rate": 1.182536490431398e-05,
      "loss": 2.4596,
      "step": 1178400
    },
    {
      "epoch": 382.0,
      "eval_bleu": 0.9758395442729672,
      "eval_loss": 4.243289470672607,
      "eval_runtime": 4.0809,
      "eval_samples_per_second": 120.563,
      "eval_steps_per_second": 1.96,
      "step": 1178470
    },
    {
      "epoch": 382.0097244732577,
      "grad_norm": 1.32680082321167,
      "learning_rate": 1.1822121310411936e-05,
      "loss": 2.5041,
      "step": 1178500
    },
    {
      "epoch": 382.0421393841167,
      "grad_norm": 1.3755345344543457,
      "learning_rate": 1.1818877716509893e-05,
      "loss": 2.4836,
      "step": 1178600
    },
    {
      "epoch": 382.0745542949757,
      "grad_norm": 1.2255090475082397,
      "learning_rate": 1.181566655854687e-05,
      "loss": 2.4711,
      "step": 1178700
    },
    {
      "epoch": 382.1069692058347,
      "grad_norm": 1.519047498703003,
      "learning_rate": 1.1812422964644828e-05,
      "loss": 2.5015,
      "step": 1178800
    },
    {
      "epoch": 382.1393841166937,
      "grad_norm": 1.5331758260726929,
      "learning_rate": 1.1809179370742783e-05,
      "loss": 2.4929,
      "step": 1178900
    },
    {
      "epoch": 382.17179902755265,
      "grad_norm": 1.3744785785675049,
      "learning_rate": 1.180593577684074e-05,
      "loss": 2.4845,
      "step": 1179000
    },
    {
      "epoch": 382.20421393841167,
      "grad_norm": 1.4324779510498047,
      "learning_rate": 1.1802692182938696e-05,
      "loss": 2.4663,
      "step": 1179100
    },
    {
      "epoch": 382.2366288492707,
      "grad_norm": 1.4354249238967896,
      "learning_rate": 1.1799448589036653e-05,
      "loss": 2.4858,
      "step": 1179200
    },
    {
      "epoch": 382.26904376012965,
      "grad_norm": 1.812395691871643,
      "learning_rate": 1.1796204995134608e-05,
      "loss": 2.4761,
      "step": 1179300
    },
    {
      "epoch": 382.30145867098867,
      "grad_norm": 1.4070557355880737,
      "learning_rate": 1.1792961401232567e-05,
      "loss": 2.4784,
      "step": 1179400
    },
    {
      "epoch": 382.3338735818476,
      "grad_norm": 1.4686590433120728,
      "learning_rate": 1.1789717807330524e-05,
      "loss": 2.4829,
      "step": 1179500
    },
    {
      "epoch": 382.36628849270664,
      "grad_norm": 1.4015856981277466,
      "learning_rate": 1.178647421342848e-05,
      "loss": 2.4764,
      "step": 1179600
    },
    {
      "epoch": 382.39870340356566,
      "grad_norm": 1.2884279489517212,
      "learning_rate": 1.1783230619526436e-05,
      "loss": 2.4954,
      "step": 1179700
    },
    {
      "epoch": 382.4311183144246,
      "grad_norm": 1.3344606161117554,
      "learning_rate": 1.1779987025624392e-05,
      "loss": 2.4938,
      "step": 1179800
    },
    {
      "epoch": 382.46353322528364,
      "grad_norm": 1.5651637315750122,
      "learning_rate": 1.1776743431722349e-05,
      "loss": 2.4953,
      "step": 1179900
    },
    {
      "epoch": 382.4959481361426,
      "grad_norm": 1.4026545286178589,
      "learning_rate": 1.1773499837820306e-05,
      "loss": 2.4841,
      "step": 1180000
    },
    {
      "epoch": 382.5283630470016,
      "grad_norm": 1.262216329574585,
      "learning_rate": 1.1770256243918263e-05,
      "loss": 2.4845,
      "step": 1180100
    },
    {
      "epoch": 382.56077795786064,
      "grad_norm": 1.5109035968780518,
      "learning_rate": 1.1767012650016218e-05,
      "loss": 2.4907,
      "step": 1180200
    },
    {
      "epoch": 382.5931928687196,
      "grad_norm": 1.1568872928619385,
      "learning_rate": 1.1763769056114175e-05,
      "loss": 2.4842,
      "step": 1180300
    },
    {
      "epoch": 382.6256077795786,
      "grad_norm": 1.4856302738189697,
      "learning_rate": 1.176052546221213e-05,
      "loss": 2.5279,
      "step": 1180400
    },
    {
      "epoch": 382.6580226904376,
      "grad_norm": 1.3135533332824707,
      "learning_rate": 1.1757281868310088e-05,
      "loss": 2.5084,
      "step": 1180500
    },
    {
      "epoch": 382.6904376012966,
      "grad_norm": 1.2483296394348145,
      "learning_rate": 1.1754038274408045e-05,
      "loss": 2.4938,
      "step": 1180600
    },
    {
      "epoch": 382.7228525121556,
      "grad_norm": 1.5077383518218994,
      "learning_rate": 1.1750794680506002e-05,
      "loss": 2.4981,
      "step": 1180700
    },
    {
      "epoch": 382.7552674230146,
      "grad_norm": 1.3325551748275757,
      "learning_rate": 1.1747551086603959e-05,
      "loss": 2.4968,
      "step": 1180800
    },
    {
      "epoch": 382.7876823338736,
      "grad_norm": 1.5908823013305664,
      "learning_rate": 1.1744307492701914e-05,
      "loss": 2.4921,
      "step": 1180900
    },
    {
      "epoch": 382.82009724473255,
      "grad_norm": 1.2482906579971313,
      "learning_rate": 1.1741063898799871e-05,
      "loss": 2.4908,
      "step": 1181000
    },
    {
      "epoch": 382.85251215559157,
      "grad_norm": 1.315824270248413,
      "learning_rate": 1.1737820304897827e-05,
      "loss": 2.5074,
      "step": 1181100
    },
    {
      "epoch": 382.8849270664506,
      "grad_norm": 1.3640815019607544,
      "learning_rate": 1.1734576710995784e-05,
      "loss": 2.4817,
      "step": 1181200
    },
    {
      "epoch": 382.91734197730955,
      "grad_norm": 1.239447832107544,
      "learning_rate": 1.173133311709374e-05,
      "loss": 2.4854,
      "step": 1181300
    },
    {
      "epoch": 382.94975688816857,
      "grad_norm": 1.3448302745819092,
      "learning_rate": 1.1728089523191698e-05,
      "loss": 2.4865,
      "step": 1181400
    },
    {
      "epoch": 382.9821717990275,
      "grad_norm": 1.2515780925750732,
      "learning_rate": 1.1724845929289653e-05,
      "loss": 2.4951,
      "step": 1181500
    },
    {
      "epoch": 383.0,
      "eval_bleu": 1.0082956085540005,
      "eval_loss": 4.240997314453125,
      "eval_runtime": 4.5488,
      "eval_samples_per_second": 108.16,
      "eval_steps_per_second": 1.759,
      "step": 1181555
    },
    {
      "epoch": 383.01458670988654,
      "grad_norm": 1.516841173171997,
      "learning_rate": 1.172160233538761e-05,
      "loss": 2.4964,
      "step": 1181600
    },
    {
      "epoch": 383.04700162074556,
      "grad_norm": 1.2888846397399902,
      "learning_rate": 1.1718358741485566e-05,
      "loss": 2.4982,
      "step": 1181700
    },
    {
      "epoch": 383.0794165316045,
      "grad_norm": 1.3775080442428589,
      "learning_rate": 1.1715115147583523e-05,
      "loss": 2.492,
      "step": 1181800
    },
    {
      "epoch": 383.11183144246354,
      "grad_norm": 1.3684146404266357,
      "learning_rate": 1.171187155368148e-05,
      "loss": 2.4652,
      "step": 1181900
    },
    {
      "epoch": 383.1442463533225,
      "grad_norm": 1.6221832036972046,
      "learning_rate": 1.1708627959779437e-05,
      "loss": 2.4854,
      "step": 1182000
    },
    {
      "epoch": 383.1766612641815,
      "grad_norm": 1.3786379098892212,
      "learning_rate": 1.1705384365877392e-05,
      "loss": 2.4862,
      "step": 1182100
    },
    {
      "epoch": 383.20907617504054,
      "grad_norm": 1.447773814201355,
      "learning_rate": 1.1702140771975349e-05,
      "loss": 2.5102,
      "step": 1182200
    },
    {
      "epoch": 383.2414910858995,
      "grad_norm": 1.5991579294204712,
      "learning_rate": 1.1698897178073306e-05,
      "loss": 2.469,
      "step": 1182300
    },
    {
      "epoch": 383.2739059967585,
      "grad_norm": 1.3686450719833374,
      "learning_rate": 1.1695653584171261e-05,
      "loss": 2.5082,
      "step": 1182400
    },
    {
      "epoch": 383.3063209076175,
      "grad_norm": 1.4395396709442139,
      "learning_rate": 1.169240999026922e-05,
      "loss": 2.4912,
      "step": 1182500
    },
    {
      "epoch": 383.3387358184765,
      "grad_norm": 1.351499319076538,
      "learning_rate": 1.1689166396367176e-05,
      "loss": 2.4635,
      "step": 1182600
    },
    {
      "epoch": 383.3711507293355,
      "grad_norm": 1.2752947807312012,
      "learning_rate": 1.1685955238404152e-05,
      "loss": 2.4793,
      "step": 1182700
    },
    {
      "epoch": 383.4035656401945,
      "grad_norm": 1.5152205228805542,
      "learning_rate": 1.1682711644502109e-05,
      "loss": 2.5124,
      "step": 1182800
    },
    {
      "epoch": 383.4359805510535,
      "grad_norm": 1.5406348705291748,
      "learning_rate": 1.1679468050600064e-05,
      "loss": 2.486,
      "step": 1182900
    },
    {
      "epoch": 383.4683954619125,
      "grad_norm": 1.4028470516204834,
      "learning_rate": 1.1676224456698021e-05,
      "loss": 2.4874,
      "step": 1183000
    },
    {
      "epoch": 383.50081037277147,
      "grad_norm": 1.4815611839294434,
      "learning_rate": 1.167298086279598e-05,
      "loss": 2.4828,
      "step": 1183100
    },
    {
      "epoch": 383.5332252836305,
      "grad_norm": 1.4731249809265137,
      "learning_rate": 1.1669737268893935e-05,
      "loss": 2.4829,
      "step": 1183200
    },
    {
      "epoch": 383.56564019448945,
      "grad_norm": 1.315757393836975,
      "learning_rate": 1.1666493674991892e-05,
      "loss": 2.478,
      "step": 1183300
    },
    {
      "epoch": 383.59805510534846,
      "grad_norm": 1.3955419063568115,
      "learning_rate": 1.1663250081089848e-05,
      "loss": 2.4869,
      "step": 1183400
    },
    {
      "epoch": 383.6304700162075,
      "grad_norm": 1.461859941482544,
      "learning_rate": 1.1660006487187805e-05,
      "loss": 2.478,
      "step": 1183500
    },
    {
      "epoch": 383.66288492706644,
      "grad_norm": 1.2831463813781738,
      "learning_rate": 1.1656762893285762e-05,
      "loss": 2.4842,
      "step": 1183600
    },
    {
      "epoch": 383.69529983792546,
      "grad_norm": 1.4835829734802246,
      "learning_rate": 1.1653519299383719e-05,
      "loss": 2.5015,
      "step": 1183700
    },
    {
      "epoch": 383.7277147487844,
      "grad_norm": 1.383235216140747,
      "learning_rate": 1.1650275705481674e-05,
      "loss": 2.494,
      "step": 1183800
    },
    {
      "epoch": 383.76012965964344,
      "grad_norm": 1.4798327684402466,
      "learning_rate": 1.1647032111579631e-05,
      "loss": 2.4882,
      "step": 1183900
    },
    {
      "epoch": 383.79254457050246,
      "grad_norm": 1.2199208736419678,
      "learning_rate": 1.1643788517677587e-05,
      "loss": 2.5036,
      "step": 1184000
    },
    {
      "epoch": 383.8249594813614,
      "grad_norm": 1.354194164276123,
      "learning_rate": 1.1640544923775544e-05,
      "loss": 2.4859,
      "step": 1184100
    },
    {
      "epoch": 383.85737439222044,
      "grad_norm": 1.6872200965881348,
      "learning_rate": 1.16373013298735e-05,
      "loss": 2.5166,
      "step": 1184200
    },
    {
      "epoch": 383.8897893030794,
      "grad_norm": 1.492287278175354,
      "learning_rate": 1.1634057735971458e-05,
      "loss": 2.5031,
      "step": 1184300
    },
    {
      "epoch": 383.9222042139384,
      "grad_norm": 1.2508221864700317,
      "learning_rate": 1.1630814142069413e-05,
      "loss": 2.4982,
      "step": 1184400
    },
    {
      "epoch": 383.95461912479743,
      "grad_norm": 1.271474838256836,
      "learning_rate": 1.162757054816737e-05,
      "loss": 2.5072,
      "step": 1184500
    },
    {
      "epoch": 383.9870340356564,
      "grad_norm": 1.392716407775879,
      "learning_rate": 1.1624326954265327e-05,
      "loss": 2.4796,
      "step": 1184600
    },
    {
      "epoch": 384.0,
      "eval_bleu": 1.0321510605637128,
      "eval_loss": 4.237929821014404,
      "eval_runtime": 4.2921,
      "eval_samples_per_second": 114.628,
      "eval_steps_per_second": 1.864,
      "step": 1184640
    },
    {
      "epoch": 384.0194489465154,
      "grad_norm": 1.3033497333526611,
      "learning_rate": 1.1621115796302303e-05,
      "loss": 2.4888,
      "step": 1184700
    },
    {
      "epoch": 384.05186385737437,
      "grad_norm": 1.4192928075790405,
      "learning_rate": 1.161787220240026e-05,
      "loss": 2.4798,
      "step": 1184800
    },
    {
      "epoch": 384.0842787682334,
      "grad_norm": 1.3079763650894165,
      "learning_rate": 1.1614628608498217e-05,
      "loss": 2.4868,
      "step": 1184900
    },
    {
      "epoch": 384.1166936790924,
      "grad_norm": 1.3855204582214355,
      "learning_rate": 1.1611385014596173e-05,
      "loss": 2.4788,
      "step": 1185000
    },
    {
      "epoch": 384.14910858995137,
      "grad_norm": 1.3873480558395386,
      "learning_rate": 1.160814142069413e-05,
      "loss": 2.4999,
      "step": 1185100
    },
    {
      "epoch": 384.1815235008104,
      "grad_norm": 1.3688569068908691,
      "learning_rate": 1.1604897826792087e-05,
      "loss": 2.4781,
      "step": 1185200
    },
    {
      "epoch": 384.21393841166935,
      "grad_norm": 1.3701874017715454,
      "learning_rate": 1.1601654232890042e-05,
      "loss": 2.4881,
      "step": 1185300
    },
    {
      "epoch": 384.24635332252836,
      "grad_norm": 1.258298635482788,
      "learning_rate": 1.1598410638988e-05,
      "loss": 2.4836,
      "step": 1185400
    },
    {
      "epoch": 384.2787682333874,
      "grad_norm": 1.5234997272491455,
      "learning_rate": 1.1595167045085956e-05,
      "loss": 2.483,
      "step": 1185500
    },
    {
      "epoch": 384.31118314424634,
      "grad_norm": 1.395573377609253,
      "learning_rate": 1.1591923451183913e-05,
      "loss": 2.4889,
      "step": 1185600
    },
    {
      "epoch": 384.34359805510536,
      "grad_norm": 1.3516597747802734,
      "learning_rate": 1.1588679857281869e-05,
      "loss": 2.4928,
      "step": 1185700
    },
    {
      "epoch": 384.3760129659643,
      "grad_norm": 1.4121893644332886,
      "learning_rate": 1.1585436263379826e-05,
      "loss": 2.481,
      "step": 1185800
    },
    {
      "epoch": 384.40842787682334,
      "grad_norm": 1.492276668548584,
      "learning_rate": 1.1582192669477781e-05,
      "loss": 2.5004,
      "step": 1185900
    },
    {
      "epoch": 384.44084278768236,
      "grad_norm": 1.4721287488937378,
      "learning_rate": 1.1578949075575738e-05,
      "loss": 2.4829,
      "step": 1186000
    },
    {
      "epoch": 384.4732576985413,
      "grad_norm": 1.4448097944259644,
      "learning_rate": 1.1575705481673695e-05,
      "loss": 2.4709,
      "step": 1186100
    },
    {
      "epoch": 384.50567260940034,
      "grad_norm": 1.4846011400222778,
      "learning_rate": 1.1572461887771652e-05,
      "loss": 2.5123,
      "step": 1186200
    },
    {
      "epoch": 384.5380875202593,
      "grad_norm": 1.5382838249206543,
      "learning_rate": 1.1569218293869608e-05,
      "loss": 2.4879,
      "step": 1186300
    },
    {
      "epoch": 384.5705024311183,
      "grad_norm": 1.3929823637008667,
      "learning_rate": 1.1565974699967565e-05,
      "loss": 2.4969,
      "step": 1186400
    },
    {
      "epoch": 384.60291734197733,
      "grad_norm": 1.4885398149490356,
      "learning_rate": 1.156273110606552e-05,
      "loss": 2.5137,
      "step": 1186500
    },
    {
      "epoch": 384.6353322528363,
      "grad_norm": 1.2336338758468628,
      "learning_rate": 1.1559487512163477e-05,
      "loss": 2.4782,
      "step": 1186600
    },
    {
      "epoch": 384.6677471636953,
      "grad_norm": 1.5016154050827026,
      "learning_rate": 1.1556276354200455e-05,
      "loss": 2.481,
      "step": 1186700
    },
    {
      "epoch": 384.70016207455427,
      "grad_norm": 1.4811182022094727,
      "learning_rate": 1.1553032760298412e-05,
      "loss": 2.4871,
      "step": 1186800
    },
    {
      "epoch": 384.7325769854133,
      "grad_norm": 1.2640198469161987,
      "learning_rate": 1.1549789166396367e-05,
      "loss": 2.4888,
      "step": 1186900
    },
    {
      "epoch": 384.7649918962723,
      "grad_norm": 1.4116096496582031,
      "learning_rate": 1.1546545572494324e-05,
      "loss": 2.4853,
      "step": 1187000
    },
    {
      "epoch": 384.79740680713127,
      "grad_norm": 1.477982997894287,
      "learning_rate": 1.154330197859228e-05,
      "loss": 2.4888,
      "step": 1187100
    },
    {
      "epoch": 384.8298217179903,
      "grad_norm": 1.2119032144546509,
      "learning_rate": 1.1540058384690237e-05,
      "loss": 2.485,
      "step": 1187200
    },
    {
      "epoch": 384.86223662884925,
      "grad_norm": 1.3196487426757812,
      "learning_rate": 1.1536814790788194e-05,
      "loss": 2.4664,
      "step": 1187300
    },
    {
      "epoch": 384.89465153970826,
      "grad_norm": 1.4357048273086548,
      "learning_rate": 1.153357119688615e-05,
      "loss": 2.5013,
      "step": 1187400
    },
    {
      "epoch": 384.9270664505673,
      "grad_norm": 1.2672947645187378,
      "learning_rate": 1.1530327602984108e-05,
      "loss": 2.5075,
      "step": 1187500
    },
    {
      "epoch": 384.95948136142624,
      "grad_norm": 1.5704582929611206,
      "learning_rate": 1.1527084009082063e-05,
      "loss": 2.4871,
      "step": 1187600
    },
    {
      "epoch": 384.99189627228526,
      "grad_norm": 1.4638766050338745,
      "learning_rate": 1.152384041518002e-05,
      "loss": 2.481,
      "step": 1187700
    },
    {
      "epoch": 385.0,
      "eval_bleu": 0.9671856375868663,
      "eval_loss": 4.242176055908203,
      "eval_runtime": 4.3934,
      "eval_samples_per_second": 111.985,
      "eval_steps_per_second": 1.821,
      "step": 1187725
    },
    {
      "epoch": 385.0243111831442,
      "grad_norm": 1.5088238716125488,
      "learning_rate": 1.1520596821277976e-05,
      "loss": 2.4805,
      "step": 1187800
    },
    {
      "epoch": 385.05672609400324,
      "grad_norm": 1.3098938465118408,
      "learning_rate": 1.1517353227375934e-05,
      "loss": 2.5025,
      "step": 1187900
    },
    {
      "epoch": 385.08914100486226,
      "grad_norm": 1.3767402172088623,
      "learning_rate": 1.151410963347389e-05,
      "loss": 2.4811,
      "step": 1188000
    },
    {
      "epoch": 385.1215559157212,
      "grad_norm": 1.460996389389038,
      "learning_rate": 1.1510866039571847e-05,
      "loss": 2.4884,
      "step": 1188100
    },
    {
      "epoch": 385.15397082658023,
      "grad_norm": 1.447776436805725,
      "learning_rate": 1.1507622445669802e-05,
      "loss": 2.4875,
      "step": 1188200
    },
    {
      "epoch": 385.1863857374392,
      "grad_norm": 1.4629672765731812,
      "learning_rate": 1.1504378851767759e-05,
      "loss": 2.455,
      "step": 1188300
    },
    {
      "epoch": 385.2188006482982,
      "grad_norm": 1.5257552862167358,
      "learning_rate": 1.1501135257865714e-05,
      "loss": 2.4917,
      "step": 1188400
    },
    {
      "epoch": 385.25121555915723,
      "grad_norm": 1.658700704574585,
      "learning_rate": 1.1497891663963673e-05,
      "loss": 2.5089,
      "step": 1188500
    },
    {
      "epoch": 385.2836304700162,
      "grad_norm": 1.4467413425445557,
      "learning_rate": 1.1494648070061629e-05,
      "loss": 2.4869,
      "step": 1188600
    },
    {
      "epoch": 385.3160453808752,
      "grad_norm": 1.4865362644195557,
      "learning_rate": 1.1491436912098606e-05,
      "loss": 2.4828,
      "step": 1188700
    },
    {
      "epoch": 385.34846029173417,
      "grad_norm": 1.4043121337890625,
      "learning_rate": 1.1488193318196562e-05,
      "loss": 2.4716,
      "step": 1188800
    },
    {
      "epoch": 385.3808752025932,
      "grad_norm": 1.4223482608795166,
      "learning_rate": 1.1484949724294519e-05,
      "loss": 2.4954,
      "step": 1188900
    },
    {
      "epoch": 385.4132901134522,
      "grad_norm": 1.4797656536102295,
      "learning_rate": 1.1481706130392476e-05,
      "loss": 2.4797,
      "step": 1189000
    },
    {
      "epoch": 385.44570502431117,
      "grad_norm": 1.3653842210769653,
      "learning_rate": 1.1478462536490433e-05,
      "loss": 2.4919,
      "step": 1189100
    },
    {
      "epoch": 385.4781199351702,
      "grad_norm": 1.4025574922561646,
      "learning_rate": 1.1475218942588388e-05,
      "loss": 2.4886,
      "step": 1189200
    },
    {
      "epoch": 385.51053484602915,
      "grad_norm": 1.4905071258544922,
      "learning_rate": 1.1471975348686345e-05,
      "loss": 2.4979,
      "step": 1189300
    },
    {
      "epoch": 385.54294975688816,
      "grad_norm": 1.4575949907302856,
      "learning_rate": 1.14687317547843e-05,
      "loss": 2.4775,
      "step": 1189400
    },
    {
      "epoch": 385.5753646677472,
      "grad_norm": 1.3720662593841553,
      "learning_rate": 1.1465488160882258e-05,
      "loss": 2.4697,
      "step": 1189500
    },
    {
      "epoch": 385.60777957860614,
      "grad_norm": 1.2462679147720337,
      "learning_rate": 1.1462244566980215e-05,
      "loss": 2.4807,
      "step": 1189600
    },
    {
      "epoch": 385.64019448946516,
      "grad_norm": 1.5227065086364746,
      "learning_rate": 1.1459000973078172e-05,
      "loss": 2.4822,
      "step": 1189700
    },
    {
      "epoch": 385.6726094003242,
      "grad_norm": 1.37566077709198,
      "learning_rate": 1.1455757379176129e-05,
      "loss": 2.4933,
      "step": 1189800
    },
    {
      "epoch": 385.70502431118314,
      "grad_norm": 1.212364673614502,
      "learning_rate": 1.1452513785274084e-05,
      "loss": 2.5056,
      "step": 1189900
    },
    {
      "epoch": 385.73743922204216,
      "grad_norm": 1.399558186531067,
      "learning_rate": 1.1449270191372041e-05,
      "loss": 2.5051,
      "step": 1190000
    },
    {
      "epoch": 385.7698541329011,
      "grad_norm": 1.3965423107147217,
      "learning_rate": 1.1446026597469997e-05,
      "loss": 2.459,
      "step": 1190100
    },
    {
      "epoch": 385.80226904376013,
      "grad_norm": 1.368438720703125,
      "learning_rate": 1.1442783003567954e-05,
      "loss": 2.4968,
      "step": 1190200
    },
    {
      "epoch": 385.83468395461915,
      "grad_norm": 1.3151565790176392,
      "learning_rate": 1.143953940966591e-05,
      "loss": 2.4901,
      "step": 1190300
    },
    {
      "epoch": 385.8670988654781,
      "grad_norm": 1.4543418884277344,
      "learning_rate": 1.1436295815763868e-05,
      "loss": 2.4888,
      "step": 1190400
    },
    {
      "epoch": 385.89951377633713,
      "grad_norm": 1.4225555658340454,
      "learning_rate": 1.1433052221861823e-05,
      "loss": 2.4936,
      "step": 1190500
    },
    {
      "epoch": 385.9319286871961,
      "grad_norm": 1.4149898290634155,
      "learning_rate": 1.142980862795978e-05,
      "loss": 2.5039,
      "step": 1190600
    },
    {
      "epoch": 385.9643435980551,
      "grad_norm": 1.469802975654602,
      "learning_rate": 1.1426597469996756e-05,
      "loss": 2.4895,
      "step": 1190700
    },
    {
      "epoch": 385.9967585089141,
      "grad_norm": 1.2663439512252808,
      "learning_rate": 1.1423353876094713e-05,
      "loss": 2.4964,
      "step": 1190800
    },
    {
      "epoch": 386.0,
      "eval_bleu": 0.9985219721944548,
      "eval_loss": 4.2464447021484375,
      "eval_runtime": 4.3695,
      "eval_samples_per_second": 112.6,
      "eval_steps_per_second": 1.831,
      "step": 1190810
    },
    {
      "epoch": 386.0291734197731,
      "grad_norm": 1.5207138061523438,
      "learning_rate": 1.142011028219267e-05,
      "loss": 2.4916,
      "step": 1190900
    },
    {
      "epoch": 386.0615883306321,
      "grad_norm": 1.2276331186294556,
      "learning_rate": 1.1416866688290627e-05,
      "loss": 2.501,
      "step": 1191000
    },
    {
      "epoch": 386.09400324149107,
      "grad_norm": 1.515320062637329,
      "learning_rate": 1.1413655530327603e-05,
      "loss": 2.4696,
      "step": 1191100
    },
    {
      "epoch": 386.1264181523501,
      "grad_norm": 1.495035171508789,
      "learning_rate": 1.141041193642556e-05,
      "loss": 2.4793,
      "step": 1191200
    },
    {
      "epoch": 386.1588330632091,
      "grad_norm": 1.2872021198272705,
      "learning_rate": 1.1407168342523516e-05,
      "loss": 2.4873,
      "step": 1191300
    },
    {
      "epoch": 386.19124797406806,
      "grad_norm": 1.3832228183746338,
      "learning_rate": 1.1403924748621473e-05,
      "loss": 2.4814,
      "step": 1191400
    },
    {
      "epoch": 386.2236628849271,
      "grad_norm": 1.6068228483200073,
      "learning_rate": 1.140068115471943e-05,
      "loss": 2.491,
      "step": 1191500
    },
    {
      "epoch": 386.25607779578604,
      "grad_norm": 1.6613250970840454,
      "learning_rate": 1.1397437560817387e-05,
      "loss": 2.4911,
      "step": 1191600
    },
    {
      "epoch": 386.28849270664506,
      "grad_norm": 1.4232995510101318,
      "learning_rate": 1.1394193966915342e-05,
      "loss": 2.4966,
      "step": 1191700
    },
    {
      "epoch": 386.3209076175041,
      "grad_norm": 1.5677821636199951,
      "learning_rate": 1.13909503730133e-05,
      "loss": 2.484,
      "step": 1191800
    },
    {
      "epoch": 386.35332252836304,
      "grad_norm": 1.4917185306549072,
      "learning_rate": 1.1387706779111255e-05,
      "loss": 2.49,
      "step": 1191900
    },
    {
      "epoch": 386.38573743922205,
      "grad_norm": 1.7447233200073242,
      "learning_rate": 1.1384463185209212e-05,
      "loss": 2.4956,
      "step": 1192000
    },
    {
      "epoch": 386.418152350081,
      "grad_norm": 1.3011177778244019,
      "learning_rate": 1.1381219591307169e-05,
      "loss": 2.4771,
      "step": 1192100
    },
    {
      "epoch": 386.45056726094003,
      "grad_norm": 1.3958643674850464,
      "learning_rate": 1.1377975997405126e-05,
      "loss": 2.4875,
      "step": 1192200
    },
    {
      "epoch": 386.48298217179905,
      "grad_norm": 1.2871404886245728,
      "learning_rate": 1.1374732403503081e-05,
      "loss": 2.4813,
      "step": 1192300
    },
    {
      "epoch": 386.515397082658,
      "grad_norm": 1.477735996246338,
      "learning_rate": 1.1371488809601038e-05,
      "loss": 2.4996,
      "step": 1192400
    },
    {
      "epoch": 386.54781199351703,
      "grad_norm": 1.399451494216919,
      "learning_rate": 1.1368245215698994e-05,
      "loss": 2.4855,
      "step": 1192500
    },
    {
      "epoch": 386.580226904376,
      "grad_norm": 1.3369488716125488,
      "learning_rate": 1.136500162179695e-05,
      "loss": 2.4886,
      "step": 1192600
    },
    {
      "epoch": 386.612641815235,
      "grad_norm": 1.4399060010910034,
      "learning_rate": 1.136175802789491e-05,
      "loss": 2.5028,
      "step": 1192700
    },
    {
      "epoch": 386.645056726094,
      "grad_norm": 1.4082612991333008,
      "learning_rate": 1.1358514433992865e-05,
      "loss": 2.4881,
      "step": 1192800
    },
    {
      "epoch": 386.677471636953,
      "grad_norm": 1.4848759174346924,
      "learning_rate": 1.1355270840090822e-05,
      "loss": 2.4967,
      "step": 1192900
    },
    {
      "epoch": 386.709886547812,
      "grad_norm": 1.4728972911834717,
      "learning_rate": 1.1352027246188777e-05,
      "loss": 2.4894,
      "step": 1193000
    },
    {
      "epoch": 386.74230145867097,
      "grad_norm": 1.2330394983291626,
      "learning_rate": 1.1348783652286734e-05,
      "loss": 2.4979,
      "step": 1193100
    },
    {
      "epoch": 386.77471636953,
      "grad_norm": 1.301705002784729,
      "learning_rate": 1.134554005838469e-05,
      "loss": 2.4942,
      "step": 1193200
    },
    {
      "epoch": 386.807131280389,
      "grad_norm": 1.319280982017517,
      "learning_rate": 1.1342328900421667e-05,
      "loss": 2.4916,
      "step": 1193300
    },
    {
      "epoch": 386.83954619124796,
      "grad_norm": 1.3809781074523926,
      "learning_rate": 1.1339085306519624e-05,
      "loss": 2.4655,
      "step": 1193400
    },
    {
      "epoch": 386.871961102107,
      "grad_norm": 1.3473045825958252,
      "learning_rate": 1.1335841712617582e-05,
      "loss": 2.4795,
      "step": 1193500
    },
    {
      "epoch": 386.90437601296594,
      "grad_norm": 1.3754438161849976,
      "learning_rate": 1.1332598118715537e-05,
      "loss": 2.4935,
      "step": 1193600
    },
    {
      "epoch": 386.93679092382496,
      "grad_norm": 1.4264541864395142,
      "learning_rate": 1.1329354524813494e-05,
      "loss": 2.5045,
      "step": 1193700
    },
    {
      "epoch": 386.969205834684,
      "grad_norm": 1.4005850553512573,
      "learning_rate": 1.1326110930911451e-05,
      "loss": 2.4809,
      "step": 1193800
    },
    {
      "epoch": 387.0,
      "eval_bleu": 0.9895672445274141,
      "eval_loss": 4.248386383056641,
      "eval_runtime": 4.9856,
      "eval_samples_per_second": 98.683,
      "eval_steps_per_second": 1.605,
      "step": 1193895
    },
    {
      "epoch": 387.00162074554294,
      "grad_norm": 1.4627853631973267,
      "learning_rate": 1.1322867337009408e-05,
      "loss": 2.4832,
      "step": 1193900
    },
    {
      "epoch": 387.03403565640195,
      "grad_norm": 1.1991740465164185,
      "learning_rate": 1.1319623743107363e-05,
      "loss": 2.4907,
      "step": 1194000
    },
    {
      "epoch": 387.0664505672609,
      "grad_norm": 1.5943418741226196,
      "learning_rate": 1.131638014920532e-05,
      "loss": 2.481,
      "step": 1194100
    },
    {
      "epoch": 387.09886547811993,
      "grad_norm": 1.5172480344772339,
      "learning_rate": 1.1313136555303276e-05,
      "loss": 2.4721,
      "step": 1194200
    },
    {
      "epoch": 387.13128038897895,
      "grad_norm": 1.5893611907958984,
      "learning_rate": 1.1309892961401233e-05,
      "loss": 2.4959,
      "step": 1194300
    },
    {
      "epoch": 387.1636952998379,
      "grad_norm": 1.5816482305526733,
      "learning_rate": 1.130664936749919e-05,
      "loss": 2.4867,
      "step": 1194400
    },
    {
      "epoch": 387.19611021069693,
      "grad_norm": 1.5021140575408936,
      "learning_rate": 1.1303405773597147e-05,
      "loss": 2.4678,
      "step": 1194500
    },
    {
      "epoch": 387.2285251215559,
      "grad_norm": 1.3415806293487549,
      "learning_rate": 1.1300162179695102e-05,
      "loss": 2.5004,
      "step": 1194600
    },
    {
      "epoch": 387.2609400324149,
      "grad_norm": 1.656075358390808,
      "learning_rate": 1.129691858579306e-05,
      "loss": 2.48,
      "step": 1194700
    },
    {
      "epoch": 387.2933549432739,
      "grad_norm": 1.4311292171478271,
      "learning_rate": 1.1293674991891016e-05,
      "loss": 2.4997,
      "step": 1194800
    },
    {
      "epoch": 387.3257698541329,
      "grad_norm": 1.3354992866516113,
      "learning_rate": 1.1290431397988972e-05,
      "loss": 2.4809,
      "step": 1194900
    },
    {
      "epoch": 387.3581847649919,
      "grad_norm": 1.3000116348266602,
      "learning_rate": 1.1287187804086929e-05,
      "loss": 2.4797,
      "step": 1195000
    },
    {
      "epoch": 387.39059967585086,
      "grad_norm": 1.423622965812683,
      "learning_rate": 1.1283944210184886e-05,
      "loss": 2.4854,
      "step": 1195100
    },
    {
      "epoch": 387.4230145867099,
      "grad_norm": 1.3758314847946167,
      "learning_rate": 1.1280700616282843e-05,
      "loss": 2.4884,
      "step": 1195200
    },
    {
      "epoch": 387.4554294975689,
      "grad_norm": 1.5784049034118652,
      "learning_rate": 1.1277457022380798e-05,
      "loss": 2.4921,
      "step": 1195300
    },
    {
      "epoch": 387.48784440842786,
      "grad_norm": 1.3464388847351074,
      "learning_rate": 1.1274213428478755e-05,
      "loss": 2.4929,
      "step": 1195400
    },
    {
      "epoch": 387.5202593192869,
      "grad_norm": 1.4288804531097412,
      "learning_rate": 1.127096983457671e-05,
      "loss": 2.492,
      "step": 1195500
    },
    {
      "epoch": 387.55267423014584,
      "grad_norm": 1.6335945129394531,
      "learning_rate": 1.1267726240674668e-05,
      "loss": 2.4872,
      "step": 1195600
    },
    {
      "epoch": 387.58508914100486,
      "grad_norm": 1.4619334936141968,
      "learning_rate": 1.1264482646772625e-05,
      "loss": 2.5004,
      "step": 1195700
    },
    {
      "epoch": 387.6175040518639,
      "grad_norm": 1.3051949739456177,
      "learning_rate": 1.1261239052870582e-05,
      "loss": 2.4734,
      "step": 1195800
    },
    {
      "epoch": 387.64991896272284,
      "grad_norm": 1.3233164548873901,
      "learning_rate": 1.1257995458968537e-05,
      "loss": 2.4742,
      "step": 1195900
    },
    {
      "epoch": 387.68233387358185,
      "grad_norm": 1.3150112628936768,
      "learning_rate": 1.1254751865066494e-05,
      "loss": 2.498,
      "step": 1196000
    },
    {
      "epoch": 387.7147487844408,
      "grad_norm": 1.195103406906128,
      "learning_rate": 1.125150827116445e-05,
      "loss": 2.5084,
      "step": 1196100
    },
    {
      "epoch": 387.74716369529983,
      "grad_norm": 1.3392549753189087,
      "learning_rate": 1.1248264677262407e-05,
      "loss": 2.4858,
      "step": 1196200
    },
    {
      "epoch": 387.77957860615885,
      "grad_norm": 1.331332802772522,
      "learning_rate": 1.1245021083360364e-05,
      "loss": 2.4804,
      "step": 1196300
    },
    {
      "epoch": 387.8119935170178,
      "grad_norm": 1.410278558731079,
      "learning_rate": 1.124177748945832e-05,
      "loss": 2.4991,
      "step": 1196400
    },
    {
      "epoch": 387.84440842787683,
      "grad_norm": 1.5349187850952148,
      "learning_rate": 1.1238533895556278e-05,
      "loss": 2.4764,
      "step": 1196500
    },
    {
      "epoch": 387.87682333873585,
      "grad_norm": 1.492103099822998,
      "learning_rate": 1.1235290301654233e-05,
      "loss": 2.4999,
      "step": 1196600
    },
    {
      "epoch": 387.9092382495948,
      "grad_norm": 1.4338346719741821,
      "learning_rate": 1.123207914369121e-05,
      "loss": 2.4937,
      "step": 1196700
    },
    {
      "epoch": 387.9416531604538,
      "grad_norm": 1.418195366859436,
      "learning_rate": 1.1228835549789166e-05,
      "loss": 2.4831,
      "step": 1196800
    },
    {
      "epoch": 387.9740680713128,
      "grad_norm": 1.2959247827529907,
      "learning_rate": 1.1225591955887123e-05,
      "loss": 2.4835,
      "step": 1196900
    },
    {
      "epoch": 388.0,
      "eval_bleu": 0.9311801927459363,
      "eval_loss": 4.248363018035889,
      "eval_runtime": 4.6559,
      "eval_samples_per_second": 105.671,
      "eval_steps_per_second": 1.718,
      "step": 1196980
    },
    {
      "epoch": 388.0064829821718,
      "grad_norm": 1.3659697771072388,
      "learning_rate": 1.122234836198508e-05,
      "loss": 2.4865,
      "step": 1197000
    },
    {
      "epoch": 388.0388978930308,
      "grad_norm": 1.44679856300354,
      "learning_rate": 1.1219104768083037e-05,
      "loss": 2.5021,
      "step": 1197100
    },
    {
      "epoch": 388.0713128038898,
      "grad_norm": 1.538874864578247,
      "learning_rate": 1.1215861174180993e-05,
      "loss": 2.4625,
      "step": 1197200
    },
    {
      "epoch": 388.1037277147488,
      "grad_norm": 1.4831185340881348,
      "learning_rate": 1.121261758027895e-05,
      "loss": 2.4985,
      "step": 1197300
    },
    {
      "epoch": 388.13614262560776,
      "grad_norm": 1.2355246543884277,
      "learning_rate": 1.1209373986376905e-05,
      "loss": 2.4749,
      "step": 1197400
    },
    {
      "epoch": 388.1685575364668,
      "grad_norm": 1.373432993888855,
      "learning_rate": 1.1206130392474864e-05,
      "loss": 2.4843,
      "step": 1197500
    },
    {
      "epoch": 388.2009724473258,
      "grad_norm": 1.4022150039672852,
      "learning_rate": 1.120288679857282e-05,
      "loss": 2.472,
      "step": 1197600
    },
    {
      "epoch": 388.23338735818476,
      "grad_norm": 1.6183456182479858,
      "learning_rate": 1.1199643204670776e-05,
      "loss": 2.5049,
      "step": 1197700
    },
    {
      "epoch": 388.2658022690438,
      "grad_norm": 1.2864242792129517,
      "learning_rate": 1.1196399610768732e-05,
      "loss": 2.4821,
      "step": 1197800
    },
    {
      "epoch": 388.29821717990274,
      "grad_norm": 1.3731824159622192,
      "learning_rate": 1.1193156016866689e-05,
      "loss": 2.4841,
      "step": 1197900
    },
    {
      "epoch": 388.33063209076175,
      "grad_norm": 1.5043673515319824,
      "learning_rate": 1.1189912422964644e-05,
      "loss": 2.4973,
      "step": 1198000
    },
    {
      "epoch": 388.36304700162077,
      "grad_norm": 1.3482980728149414,
      "learning_rate": 1.1186668829062603e-05,
      "loss": 2.477,
      "step": 1198100
    },
    {
      "epoch": 388.39546191247973,
      "grad_norm": 1.526394009590149,
      "learning_rate": 1.1183425235160558e-05,
      "loss": 2.5001,
      "step": 1198200
    },
    {
      "epoch": 388.42787682333875,
      "grad_norm": 1.5496819019317627,
      "learning_rate": 1.1180181641258515e-05,
      "loss": 2.4871,
      "step": 1198300
    },
    {
      "epoch": 388.4602917341977,
      "grad_norm": 1.4292558431625366,
      "learning_rate": 1.117693804735647e-05,
      "loss": 2.4909,
      "step": 1198400
    },
    {
      "epoch": 388.4927066450567,
      "grad_norm": 1.228140950202942,
      "learning_rate": 1.1173694453454428e-05,
      "loss": 2.5056,
      "step": 1198500
    },
    {
      "epoch": 388.52512155591575,
      "grad_norm": 1.6272385120391846,
      "learning_rate": 1.1170450859552385e-05,
      "loss": 2.4731,
      "step": 1198600
    },
    {
      "epoch": 388.5575364667747,
      "grad_norm": 1.36575448513031,
      "learning_rate": 1.1167207265650342e-05,
      "loss": 2.474,
      "step": 1198700
    },
    {
      "epoch": 388.5899513776337,
      "grad_norm": 1.3160287141799927,
      "learning_rate": 1.1163963671748299e-05,
      "loss": 2.4839,
      "step": 1198800
    },
    {
      "epoch": 388.6223662884927,
      "grad_norm": 1.3999907970428467,
      "learning_rate": 1.1160720077846254e-05,
      "loss": 2.4758,
      "step": 1198900
    },
    {
      "epoch": 388.6547811993517,
      "grad_norm": 1.3616000413894653,
      "learning_rate": 1.1157476483944211e-05,
      "loss": 2.4685,
      "step": 1199000
    },
    {
      "epoch": 388.6871961102107,
      "grad_norm": 1.4376205205917358,
      "learning_rate": 1.1154232890042167e-05,
      "loss": 2.4876,
      "step": 1199100
    },
    {
      "epoch": 388.7196110210697,
      "grad_norm": 1.3178608417510986,
      "learning_rate": 1.1150989296140124e-05,
      "loss": 2.4922,
      "step": 1199200
    },
    {
      "epoch": 388.7520259319287,
      "grad_norm": 1.4990830421447754,
      "learning_rate": 1.114774570223808e-05,
      "loss": 2.4871,
      "step": 1199300
    },
    {
      "epoch": 388.78444084278766,
      "grad_norm": 1.3108950853347778,
      "learning_rate": 1.1144502108336038e-05,
      "loss": 2.4962,
      "step": 1199400
    },
    {
      "epoch": 388.8168557536467,
      "grad_norm": 1.503867506980896,
      "learning_rate": 1.1141258514433993e-05,
      "loss": 2.4791,
      "step": 1199500
    },
    {
      "epoch": 388.8492706645057,
      "grad_norm": 1.3266915082931519,
      "learning_rate": 1.113801492053195e-05,
      "loss": 2.4859,
      "step": 1199600
    },
    {
      "epoch": 388.88168557536466,
      "grad_norm": 1.5347892045974731,
      "learning_rate": 1.1134771326629905e-05,
      "loss": 2.4728,
      "step": 1199700
    },
    {
      "epoch": 388.9141004862237,
      "grad_norm": 1.475372076034546,
      "learning_rate": 1.1131527732727862e-05,
      "loss": 2.4922,
      "step": 1199800
    },
    {
      "epoch": 388.94651539708263,
      "grad_norm": 1.4422506093978882,
      "learning_rate": 1.112828413882582e-05,
      "loss": 2.5055,
      "step": 1199900
    },
    {
      "epoch": 388.97893030794165,
      "grad_norm": 1.4623452425003052,
      "learning_rate": 1.1125040544923777e-05,
      "loss": 2.49,
      "step": 1200000
    },
    {
      "epoch": 389.0,
      "eval_bleu": 1.0827660421113379,
      "eval_loss": 4.247505187988281,
      "eval_runtime": 4.3544,
      "eval_samples_per_second": 112.988,
      "eval_steps_per_second": 1.837,
      "step": 1200065
    },
    {
      "epoch": 389.01134521880067,
      "grad_norm": 1.42789626121521,
      "learning_rate": 1.1121796951021734e-05,
      "loss": 2.4786,
      "step": 1200100
    },
    {
      "epoch": 389.04376012965963,
      "grad_norm": 1.4769889116287231,
      "learning_rate": 1.1118553357119689e-05,
      "loss": 2.4898,
      "step": 1200200
    },
    {
      "epoch": 389.07617504051865,
      "grad_norm": 1.662103295326233,
      "learning_rate": 1.1115309763217646e-05,
      "loss": 2.4829,
      "step": 1200300
    },
    {
      "epoch": 389.1085899513776,
      "grad_norm": 1.3848568201065063,
      "learning_rate": 1.1112066169315601e-05,
      "loss": 2.4859,
      "step": 1200400
    },
    {
      "epoch": 389.1410048622366,
      "grad_norm": 1.5340445041656494,
      "learning_rate": 1.1108822575413558e-05,
      "loss": 2.4948,
      "step": 1200500
    },
    {
      "epoch": 389.17341977309565,
      "grad_norm": 1.7641433477401733,
      "learning_rate": 1.1105578981511515e-05,
      "loss": 2.4662,
      "step": 1200600
    },
    {
      "epoch": 389.2058346839546,
      "grad_norm": 1.5022797584533691,
      "learning_rate": 1.1102367823548493e-05,
      "loss": 2.489,
      "step": 1200700
    },
    {
      "epoch": 389.2382495948136,
      "grad_norm": 1.488935112953186,
      "learning_rate": 1.1099124229646449e-05,
      "loss": 2.4777,
      "step": 1200800
    },
    {
      "epoch": 389.2706645056726,
      "grad_norm": 1.5151277780532837,
      "learning_rate": 1.1095880635744406e-05,
      "loss": 2.4825,
      "step": 1200900
    },
    {
      "epoch": 389.3030794165316,
      "grad_norm": 1.2964049577713013,
      "learning_rate": 1.1092637041842361e-05,
      "loss": 2.4922,
      "step": 1201000
    },
    {
      "epoch": 389.3354943273906,
      "grad_norm": 1.4113870859146118,
      "learning_rate": 1.1089393447940318e-05,
      "loss": 2.4771,
      "step": 1201100
    },
    {
      "epoch": 389.3679092382496,
      "grad_norm": 1.4171770811080933,
      "learning_rate": 1.1086149854038275e-05,
      "loss": 2.4647,
      "step": 1201200
    },
    {
      "epoch": 389.4003241491086,
      "grad_norm": 1.5153502225875854,
      "learning_rate": 1.1082906260136232e-05,
      "loss": 2.4628,
      "step": 1201300
    },
    {
      "epoch": 389.43273905996756,
      "grad_norm": 1.41838800907135,
      "learning_rate": 1.1079662666234188e-05,
      "loss": 2.4904,
      "step": 1201400
    },
    {
      "epoch": 389.4651539708266,
      "grad_norm": 1.5252355337142944,
      "learning_rate": 1.1076419072332145e-05,
      "loss": 2.487,
      "step": 1201500
    },
    {
      "epoch": 389.4975688816856,
      "grad_norm": 1.4588754177093506,
      "learning_rate": 1.10731754784301e-05,
      "loss": 2.486,
      "step": 1201600
    },
    {
      "epoch": 389.52998379254456,
      "grad_norm": 1.5922513008117676,
      "learning_rate": 1.1069931884528057e-05,
      "loss": 2.4846,
      "step": 1201700
    },
    {
      "epoch": 389.5623987034036,
      "grad_norm": 1.3206714391708374,
      "learning_rate": 1.1066688290626014e-05,
      "loss": 2.5125,
      "step": 1201800
    },
    {
      "epoch": 389.59481361426253,
      "grad_norm": 1.2948875427246094,
      "learning_rate": 1.1063477132662992e-05,
      "loss": 2.469,
      "step": 1201900
    },
    {
      "epoch": 389.62722852512155,
      "grad_norm": 1.2377545833587646,
      "learning_rate": 1.1060233538760947e-05,
      "loss": 2.4952,
      "step": 1202000
    },
    {
      "epoch": 389.65964343598057,
      "grad_norm": 1.4061754941940308,
      "learning_rate": 1.1056989944858904e-05,
      "loss": 2.4914,
      "step": 1202100
    },
    {
      "epoch": 389.69205834683953,
      "grad_norm": 1.4173611402511597,
      "learning_rate": 1.105374635095686e-05,
      "loss": 2.5082,
      "step": 1202200
    },
    {
      "epoch": 389.72447325769855,
      "grad_norm": 1.5075106620788574,
      "learning_rate": 1.1050502757054818e-05,
      "loss": 2.4765,
      "step": 1202300
    },
    {
      "epoch": 389.7568881685575,
      "grad_norm": 1.3773213624954224,
      "learning_rate": 1.1047259163152774e-05,
      "loss": 2.4958,
      "step": 1202400
    },
    {
      "epoch": 389.7893030794165,
      "grad_norm": 1.3619366884231567,
      "learning_rate": 1.104401556925073e-05,
      "loss": 2.4924,
      "step": 1202500
    },
    {
      "epoch": 389.82171799027554,
      "grad_norm": 1.3938591480255127,
      "learning_rate": 1.1040771975348686e-05,
      "loss": 2.4515,
      "step": 1202600
    },
    {
      "epoch": 389.8541329011345,
      "grad_norm": 1.679190754890442,
      "learning_rate": 1.1037528381446643e-05,
      "loss": 2.4912,
      "step": 1202700
    },
    {
      "epoch": 389.8865478119935,
      "grad_norm": 1.3233622312545776,
      "learning_rate": 1.1034284787544599e-05,
      "loss": 2.493,
      "step": 1202800
    },
    {
      "epoch": 389.9189627228525,
      "grad_norm": 1.327465295791626,
      "learning_rate": 1.1031041193642557e-05,
      "loss": 2.5148,
      "step": 1202900
    },
    {
      "epoch": 389.9513776337115,
      "grad_norm": 1.4624712467193604,
      "learning_rate": 1.1027797599740514e-05,
      "loss": 2.4866,
      "step": 1203000
    },
    {
      "epoch": 389.9837925445705,
      "grad_norm": 1.517313003540039,
      "learning_rate": 1.102455400583847e-05,
      "loss": 2.4838,
      "step": 1203100
    },
    {
      "epoch": 390.0,
      "eval_bleu": 0.9877598303113501,
      "eval_loss": 4.2459917068481445,
      "eval_runtime": 4.592,
      "eval_samples_per_second": 107.143,
      "eval_steps_per_second": 1.742,
      "step": 1203150
    },
    {
      "epoch": 390.0162074554295,
      "grad_norm": 1.466739535331726,
      "learning_rate": 1.1021310411936427e-05,
      "loss": 2.4966,
      "step": 1203200
    },
    {
      "epoch": 390.0486223662885,
      "grad_norm": 1.33366060256958,
      "learning_rate": 1.1018066818034382e-05,
      "loss": 2.4976,
      "step": 1203300
    },
    {
      "epoch": 390.0810372771475,
      "grad_norm": 1.5143498182296753,
      "learning_rate": 1.1014823224132339e-05,
      "loss": 2.4846,
      "step": 1203400
    },
    {
      "epoch": 390.1134521880065,
      "grad_norm": 1.2988265752792358,
      "learning_rate": 1.1011579630230296e-05,
      "loss": 2.4483,
      "step": 1203500
    },
    {
      "epoch": 390.1458670988655,
      "grad_norm": 1.2982275485992432,
      "learning_rate": 1.1008336036328253e-05,
      "loss": 2.4938,
      "step": 1203600
    },
    {
      "epoch": 390.17828200972446,
      "grad_norm": 1.3849819898605347,
      "learning_rate": 1.1005092442426209e-05,
      "loss": 2.4953,
      "step": 1203700
    },
    {
      "epoch": 390.2106969205835,
      "grad_norm": 1.5033851861953735,
      "learning_rate": 1.1001848848524166e-05,
      "loss": 2.4826,
      "step": 1203800
    },
    {
      "epoch": 390.2431118314425,
      "grad_norm": 1.6123921871185303,
      "learning_rate": 1.0998605254622121e-05,
      "loss": 2.475,
      "step": 1203900
    },
    {
      "epoch": 390.27552674230145,
      "grad_norm": 1.4859707355499268,
      "learning_rate": 1.0995361660720078e-05,
      "loss": 2.4816,
      "step": 1204000
    },
    {
      "epoch": 390.30794165316047,
      "grad_norm": 1.4713084697723389,
      "learning_rate": 1.0992118066818035e-05,
      "loss": 2.4952,
      "step": 1204100
    },
    {
      "epoch": 390.34035656401943,
      "grad_norm": 1.3785091638565063,
      "learning_rate": 1.0988874472915992e-05,
      "loss": 2.4756,
      "step": 1204200
    },
    {
      "epoch": 390.37277147487845,
      "grad_norm": 1.4040610790252686,
      "learning_rate": 1.0985630879013949e-05,
      "loss": 2.4786,
      "step": 1204300
    },
    {
      "epoch": 390.40518638573747,
      "grad_norm": 1.4334189891815186,
      "learning_rate": 1.0982387285111905e-05,
      "loss": 2.4838,
      "step": 1204400
    },
    {
      "epoch": 390.4376012965964,
      "grad_norm": 1.5982321500778198,
      "learning_rate": 1.0979143691209862e-05,
      "loss": 2.5006,
      "step": 1204500
    },
    {
      "epoch": 390.47001620745544,
      "grad_norm": 1.741786241531372,
      "learning_rate": 1.0975900097307817e-05,
      "loss": 2.4813,
      "step": 1204600
    },
    {
      "epoch": 390.5024311183144,
      "grad_norm": 1.543837547302246,
      "learning_rate": 1.0972656503405774e-05,
      "loss": 2.4929,
      "step": 1204700
    },
    {
      "epoch": 390.5348460291734,
      "grad_norm": 1.4315056800842285,
      "learning_rate": 1.0969412909503731e-05,
      "loss": 2.4724,
      "step": 1204800
    },
    {
      "epoch": 390.56726094003244,
      "grad_norm": 1.3420931100845337,
      "learning_rate": 1.0966169315601688e-05,
      "loss": 2.4982,
      "step": 1204900
    },
    {
      "epoch": 390.5996758508914,
      "grad_norm": 1.4602419137954712,
      "learning_rate": 1.0962925721699643e-05,
      "loss": 2.4793,
      "step": 1205000
    },
    {
      "epoch": 390.6320907617504,
      "grad_norm": 1.450160264968872,
      "learning_rate": 1.09596821277976e-05,
      "loss": 2.4835,
      "step": 1205100
    },
    {
      "epoch": 390.6645056726094,
      "grad_norm": 1.4424042701721191,
      "learning_rate": 1.0956438533895556e-05,
      "loss": 2.4917,
      "step": 1205200
    },
    {
      "epoch": 390.6969205834684,
      "grad_norm": 1.454807162284851,
      "learning_rate": 1.0953194939993513e-05,
      "loss": 2.4882,
      "step": 1205300
    },
    {
      "epoch": 390.7293354943274,
      "grad_norm": 1.593490719795227,
      "learning_rate": 1.094995134609147e-05,
      "loss": 2.4765,
      "step": 1205400
    },
    {
      "epoch": 390.7617504051864,
      "grad_norm": 1.4879151582717896,
      "learning_rate": 1.0946707752189427e-05,
      "loss": 2.4889,
      "step": 1205500
    },
    {
      "epoch": 390.7941653160454,
      "grad_norm": 1.349141240119934,
      "learning_rate": 1.0943464158287382e-05,
      "loss": 2.4765,
      "step": 1205600
    },
    {
      "epoch": 390.82658022690435,
      "grad_norm": 1.2235854864120483,
      "learning_rate": 1.094025300032436e-05,
      "loss": 2.4958,
      "step": 1205700
    },
    {
      "epoch": 390.8589951377634,
      "grad_norm": 1.3137576580047607,
      "learning_rate": 1.0937009406422315e-05,
      "loss": 2.4853,
      "step": 1205800
    },
    {
      "epoch": 390.8914100486224,
      "grad_norm": 1.3025604486465454,
      "learning_rate": 1.0933765812520273e-05,
      "loss": 2.4835,
      "step": 1205900
    },
    {
      "epoch": 390.92382495948135,
      "grad_norm": 1.4677654504776,
      "learning_rate": 1.093052221861823e-05,
      "loss": 2.4881,
      "step": 1206000
    },
    {
      "epoch": 390.95623987034037,
      "grad_norm": 1.3367830514907837,
      "learning_rate": 1.0927278624716187e-05,
      "loss": 2.4931,
      "step": 1206100
    },
    {
      "epoch": 390.98865478119933,
      "grad_norm": 1.3739116191864014,
      "learning_rate": 1.0924035030814142e-05,
      "loss": 2.4708,
      "step": 1206200
    },
    {
      "epoch": 391.0,
      "eval_bleu": 0.9626874310795396,
      "eval_loss": 4.245560646057129,
      "eval_runtime": 4.6237,
      "eval_samples_per_second": 106.409,
      "eval_steps_per_second": 1.73,
      "step": 1206235
    },
    {
      "epoch": 391.02106969205835,
      "grad_norm": 1.456661581993103,
      "learning_rate": 1.0920791436912099e-05,
      "loss": 2.4839,
      "step": 1206300
    },
    {
      "epoch": 391.05348460291737,
      "grad_norm": 1.2946878671646118,
      "learning_rate": 1.0917547843010054e-05,
      "loss": 2.4762,
      "step": 1206400
    },
    {
      "epoch": 391.0858995137763,
      "grad_norm": 1.611306071281433,
      "learning_rate": 1.0914304249108011e-05,
      "loss": 2.469,
      "step": 1206500
    },
    {
      "epoch": 391.11831442463534,
      "grad_norm": 1.3178001642227173,
      "learning_rate": 1.091106065520597e-05,
      "loss": 2.4841,
      "step": 1206600
    },
    {
      "epoch": 391.1507293354943,
      "grad_norm": 1.2691439390182495,
      "learning_rate": 1.0907817061303926e-05,
      "loss": 2.504,
      "step": 1206700
    },
    {
      "epoch": 391.1831442463533,
      "grad_norm": 1.413041591644287,
      "learning_rate": 1.0904573467401883e-05,
      "loss": 2.474,
      "step": 1206800
    },
    {
      "epoch": 391.21555915721234,
      "grad_norm": 1.5398417711257935,
      "learning_rate": 1.0901329873499838e-05,
      "loss": 2.4973,
      "step": 1206900
    },
    {
      "epoch": 391.2479740680713,
      "grad_norm": 1.398475170135498,
      "learning_rate": 1.0898086279597795e-05,
      "loss": 2.4994,
      "step": 1207000
    },
    {
      "epoch": 391.2803889789303,
      "grad_norm": 1.3630934953689575,
      "learning_rate": 1.089484268569575e-05,
      "loss": 2.4876,
      "step": 1207100
    },
    {
      "epoch": 391.3128038897893,
      "grad_norm": 1.5979810953140259,
      "learning_rate": 1.0891599091793709e-05,
      "loss": 2.4618,
      "step": 1207200
    },
    {
      "epoch": 391.3452188006483,
      "grad_norm": 1.4459084272384644,
      "learning_rate": 1.0888355497891664e-05,
      "loss": 2.4913,
      "step": 1207300
    },
    {
      "epoch": 391.3776337115073,
      "grad_norm": 1.3590736389160156,
      "learning_rate": 1.0885111903989621e-05,
      "loss": 2.4963,
      "step": 1207400
    },
    {
      "epoch": 391.4100486223663,
      "grad_norm": 1.5596935749053955,
      "learning_rate": 1.0881868310087577e-05,
      "loss": 2.5029,
      "step": 1207500
    },
    {
      "epoch": 391.4424635332253,
      "grad_norm": 1.3619099855422974,
      "learning_rate": 1.0878624716185534e-05,
      "loss": 2.467,
      "step": 1207600
    },
    {
      "epoch": 391.47487844408425,
      "grad_norm": 1.274218201637268,
      "learning_rate": 1.087538112228349e-05,
      "loss": 2.5071,
      "step": 1207700
    },
    {
      "epoch": 391.5072933549433,
      "grad_norm": 1.4716346263885498,
      "learning_rate": 1.0872137528381448e-05,
      "loss": 2.4943,
      "step": 1207800
    },
    {
      "epoch": 391.5397082658023,
      "grad_norm": 1.4668210744857788,
      "learning_rate": 1.0868893934479403e-05,
      "loss": 2.5144,
      "step": 1207900
    },
    {
      "epoch": 391.57212317666125,
      "grad_norm": 1.386898159980774,
      "learning_rate": 1.086565034057736e-05,
      "loss": 2.4961,
      "step": 1208000
    },
    {
      "epoch": 391.60453808752027,
      "grad_norm": 1.495847225189209,
      "learning_rate": 1.0862406746675317e-05,
      "loss": 2.4994,
      "step": 1208100
    },
    {
      "epoch": 391.63695299837923,
      "grad_norm": 1.4240285158157349,
      "learning_rate": 1.0859163152773273e-05,
      "loss": 2.482,
      "step": 1208200
    },
    {
      "epoch": 391.66936790923825,
      "grad_norm": 1.4462528228759766,
      "learning_rate": 1.085591955887123e-05,
      "loss": 2.4642,
      "step": 1208300
    },
    {
      "epoch": 391.70178282009726,
      "grad_norm": 1.2783927917480469,
      "learning_rate": 1.0852675964969187e-05,
      "loss": 2.4786,
      "step": 1208400
    },
    {
      "epoch": 391.7341977309562,
      "grad_norm": 1.2867547273635864,
      "learning_rate": 1.0849432371067144e-05,
      "loss": 2.4701,
      "step": 1208500
    },
    {
      "epoch": 391.76661264181524,
      "grad_norm": 1.501872181892395,
      "learning_rate": 1.08461887771651e-05,
      "loss": 2.4852,
      "step": 1208600
    },
    {
      "epoch": 391.7990275526742,
      "grad_norm": 1.3192113637924194,
      "learning_rate": 1.0842945183263056e-05,
      "loss": 2.4626,
      "step": 1208700
    },
    {
      "epoch": 391.8314424635332,
      "grad_norm": 1.398039698600769,
      "learning_rate": 1.0839701589361012e-05,
      "loss": 2.4823,
      "step": 1208800
    },
    {
      "epoch": 391.86385737439224,
      "grad_norm": 1.6468425989151,
      "learning_rate": 1.0836457995458969e-05,
      "loss": 2.4963,
      "step": 1208900
    },
    {
      "epoch": 391.8962722852512,
      "grad_norm": 1.621814489364624,
      "learning_rate": 1.0833214401556926e-05,
      "loss": 2.4555,
      "step": 1209000
    },
    {
      "epoch": 391.9286871961102,
      "grad_norm": 1.4601726531982422,
      "learning_rate": 1.0829970807654883e-05,
      "loss": 2.4738,
      "step": 1209100
    },
    {
      "epoch": 391.9611021069692,
      "grad_norm": 1.401347041130066,
      "learning_rate": 1.0826727213752838e-05,
      "loss": 2.4874,
      "step": 1209200
    },
    {
      "epoch": 391.9935170178282,
      "grad_norm": 1.863240122795105,
      "learning_rate": 1.0823483619850795e-05,
      "loss": 2.479,
      "step": 1209300
    },
    {
      "epoch": 392.0,
      "eval_bleu": 0.9300566842827518,
      "eval_loss": 4.246567726135254,
      "eval_runtime": 4.0616,
      "eval_samples_per_second": 121.135,
      "eval_steps_per_second": 1.97,
      "step": 1209320
    },
    {
      "epoch": 392.0259319286872,
      "grad_norm": 1.2898662090301514,
      "learning_rate": 1.0820240025948752e-05,
      "loss": 2.4673,
      "step": 1209400
    },
    {
      "epoch": 392.0583468395462,
      "grad_norm": 1.392303466796875,
      "learning_rate": 1.0816996432046708e-05,
      "loss": 2.4926,
      "step": 1209500
    },
    {
      "epoch": 392.0907617504052,
      "grad_norm": 1.3930903673171997,
      "learning_rate": 1.0813752838144665e-05,
      "loss": 2.4793,
      "step": 1209600
    },
    {
      "epoch": 392.12317666126415,
      "grad_norm": 1.5735700130462646,
      "learning_rate": 1.0810541680181642e-05,
      "loss": 2.4712,
      "step": 1209700
    },
    {
      "epoch": 392.15559157212317,
      "grad_norm": 1.256151556968689,
      "learning_rate": 1.0807298086279598e-05,
      "loss": 2.5047,
      "step": 1209800
    },
    {
      "epoch": 392.1880064829822,
      "grad_norm": 1.372300624847412,
      "learning_rate": 1.0804054492377555e-05,
      "loss": 2.4528,
      "step": 1209900
    },
    {
      "epoch": 392.22042139384115,
      "grad_norm": 1.3939448595046997,
      "learning_rate": 1.080081089847551e-05,
      "loss": 2.4721,
      "step": 1210000
    },
    {
      "epoch": 392.25283630470017,
      "grad_norm": 1.287696123123169,
      "learning_rate": 1.0797567304573467e-05,
      "loss": 2.4685,
      "step": 1210100
    },
    {
      "epoch": 392.2852512155592,
      "grad_norm": 1.498481035232544,
      "learning_rate": 1.0794323710671424e-05,
      "loss": 2.5007,
      "step": 1210200
    },
    {
      "epoch": 392.31766612641815,
      "grad_norm": 1.3913496732711792,
      "learning_rate": 1.0791080116769381e-05,
      "loss": 2.4785,
      "step": 1210300
    },
    {
      "epoch": 392.35008103727716,
      "grad_norm": 1.3294402360916138,
      "learning_rate": 1.0787836522867338e-05,
      "loss": 2.495,
      "step": 1210400
    },
    {
      "epoch": 392.3824959481361,
      "grad_norm": 1.7270628213882446,
      "learning_rate": 1.0784592928965294e-05,
      "loss": 2.4775,
      "step": 1210500
    },
    {
      "epoch": 392.41491085899514,
      "grad_norm": 1.721888542175293,
      "learning_rate": 1.0781349335063251e-05,
      "loss": 2.4572,
      "step": 1210600
    },
    {
      "epoch": 392.44732576985416,
      "grad_norm": 1.6209139823913574,
      "learning_rate": 1.0778105741161206e-05,
      "loss": 2.4826,
      "step": 1210700
    },
    {
      "epoch": 392.4797406807131,
      "grad_norm": 1.5196192264556885,
      "learning_rate": 1.0774862147259163e-05,
      "loss": 2.512,
      "step": 1210800
    },
    {
      "epoch": 392.51215559157214,
      "grad_norm": 1.5090893507003784,
      "learning_rate": 1.077161855335712e-05,
      "loss": 2.4882,
      "step": 1210900
    },
    {
      "epoch": 392.5445705024311,
      "grad_norm": 1.4340699911117554,
      "learning_rate": 1.0768374959455077e-05,
      "loss": 2.486,
      "step": 1211000
    },
    {
      "epoch": 392.5769854132901,
      "grad_norm": 1.3261959552764893,
      "learning_rate": 1.0765131365553033e-05,
      "loss": 2.5123,
      "step": 1211100
    },
    {
      "epoch": 392.60940032414914,
      "grad_norm": 1.2734017372131348,
      "learning_rate": 1.076188777165099e-05,
      "loss": 2.4838,
      "step": 1211200
    },
    {
      "epoch": 392.6418152350081,
      "grad_norm": 1.5869861841201782,
      "learning_rate": 1.0758644177748945e-05,
      "loss": 2.5008,
      "step": 1211300
    },
    {
      "epoch": 392.6742301458671,
      "grad_norm": 1.4430158138275146,
      "learning_rate": 1.0755400583846902e-05,
      "loss": 2.4624,
      "step": 1211400
    },
    {
      "epoch": 392.7066450567261,
      "grad_norm": 1.543907880783081,
      "learning_rate": 1.075215698994486e-05,
      "loss": 2.4599,
      "step": 1211500
    },
    {
      "epoch": 392.7390599675851,
      "grad_norm": 1.3334667682647705,
      "learning_rate": 1.0748913396042816e-05,
      "loss": 2.4862,
      "step": 1211600
    },
    {
      "epoch": 392.7714748784441,
      "grad_norm": 1.2543673515319824,
      "learning_rate": 1.0745702238079792e-05,
      "loss": 2.4943,
      "step": 1211700
    },
    {
      "epoch": 392.80388978930307,
      "grad_norm": 1.4044697284698486,
      "learning_rate": 1.074245864417775e-05,
      "loss": 2.4979,
      "step": 1211800
    },
    {
      "epoch": 392.8363047001621,
      "grad_norm": 1.4450440406799316,
      "learning_rate": 1.0739215050275705e-05,
      "loss": 2.4866,
      "step": 1211900
    },
    {
      "epoch": 392.86871961102105,
      "grad_norm": 1.3153969049453735,
      "learning_rate": 1.0735971456373664e-05,
      "loss": 2.4804,
      "step": 1212000
    },
    {
      "epoch": 392.90113452188007,
      "grad_norm": 1.2700004577636719,
      "learning_rate": 1.0732727862471619e-05,
      "loss": 2.4791,
      "step": 1212100
    },
    {
      "epoch": 392.9335494327391,
      "grad_norm": 1.4874945878982544,
      "learning_rate": 1.0729484268569576e-05,
      "loss": 2.4911,
      "step": 1212200
    },
    {
      "epoch": 392.96596434359805,
      "grad_norm": 1.4138156175613403,
      "learning_rate": 1.0726273110606552e-05,
      "loss": 2.4832,
      "step": 1212300
    },
    {
      "epoch": 392.99837925445706,
      "grad_norm": 1.4038808345794678,
      "learning_rate": 1.0723029516704509e-05,
      "loss": 2.5061,
      "step": 1212400
    },
    {
      "epoch": 393.0,
      "eval_bleu": 1.0577631532635827,
      "eval_loss": 4.244202613830566,
      "eval_runtime": 4.4031,
      "eval_samples_per_second": 111.74,
      "eval_steps_per_second": 1.817,
      "step": 1212405
    },
    {
      "epoch": 393.030794165316,
      "grad_norm": 1.3632065057754517,
      "learning_rate": 1.0719785922802464e-05,
      "loss": 2.4742,
      "step": 1212500
    },
    {
      "epoch": 393.06320907617504,
      "grad_norm": 1.4030566215515137,
      "learning_rate": 1.0716542328900423e-05,
      "loss": 2.4653,
      "step": 1212600
    },
    {
      "epoch": 393.09562398703406,
      "grad_norm": 1.3882579803466797,
      "learning_rate": 1.0713298734998379e-05,
      "loss": 2.4688,
      "step": 1212700
    },
    {
      "epoch": 393.128038897893,
      "grad_norm": 1.4625244140625,
      "learning_rate": 1.0710055141096336e-05,
      "loss": 2.4761,
      "step": 1212800
    },
    {
      "epoch": 393.16045380875204,
      "grad_norm": 1.5215871334075928,
      "learning_rate": 1.0706811547194291e-05,
      "loss": 2.4749,
      "step": 1212900
    },
    {
      "epoch": 393.192868719611,
      "grad_norm": 1.360482931137085,
      "learning_rate": 1.0703567953292248e-05,
      "loss": 2.493,
      "step": 1213000
    },
    {
      "epoch": 393.22528363047,
      "grad_norm": 1.3986009359359741,
      "learning_rate": 1.0700324359390203e-05,
      "loss": 2.4879,
      "step": 1213100
    },
    {
      "epoch": 393.25769854132903,
      "grad_norm": 1.4025282859802246,
      "learning_rate": 1.0697080765488162e-05,
      "loss": 2.484,
      "step": 1213200
    },
    {
      "epoch": 393.290113452188,
      "grad_norm": 1.5851800441741943,
      "learning_rate": 1.0693837171586119e-05,
      "loss": 2.4976,
      "step": 1213300
    },
    {
      "epoch": 393.322528363047,
      "grad_norm": 1.3279374837875366,
      "learning_rate": 1.0690593577684074e-05,
      "loss": 2.4699,
      "step": 1213400
    },
    {
      "epoch": 393.354943273906,
      "grad_norm": 1.3033651113510132,
      "learning_rate": 1.0687349983782032e-05,
      "loss": 2.4909,
      "step": 1213500
    },
    {
      "epoch": 393.387358184765,
      "grad_norm": 1.3840289115905762,
      "learning_rate": 1.0684106389879987e-05,
      "loss": 2.4728,
      "step": 1213600
    },
    {
      "epoch": 393.419773095624,
      "grad_norm": 1.3780291080474854,
      "learning_rate": 1.0680862795977944e-05,
      "loss": 2.4739,
      "step": 1213700
    },
    {
      "epoch": 393.45218800648297,
      "grad_norm": 1.3422385454177856,
      "learning_rate": 1.0677619202075901e-05,
      "loss": 2.4883,
      "step": 1213800
    },
    {
      "epoch": 393.484602917342,
      "grad_norm": 1.562511920928955,
      "learning_rate": 1.0674375608173858e-05,
      "loss": 2.4817,
      "step": 1213900
    },
    {
      "epoch": 393.51701782820095,
      "grad_norm": 1.425376057624817,
      "learning_rate": 1.0671132014271813e-05,
      "loss": 2.4598,
      "step": 1214000
    },
    {
      "epoch": 393.54943273905997,
      "grad_norm": 1.220634937286377,
      "learning_rate": 1.066788842036977e-05,
      "loss": 2.4943,
      "step": 1214100
    },
    {
      "epoch": 393.581847649919,
      "grad_norm": 1.366797924041748,
      "learning_rate": 1.0664644826467726e-05,
      "loss": 2.4882,
      "step": 1214200
    },
    {
      "epoch": 393.61426256077795,
      "grad_norm": 1.679309368133545,
      "learning_rate": 1.0661401232565683e-05,
      "loss": 2.4751,
      "step": 1214300
    },
    {
      "epoch": 393.64667747163696,
      "grad_norm": 1.482911467552185,
      "learning_rate": 1.065815763866364e-05,
      "loss": 2.4857,
      "step": 1214400
    },
    {
      "epoch": 393.6790923824959,
      "grad_norm": 1.2945284843444824,
      "learning_rate": 1.0654914044761597e-05,
      "loss": 2.4689,
      "step": 1214500
    },
    {
      "epoch": 393.71150729335494,
      "grad_norm": 1.5813066959381104,
      "learning_rate": 1.0651670450859554e-05,
      "loss": 2.4889,
      "step": 1214600
    },
    {
      "epoch": 393.74392220421396,
      "grad_norm": 1.5450907945632935,
      "learning_rate": 1.064842685695751e-05,
      "loss": 2.4932,
      "step": 1214700
    },
    {
      "epoch": 393.7763371150729,
      "grad_norm": 1.3514522314071655,
      "learning_rate": 1.0645183263055466e-05,
      "loss": 2.4931,
      "step": 1214800
    },
    {
      "epoch": 393.80875202593194,
      "grad_norm": 1.6550333499908447,
      "learning_rate": 1.0641939669153422e-05,
      "loss": 2.4812,
      "step": 1214900
    },
    {
      "epoch": 393.8411669367909,
      "grad_norm": 1.3308888673782349,
      "learning_rate": 1.0638696075251379e-05,
      "loss": 2.4881,
      "step": 1215000
    },
    {
      "epoch": 393.8735818476499,
      "grad_norm": 1.3960390090942383,
      "learning_rate": 1.0635452481349336e-05,
      "loss": 2.4956,
      "step": 1215100
    },
    {
      "epoch": 393.90599675850893,
      "grad_norm": 1.2863168716430664,
      "learning_rate": 1.0632208887447293e-05,
      "loss": 2.4858,
      "step": 1215200
    },
    {
      "epoch": 393.9384116693679,
      "grad_norm": 1.3145970106124878,
      "learning_rate": 1.0628997729484269e-05,
      "loss": 2.4972,
      "step": 1215300
    },
    {
      "epoch": 393.9708265802269,
      "grad_norm": 1.3287287950515747,
      "learning_rate": 1.0625754135582226e-05,
      "loss": 2.4962,
      "step": 1215400
    },
    {
      "epoch": 394.0,
      "eval_bleu": 0.7808398524851944,
      "eval_loss": 4.244668006896973,
      "eval_runtime": 4.4216,
      "eval_samples_per_second": 111.273,
      "eval_steps_per_second": 1.809,
      "step": 1215490
    },
    {
      "epoch": 394.0032414910859,
      "grad_norm": 1.33113431930542,
      "learning_rate": 1.0622510541680181e-05,
      "loss": 2.4909,
      "step": 1215500
    },
    {
      "epoch": 394.0356564019449,
      "grad_norm": 1.3379065990447998,
      "learning_rate": 1.0619266947778138e-05,
      "loss": 2.4956,
      "step": 1215600
    },
    {
      "epoch": 394.0680713128039,
      "grad_norm": 1.4794790744781494,
      "learning_rate": 1.0616023353876095e-05,
      "loss": 2.4753,
      "step": 1215700
    },
    {
      "epoch": 394.10048622366287,
      "grad_norm": 1.5815619230270386,
      "learning_rate": 1.0612779759974053e-05,
      "loss": 2.4573,
      "step": 1215800
    },
    {
      "epoch": 394.1329011345219,
      "grad_norm": 1.702802062034607,
      "learning_rate": 1.0609536166072008e-05,
      "loss": 2.4671,
      "step": 1215900
    },
    {
      "epoch": 394.16531604538085,
      "grad_norm": 1.4143924713134766,
      "learning_rate": 1.0606292572169965e-05,
      "loss": 2.4648,
      "step": 1216000
    },
    {
      "epoch": 394.19773095623987,
      "grad_norm": 1.4908881187438965,
      "learning_rate": 1.060304897826792e-05,
      "loss": 2.4834,
      "step": 1216100
    },
    {
      "epoch": 394.2301458670989,
      "grad_norm": 1.3361238241195679,
      "learning_rate": 1.0599805384365877e-05,
      "loss": 2.4921,
      "step": 1216200
    },
    {
      "epoch": 394.26256077795784,
      "grad_norm": 1.5253312587738037,
      "learning_rate": 1.0596561790463834e-05,
      "loss": 2.5031,
      "step": 1216300
    },
    {
      "epoch": 394.29497568881686,
      "grad_norm": 1.2953641414642334,
      "learning_rate": 1.0593318196561791e-05,
      "loss": 2.472,
      "step": 1216400
    },
    {
      "epoch": 394.3273905996758,
      "grad_norm": 1.590342402458191,
      "learning_rate": 1.0590074602659747e-05,
      "loss": 2.483,
      "step": 1216500
    },
    {
      "epoch": 394.35980551053484,
      "grad_norm": 1.4965072870254517,
      "learning_rate": 1.0586831008757704e-05,
      "loss": 2.4762,
      "step": 1216600
    },
    {
      "epoch": 394.39222042139386,
      "grad_norm": 1.4863718748092651,
      "learning_rate": 1.058358741485566e-05,
      "loss": 2.4796,
      "step": 1216700
    },
    {
      "epoch": 394.4246353322528,
      "grad_norm": 1.2595864534378052,
      "learning_rate": 1.0580343820953616e-05,
      "loss": 2.4811,
      "step": 1216800
    },
    {
      "epoch": 394.45705024311184,
      "grad_norm": 1.4227702617645264,
      "learning_rate": 1.0577100227051575e-05,
      "loss": 2.4739,
      "step": 1216900
    },
    {
      "epoch": 394.48946515397085,
      "grad_norm": 1.3228724002838135,
      "learning_rate": 1.057385663314953e-05,
      "loss": 2.4766,
      "step": 1217000
    },
    {
      "epoch": 394.5218800648298,
      "grad_norm": 1.4551600217819214,
      "learning_rate": 1.0570613039247487e-05,
      "loss": 2.4815,
      "step": 1217100
    },
    {
      "epoch": 394.55429497568883,
      "grad_norm": 1.3068888187408447,
      "learning_rate": 1.0567369445345443e-05,
      "loss": 2.487,
      "step": 1217200
    },
    {
      "epoch": 394.5867098865478,
      "grad_norm": 1.51137375831604,
      "learning_rate": 1.05641258514434e-05,
      "loss": 2.4833,
      "step": 1217300
    },
    {
      "epoch": 394.6191247974068,
      "grad_norm": 1.4478071928024292,
      "learning_rate": 1.0560882257541357e-05,
      "loss": 2.4844,
      "step": 1217400
    },
    {
      "epoch": 394.65153970826583,
      "grad_norm": 1.381000280380249,
      "learning_rate": 1.0557638663639314e-05,
      "loss": 2.4597,
      "step": 1217500
    },
    {
      "epoch": 394.6839546191248,
      "grad_norm": 1.4655745029449463,
      "learning_rate": 1.055439506973727e-05,
      "loss": 2.5019,
      "step": 1217600
    },
    {
      "epoch": 394.7163695299838,
      "grad_norm": 1.4793621301651,
      "learning_rate": 1.0551151475835226e-05,
      "loss": 2.4958,
      "step": 1217700
    },
    {
      "epoch": 394.74878444084277,
      "grad_norm": 1.3242814540863037,
      "learning_rate": 1.0547907881933182e-05,
      "loss": 2.4865,
      "step": 1217800
    },
    {
      "epoch": 394.7811993517018,
      "grad_norm": 1.3218575716018677,
      "learning_rate": 1.0544664288031139e-05,
      "loss": 2.5123,
      "step": 1217900
    },
    {
      "epoch": 394.8136142625608,
      "grad_norm": 1.4812977313995361,
      "learning_rate": 1.0541420694129096e-05,
      "loss": 2.4957,
      "step": 1218000
    },
    {
      "epoch": 394.84602917341977,
      "grad_norm": 1.4176173210144043,
      "learning_rate": 1.0538177100227053e-05,
      "loss": 2.4958,
      "step": 1218100
    },
    {
      "epoch": 394.8784440842788,
      "grad_norm": 1.6758642196655273,
      "learning_rate": 1.053493350632501e-05,
      "loss": 2.4818,
      "step": 1218200
    },
    {
      "epoch": 394.91085899513774,
      "grad_norm": 1.2040287256240845,
      "learning_rate": 1.0531689912422965e-05,
      "loss": 2.4678,
      "step": 1218300
    },
    {
      "epoch": 394.94327390599676,
      "grad_norm": 1.4367523193359375,
      "learning_rate": 1.0528446318520922e-05,
      "loss": 2.4738,
      "step": 1218400
    },
    {
      "epoch": 394.9756888168558,
      "grad_norm": 1.513162612915039,
      "learning_rate": 1.0525202724618878e-05,
      "loss": 2.481,
      "step": 1218500
    },
    {
      "epoch": 395.0,
      "eval_bleu": 0.9228669536509498,
      "eval_loss": 4.249176025390625,
      "eval_runtime": 4.304,
      "eval_samples_per_second": 114.312,
      "eval_steps_per_second": 1.859,
      "step": 1218575
    },
    {
      "epoch": 395.00810372771474,
      "grad_norm": 1.3609364032745361,
      "learning_rate": 1.0521959130716835e-05,
      "loss": 2.4876,
      "step": 1218600
    },
    {
      "epoch": 395.04051863857376,
      "grad_norm": 1.3792654275894165,
      "learning_rate": 1.0518715536814792e-05,
      "loss": 2.4863,
      "step": 1218700
    },
    {
      "epoch": 395.0729335494327,
      "grad_norm": 1.3842713832855225,
      "learning_rate": 1.0515471942912749e-05,
      "loss": 2.4739,
      "step": 1218800
    },
    {
      "epoch": 395.10534846029174,
      "grad_norm": 1.5479930639266968,
      "learning_rate": 1.0512228349010704e-05,
      "loss": 2.4933,
      "step": 1218900
    },
    {
      "epoch": 395.13776337115075,
      "grad_norm": 1.6509650945663452,
      "learning_rate": 1.0508984755108661e-05,
      "loss": 2.4889,
      "step": 1219000
    },
    {
      "epoch": 395.1701782820097,
      "grad_norm": 1.4134162664413452,
      "learning_rate": 1.0505741161206617e-05,
      "loss": 2.4865,
      "step": 1219100
    },
    {
      "epoch": 395.20259319286873,
      "grad_norm": 1.379408836364746,
      "learning_rate": 1.0502497567304574e-05,
      "loss": 2.4704,
      "step": 1219200
    },
    {
      "epoch": 395.2350081037277,
      "grad_norm": 1.3142921924591064,
      "learning_rate": 1.0499286409341551e-05,
      "loss": 2.4889,
      "step": 1219300
    },
    {
      "epoch": 395.2674230145867,
      "grad_norm": 1.3458112478256226,
      "learning_rate": 1.0496042815439508e-05,
      "loss": 2.4972,
      "step": 1219400
    },
    {
      "epoch": 395.29983792544573,
      "grad_norm": 1.4483340978622437,
      "learning_rate": 1.0492799221537464e-05,
      "loss": 2.5064,
      "step": 1219500
    },
    {
      "epoch": 395.3322528363047,
      "grad_norm": 1.3584256172180176,
      "learning_rate": 1.048955562763542e-05,
      "loss": 2.4879,
      "step": 1219600
    },
    {
      "epoch": 395.3646677471637,
      "grad_norm": 1.4986101388931274,
      "learning_rate": 1.0486312033733376e-05,
      "loss": 2.4923,
      "step": 1219700
    },
    {
      "epoch": 395.39708265802267,
      "grad_norm": 1.2033822536468506,
      "learning_rate": 1.0483068439831333e-05,
      "loss": 2.4425,
      "step": 1219800
    },
    {
      "epoch": 395.4294975688817,
      "grad_norm": 1.4037754535675049,
      "learning_rate": 1.047982484592929e-05,
      "loss": 2.4932,
      "step": 1219900
    },
    {
      "epoch": 395.4619124797407,
      "grad_norm": 1.4116079807281494,
      "learning_rate": 1.0476581252027247e-05,
      "loss": 2.4828,
      "step": 1220000
    },
    {
      "epoch": 395.49432739059966,
      "grad_norm": 1.5322215557098389,
      "learning_rate": 1.0473337658125203e-05,
      "loss": 2.4793,
      "step": 1220100
    },
    {
      "epoch": 395.5267423014587,
      "grad_norm": 1.454514980316162,
      "learning_rate": 1.047009406422316e-05,
      "loss": 2.4643,
      "step": 1220200
    },
    {
      "epoch": 395.55915721231764,
      "grad_norm": 1.333728313446045,
      "learning_rate": 1.0466850470321115e-05,
      "loss": 2.4869,
      "step": 1220300
    },
    {
      "epoch": 395.59157212317666,
      "grad_norm": 1.3416376113891602,
      "learning_rate": 1.0463606876419072e-05,
      "loss": 2.4942,
      "step": 1220400
    },
    {
      "epoch": 395.6239870340357,
      "grad_norm": 1.4209171533584595,
      "learning_rate": 1.0460363282517031e-05,
      "loss": 2.475,
      "step": 1220500
    },
    {
      "epoch": 395.65640194489464,
      "grad_norm": 1.2762982845306396,
      "learning_rate": 1.0457119688614986e-05,
      "loss": 2.4883,
      "step": 1220600
    },
    {
      "epoch": 395.68881685575366,
      "grad_norm": 1.4373458623886108,
      "learning_rate": 1.0453876094712943e-05,
      "loss": 2.4779,
      "step": 1220700
    },
    {
      "epoch": 395.7212317666126,
      "grad_norm": 1.3170123100280762,
      "learning_rate": 1.0450632500810899e-05,
      "loss": 2.4755,
      "step": 1220800
    },
    {
      "epoch": 395.75364667747164,
      "grad_norm": 1.4047240018844604,
      "learning_rate": 1.0447388906908856e-05,
      "loss": 2.4953,
      "step": 1220900
    },
    {
      "epoch": 395.78606158833065,
      "grad_norm": 1.5353162288665771,
      "learning_rate": 1.0444177748945832e-05,
      "loss": 2.4785,
      "step": 1221000
    },
    {
      "epoch": 395.8184764991896,
      "grad_norm": 1.5038011074066162,
      "learning_rate": 1.0440934155043789e-05,
      "loss": 2.4773,
      "step": 1221100
    },
    {
      "epoch": 395.85089141004863,
      "grad_norm": 1.344015121459961,
      "learning_rate": 1.0437690561141746e-05,
      "loss": 2.4665,
      "step": 1221200
    },
    {
      "epoch": 395.8833063209076,
      "grad_norm": 1.476162075996399,
      "learning_rate": 1.0434446967239703e-05,
      "loss": 2.4809,
      "step": 1221300
    },
    {
      "epoch": 395.9157212317666,
      "grad_norm": 1.494876742362976,
      "learning_rate": 1.0431203373337658e-05,
      "loss": 2.4556,
      "step": 1221400
    },
    {
      "epoch": 395.94813614262563,
      "grad_norm": 1.5434290170669556,
      "learning_rate": 1.0427959779435615e-05,
      "loss": 2.4951,
      "step": 1221500
    },
    {
      "epoch": 395.9805510534846,
      "grad_norm": 1.4250153303146362,
      "learning_rate": 1.042471618553357e-05,
      "loss": 2.4937,
      "step": 1221600
    },
    {
      "epoch": 396.0,
      "eval_bleu": 0.9711378856931832,
      "eval_loss": 4.25125789642334,
      "eval_runtime": 4.2751,
      "eval_samples_per_second": 115.084,
      "eval_steps_per_second": 1.871,
      "step": 1221660
    },
    {
      "epoch": 396.0129659643436,
      "grad_norm": 1.4291293621063232,
      "learning_rate": 1.042147259163153e-05,
      "loss": 2.463,
      "step": 1221700
    },
    {
      "epoch": 396.04538087520257,
      "grad_norm": 1.335253119468689,
      "learning_rate": 1.0418228997729485e-05,
      "loss": 2.459,
      "step": 1221800
    },
    {
      "epoch": 396.0777957860616,
      "grad_norm": 1.4690725803375244,
      "learning_rate": 1.0414985403827442e-05,
      "loss": 2.4524,
      "step": 1221900
    },
    {
      "epoch": 396.1102106969206,
      "grad_norm": 1.316460371017456,
      "learning_rate": 1.0411741809925397e-05,
      "loss": 2.4962,
      "step": 1222000
    },
    {
      "epoch": 396.14262560777956,
      "grad_norm": 1.3627902269363403,
      "learning_rate": 1.0408498216023354e-05,
      "loss": 2.455,
      "step": 1222100
    },
    {
      "epoch": 396.1750405186386,
      "grad_norm": 1.2423516511917114,
      "learning_rate": 1.040525462212131e-05,
      "loss": 2.4742,
      "step": 1222200
    },
    {
      "epoch": 396.20745542949754,
      "grad_norm": 1.3857448101043701,
      "learning_rate": 1.0402011028219268e-05,
      "loss": 2.4799,
      "step": 1222300
    },
    {
      "epoch": 396.23987034035656,
      "grad_norm": 1.4784812927246094,
      "learning_rate": 1.0398767434317224e-05,
      "loss": 2.4666,
      "step": 1222400
    },
    {
      "epoch": 396.2722852512156,
      "grad_norm": 1.2923333644866943,
      "learning_rate": 1.039552384041518e-05,
      "loss": 2.4836,
      "step": 1222500
    },
    {
      "epoch": 396.30470016207454,
      "grad_norm": 1.2885810136795044,
      "learning_rate": 1.0392280246513136e-05,
      "loss": 2.4947,
      "step": 1222600
    },
    {
      "epoch": 396.33711507293356,
      "grad_norm": 1.2938594818115234,
      "learning_rate": 1.0389036652611093e-05,
      "loss": 2.4756,
      "step": 1222700
    },
    {
      "epoch": 396.3695299837925,
      "grad_norm": 1.3587697744369507,
      "learning_rate": 1.038579305870905e-05,
      "loss": 2.4902,
      "step": 1222800
    },
    {
      "epoch": 396.40194489465154,
      "grad_norm": 1.4967938661575317,
      "learning_rate": 1.0382549464807007e-05,
      "loss": 2.4847,
      "step": 1222900
    },
    {
      "epoch": 396.43435980551055,
      "grad_norm": 1.4326554536819458,
      "learning_rate": 1.0379305870904964e-05,
      "loss": 2.489,
      "step": 1223000
    },
    {
      "epoch": 396.4667747163695,
      "grad_norm": 1.5289074182510376,
      "learning_rate": 1.037606227700292e-05,
      "loss": 2.4836,
      "step": 1223100
    },
    {
      "epoch": 396.49918962722853,
      "grad_norm": 1.3500359058380127,
      "learning_rate": 1.0372818683100877e-05,
      "loss": 2.4853,
      "step": 1223200
    },
    {
      "epoch": 396.5316045380875,
      "grad_norm": 1.4009382724761963,
      "learning_rate": 1.0369575089198832e-05,
      "loss": 2.4691,
      "step": 1223300
    },
    {
      "epoch": 396.5640194489465,
      "grad_norm": 1.3238085508346558,
      "learning_rate": 1.0366331495296789e-05,
      "loss": 2.4857,
      "step": 1223400
    },
    {
      "epoch": 396.5964343598055,
      "grad_norm": 1.5029155015945435,
      "learning_rate": 1.0363087901394746e-05,
      "loss": 2.4833,
      "step": 1223500
    },
    {
      "epoch": 396.6288492706645,
      "grad_norm": 1.4137349128723145,
      "learning_rate": 1.0359844307492703e-05,
      "loss": 2.4981,
      "step": 1223600
    },
    {
      "epoch": 396.6612641815235,
      "grad_norm": 1.442465901374817,
      "learning_rate": 1.0356600713590659e-05,
      "loss": 2.4787,
      "step": 1223700
    },
    {
      "epoch": 396.6936790923825,
      "grad_norm": 1.212783694267273,
      "learning_rate": 1.0353357119688616e-05,
      "loss": 2.4705,
      "step": 1223800
    },
    {
      "epoch": 396.7260940032415,
      "grad_norm": 1.247627854347229,
      "learning_rate": 1.0350113525786571e-05,
      "loss": 2.4904,
      "step": 1223900
    },
    {
      "epoch": 396.7585089141005,
      "grad_norm": 1.5235257148742676,
      "learning_rate": 1.0346869931884528e-05,
      "loss": 2.5054,
      "step": 1224000
    },
    {
      "epoch": 396.79092382495946,
      "grad_norm": 1.496803641319275,
      "learning_rate": 1.0343626337982485e-05,
      "loss": 2.4709,
      "step": 1224100
    },
    {
      "epoch": 396.8233387358185,
      "grad_norm": 1.356631875038147,
      "learning_rate": 1.0340382744080442e-05,
      "loss": 2.5031,
      "step": 1224200
    },
    {
      "epoch": 396.8557536466775,
      "grad_norm": 1.490052342414856,
      "learning_rate": 1.0337139150178399e-05,
      "loss": 2.4868,
      "step": 1224300
    },
    {
      "epoch": 396.88816855753646,
      "grad_norm": 1.810654640197754,
      "learning_rate": 1.0333895556276354e-05,
      "loss": 2.4963,
      "step": 1224400
    },
    {
      "epoch": 396.9205834683955,
      "grad_norm": 1.4909875392913818,
      "learning_rate": 1.0330651962374312e-05,
      "loss": 2.4566,
      "step": 1224500
    },
    {
      "epoch": 396.95299837925444,
      "grad_norm": 1.3859028816223145,
      "learning_rate": 1.0327408368472267e-05,
      "loss": 2.4914,
      "step": 1224600
    },
    {
      "epoch": 396.98541329011346,
      "grad_norm": 1.4653345346450806,
      "learning_rate": 1.0324164774570224e-05,
      "loss": 2.4726,
      "step": 1224700
    },
    {
      "epoch": 397.0,
      "eval_bleu": 0.9564372755827742,
      "eval_loss": 4.251373291015625,
      "eval_runtime": 4.1972,
      "eval_samples_per_second": 117.222,
      "eval_steps_per_second": 1.906,
      "step": 1224745
    },
    {
      "epoch": 397.0178282009725,
      "grad_norm": 1.3770281076431274,
      "learning_rate": 1.0320921180668181e-05,
      "loss": 2.5005,
      "step": 1224800
    },
    {
      "epoch": 397.05024311183143,
      "grad_norm": 1.517142415046692,
      "learning_rate": 1.0317677586766138e-05,
      "loss": 2.4826,
      "step": 1224900
    },
    {
      "epoch": 397.08265802269045,
      "grad_norm": 1.406110167503357,
      "learning_rate": 1.0314466428803114e-05,
      "loss": 2.4965,
      "step": 1225000
    },
    {
      "epoch": 397.1150729335494,
      "grad_norm": 1.5787042379379272,
      "learning_rate": 1.0311222834901071e-05,
      "loss": 2.487,
      "step": 1225100
    },
    {
      "epoch": 397.14748784440843,
      "grad_norm": 1.346115231513977,
      "learning_rate": 1.0307979240999027e-05,
      "loss": 2.4637,
      "step": 1225200
    },
    {
      "epoch": 397.17990275526745,
      "grad_norm": 1.3124160766601562,
      "learning_rate": 1.0304735647096984e-05,
      "loss": 2.4727,
      "step": 1225300
    },
    {
      "epoch": 397.2123176661264,
      "grad_norm": 1.4422383308410645,
      "learning_rate": 1.030149205319494e-05,
      "loss": 2.4948,
      "step": 1225400
    },
    {
      "epoch": 397.2447325769854,
      "grad_norm": 1.4337912797927856,
      "learning_rate": 1.0298248459292898e-05,
      "loss": 2.4816,
      "step": 1225500
    },
    {
      "epoch": 397.2771474878444,
      "grad_norm": 1.5538806915283203,
      "learning_rate": 1.0295004865390853e-05,
      "loss": 2.4691,
      "step": 1225600
    },
    {
      "epoch": 397.3095623987034,
      "grad_norm": 1.5852131843566895,
      "learning_rate": 1.029176127148881e-05,
      "loss": 2.4688,
      "step": 1225700
    },
    {
      "epoch": 397.3419773095624,
      "grad_norm": 1.5873663425445557,
      "learning_rate": 1.0288517677586765e-05,
      "loss": 2.4998,
      "step": 1225800
    },
    {
      "epoch": 397.3743922204214,
      "grad_norm": 1.5335460901260376,
      "learning_rate": 1.0285306519623745e-05,
      "loss": 2.4876,
      "step": 1225900
    },
    {
      "epoch": 397.4068071312804,
      "grad_norm": 1.3261289596557617,
      "learning_rate": 1.02820629257217e-05,
      "loss": 2.4784,
      "step": 1226000
    },
    {
      "epoch": 397.43922204213936,
      "grad_norm": 1.4753739833831787,
      "learning_rate": 1.0278819331819657e-05,
      "loss": 2.4896,
      "step": 1226100
    },
    {
      "epoch": 397.4716369529984,
      "grad_norm": 1.3873848915100098,
      "learning_rate": 1.0275575737917613e-05,
      "loss": 2.4732,
      "step": 1226200
    },
    {
      "epoch": 397.5040518638574,
      "grad_norm": 1.6279100179672241,
      "learning_rate": 1.027233214401557e-05,
      "loss": 2.4609,
      "step": 1226300
    },
    {
      "epoch": 397.53646677471636,
      "grad_norm": 1.5394455194473267,
      "learning_rate": 1.0269088550113525e-05,
      "loss": 2.4886,
      "step": 1226400
    },
    {
      "epoch": 397.5688816855754,
      "grad_norm": 1.3371121883392334,
      "learning_rate": 1.0265844956211484e-05,
      "loss": 2.4648,
      "step": 1226500
    },
    {
      "epoch": 397.60129659643434,
      "grad_norm": 1.6660711765289307,
      "learning_rate": 1.026260136230944e-05,
      "loss": 2.4711,
      "step": 1226600
    },
    {
      "epoch": 397.63371150729336,
      "grad_norm": 1.3406230211257935,
      "learning_rate": 1.0259357768407396e-05,
      "loss": 2.4891,
      "step": 1226700
    },
    {
      "epoch": 397.6661264181524,
      "grad_norm": 1.3363434076309204,
      "learning_rate": 1.0256114174505352e-05,
      "loss": 2.4824,
      "step": 1226800
    },
    {
      "epoch": 397.69854132901133,
      "grad_norm": 1.3236385583877563,
      "learning_rate": 1.0252870580603309e-05,
      "loss": 2.4774,
      "step": 1226900
    },
    {
      "epoch": 397.73095623987035,
      "grad_norm": 1.5540274381637573,
      "learning_rate": 1.0249626986701264e-05,
      "loss": 2.4927,
      "step": 1227000
    },
    {
      "epoch": 397.7633711507293,
      "grad_norm": 1.391890287399292,
      "learning_rate": 1.0246383392799223e-05,
      "loss": 2.4694,
      "step": 1227100
    },
    {
      "epoch": 397.79578606158833,
      "grad_norm": 1.6735116243362427,
      "learning_rate": 1.024313979889718e-05,
      "loss": 2.4828,
      "step": 1227200
    },
    {
      "epoch": 397.82820097244735,
      "grad_norm": 1.3660465478897095,
      "learning_rate": 1.0239896204995135e-05,
      "loss": 2.4751,
      "step": 1227300
    },
    {
      "epoch": 397.8606158833063,
      "grad_norm": 1.5844790935516357,
      "learning_rate": 1.0236652611093092e-05,
      "loss": 2.4967,
      "step": 1227400
    },
    {
      "epoch": 397.8930307941653,
      "grad_norm": 1.4067357778549194,
      "learning_rate": 1.0233409017191048e-05,
      "loss": 2.4752,
      "step": 1227500
    },
    {
      "epoch": 397.9254457050243,
      "grad_norm": 1.4336469173431396,
      "learning_rate": 1.0230165423289005e-05,
      "loss": 2.4983,
      "step": 1227600
    },
    {
      "epoch": 397.9578606158833,
      "grad_norm": 1.3876861333847046,
      "learning_rate": 1.0226921829386962e-05,
      "loss": 2.4883,
      "step": 1227700
    },
    {
      "epoch": 397.9902755267423,
      "grad_norm": 1.4819965362548828,
      "learning_rate": 1.0223678235484919e-05,
      "loss": 2.4601,
      "step": 1227800
    },
    {
      "epoch": 398.0,
      "eval_bleu": 0.9661264063893007,
      "eval_loss": 4.252023696899414,
      "eval_runtime": 4.1171,
      "eval_samples_per_second": 119.503,
      "eval_steps_per_second": 1.943,
      "step": 1227830
    },
    {
      "epoch": 398.0226904376013,
      "grad_norm": 1.428123950958252,
      "learning_rate": 1.0220434641582874e-05,
      "loss": 2.4884,
      "step": 1227900
    },
    {
      "epoch": 398.0551053484603,
      "grad_norm": 1.4358127117156982,
      "learning_rate": 1.0217191047680831e-05,
      "loss": 2.4844,
      "step": 1228000
    },
    {
      "epoch": 398.08752025931926,
      "grad_norm": 1.2949639558792114,
      "learning_rate": 1.0213947453778786e-05,
      "loss": 2.4829,
      "step": 1228100
    },
    {
      "epoch": 398.1199351701783,
      "grad_norm": 1.6146819591522217,
      "learning_rate": 1.0210703859876744e-05,
      "loss": 2.4636,
      "step": 1228200
    },
    {
      "epoch": 398.1523500810373,
      "grad_norm": 1.2647978067398071,
      "learning_rate": 1.02074602659747e-05,
      "loss": 2.482,
      "step": 1228300
    },
    {
      "epoch": 398.18476499189626,
      "grad_norm": 1.5624935626983643,
      "learning_rate": 1.0204216672072658e-05,
      "loss": 2.4876,
      "step": 1228400
    },
    {
      "epoch": 398.2171799027553,
      "grad_norm": 1.2176002264022827,
      "learning_rate": 1.0200973078170615e-05,
      "loss": 2.5011,
      "step": 1228500
    },
    {
      "epoch": 398.24959481361424,
      "grad_norm": 1.348400354385376,
      "learning_rate": 1.019772948426857e-05,
      "loss": 2.4614,
      "step": 1228600
    },
    {
      "epoch": 398.28200972447326,
      "grad_norm": 1.4932669401168823,
      "learning_rate": 1.0194485890366527e-05,
      "loss": 2.4801,
      "step": 1228700
    },
    {
      "epoch": 398.3144246353323,
      "grad_norm": 1.5945826768875122,
      "learning_rate": 1.0191242296464482e-05,
      "loss": 2.4913,
      "step": 1228800
    },
    {
      "epoch": 398.34683954619123,
      "grad_norm": 1.3569780588150024,
      "learning_rate": 1.018799870256244e-05,
      "loss": 2.5048,
      "step": 1228900
    },
    {
      "epoch": 398.37925445705025,
      "grad_norm": 1.5084391832351685,
      "learning_rate": 1.0184755108660397e-05,
      "loss": 2.4703,
      "step": 1229000
    },
    {
      "epoch": 398.4116693679092,
      "grad_norm": 1.5309308767318726,
      "learning_rate": 1.0181511514758354e-05,
      "loss": 2.479,
      "step": 1229100
    },
    {
      "epoch": 398.44408427876823,
      "grad_norm": 1.5196171998977661,
      "learning_rate": 1.0178267920856309e-05,
      "loss": 2.4662,
      "step": 1229200
    },
    {
      "epoch": 398.47649918962725,
      "grad_norm": 1.3408865928649902,
      "learning_rate": 1.0175024326954266e-05,
      "loss": 2.469,
      "step": 1229300
    },
    {
      "epoch": 398.5089141004862,
      "grad_norm": 1.4765539169311523,
      "learning_rate": 1.0171780733052221e-05,
      "loss": 2.4858,
      "step": 1229400
    },
    {
      "epoch": 398.5413290113452,
      "grad_norm": 1.2246065139770508,
      "learning_rate": 1.0168537139150178e-05,
      "loss": 2.4931,
      "step": 1229500
    },
    {
      "epoch": 398.5737439222042,
      "grad_norm": 1.3923066854476929,
      "learning_rate": 1.0165293545248135e-05,
      "loss": 2.4893,
      "step": 1229600
    },
    {
      "epoch": 398.6061588330632,
      "grad_norm": 1.5337138175964355,
      "learning_rate": 1.0162049951346092e-05,
      "loss": 2.472,
      "step": 1229700
    },
    {
      "epoch": 398.6385737439222,
      "grad_norm": 1.344841480255127,
      "learning_rate": 1.0158806357444048e-05,
      "loss": 2.4786,
      "step": 1229800
    },
    {
      "epoch": 398.6709886547812,
      "grad_norm": 1.756034016609192,
      "learning_rate": 1.0155595199481026e-05,
      "loss": 2.4656,
      "step": 1229900
    },
    {
      "epoch": 398.7034035656402,
      "grad_norm": 1.7520253658294678,
      "learning_rate": 1.0152351605578981e-05,
      "loss": 2.4694,
      "step": 1230000
    },
    {
      "epoch": 398.73581847649916,
      "grad_norm": 1.4394763708114624,
      "learning_rate": 1.0149108011676938e-05,
      "loss": 2.4857,
      "step": 1230100
    },
    {
      "epoch": 398.7682333873582,
      "grad_norm": 1.269775152206421,
      "learning_rate": 1.0145864417774895e-05,
      "loss": 2.4726,
      "step": 1230200
    },
    {
      "epoch": 398.8006482982172,
      "grad_norm": 1.342657208442688,
      "learning_rate": 1.0142620823872852e-05,
      "loss": 2.4826,
      "step": 1230300
    },
    {
      "epoch": 398.83306320907616,
      "grad_norm": 1.297289252281189,
      "learning_rate": 1.0139377229970807e-05,
      "loss": 2.4728,
      "step": 1230400
    },
    {
      "epoch": 398.8654781199352,
      "grad_norm": 1.3456453084945679,
      "learning_rate": 1.0136133636068765e-05,
      "loss": 2.4853,
      "step": 1230500
    },
    {
      "epoch": 398.8978930307942,
      "grad_norm": 1.6203457117080688,
      "learning_rate": 1.013289004216672e-05,
      "loss": 2.4705,
      "step": 1230600
    },
    {
      "epoch": 398.93030794165315,
      "grad_norm": 1.5235662460327148,
      "learning_rate": 1.0129646448264677e-05,
      "loss": 2.4945,
      "step": 1230700
    },
    {
      "epoch": 398.9627228525122,
      "grad_norm": 1.3601529598236084,
      "learning_rate": 1.0126402854362636e-05,
      "loss": 2.4723,
      "step": 1230800
    },
    {
      "epoch": 398.99513776337113,
      "grad_norm": 1.5196632146835327,
      "learning_rate": 1.0123159260460591e-05,
      "loss": 2.4909,
      "step": 1230900
    },
    {
      "epoch": 399.0,
      "eval_bleu": 1.0740270810270096,
      "eval_loss": 4.256314754486084,
      "eval_runtime": 4.0245,
      "eval_samples_per_second": 122.252,
      "eval_steps_per_second": 1.988,
      "step": 1230915
    },
    {
      "epoch": 399.02755267423015,
      "grad_norm": 1.50160813331604,
      "learning_rate": 1.0119915666558548e-05,
      "loss": 2.4799,
      "step": 1231000
    },
    {
      "epoch": 399.05996758508917,
      "grad_norm": 1.5939215421676636,
      "learning_rate": 1.0116672072656503e-05,
      "loss": 2.4597,
      "step": 1231100
    },
    {
      "epoch": 399.09238249594813,
      "grad_norm": 1.3657691478729248,
      "learning_rate": 1.011342847875446e-05,
      "loss": 2.4893,
      "step": 1231200
    },
    {
      "epoch": 399.12479740680715,
      "grad_norm": 1.4469292163848877,
      "learning_rate": 1.0110184884852416e-05,
      "loss": 2.4818,
      "step": 1231300
    },
    {
      "epoch": 399.1572123176661,
      "grad_norm": 1.3458341360092163,
      "learning_rate": 1.0106941290950375e-05,
      "loss": 2.4851,
      "step": 1231400
    },
    {
      "epoch": 399.1896272285251,
      "grad_norm": 1.3239836692810059,
      "learning_rate": 1.010369769704833e-05,
      "loss": 2.4802,
      "step": 1231500
    },
    {
      "epoch": 399.22204213938414,
      "grad_norm": 1.3491631746292114,
      "learning_rate": 1.0100454103146287e-05,
      "loss": 2.4778,
      "step": 1231600
    },
    {
      "epoch": 399.2544570502431,
      "grad_norm": 1.4639936685562134,
      "learning_rate": 1.0097210509244242e-05,
      "loss": 2.4755,
      "step": 1231700
    },
    {
      "epoch": 399.2868719611021,
      "grad_norm": 1.4115012884140015,
      "learning_rate": 1.00939669153422e-05,
      "loss": 2.484,
      "step": 1231800
    },
    {
      "epoch": 399.3192868719611,
      "grad_norm": 1.4000645875930786,
      "learning_rate": 1.0090788193318198e-05,
      "loss": 2.4877,
      "step": 1231900
    },
    {
      "epoch": 399.3517017828201,
      "grad_norm": 1.3541442155838013,
      "learning_rate": 1.0087544599416153e-05,
      "loss": 2.4608,
      "step": 1232000
    },
    {
      "epoch": 399.3841166936791,
      "grad_norm": 1.5458282232284546,
      "learning_rate": 1.008430100551411e-05,
      "loss": 2.4661,
      "step": 1232100
    },
    {
      "epoch": 399.4165316045381,
      "grad_norm": 1.304647445678711,
      "learning_rate": 1.0081057411612066e-05,
      "loss": 2.4846,
      "step": 1232200
    },
    {
      "epoch": 399.4489465153971,
      "grad_norm": 1.640097737312317,
      "learning_rate": 1.0077813817710023e-05,
      "loss": 2.4958,
      "step": 1232300
    },
    {
      "epoch": 399.48136142625606,
      "grad_norm": 1.2748695611953735,
      "learning_rate": 1.007457022380798e-05,
      "loss": 2.4739,
      "step": 1232400
    },
    {
      "epoch": 399.5137763371151,
      "grad_norm": 1.5743863582611084,
      "learning_rate": 1.0071326629905937e-05,
      "loss": 2.4808,
      "step": 1232500
    },
    {
      "epoch": 399.5461912479741,
      "grad_norm": 1.5417903661727905,
      "learning_rate": 1.0068083036003894e-05,
      "loss": 2.4836,
      "step": 1232600
    },
    {
      "epoch": 399.57860615883305,
      "grad_norm": 1.2713690996170044,
      "learning_rate": 1.006483944210185e-05,
      "loss": 2.4938,
      "step": 1232700
    },
    {
      "epoch": 399.6110210696921,
      "grad_norm": 1.349915623664856,
      "learning_rate": 1.0061595848199806e-05,
      "loss": 2.4744,
      "step": 1232800
    },
    {
      "epoch": 399.64343598055103,
      "grad_norm": 1.356063961982727,
      "learning_rate": 1.0058352254297762e-05,
      "loss": 2.4806,
      "step": 1232900
    },
    {
      "epoch": 399.67585089141005,
      "grad_norm": 1.2703123092651367,
      "learning_rate": 1.0055108660395719e-05,
      "loss": 2.4703,
      "step": 1233000
    },
    {
      "epoch": 399.70826580226907,
      "grad_norm": 1.5604312419891357,
      "learning_rate": 1.0051865066493676e-05,
      "loss": 2.4696,
      "step": 1233100
    },
    {
      "epoch": 399.74068071312803,
      "grad_norm": 1.5010640621185303,
      "learning_rate": 1.0048621472591633e-05,
      "loss": 2.4897,
      "step": 1233200
    },
    {
      "epoch": 399.77309562398705,
      "grad_norm": 1.4556609392166138,
      "learning_rate": 1.0045377878689588e-05,
      "loss": 2.4759,
      "step": 1233300
    },
    {
      "epoch": 399.805510534846,
      "grad_norm": 1.505797028541565,
      "learning_rate": 1.0042134284787545e-05,
      "loss": 2.4863,
      "step": 1233400
    },
    {
      "epoch": 399.837925445705,
      "grad_norm": 1.5396450757980347,
      "learning_rate": 1.00388906908855e-05,
      "loss": 2.4936,
      "step": 1233500
    },
    {
      "epoch": 399.87034035656404,
      "grad_norm": 1.5004093647003174,
      "learning_rate": 1.0035647096983458e-05,
      "loss": 2.4852,
      "step": 1233600
    },
    {
      "epoch": 399.902755267423,
      "grad_norm": 1.475704312324524,
      "learning_rate": 1.0032403503081415e-05,
      "loss": 2.4588,
      "step": 1233700
    },
    {
      "epoch": 399.935170178282,
      "grad_norm": 1.263201355934143,
      "learning_rate": 1.0029159909179372e-05,
      "loss": 2.4743,
      "step": 1233800
    },
    {
      "epoch": 399.967585089141,
      "grad_norm": 1.4105628728866577,
      "learning_rate": 1.0025916315277329e-05,
      "loss": 2.5016,
      "step": 1233900
    },
    {
      "epoch": 400.0,
      "grad_norm": 1.2766053676605225,
      "learning_rate": 1.0022672721375284e-05,
      "loss": 2.481,
      "step": 1234000
    },
    {
      "epoch": 400.0,
      "eval_bleu": 0.86536240219496,
      "eval_loss": 4.251575469970703,
      "eval_runtime": 3.9253,
      "eval_samples_per_second": 125.341,
      "eval_steps_per_second": 2.038,
      "step": 1234000
    },
    {
      "epoch": 400.032414910859,
      "grad_norm": 1.4930323362350464,
      "learning_rate": 1.0019429127473241e-05,
      "loss": 2.4872,
      "step": 1234100
    },
    {
      "epoch": 400.064829821718,
      "grad_norm": 1.53838312625885,
      "learning_rate": 1.0016185533571197e-05,
      "loss": 2.4791,
      "step": 1234200
    },
    {
      "epoch": 400.097244732577,
      "grad_norm": 1.424485206604004,
      "learning_rate": 1.0012941939669154e-05,
      "loss": 2.4776,
      "step": 1234300
    },
    {
      "epoch": 400.12965964343596,
      "grad_norm": 1.3797513246536255,
      "learning_rate": 1.000969834576711e-05,
      "loss": 2.469,
      "step": 1234400
    },
    {
      "epoch": 400.162074554295,
      "grad_norm": 1.5656981468200684,
      "learning_rate": 1.0006454751865068e-05,
      "loss": 2.4775,
      "step": 1234500
    },
    {
      "epoch": 400.194489465154,
      "grad_norm": 1.403684139251709,
      "learning_rate": 1.0003211157963023e-05,
      "loss": 2.4702,
      "step": 1234600
    },
    {
      "epoch": 400.22690437601295,
      "grad_norm": 1.3735649585723877,
      "learning_rate": 9.99996756406098e-06,
      "loss": 2.4987,
      "step": 1234700
    },
    {
      "epoch": 400.25931928687197,
      "grad_norm": 1.3617914915084839,
      "learning_rate": 9.996723970158935e-06,
      "loss": 2.4912,
      "step": 1234800
    },
    {
      "epoch": 400.29173419773093,
      "grad_norm": 1.3946071863174438,
      "learning_rate": 9.993480376256892e-06,
      "loss": 2.4775,
      "step": 1234900
    },
    {
      "epoch": 400.32414910858995,
      "grad_norm": 1.3816382884979248,
      "learning_rate": 9.99023678235485e-06,
      "loss": 2.4948,
      "step": 1235000
    },
    {
      "epoch": 400.35656401944897,
      "grad_norm": 1.3950978517532349,
      "learning_rate": 9.986993188452807e-06,
      "loss": 2.4743,
      "step": 1235100
    },
    {
      "epoch": 400.3889789303079,
      "grad_norm": 1.45931077003479,
      "learning_rate": 9.983749594550764e-06,
      "loss": 2.4741,
      "step": 1235200
    },
    {
      "epoch": 400.42139384116695,
      "grad_norm": 1.4163786172866821,
      "learning_rate": 9.980506000648719e-06,
      "loss": 2.4784,
      "step": 1235300
    },
    {
      "epoch": 400.4538087520259,
      "grad_norm": 1.4535592794418335,
      "learning_rate": 9.977262406746676e-06,
      "loss": 2.4693,
      "step": 1235400
    },
    {
      "epoch": 400.4862236628849,
      "grad_norm": 1.5221513509750366,
      "learning_rate": 9.974018812844631e-06,
      "loss": 2.4726,
      "step": 1235500
    },
    {
      "epoch": 400.51863857374394,
      "grad_norm": 1.3386114835739136,
      "learning_rate": 9.97080765488161e-06,
      "loss": 2.4771,
      "step": 1235600
    },
    {
      "epoch": 400.5510534846029,
      "grad_norm": 1.523362636566162,
      "learning_rate": 9.967564060979566e-06,
      "loss": 2.4664,
      "step": 1235700
    },
    {
      "epoch": 400.5834683954619,
      "grad_norm": 1.487674593925476,
      "learning_rate": 9.964320467077522e-06,
      "loss": 2.4996,
      "step": 1235800
    },
    {
      "epoch": 400.6158833063209,
      "grad_norm": 1.5663233995437622,
      "learning_rate": 9.961076873175479e-06,
      "loss": 2.483,
      "step": 1235900
    },
    {
      "epoch": 400.6482982171799,
      "grad_norm": 1.3420721292495728,
      "learning_rate": 9.957833279273436e-06,
      "loss": 2.4912,
      "step": 1236000
    },
    {
      "epoch": 400.6807131280389,
      "grad_norm": 1.399410605430603,
      "learning_rate": 9.954589685371391e-06,
      "loss": 2.475,
      "step": 1236100
    },
    {
      "epoch": 400.7131280388979,
      "grad_norm": 1.4201747179031372,
      "learning_rate": 9.95134609146935e-06,
      "loss": 2.4872,
      "step": 1236200
    },
    {
      "epoch": 400.7455429497569,
      "grad_norm": 1.2543481588363647,
      "learning_rate": 9.948102497567305e-06,
      "loss": 2.4963,
      "step": 1236300
    },
    {
      "epoch": 400.77795786061586,
      "grad_norm": 1.5038971900939941,
      "learning_rate": 9.944858903665262e-06,
      "loss": 2.4797,
      "step": 1236400
    },
    {
      "epoch": 400.8103727714749,
      "grad_norm": 1.6360903978347778,
      "learning_rate": 9.941647745702238e-06,
      "loss": 2.4733,
      "step": 1236500
    },
    {
      "epoch": 400.8427876823339,
      "grad_norm": 1.2890124320983887,
      "learning_rate": 9.938404151800194e-06,
      "loss": 2.4886,
      "step": 1236600
    },
    {
      "epoch": 400.87520259319285,
      "grad_norm": 1.3922358751296997,
      "learning_rate": 9.935160557898152e-06,
      "loss": 2.4828,
      "step": 1236700
    },
    {
      "epoch": 400.90761750405187,
      "grad_norm": 1.2781996726989746,
      "learning_rate": 9.93191696399611e-06,
      "loss": 2.4553,
      "step": 1236800
    },
    {
      "epoch": 400.94003241491083,
      "grad_norm": 1.303287148475647,
      "learning_rate": 9.928673370094065e-06,
      "loss": 2.5008,
      "step": 1236900
    },
    {
      "epoch": 400.97244732576985,
      "grad_norm": 1.3503308296203613,
      "learning_rate": 9.925429776192022e-06,
      "loss": 2.4553,
      "step": 1237000
    },
    {
      "epoch": 401.0,
      "eval_bleu": 0.8979264337047478,
      "eval_loss": 4.25404691696167,
      "eval_runtime": 4.1957,
      "eval_samples_per_second": 117.262,
      "eval_steps_per_second": 1.907,
      "step": 1237085
    },
    {
      "epoch": 401.00486223662887,
      "grad_norm": 1.356667399406433,
      "learning_rate": 9.922186182289977e-06,
      "loss": 2.4605,
      "step": 1237100
    },
    {
      "epoch": 401.0372771474878,
      "grad_norm": 1.4241840839385986,
      "learning_rate": 9.918942588387934e-06,
      "loss": 2.482,
      "step": 1237200
    },
    {
      "epoch": 401.06969205834685,
      "grad_norm": 1.2412843704223633,
      "learning_rate": 9.915698994485891e-06,
      "loss": 2.4649,
      "step": 1237300
    },
    {
      "epoch": 401.1021069692058,
      "grad_norm": 1.3133150339126587,
      "learning_rate": 9.912455400583848e-06,
      "loss": 2.4718,
      "step": 1237400
    },
    {
      "epoch": 401.1345218800648,
      "grad_norm": 1.4108659029006958,
      "learning_rate": 9.909211806681804e-06,
      "loss": 2.5063,
      "step": 1237500
    },
    {
      "epoch": 401.16693679092384,
      "grad_norm": 1.3406940698623657,
      "learning_rate": 9.90596821277976e-06,
      "loss": 2.4999,
      "step": 1237600
    },
    {
      "epoch": 401.1993517017828,
      "grad_norm": 1.1647167205810547,
      "learning_rate": 9.902724618877716e-06,
      "loss": 2.4804,
      "step": 1237700
    },
    {
      "epoch": 401.2317666126418,
      "grad_norm": 1.5263962745666504,
      "learning_rate": 9.899481024975673e-06,
      "loss": 2.4729,
      "step": 1237800
    },
    {
      "epoch": 401.26418152350084,
      "grad_norm": 1.5995891094207764,
      "learning_rate": 9.89623743107363e-06,
      "loss": 2.488,
      "step": 1237900
    },
    {
      "epoch": 401.2965964343598,
      "grad_norm": 1.397430419921875,
      "learning_rate": 9.892993837171587e-06,
      "loss": 2.4819,
      "step": 1238000
    },
    {
      "epoch": 401.3290113452188,
      "grad_norm": 1.3788609504699707,
      "learning_rate": 9.889782679208563e-06,
      "loss": 2.4458,
      "step": 1238100
    },
    {
      "epoch": 401.3614262560778,
      "grad_norm": 1.3641310930252075,
      "learning_rate": 9.88653908530652e-06,
      "loss": 2.4921,
      "step": 1238200
    },
    {
      "epoch": 401.3938411669368,
      "grad_norm": 1.4584190845489502,
      "learning_rate": 9.883295491404476e-06,
      "loss": 2.4888,
      "step": 1238300
    },
    {
      "epoch": 401.4262560777958,
      "grad_norm": 1.5388197898864746,
      "learning_rate": 9.880051897502433e-06,
      "loss": 2.4797,
      "step": 1238400
    },
    {
      "epoch": 401.4586709886548,
      "grad_norm": 1.427516222000122,
      "learning_rate": 9.87680830360039e-06,
      "loss": 2.4743,
      "step": 1238500
    },
    {
      "epoch": 401.4910858995138,
      "grad_norm": 1.3250161409378052,
      "learning_rate": 9.873564709698347e-06,
      "loss": 2.4692,
      "step": 1238600
    },
    {
      "epoch": 401.52350081037275,
      "grad_norm": 1.4784890413284302,
      "learning_rate": 9.870321115796302e-06,
      "loss": 2.4808,
      "step": 1238700
    },
    {
      "epoch": 401.55591572123177,
      "grad_norm": 1.361490249633789,
      "learning_rate": 9.86707752189426e-06,
      "loss": 2.4689,
      "step": 1238800
    },
    {
      "epoch": 401.5883306320908,
      "grad_norm": 1.4722654819488525,
      "learning_rate": 9.863833927992216e-06,
      "loss": 2.4674,
      "step": 1238900
    },
    {
      "epoch": 401.62074554294975,
      "grad_norm": 1.3546420335769653,
      "learning_rate": 9.860590334090172e-06,
      "loss": 2.4906,
      "step": 1239000
    },
    {
      "epoch": 401.65316045380877,
      "grad_norm": 1.3846555948257446,
      "learning_rate": 9.857346740188129e-06,
      "loss": 2.4834,
      "step": 1239100
    },
    {
      "epoch": 401.6855753646677,
      "grad_norm": 1.5048186779022217,
      "learning_rate": 9.854103146286086e-06,
      "loss": 2.4842,
      "step": 1239200
    },
    {
      "epoch": 401.71799027552674,
      "grad_norm": 1.3193806409835815,
      "learning_rate": 9.850859552384043e-06,
      "loss": 2.488,
      "step": 1239300
    },
    {
      "epoch": 401.75040518638576,
      "grad_norm": 1.3441245555877686,
      "learning_rate": 9.847615958481998e-06,
      "loss": 2.4695,
      "step": 1239400
    },
    {
      "epoch": 401.7828200972447,
      "grad_norm": 1.3612111806869507,
      "learning_rate": 9.844372364579955e-06,
      "loss": 2.4763,
      "step": 1239500
    },
    {
      "epoch": 401.81523500810374,
      "grad_norm": 1.4104102849960327,
      "learning_rate": 9.84112877067791e-06,
      "loss": 2.4842,
      "step": 1239600
    },
    {
      "epoch": 401.8476499189627,
      "grad_norm": 1.3506501913070679,
      "learning_rate": 9.837885176775868e-06,
      "loss": 2.4568,
      "step": 1239700
    },
    {
      "epoch": 401.8800648298217,
      "grad_norm": 1.5210517644882202,
      "learning_rate": 9.834641582873825e-06,
      "loss": 2.4949,
      "step": 1239800
    },
    {
      "epoch": 401.91247974068074,
      "grad_norm": 1.3606219291687012,
      "learning_rate": 9.831397988971782e-06,
      "loss": 2.4816,
      "step": 1239900
    },
    {
      "epoch": 401.9448946515397,
      "grad_norm": 1.3592593669891357,
      "learning_rate": 9.828154395069737e-06,
      "loss": 2.4785,
      "step": 1240000
    },
    {
      "epoch": 401.9773095623987,
      "grad_norm": 1.4206355810165405,
      "learning_rate": 9.824910801167694e-06,
      "loss": 2.4851,
      "step": 1240100
    },
    {
      "epoch": 402.0,
      "eval_bleu": 0.8973972228335542,
      "eval_loss": 4.254619598388672,
      "eval_runtime": 4.3726,
      "eval_samples_per_second": 112.519,
      "eval_steps_per_second": 1.83,
      "step": 1240170
    },
    {
      "epoch": 402.0097244732577,
      "grad_norm": 1.5161126852035522,
      "learning_rate": 9.82166720726565e-06,
      "loss": 2.4693,
      "step": 1240200
    },
    {
      "epoch": 402.0421393841167,
      "grad_norm": 1.4294358491897583,
      "learning_rate": 9.818423613363607e-06,
      "loss": 2.4803,
      "step": 1240300
    },
    {
      "epoch": 402.0745542949757,
      "grad_norm": 1.4200228452682495,
      "learning_rate": 9.815180019461565e-06,
      "loss": 2.482,
      "step": 1240400
    },
    {
      "epoch": 402.1069692058347,
      "grad_norm": 1.5268888473510742,
      "learning_rate": 9.81193642555952e-06,
      "loss": 2.4839,
      "step": 1240500
    },
    {
      "epoch": 402.1393841166937,
      "grad_norm": 1.3284951448440552,
      "learning_rate": 9.808692831657478e-06,
      "loss": 2.4662,
      "step": 1240600
    },
    {
      "epoch": 402.17179902755265,
      "grad_norm": 1.6775908470153809,
      "learning_rate": 9.805449237755433e-06,
      "loss": 2.4644,
      "step": 1240700
    },
    {
      "epoch": 402.20421393841167,
      "grad_norm": 1.5154303312301636,
      "learning_rate": 9.80220564385339e-06,
      "loss": 2.4742,
      "step": 1240800
    },
    {
      "epoch": 402.2366288492707,
      "grad_norm": 1.3862494230270386,
      "learning_rate": 9.798962049951345e-06,
      "loss": 2.4663,
      "step": 1240900
    },
    {
      "epoch": 402.26904376012965,
      "grad_norm": 1.5764714479446411,
      "learning_rate": 9.795718456049304e-06,
      "loss": 2.4722,
      "step": 1241000
    },
    {
      "epoch": 402.30145867098867,
      "grad_norm": 1.353210687637329,
      "learning_rate": 9.79247486214726e-06,
      "loss": 2.482,
      "step": 1241100
    },
    {
      "epoch": 402.3338735818476,
      "grad_norm": 1.4811865091323853,
      "learning_rate": 9.789231268245217e-06,
      "loss": 2.4775,
      "step": 1241200
    },
    {
      "epoch": 402.36628849270664,
      "grad_norm": 1.427869200706482,
      "learning_rate": 9.785987674343172e-06,
      "loss": 2.4766,
      "step": 1241300
    },
    {
      "epoch": 402.39870340356566,
      "grad_norm": 1.6000701189041138,
      "learning_rate": 9.782744080441129e-06,
      "loss": 2.4842,
      "step": 1241400
    },
    {
      "epoch": 402.4311183144246,
      "grad_norm": 1.305376410484314,
      "learning_rate": 9.779500486539084e-06,
      "loss": 2.4572,
      "step": 1241500
    },
    {
      "epoch": 402.46353322528364,
      "grad_norm": 1.5351065397262573,
      "learning_rate": 9.776256892637043e-06,
      "loss": 2.4783,
      "step": 1241600
    },
    {
      "epoch": 402.4959481361426,
      "grad_norm": 1.3910356760025024,
      "learning_rate": 9.773013298734998e-06,
      "loss": 2.4746,
      "step": 1241700
    },
    {
      "epoch": 402.5283630470016,
      "grad_norm": 1.5078461170196533,
      "learning_rate": 9.769769704832956e-06,
      "loss": 2.4628,
      "step": 1241800
    },
    {
      "epoch": 402.56077795786064,
      "grad_norm": 1.1696780920028687,
      "learning_rate": 9.766526110930913e-06,
      "loss": 2.4812,
      "step": 1241900
    },
    {
      "epoch": 402.5931928687196,
      "grad_norm": 1.4018454551696777,
      "learning_rate": 9.763282517028868e-06,
      "loss": 2.4723,
      "step": 1242000
    },
    {
      "epoch": 402.6256077795786,
      "grad_norm": 1.4196326732635498,
      "learning_rate": 9.760038923126825e-06,
      "loss": 2.4735,
      "step": 1242100
    },
    {
      "epoch": 402.6580226904376,
      "grad_norm": 1.2487722635269165,
      "learning_rate": 9.756795329224782e-06,
      "loss": 2.4797,
      "step": 1242200
    },
    {
      "epoch": 402.6904376012966,
      "grad_norm": 1.275553822517395,
      "learning_rate": 9.753551735322739e-06,
      "loss": 2.506,
      "step": 1242300
    },
    {
      "epoch": 402.7228525121556,
      "grad_norm": 1.4630683660507202,
      "learning_rate": 9.750308141420694e-06,
      "loss": 2.4862,
      "step": 1242400
    },
    {
      "epoch": 402.7552674230146,
      "grad_norm": 1.411877989768982,
      "learning_rate": 9.747064547518651e-06,
      "loss": 2.4933,
      "step": 1242500
    },
    {
      "epoch": 402.7876823338736,
      "grad_norm": 1.41409432888031,
      "learning_rate": 9.743820953616607e-06,
      "loss": 2.5044,
      "step": 1242600
    },
    {
      "epoch": 402.82009724473255,
      "grad_norm": 1.751745343208313,
      "learning_rate": 9.740577359714564e-06,
      "loss": 2.4822,
      "step": 1242700
    },
    {
      "epoch": 402.85251215559157,
      "grad_norm": 1.2064141035079956,
      "learning_rate": 9.737333765812521e-06,
      "loss": 2.4903,
      "step": 1242800
    },
    {
      "epoch": 402.8849270664506,
      "grad_norm": 1.5131175518035889,
      "learning_rate": 9.734090171910478e-06,
      "loss": 2.4768,
      "step": 1242900
    },
    {
      "epoch": 402.91734197730955,
      "grad_norm": 1.6345807313919067,
      "learning_rate": 9.730846578008433e-06,
      "loss": 2.4808,
      "step": 1243000
    },
    {
      "epoch": 402.94975688816857,
      "grad_norm": 1.2849174737930298,
      "learning_rate": 9.72760298410639e-06,
      "loss": 2.4872,
      "step": 1243100
    },
    {
      "epoch": 402.9821717990275,
      "grad_norm": 1.6846566200256348,
      "learning_rate": 9.724359390204347e-06,
      "loss": 2.4676,
      "step": 1243200
    },
    {
      "epoch": 403.0,
      "eval_bleu": 1.100880380665639,
      "eval_loss": 4.252397537231445,
      "eval_runtime": 4.0913,
      "eval_samples_per_second": 120.255,
      "eval_steps_per_second": 1.955,
      "step": 1243255
    },
    {
      "epoch": 403.01458670988654,
      "grad_norm": 1.428877353668213,
      "learning_rate": 9.721115796302303e-06,
      "loss": 2.4817,
      "step": 1243300
    },
    {
      "epoch": 403.04700162074556,
      "grad_norm": 1.2335240840911865,
      "learning_rate": 9.71787220240026e-06,
      "loss": 2.512,
      "step": 1243400
    },
    {
      "epoch": 403.0794165316045,
      "grad_norm": 1.148383617401123,
      "learning_rate": 9.714628608498217e-06,
      "loss": 2.4774,
      "step": 1243500
    },
    {
      "epoch": 403.11183144246354,
      "grad_norm": 1.4159247875213623,
      "learning_rate": 9.711385014596174e-06,
      "loss": 2.4625,
      "step": 1243600
    },
    {
      "epoch": 403.1442463533225,
      "grad_norm": 1.289428472518921,
      "learning_rate": 9.70814142069413e-06,
      "loss": 2.4646,
      "step": 1243700
    },
    {
      "epoch": 403.1766612641815,
      "grad_norm": 1.504676103591919,
      "learning_rate": 9.704897826792086e-06,
      "loss": 2.4836,
      "step": 1243800
    },
    {
      "epoch": 403.20907617504054,
      "grad_norm": 1.6136448383331299,
      "learning_rate": 9.701654232890042e-06,
      "loss": 2.4885,
      "step": 1243900
    },
    {
      "epoch": 403.2414910858995,
      "grad_norm": 1.4254505634307861,
      "learning_rate": 9.698410638987999e-06,
      "loss": 2.4721,
      "step": 1244000
    },
    {
      "epoch": 403.2739059967585,
      "grad_norm": 1.3382660150527954,
      "learning_rate": 9.695167045085956e-06,
      "loss": 2.4838,
      "step": 1244100
    },
    {
      "epoch": 403.3063209076175,
      "grad_norm": 1.2625483274459839,
      "learning_rate": 9.691955887122934e-06,
      "loss": 2.4586,
      "step": 1244200
    },
    {
      "epoch": 403.3387358184765,
      "grad_norm": 1.3873271942138672,
      "learning_rate": 9.688712293220889e-06,
      "loss": 2.4837,
      "step": 1244300
    },
    {
      "epoch": 403.3711507293355,
      "grad_norm": 1.3618416786193848,
      "learning_rate": 9.685468699318846e-06,
      "loss": 2.479,
      "step": 1244400
    },
    {
      "epoch": 403.4035656401945,
      "grad_norm": 1.2693849802017212,
      "learning_rate": 9.682225105416801e-06,
      "loss": 2.4821,
      "step": 1244500
    },
    {
      "epoch": 403.4359805510535,
      "grad_norm": 1.2838144302368164,
      "learning_rate": 9.678981511514758e-06,
      "loss": 2.4786,
      "step": 1244600
    },
    {
      "epoch": 403.4683954619125,
      "grad_norm": 1.4229097366333008,
      "learning_rate": 9.675737917612715e-06,
      "loss": 2.4681,
      "step": 1244700
    },
    {
      "epoch": 403.50081037277147,
      "grad_norm": 1.2420798540115356,
      "learning_rate": 9.672494323710672e-06,
      "loss": 2.4871,
      "step": 1244800
    },
    {
      "epoch": 403.5332252836305,
      "grad_norm": 1.422346591949463,
      "learning_rate": 9.669250729808628e-06,
      "loss": 2.4724,
      "step": 1244900
    },
    {
      "epoch": 403.56564019448945,
      "grad_norm": 1.4806829690933228,
      "learning_rate": 9.666007135906585e-06,
      "loss": 2.4651,
      "step": 1245000
    },
    {
      "epoch": 403.59805510534846,
      "grad_norm": 1.6163817644119263,
      "learning_rate": 9.66276354200454e-06,
      "loss": 2.4722,
      "step": 1245100
    },
    {
      "epoch": 403.6304700162075,
      "grad_norm": 1.2915061712265015,
      "learning_rate": 9.659519948102497e-06,
      "loss": 2.4782,
      "step": 1245200
    },
    {
      "epoch": 403.66288492706644,
      "grad_norm": 1.3747152090072632,
      "learning_rate": 9.656276354200454e-06,
      "loss": 2.4792,
      "step": 1245300
    },
    {
      "epoch": 403.69529983792546,
      "grad_norm": 1.6279045343399048,
      "learning_rate": 9.653032760298411e-06,
      "loss": 2.4558,
      "step": 1245400
    },
    {
      "epoch": 403.7277147487844,
      "grad_norm": 1.4320446252822876,
      "learning_rate": 9.649789166396368e-06,
      "loss": 2.4928,
      "step": 1245500
    },
    {
      "epoch": 403.76012965964344,
      "grad_norm": 1.4128423929214478,
      "learning_rate": 9.646545572494324e-06,
      "loss": 2.4847,
      "step": 1245600
    },
    {
      "epoch": 403.79254457050246,
      "grad_norm": 1.3806703090667725,
      "learning_rate": 9.64330197859228e-06,
      "loss": 2.4623,
      "step": 1245700
    },
    {
      "epoch": 403.8249594813614,
      "grad_norm": 1.2810053825378418,
      "learning_rate": 9.640058384690236e-06,
      "loss": 2.4759,
      "step": 1245800
    },
    {
      "epoch": 403.85737439222044,
      "grad_norm": 1.4290708303451538,
      "learning_rate": 9.636814790788195e-06,
      "loss": 2.4596,
      "step": 1245900
    },
    {
      "epoch": 403.8897893030794,
      "grad_norm": 1.49220609664917,
      "learning_rate": 9.63357119688615e-06,
      "loss": 2.499,
      "step": 1246000
    },
    {
      "epoch": 403.9222042139384,
      "grad_norm": 1.3813782930374146,
      "learning_rate": 9.630327602984107e-06,
      "loss": 2.4849,
      "step": 1246100
    },
    {
      "epoch": 403.95461912479743,
      "grad_norm": 1.2880561351776123,
      "learning_rate": 9.627084009082063e-06,
      "loss": 2.4809,
      "step": 1246200
    },
    {
      "epoch": 403.9870340356564,
      "grad_norm": 1.3118716478347778,
      "learning_rate": 9.62384041518002e-06,
      "loss": 2.4654,
      "step": 1246300
    },
    {
      "epoch": 404.0,
      "eval_bleu": 1.0382383003442823,
      "eval_loss": 4.2574028968811035,
      "eval_runtime": 4.3184,
      "eval_samples_per_second": 113.932,
      "eval_steps_per_second": 1.853,
      "step": 1246340
    },
    {
      "epoch": 404.0194489465154,
      "grad_norm": 1.4947798252105713,
      "learning_rate": 9.620596821277977e-06,
      "loss": 2.4927,
      "step": 1246400
    },
    {
      "epoch": 404.05186385737437,
      "grad_norm": 1.4918861389160156,
      "learning_rate": 9.617353227375934e-06,
      "loss": 2.4671,
      "step": 1246500
    },
    {
      "epoch": 404.0842787682334,
      "grad_norm": 1.4119060039520264,
      "learning_rate": 9.61410963347389e-06,
      "loss": 2.4828,
      "step": 1246600
    },
    {
      "epoch": 404.1166936790924,
      "grad_norm": 1.4398272037506104,
      "learning_rate": 9.610866039571846e-06,
      "loss": 2.4497,
      "step": 1246700
    },
    {
      "epoch": 404.14910858995137,
      "grad_norm": 1.5246846675872803,
      "learning_rate": 9.607622445669802e-06,
      "loss": 2.4694,
      "step": 1246800
    },
    {
      "epoch": 404.1815235008104,
      "grad_norm": 1.4550069570541382,
      "learning_rate": 9.604378851767759e-06,
      "loss": 2.4846,
      "step": 1246900
    },
    {
      "epoch": 404.21393841166935,
      "grad_norm": 1.3234846591949463,
      "learning_rate": 9.601135257865716e-06,
      "loss": 2.4844,
      "step": 1247000
    },
    {
      "epoch": 404.24635332252836,
      "grad_norm": 1.4902423620224,
      "learning_rate": 9.597891663963673e-06,
      "loss": 2.481,
      "step": 1247100
    },
    {
      "epoch": 404.2787682333874,
      "grad_norm": 1.6058975458145142,
      "learning_rate": 9.594680506000649e-06,
      "loss": 2.4767,
      "step": 1247200
    },
    {
      "epoch": 404.31118314424634,
      "grad_norm": 1.3969683647155762,
      "learning_rate": 9.591436912098606e-06,
      "loss": 2.4809,
      "step": 1247300
    },
    {
      "epoch": 404.34359805510536,
      "grad_norm": 1.357113003730774,
      "learning_rate": 9.588193318196561e-06,
      "loss": 2.4677,
      "step": 1247400
    },
    {
      "epoch": 404.3760129659643,
      "grad_norm": 1.4267596006393433,
      "learning_rate": 9.584949724294518e-06,
      "loss": 2.4596,
      "step": 1247500
    },
    {
      "epoch": 404.40842787682334,
      "grad_norm": 1.3903255462646484,
      "learning_rate": 9.581706130392475e-06,
      "loss": 2.4652,
      "step": 1247600
    },
    {
      "epoch": 404.44084278768236,
      "grad_norm": 1.4552865028381348,
      "learning_rate": 9.578462536490432e-06,
      "loss": 2.4947,
      "step": 1247700
    },
    {
      "epoch": 404.4732576985413,
      "grad_norm": 1.476628065109253,
      "learning_rate": 9.57521894258839e-06,
      "loss": 2.4741,
      "step": 1247800
    },
    {
      "epoch": 404.50567260940034,
      "grad_norm": 1.467458963394165,
      "learning_rate": 9.571975348686345e-06,
      "loss": 2.4702,
      "step": 1247900
    },
    {
      "epoch": 404.5380875202593,
      "grad_norm": 1.3107624053955078,
      "learning_rate": 9.568731754784302e-06,
      "loss": 2.495,
      "step": 1248000
    },
    {
      "epoch": 404.5705024311183,
      "grad_norm": 1.6492079496383667,
      "learning_rate": 9.565488160882257e-06,
      "loss": 2.484,
      "step": 1248100
    },
    {
      "epoch": 404.60291734197733,
      "grad_norm": 1.3869067430496216,
      "learning_rate": 9.562244566980214e-06,
      "loss": 2.4826,
      "step": 1248200
    },
    {
      "epoch": 404.6353322528363,
      "grad_norm": 1.478518009185791,
      "learning_rate": 9.559000973078171e-06,
      "loss": 2.4679,
      "step": 1248300
    },
    {
      "epoch": 404.6677471636953,
      "grad_norm": 1.3691291809082031,
      "learning_rate": 9.555757379176128e-06,
      "loss": 2.4812,
      "step": 1248400
    },
    {
      "epoch": 404.70016207455427,
      "grad_norm": 1.5639750957489014,
      "learning_rate": 9.552513785274084e-06,
      "loss": 2.4625,
      "step": 1248500
    },
    {
      "epoch": 404.7325769854133,
      "grad_norm": 1.3189760446548462,
      "learning_rate": 9.54927019137204e-06,
      "loss": 2.474,
      "step": 1248600
    },
    {
      "epoch": 404.7649918962723,
      "grad_norm": 1.48429536819458,
      "learning_rate": 9.546026597469996e-06,
      "loss": 2.4841,
      "step": 1248700
    },
    {
      "epoch": 404.79740680713127,
      "grad_norm": 1.4222767353057861,
      "learning_rate": 9.542783003567953e-06,
      "loss": 2.5044,
      "step": 1248800
    },
    {
      "epoch": 404.8298217179903,
      "grad_norm": 1.6645023822784424,
      "learning_rate": 9.53953940966591e-06,
      "loss": 2.4862,
      "step": 1248900
    },
    {
      "epoch": 404.86223662884925,
      "grad_norm": 1.4039310216903687,
      "learning_rate": 9.536295815763867e-06,
      "loss": 2.4837,
      "step": 1249000
    },
    {
      "epoch": 404.89465153970826,
      "grad_norm": 1.414670705795288,
      "learning_rate": 9.533052221861824e-06,
      "loss": 2.4938,
      "step": 1249100
    },
    {
      "epoch": 404.9270664505673,
      "grad_norm": 1.4466683864593506,
      "learning_rate": 9.52980862795978e-06,
      "loss": 2.4422,
      "step": 1249200
    },
    {
      "epoch": 404.95948136142624,
      "grad_norm": 1.4051673412322998,
      "learning_rate": 9.526565034057737e-06,
      "loss": 2.4805,
      "step": 1249300
    },
    {
      "epoch": 404.99189627228526,
      "grad_norm": 1.4752228260040283,
      "learning_rate": 9.523353876094713e-06,
      "loss": 2.472,
      "step": 1249400
    },
    {
      "epoch": 405.0,
      "eval_bleu": 1.1128855870343002,
      "eval_loss": 4.2540411949157715,
      "eval_runtime": 4.3065,
      "eval_samples_per_second": 114.245,
      "eval_steps_per_second": 1.858,
      "step": 1249425
    },
    {
      "epoch": 405.0243111831442,
      "grad_norm": 1.3888345956802368,
      "learning_rate": 9.52011028219267e-06,
      "loss": 2.4864,
      "step": 1249500
    },
    {
      "epoch": 405.05672609400324,
      "grad_norm": 1.346295952796936,
      "learning_rate": 9.516866688290627e-06,
      "loss": 2.4654,
      "step": 1249600
    },
    {
      "epoch": 405.08914100486226,
      "grad_norm": 1.3311965465545654,
      "learning_rate": 9.513623094388582e-06,
      "loss": 2.4762,
      "step": 1249700
    },
    {
      "epoch": 405.1215559157212,
      "grad_norm": 1.6207258701324463,
      "learning_rate": 9.51037950048654e-06,
      "loss": 2.473,
      "step": 1249800
    },
    {
      "epoch": 405.15397082658023,
      "grad_norm": 1.6627708673477173,
      "learning_rate": 9.507135906584496e-06,
      "loss": 2.4794,
      "step": 1249900
    },
    {
      "epoch": 405.1863857374392,
      "grad_norm": 1.441981554031372,
      "learning_rate": 9.503892312682452e-06,
      "loss": 2.4587,
      "step": 1250000
    },
    {
      "epoch": 405.2188006482982,
      "grad_norm": 1.4891586303710938,
      "learning_rate": 9.50064871878041e-06,
      "loss": 2.4774,
      "step": 1250100
    },
    {
      "epoch": 405.25121555915723,
      "grad_norm": 1.524009346961975,
      "learning_rate": 9.497405124878366e-06,
      "loss": 2.4836,
      "step": 1250200
    },
    {
      "epoch": 405.2836304700162,
      "grad_norm": 1.8979737758636475,
      "learning_rate": 9.494161530976323e-06,
      "loss": 2.4736,
      "step": 1250300
    },
    {
      "epoch": 405.3160453808752,
      "grad_norm": 1.480893850326538,
      "learning_rate": 9.490917937074278e-06,
      "loss": 2.4891,
      "step": 1250400
    },
    {
      "epoch": 405.34846029173417,
      "grad_norm": 1.5283313989639282,
      "learning_rate": 9.487674343172235e-06,
      "loss": 2.4981,
      "step": 1250500
    },
    {
      "epoch": 405.3808752025932,
      "grad_norm": 1.459230899810791,
      "learning_rate": 9.48443074927019e-06,
      "loss": 2.5047,
      "step": 1250600
    },
    {
      "epoch": 405.4132901134522,
      "grad_norm": 1.3904870748519897,
      "learning_rate": 9.48118715536815e-06,
      "loss": 2.4831,
      "step": 1250700
    },
    {
      "epoch": 405.44570502431117,
      "grad_norm": 1.4612643718719482,
      "learning_rate": 9.477943561466105e-06,
      "loss": 2.4799,
      "step": 1250800
    },
    {
      "epoch": 405.4781199351702,
      "grad_norm": 1.4393339157104492,
      "learning_rate": 9.474699967564062e-06,
      "loss": 2.4659,
      "step": 1250900
    },
    {
      "epoch": 405.51053484602915,
      "grad_norm": 1.5841586589813232,
      "learning_rate": 9.471456373662017e-06,
      "loss": 2.4465,
      "step": 1251000
    },
    {
      "epoch": 405.54294975688816,
      "grad_norm": 1.3499524593353271,
      "learning_rate": 9.468212779759974e-06,
      "loss": 2.4719,
      "step": 1251100
    },
    {
      "epoch": 405.5753646677472,
      "grad_norm": 1.337536096572876,
      "learning_rate": 9.46496918585793e-06,
      "loss": 2.4801,
      "step": 1251200
    },
    {
      "epoch": 405.60777957860614,
      "grad_norm": 1.439900279045105,
      "learning_rate": 9.461725591955888e-06,
      "loss": 2.4577,
      "step": 1251300
    },
    {
      "epoch": 405.64019448946516,
      "grad_norm": 1.3576924800872803,
      "learning_rate": 9.458481998053845e-06,
      "loss": 2.4928,
      "step": 1251400
    },
    {
      "epoch": 405.6726094003242,
      "grad_norm": 1.315853238105774,
      "learning_rate": 9.4552384041518e-06,
      "loss": 2.476,
      "step": 1251500
    },
    {
      "epoch": 405.70502431118314,
      "grad_norm": 1.4578436613082886,
      "learning_rate": 9.451994810249758e-06,
      "loss": 2.4815,
      "step": 1251600
    },
    {
      "epoch": 405.73743922204216,
      "grad_norm": 1.439695954322815,
      "learning_rate": 9.448751216347713e-06,
      "loss": 2.4655,
      "step": 1251700
    },
    {
      "epoch": 405.7698541329011,
      "grad_norm": 1.2025983333587646,
      "learning_rate": 9.44550762244567e-06,
      "loss": 2.4603,
      "step": 1251800
    },
    {
      "epoch": 405.80226904376013,
      "grad_norm": 1.477900743484497,
      "learning_rate": 9.442264028543627e-06,
      "loss": 2.4696,
      "step": 1251900
    },
    {
      "epoch": 405.83468395461915,
      "grad_norm": 1.4765822887420654,
      "learning_rate": 9.439020434641584e-06,
      "loss": 2.4724,
      "step": 1252000
    },
    {
      "epoch": 405.8670988654781,
      "grad_norm": 1.3126331567764282,
      "learning_rate": 9.43577684073954e-06,
      "loss": 2.4687,
      "step": 1252100
    },
    {
      "epoch": 405.89951377633713,
      "grad_norm": 1.3498424291610718,
      "learning_rate": 9.432533246837497e-06,
      "loss": 2.4714,
      "step": 1252200
    },
    {
      "epoch": 405.9319286871961,
      "grad_norm": 1.2549208402633667,
      "learning_rate": 9.429289652935452e-06,
      "loss": 2.4961,
      "step": 1252300
    },
    {
      "epoch": 405.9643435980551,
      "grad_norm": 1.3883947134017944,
      "learning_rate": 9.426046059033409e-06,
      "loss": 2.4922,
      "step": 1252400
    },
    {
      "epoch": 405.9967585089141,
      "grad_norm": 1.4034554958343506,
      "learning_rate": 9.422802465131366e-06,
      "loss": 2.4734,
      "step": 1252500
    },
    {
      "epoch": 406.0,
      "eval_bleu": 0.9809988887248204,
      "eval_loss": 4.255459785461426,
      "eval_runtime": 4.6409,
      "eval_samples_per_second": 106.013,
      "eval_steps_per_second": 1.724,
      "step": 1252510
    },
    {
      "epoch": 406.0291734197731,
      "grad_norm": 1.778621792793274,
      "learning_rate": 9.419591307168344e-06,
      "loss": 2.4587,
      "step": 1252600
    },
    {
      "epoch": 406.0615883306321,
      "grad_norm": 1.3050726652145386,
      "learning_rate": 9.4163477132663e-06,
      "loss": 2.448,
      "step": 1252700
    },
    {
      "epoch": 406.09400324149107,
      "grad_norm": 1.329007863998413,
      "learning_rate": 9.413136555303277e-06,
      "loss": 2.4787,
      "step": 1252800
    },
    {
      "epoch": 406.1264181523501,
      "grad_norm": 1.4378514289855957,
      "learning_rate": 9.409892961401232e-06,
      "loss": 2.4761,
      "step": 1252900
    },
    {
      "epoch": 406.1588330632091,
      "grad_norm": 1.3745819330215454,
      "learning_rate": 9.40664936749919e-06,
      "loss": 2.4837,
      "step": 1253000
    },
    {
      "epoch": 406.19124797406806,
      "grad_norm": 1.5437415838241577,
      "learning_rate": 9.403405773597146e-06,
      "loss": 2.4902,
      "step": 1253100
    },
    {
      "epoch": 406.2236628849271,
      "grad_norm": 1.3576682806015015,
      "learning_rate": 9.400162179695104e-06,
      "loss": 2.4823,
      "step": 1253200
    },
    {
      "epoch": 406.25607779578604,
      "grad_norm": 1.3707383871078491,
      "learning_rate": 9.396918585793059e-06,
      "loss": 2.4625,
      "step": 1253300
    },
    {
      "epoch": 406.28849270664506,
      "grad_norm": 1.2906614542007446,
      "learning_rate": 9.393674991891016e-06,
      "loss": 2.4937,
      "step": 1253400
    },
    {
      "epoch": 406.3209076175041,
      "grad_norm": 1.3688992261886597,
      "learning_rate": 9.390431397988971e-06,
      "loss": 2.461,
      "step": 1253500
    },
    {
      "epoch": 406.35332252836304,
      "grad_norm": 1.4573711156845093,
      "learning_rate": 9.387187804086928e-06,
      "loss": 2.4757,
      "step": 1253600
    },
    {
      "epoch": 406.38573743922205,
      "grad_norm": 1.4011783599853516,
      "learning_rate": 9.383944210184885e-06,
      "loss": 2.4866,
      "step": 1253700
    },
    {
      "epoch": 406.418152350081,
      "grad_norm": 1.6528431177139282,
      "learning_rate": 9.380700616282842e-06,
      "loss": 2.4553,
      "step": 1253800
    },
    {
      "epoch": 406.45056726094003,
      "grad_norm": 1.3385009765625,
      "learning_rate": 9.377457022380798e-06,
      "loss": 2.4804,
      "step": 1253900
    },
    {
      "epoch": 406.48298217179905,
      "grad_norm": 1.4069854021072388,
      "learning_rate": 9.374213428478755e-06,
      "loss": 2.4929,
      "step": 1254000
    },
    {
      "epoch": 406.515397082658,
      "grad_norm": 1.4747846126556396,
      "learning_rate": 9.37096983457671e-06,
      "loss": 2.4633,
      "step": 1254100
    },
    {
      "epoch": 406.54781199351703,
      "grad_norm": 1.3282742500305176,
      "learning_rate": 9.367726240674667e-06,
      "loss": 2.4764,
      "step": 1254200
    },
    {
      "epoch": 406.580226904376,
      "grad_norm": 1.4188538789749146,
      "learning_rate": 9.364482646772626e-06,
      "loss": 2.4689,
      "step": 1254300
    },
    {
      "epoch": 406.612641815235,
      "grad_norm": 1.4375026226043701,
      "learning_rate": 9.361239052870581e-06,
      "loss": 2.482,
      "step": 1254400
    },
    {
      "epoch": 406.645056726094,
      "grad_norm": 1.6509381532669067,
      "learning_rate": 9.357995458968538e-06,
      "loss": 2.4655,
      "step": 1254500
    },
    {
      "epoch": 406.677471636953,
      "grad_norm": 1.2770510911941528,
      "learning_rate": 9.354751865066494e-06,
      "loss": 2.4655,
      "step": 1254600
    },
    {
      "epoch": 406.709886547812,
      "grad_norm": 1.4793555736541748,
      "learning_rate": 9.35150827116445e-06,
      "loss": 2.5081,
      "step": 1254700
    },
    {
      "epoch": 406.74230145867097,
      "grad_norm": 1.324554443359375,
      "learning_rate": 9.348264677262406e-06,
      "loss": 2.4765,
      "step": 1254800
    },
    {
      "epoch": 406.77471636953,
      "grad_norm": 1.4282386302947998,
      "learning_rate": 9.345021083360365e-06,
      "loss": 2.4592,
      "step": 1254900
    },
    {
      "epoch": 406.807131280389,
      "grad_norm": 1.6406159400939941,
      "learning_rate": 9.34177748945832e-06,
      "loss": 2.4802,
      "step": 1255000
    },
    {
      "epoch": 406.83954619124796,
      "grad_norm": 1.4684327840805054,
      "learning_rate": 9.338533895556277e-06,
      "loss": 2.4928,
      "step": 1255100
    },
    {
      "epoch": 406.871961102107,
      "grad_norm": 1.4085807800292969,
      "learning_rate": 9.335290301654233e-06,
      "loss": 2.472,
      "step": 1255200
    },
    {
      "epoch": 406.90437601296594,
      "grad_norm": 1.6703250408172607,
      "learning_rate": 9.33204670775219e-06,
      "loss": 2.4769,
      "step": 1255300
    },
    {
      "epoch": 406.93679092382496,
      "grad_norm": 1.310104489326477,
      "learning_rate": 9.328803113850145e-06,
      "loss": 2.4909,
      "step": 1255400
    },
    {
      "epoch": 406.969205834684,
      "grad_norm": 1.4182277917861938,
      "learning_rate": 9.325559519948104e-06,
      "loss": 2.4915,
      "step": 1255500
    },
    {
      "epoch": 407.0,
      "eval_bleu": 1.0741384390227116,
      "eval_loss": 4.255568504333496,
      "eval_runtime": 4.4945,
      "eval_samples_per_second": 109.467,
      "eval_steps_per_second": 1.78,
      "step": 1255595
    },
    {
      "epoch": 407.00162074554294,
      "grad_norm": 1.377360463142395,
      "learning_rate": 9.32231592604606e-06,
      "loss": 2.4675,
      "step": 1255600
    },
    {
      "epoch": 407.03403565640195,
      "grad_norm": 1.435165524482727,
      "learning_rate": 9.319072332144016e-06,
      "loss": 2.4951,
      "step": 1255700
    },
    {
      "epoch": 407.0664505672609,
      "grad_norm": 1.5583056211471558,
      "learning_rate": 9.315828738241973e-06,
      "loss": 2.4956,
      "step": 1255800
    },
    {
      "epoch": 407.09886547811993,
      "grad_norm": 1.3146591186523438,
      "learning_rate": 9.312585144339929e-06,
      "loss": 2.4797,
      "step": 1255900
    },
    {
      "epoch": 407.13128038897895,
      "grad_norm": 1.379077672958374,
      "learning_rate": 9.309341550437886e-06,
      "loss": 2.4763,
      "step": 1256000
    },
    {
      "epoch": 407.1636952998379,
      "grad_norm": 1.5098191499710083,
      "learning_rate": 9.306097956535843e-06,
      "loss": 2.4697,
      "step": 1256100
    },
    {
      "epoch": 407.19611021069693,
      "grad_norm": 1.857495665550232,
      "learning_rate": 9.3028543626338e-06,
      "loss": 2.4913,
      "step": 1256200
    },
    {
      "epoch": 407.2285251215559,
      "grad_norm": 1.1954714059829712,
      "learning_rate": 9.299643204670776e-06,
      "loss": 2.4572,
      "step": 1256300
    },
    {
      "epoch": 407.2609400324149,
      "grad_norm": 1.3361250162124634,
      "learning_rate": 9.296399610768733e-06,
      "loss": 2.4495,
      "step": 1256400
    },
    {
      "epoch": 407.2933549432739,
      "grad_norm": 1.3080322742462158,
      "learning_rate": 9.293156016866688e-06,
      "loss": 2.4741,
      "step": 1256500
    },
    {
      "epoch": 407.3257698541329,
      "grad_norm": 1.516405463218689,
      "learning_rate": 9.289912422964645e-06,
      "loss": 2.4831,
      "step": 1256600
    },
    {
      "epoch": 407.3581847649919,
      "grad_norm": 1.376685380935669,
      "learning_rate": 9.286668829062602e-06,
      "loss": 2.4692,
      "step": 1256700
    },
    {
      "epoch": 407.39059967585086,
      "grad_norm": 1.36300528049469,
      "learning_rate": 9.28342523516056e-06,
      "loss": 2.4802,
      "step": 1256800
    },
    {
      "epoch": 407.4230145867099,
      "grad_norm": 1.5167311429977417,
      "learning_rate": 9.280181641258515e-06,
      "loss": 2.479,
      "step": 1256900
    },
    {
      "epoch": 407.4554294975689,
      "grad_norm": 1.3289241790771484,
      "learning_rate": 9.276938047356472e-06,
      "loss": 2.4951,
      "step": 1257000
    },
    {
      "epoch": 407.48784440842786,
      "grad_norm": 1.3841248750686646,
      "learning_rate": 9.273694453454427e-06,
      "loss": 2.4626,
      "step": 1257100
    },
    {
      "epoch": 407.5202593192869,
      "grad_norm": 1.2545123100280762,
      "learning_rate": 9.270450859552384e-06,
      "loss": 2.4743,
      "step": 1257200
    },
    {
      "epoch": 407.55267423014584,
      "grad_norm": 1.4114958047866821,
      "learning_rate": 9.267207265650341e-06,
      "loss": 2.4782,
      "step": 1257300
    },
    {
      "epoch": 407.58508914100486,
      "grad_norm": 1.4068684577941895,
      "learning_rate": 9.263963671748298e-06,
      "loss": 2.4559,
      "step": 1257400
    },
    {
      "epoch": 407.6175040518639,
      "grad_norm": 1.5027258396148682,
      "learning_rate": 9.260720077846254e-06,
      "loss": 2.482,
      "step": 1257500
    },
    {
      "epoch": 407.64991896272284,
      "grad_norm": 1.672845721244812,
      "learning_rate": 9.25747648394421e-06,
      "loss": 2.4404,
      "step": 1257600
    },
    {
      "epoch": 407.68233387358185,
      "grad_norm": 1.3472912311553955,
      "learning_rate": 9.254232890042166e-06,
      "loss": 2.4657,
      "step": 1257700
    },
    {
      "epoch": 407.7147487844408,
      "grad_norm": 1.1477941274642944,
      "learning_rate": 9.250989296140123e-06,
      "loss": 2.4785,
      "step": 1257800
    },
    {
      "epoch": 407.74716369529983,
      "grad_norm": 1.4192546606063843,
      "learning_rate": 9.24774570223808e-06,
      "loss": 2.4781,
      "step": 1257900
    },
    {
      "epoch": 407.77957860615885,
      "grad_norm": 1.4055981636047363,
      "learning_rate": 9.244502108336037e-06,
      "loss": 2.4901,
      "step": 1258000
    },
    {
      "epoch": 407.8119935170178,
      "grad_norm": 1.3303141593933105,
      "learning_rate": 9.241258514433994e-06,
      "loss": 2.4831,
      "step": 1258100
    },
    {
      "epoch": 407.84440842787683,
      "grad_norm": 1.3570668697357178,
      "learning_rate": 9.23801492053195e-06,
      "loss": 2.4694,
      "step": 1258200
    },
    {
      "epoch": 407.87682333873585,
      "grad_norm": 1.5251356363296509,
      "learning_rate": 9.234771326629907e-06,
      "loss": 2.4808,
      "step": 1258300
    },
    {
      "epoch": 407.9092382495948,
      "grad_norm": 1.3943309783935547,
      "learning_rate": 9.231527732727862e-06,
      "loss": 2.4834,
      "step": 1258400
    },
    {
      "epoch": 407.9416531604538,
      "grad_norm": 1.3175621032714844,
      "learning_rate": 9.228284138825819e-06,
      "loss": 2.4725,
      "step": 1258500
    },
    {
      "epoch": 407.9740680713128,
      "grad_norm": 1.3697218894958496,
      "learning_rate": 9.225040544923776e-06,
      "loss": 2.4685,
      "step": 1258600
    },
    {
      "epoch": 408.0,
      "eval_bleu": 1.094879959760988,
      "eval_loss": 4.253619194030762,
      "eval_runtime": 4.6274,
      "eval_samples_per_second": 106.324,
      "eval_steps_per_second": 1.729,
      "step": 1258680
    },
    {
      "epoch": 408.0064829821718,
      "grad_norm": 1.4633307456970215,
      "learning_rate": 9.221796951021733e-06,
      "loss": 2.4633,
      "step": 1258700
    },
    {
      "epoch": 408.0388978930308,
      "grad_norm": 1.261942982673645,
      "learning_rate": 9.218553357119689e-06,
      "loss": 2.4764,
      "step": 1258800
    },
    {
      "epoch": 408.0713128038898,
      "grad_norm": 1.3666914701461792,
      "learning_rate": 9.215309763217646e-06,
      "loss": 2.4688,
      "step": 1258900
    },
    {
      "epoch": 408.1037277147488,
      "grad_norm": 1.345116138458252,
      "learning_rate": 9.212066169315601e-06,
      "loss": 2.4779,
      "step": 1259000
    },
    {
      "epoch": 408.13614262560776,
      "grad_norm": 1.4701290130615234,
      "learning_rate": 9.208822575413558e-06,
      "loss": 2.4772,
      "step": 1259100
    },
    {
      "epoch": 408.1685575364668,
      "grad_norm": 1.2925150394439697,
      "learning_rate": 9.205578981511515e-06,
      "loss": 2.4567,
      "step": 1259200
    },
    {
      "epoch": 408.2009724473258,
      "grad_norm": 1.7059133052825928,
      "learning_rate": 9.202335387609472e-06,
      "loss": 2.4869,
      "step": 1259300
    },
    {
      "epoch": 408.23338735818476,
      "grad_norm": 1.1773386001586914,
      "learning_rate": 9.199091793707429e-06,
      "loss": 2.4794,
      "step": 1259400
    },
    {
      "epoch": 408.2658022690438,
      "grad_norm": 1.2664481401443481,
      "learning_rate": 9.195848199805384e-06,
      "loss": 2.4875,
      "step": 1259500
    },
    {
      "epoch": 408.29821717990274,
      "grad_norm": 1.4172991514205933,
      "learning_rate": 9.192604605903342e-06,
      "loss": 2.4828,
      "step": 1259600
    },
    {
      "epoch": 408.33063209076175,
      "grad_norm": 1.6805455684661865,
      "learning_rate": 9.189361012001297e-06,
      "loss": 2.4882,
      "step": 1259700
    },
    {
      "epoch": 408.36304700162077,
      "grad_norm": 1.596547245979309,
      "learning_rate": 9.186117418099256e-06,
      "loss": 2.4606,
      "step": 1259800
    },
    {
      "epoch": 408.39546191247973,
      "grad_norm": 1.791657567024231,
      "learning_rate": 9.182873824197211e-06,
      "loss": 2.4795,
      "step": 1259900
    },
    {
      "epoch": 408.42787682333875,
      "grad_norm": 1.4662243127822876,
      "learning_rate": 9.179630230295168e-06,
      "loss": 2.4707,
      "step": 1260000
    },
    {
      "epoch": 408.4602917341977,
      "grad_norm": 1.4720007181167603,
      "learning_rate": 9.176386636393123e-06,
      "loss": 2.4527,
      "step": 1260100
    },
    {
      "epoch": 408.4927066450567,
      "grad_norm": 1.4787688255310059,
      "learning_rate": 9.17314304249108e-06,
      "loss": 2.4773,
      "step": 1260200
    },
    {
      "epoch": 408.52512155591575,
      "grad_norm": 1.3856749534606934,
      "learning_rate": 9.169899448589036e-06,
      "loss": 2.4681,
      "step": 1260300
    },
    {
      "epoch": 408.5575364667747,
      "grad_norm": 1.4017964601516724,
      "learning_rate": 9.166655854686995e-06,
      "loss": 2.487,
      "step": 1260400
    },
    {
      "epoch": 408.5899513776337,
      "grad_norm": 1.5440222024917603,
      "learning_rate": 9.16341226078495e-06,
      "loss": 2.4867,
      "step": 1260500
    },
    {
      "epoch": 408.6223662884927,
      "grad_norm": 1.2867088317871094,
      "learning_rate": 9.160168666882907e-06,
      "loss": 2.4739,
      "step": 1260600
    },
    {
      "epoch": 408.6547811993517,
      "grad_norm": 1.3450127840042114,
      "learning_rate": 9.156925072980862e-06,
      "loss": 2.4847,
      "step": 1260700
    },
    {
      "epoch": 408.6871961102107,
      "grad_norm": 1.6776231527328491,
      "learning_rate": 9.15368147907882e-06,
      "loss": 2.4653,
      "step": 1260800
    },
    {
      "epoch": 408.7196110210697,
      "grad_norm": 1.4789708852767944,
      "learning_rate": 9.150437885176776e-06,
      "loss": 2.4612,
      "step": 1260900
    },
    {
      "epoch": 408.7520259319287,
      "grad_norm": 1.3248084783554077,
      "learning_rate": 9.147194291274733e-06,
      "loss": 2.4708,
      "step": 1261000
    },
    {
      "epoch": 408.78444084278766,
      "grad_norm": 1.3430720567703247,
      "learning_rate": 9.14395069737269e-06,
      "loss": 2.49,
      "step": 1261100
    },
    {
      "epoch": 408.8168557536467,
      "grad_norm": 1.2127692699432373,
      "learning_rate": 9.140707103470646e-06,
      "loss": 2.4702,
      "step": 1261200
    },
    {
      "epoch": 408.8492706645057,
      "grad_norm": 1.3815308809280396,
      "learning_rate": 9.137463509568603e-06,
      "loss": 2.4559,
      "step": 1261300
    },
    {
      "epoch": 408.88168557536466,
      "grad_norm": 1.489775538444519,
      "learning_rate": 9.134219915666558e-06,
      "loss": 2.4919,
      "step": 1261400
    },
    {
      "epoch": 408.9141004862237,
      "grad_norm": 1.3490184545516968,
      "learning_rate": 9.130976321764515e-06,
      "loss": 2.4726,
      "step": 1261500
    },
    {
      "epoch": 408.94651539708263,
      "grad_norm": 1.7309582233428955,
      "learning_rate": 9.127732727862472e-06,
      "loss": 2.4837,
      "step": 1261600
    },
    {
      "epoch": 408.97893030794165,
      "grad_norm": 1.3422561883926392,
      "learning_rate": 9.12448913396043e-06,
      "loss": 2.4655,
      "step": 1261700
    },
    {
      "epoch": 409.0,
      "eval_bleu": 0.8925810829708661,
      "eval_loss": 4.258954048156738,
      "eval_runtime": 4.2844,
      "eval_samples_per_second": 114.835,
      "eval_steps_per_second": 1.867,
      "step": 1261765
    },
    {
      "epoch": 409.01134521880067,
      "grad_norm": 1.4828604459762573,
      "learning_rate": 9.121245540058385e-06,
      "loss": 2.497,
      "step": 1261800
    },
    {
      "epoch": 409.04376012965963,
      "grad_norm": 1.6985591650009155,
      "learning_rate": 9.118001946156342e-06,
      "loss": 2.4608,
      "step": 1261900
    },
    {
      "epoch": 409.07617504051865,
      "grad_norm": 1.37802255153656,
      "learning_rate": 9.114758352254297e-06,
      "loss": 2.4896,
      "step": 1262000
    },
    {
      "epoch": 409.1085899513776,
      "grad_norm": 1.2444802522659302,
      "learning_rate": 9.111514758352254e-06,
      "loss": 2.4799,
      "step": 1262100
    },
    {
      "epoch": 409.1410048622366,
      "grad_norm": 1.4855798482894897,
      "learning_rate": 9.108271164450211e-06,
      "loss": 2.487,
      "step": 1262200
    },
    {
      "epoch": 409.17341977309565,
      "grad_norm": 1.3380745649337769,
      "learning_rate": 9.105060006487189e-06,
      "loss": 2.4625,
      "step": 1262300
    },
    {
      "epoch": 409.2058346839546,
      "grad_norm": 1.3798075914382935,
      "learning_rate": 9.101816412585144e-06,
      "loss": 2.4575,
      "step": 1262400
    },
    {
      "epoch": 409.2382495948136,
      "grad_norm": 1.4461948871612549,
      "learning_rate": 9.098572818683101e-06,
      "loss": 2.4756,
      "step": 1262500
    },
    {
      "epoch": 409.2706645056726,
      "grad_norm": 1.6438300609588623,
      "learning_rate": 9.095329224781057e-06,
      "loss": 2.458,
      "step": 1262600
    },
    {
      "epoch": 409.3030794165316,
      "grad_norm": 1.411401391029358,
      "learning_rate": 9.092085630879014e-06,
      "loss": 2.4604,
      "step": 1262700
    },
    {
      "epoch": 409.3354943273906,
      "grad_norm": 1.4415457248687744,
      "learning_rate": 9.088842036976971e-06,
      "loss": 2.485,
      "step": 1262800
    },
    {
      "epoch": 409.3679092382496,
      "grad_norm": 1.5392276048660278,
      "learning_rate": 9.085598443074928e-06,
      "loss": 2.4714,
      "step": 1262900
    },
    {
      "epoch": 409.4003241491086,
      "grad_norm": 1.6680699586868286,
      "learning_rate": 9.082354849172885e-06,
      "loss": 2.468,
      "step": 1263000
    },
    {
      "epoch": 409.43273905996756,
      "grad_norm": 1.3018572330474854,
      "learning_rate": 9.07911125527084e-06,
      "loss": 2.4848,
      "step": 1263100
    },
    {
      "epoch": 409.4651539708266,
      "grad_norm": 1.4317739009857178,
      "learning_rate": 9.075867661368797e-06,
      "loss": 2.4746,
      "step": 1263200
    },
    {
      "epoch": 409.4975688816856,
      "grad_norm": 1.6375348567962646,
      "learning_rate": 9.072624067466753e-06,
      "loss": 2.4724,
      "step": 1263300
    },
    {
      "epoch": 409.52998379254456,
      "grad_norm": 1.4698889255523682,
      "learning_rate": 9.06938047356471e-06,
      "loss": 2.4442,
      "step": 1263400
    },
    {
      "epoch": 409.5623987034036,
      "grad_norm": 1.4414546489715576,
      "learning_rate": 9.066136879662667e-06,
      "loss": 2.4895,
      "step": 1263500
    },
    {
      "epoch": 409.59481361426253,
      "grad_norm": 1.3988584280014038,
      "learning_rate": 9.062893285760624e-06,
      "loss": 2.4825,
      "step": 1263600
    },
    {
      "epoch": 409.62722852512155,
      "grad_norm": 1.371272325515747,
      "learning_rate": 9.05964969185858e-06,
      "loss": 2.4718,
      "step": 1263700
    },
    {
      "epoch": 409.65964343598057,
      "grad_norm": 1.4707980155944824,
      "learning_rate": 9.056406097956536e-06,
      "loss": 2.4802,
      "step": 1263800
    },
    {
      "epoch": 409.69205834683953,
      "grad_norm": 1.276023268699646,
      "learning_rate": 9.053162504054492e-06,
      "loss": 2.4877,
      "step": 1263900
    },
    {
      "epoch": 409.72447325769855,
      "grad_norm": 1.6357799768447876,
      "learning_rate": 9.049918910152449e-06,
      "loss": 2.4779,
      "step": 1264000
    },
    {
      "epoch": 409.7568881685575,
      "grad_norm": 1.4404387474060059,
      "learning_rate": 9.046675316250406e-06,
      "loss": 2.4819,
      "step": 1264100
    },
    {
      "epoch": 409.7893030794165,
      "grad_norm": 1.4631129503250122,
      "learning_rate": 9.043431722348363e-06,
      "loss": 2.4767,
      "step": 1264200
    },
    {
      "epoch": 409.82171799027554,
      "grad_norm": 1.332948923110962,
      "learning_rate": 9.040220564385339e-06,
      "loss": 2.4695,
      "step": 1264300
    },
    {
      "epoch": 409.8541329011345,
      "grad_norm": 1.7154022455215454,
      "learning_rate": 9.036976970483296e-06,
      "loss": 2.4924,
      "step": 1264400
    },
    {
      "epoch": 409.8865478119935,
      "grad_norm": 1.8264673948287964,
      "learning_rate": 9.033765812520272e-06,
      "loss": 2.4729,
      "step": 1264500
    },
    {
      "epoch": 409.9189627228525,
      "grad_norm": 1.3523921966552734,
      "learning_rate": 9.03052221861823e-06,
      "loss": 2.4521,
      "step": 1264600
    },
    {
      "epoch": 409.9513776337115,
      "grad_norm": 1.2827904224395752,
      "learning_rate": 9.027278624716186e-06,
      "loss": 2.4984,
      "step": 1264700
    },
    {
      "epoch": 409.9837925445705,
      "grad_norm": 1.5140665769577026,
      "learning_rate": 9.024035030814143e-06,
      "loss": 2.461,
      "step": 1264800
    },
    {
      "epoch": 410.0,
      "eval_bleu": 0.9516222323034909,
      "eval_loss": 4.2546186447143555,
      "eval_runtime": 4.8337,
      "eval_samples_per_second": 101.785,
      "eval_steps_per_second": 1.655,
      "step": 1264850
    },
    {
      "epoch": 410.0162074554295,
      "grad_norm": 1.3666331768035889,
      "learning_rate": 9.020791436912099e-06,
      "loss": 2.4827,
      "step": 1264900
    },
    {
      "epoch": 410.0486223662885,
      "grad_norm": 1.582870364189148,
      "learning_rate": 9.017547843010056e-06,
      "loss": 2.4824,
      "step": 1265000
    },
    {
      "epoch": 410.0810372771475,
      "grad_norm": 1.5577707290649414,
      "learning_rate": 9.014304249108011e-06,
      "loss": 2.4782,
      "step": 1265100
    },
    {
      "epoch": 410.1134521880065,
      "grad_norm": 1.4462029933929443,
      "learning_rate": 9.01106065520597e-06,
      "loss": 2.4673,
      "step": 1265200
    },
    {
      "epoch": 410.1458670988655,
      "grad_norm": 1.4160305261611938,
      "learning_rate": 9.007817061303925e-06,
      "loss": 2.4711,
      "step": 1265300
    },
    {
      "epoch": 410.17828200972446,
      "grad_norm": 1.427348256111145,
      "learning_rate": 9.004573467401882e-06,
      "loss": 2.4736,
      "step": 1265400
    },
    {
      "epoch": 410.2106969205835,
      "grad_norm": 1.5123058557510376,
      "learning_rate": 9.001329873499837e-06,
      "loss": 2.4882,
      "step": 1265500
    },
    {
      "epoch": 410.2431118314425,
      "grad_norm": 1.2494149208068848,
      "learning_rate": 8.998086279597795e-06,
      "loss": 2.4742,
      "step": 1265600
    },
    {
      "epoch": 410.27552674230145,
      "grad_norm": 1.648245096206665,
      "learning_rate": 8.99484268569575e-06,
      "loss": 2.4938,
      "step": 1265700
    },
    {
      "epoch": 410.30794165316047,
      "grad_norm": 1.3516913652420044,
      "learning_rate": 8.991599091793709e-06,
      "loss": 2.4708,
      "step": 1265800
    },
    {
      "epoch": 410.34035656401943,
      "grad_norm": 1.3234679698944092,
      "learning_rate": 8.988355497891664e-06,
      "loss": 2.468,
      "step": 1265900
    },
    {
      "epoch": 410.37277147487845,
      "grad_norm": 1.4385313987731934,
      "learning_rate": 8.985111903989621e-06,
      "loss": 2.4618,
      "step": 1266000
    },
    {
      "epoch": 410.40518638573747,
      "grad_norm": 1.5651122331619263,
      "learning_rate": 8.981868310087578e-06,
      "loss": 2.4682,
      "step": 1266100
    },
    {
      "epoch": 410.4376012965964,
      "grad_norm": 1.1982187032699585,
      "learning_rate": 8.978624716185533e-06,
      "loss": 2.4686,
      "step": 1266200
    },
    {
      "epoch": 410.47001620745544,
      "grad_norm": 1.4647877216339111,
      "learning_rate": 8.97538112228349e-06,
      "loss": 2.4693,
      "step": 1266300
    },
    {
      "epoch": 410.5024311183144,
      "grad_norm": 1.483963966369629,
      "learning_rate": 8.972137528381448e-06,
      "loss": 2.4657,
      "step": 1266400
    },
    {
      "epoch": 410.5348460291734,
      "grad_norm": 1.515404224395752,
      "learning_rate": 8.968893934479405e-06,
      "loss": 2.4884,
      "step": 1266500
    },
    {
      "epoch": 410.56726094003244,
      "grad_norm": 1.45082426071167,
      "learning_rate": 8.96565034057736e-06,
      "loss": 2.4702,
      "step": 1266600
    },
    {
      "epoch": 410.5996758508914,
      "grad_norm": 1.4833006858825684,
      "learning_rate": 8.962406746675317e-06,
      "loss": 2.471,
      "step": 1266700
    },
    {
      "epoch": 410.6320907617504,
      "grad_norm": 1.3318482637405396,
      "learning_rate": 8.959163152773272e-06,
      "loss": 2.4803,
      "step": 1266800
    },
    {
      "epoch": 410.6645056726094,
      "grad_norm": 1.4002790451049805,
      "learning_rate": 8.95591955887123e-06,
      "loss": 2.471,
      "step": 1266900
    },
    {
      "epoch": 410.6969205834684,
      "grad_norm": 1.5310914516448975,
      "learning_rate": 8.952675964969186e-06,
      "loss": 2.4649,
      "step": 1267000
    },
    {
      "epoch": 410.7293354943274,
      "grad_norm": 1.4904364347457886,
      "learning_rate": 8.949432371067143e-06,
      "loss": 2.4578,
      "step": 1267100
    },
    {
      "epoch": 410.7617504051864,
      "grad_norm": 1.3696203231811523,
      "learning_rate": 8.946188777165099e-06,
      "loss": 2.4732,
      "step": 1267200
    },
    {
      "epoch": 410.7941653160454,
      "grad_norm": 1.3520214557647705,
      "learning_rate": 8.942945183263056e-06,
      "loss": 2.474,
      "step": 1267300
    },
    {
      "epoch": 410.82658022690435,
      "grad_norm": 1.4520500898361206,
      "learning_rate": 8.939701589361013e-06,
      "loss": 2.4499,
      "step": 1267400
    },
    {
      "epoch": 410.8589951377634,
      "grad_norm": 1.3721425533294678,
      "learning_rate": 8.936490431397989e-06,
      "loss": 2.4962,
      "step": 1267500
    },
    {
      "epoch": 410.8914100486224,
      "grad_norm": 1.4884976148605347,
      "learning_rate": 8.933246837495946e-06,
      "loss": 2.4839,
      "step": 1267600
    },
    {
      "epoch": 410.92382495948135,
      "grad_norm": 1.4603532552719116,
      "learning_rate": 8.930003243593903e-06,
      "loss": 2.4842,
      "step": 1267700
    },
    {
      "epoch": 410.95623987034037,
      "grad_norm": 1.5467296838760376,
      "learning_rate": 8.926759649691858e-06,
      "loss": 2.4719,
      "step": 1267800
    },
    {
      "epoch": 410.98865478119933,
      "grad_norm": 1.4231369495391846,
      "learning_rate": 8.923516055789816e-06,
      "loss": 2.4672,
      "step": 1267900
    },
    {
      "epoch": 411.0,
      "eval_bleu": 0.9983633875253964,
      "eval_loss": 4.258700370788574,
      "eval_runtime": 4.296,
      "eval_samples_per_second": 114.526,
      "eval_steps_per_second": 1.862,
      "step": 1267935
    },
    {
      "epoch": 411.02106969205835,
      "grad_norm": 1.582517147064209,
      "learning_rate": 8.920272461887771e-06,
      "loss": 2.4875,
      "step": 1268000
    },
    {
      "epoch": 411.05348460291737,
      "grad_norm": 1.341732144355774,
      "learning_rate": 8.917028867985728e-06,
      "loss": 2.4801,
      "step": 1268100
    },
    {
      "epoch": 411.0858995137763,
      "grad_norm": 1.3734915256500244,
      "learning_rate": 8.913785274083685e-06,
      "loss": 2.473,
      "step": 1268200
    },
    {
      "epoch": 411.11831442463534,
      "grad_norm": 1.34367036819458,
      "learning_rate": 8.910541680181642e-06,
      "loss": 2.4566,
      "step": 1268300
    },
    {
      "epoch": 411.1507293354943,
      "grad_norm": 1.5390866994857788,
      "learning_rate": 8.907298086279599e-06,
      "loss": 2.4537,
      "step": 1268400
    },
    {
      "epoch": 411.1831442463533,
      "grad_norm": 1.2951151132583618,
      "learning_rate": 8.904054492377554e-06,
      "loss": 2.4646,
      "step": 1268500
    },
    {
      "epoch": 411.21555915721234,
      "grad_norm": 1.5530370473861694,
      "learning_rate": 8.900810898475511e-06,
      "loss": 2.4464,
      "step": 1268600
    },
    {
      "epoch": 411.2479740680713,
      "grad_norm": 1.4922418594360352,
      "learning_rate": 8.897567304573467e-06,
      "loss": 2.4595,
      "step": 1268700
    },
    {
      "epoch": 411.2803889789303,
      "grad_norm": 1.5188384056091309,
      "learning_rate": 8.894323710671424e-06,
      "loss": 2.4657,
      "step": 1268800
    },
    {
      "epoch": 411.3128038897893,
      "grad_norm": 1.3589961528778076,
      "learning_rate": 8.891080116769381e-06,
      "loss": 2.4702,
      "step": 1268900
    },
    {
      "epoch": 411.3452188006483,
      "grad_norm": 1.5290875434875488,
      "learning_rate": 8.887836522867338e-06,
      "loss": 2.4893,
      "step": 1269000
    },
    {
      "epoch": 411.3776337115073,
      "grad_norm": 1.5307202339172363,
      "learning_rate": 8.884592928965293e-06,
      "loss": 2.4677,
      "step": 1269100
    },
    {
      "epoch": 411.4100486223663,
      "grad_norm": 1.4169836044311523,
      "learning_rate": 8.88134933506325e-06,
      "loss": 2.4722,
      "step": 1269200
    },
    {
      "epoch": 411.4424635332253,
      "grad_norm": 1.494573950767517,
      "learning_rate": 8.878105741161206e-06,
      "loss": 2.4633,
      "step": 1269300
    },
    {
      "epoch": 411.47487844408425,
      "grad_norm": 1.461417555809021,
      "learning_rate": 8.874862147259163e-06,
      "loss": 2.4765,
      "step": 1269400
    },
    {
      "epoch": 411.5072933549433,
      "grad_norm": 1.5053473711013794,
      "learning_rate": 8.87161855335712e-06,
      "loss": 2.4744,
      "step": 1269500
    },
    {
      "epoch": 411.5397082658023,
      "grad_norm": 1.3412091732025146,
      "learning_rate": 8.868374959455077e-06,
      "loss": 2.4705,
      "step": 1269600
    },
    {
      "epoch": 411.57212317666125,
      "grad_norm": 1.3114447593688965,
      "learning_rate": 8.865131365553034e-06,
      "loss": 2.4722,
      "step": 1269700
    },
    {
      "epoch": 411.60453808752027,
      "grad_norm": 1.3355146646499634,
      "learning_rate": 8.86188777165099e-06,
      "loss": 2.4576,
      "step": 1269800
    },
    {
      "epoch": 411.63695299837923,
      "grad_norm": 1.3148396015167236,
      "learning_rate": 8.858644177748946e-06,
      "loss": 2.4808,
      "step": 1269900
    },
    {
      "epoch": 411.66936790923825,
      "grad_norm": 1.5439438819885254,
      "learning_rate": 8.855400583846903e-06,
      "loss": 2.4798,
      "step": 1270000
    },
    {
      "epoch": 411.70178282009726,
      "grad_norm": 1.4747105836868286,
      "learning_rate": 8.85215698994486e-06,
      "loss": 2.4857,
      "step": 1270100
    },
    {
      "epoch": 411.7341977309562,
      "grad_norm": 1.4197622537612915,
      "learning_rate": 8.848913396042816e-06,
      "loss": 2.4912,
      "step": 1270200
    },
    {
      "epoch": 411.76661264181524,
      "grad_norm": 1.436225175857544,
      "learning_rate": 8.845669802140773e-06,
      "loss": 2.4691,
      "step": 1270300
    },
    {
      "epoch": 411.7990275526742,
      "grad_norm": 1.3962626457214355,
      "learning_rate": 8.842426208238728e-06,
      "loss": 2.4762,
      "step": 1270400
    },
    {
      "epoch": 411.8314424635332,
      "grad_norm": 1.7697645425796509,
      "learning_rate": 8.839182614336685e-06,
      "loss": 2.4926,
      "step": 1270500
    },
    {
      "epoch": 411.86385737439224,
      "grad_norm": 1.516710638999939,
      "learning_rate": 8.835939020434642e-06,
      "loss": 2.4515,
      "step": 1270600
    },
    {
      "epoch": 411.8962722852512,
      "grad_norm": 1.6724333763122559,
      "learning_rate": 8.8326954265326e-06,
      "loss": 2.493,
      "step": 1270700
    },
    {
      "epoch": 411.9286871961102,
      "grad_norm": 1.5253854990005493,
      "learning_rate": 8.829451832630555e-06,
      "loss": 2.4893,
      "step": 1270800
    },
    {
      "epoch": 411.9611021069692,
      "grad_norm": 1.4005143642425537,
      "learning_rate": 8.826240674667533e-06,
      "loss": 2.4736,
      "step": 1270900
    },
    {
      "epoch": 411.9935170178282,
      "grad_norm": 1.4369679689407349,
      "learning_rate": 8.822997080765488e-06,
      "loss": 2.5071,
      "step": 1271000
    },
    {
      "epoch": 412.0,
      "eval_bleu": 1.0339115743976899,
      "eval_loss": 4.257806301116943,
      "eval_runtime": 4.3456,
      "eval_samples_per_second": 113.218,
      "eval_steps_per_second": 1.841,
      "step": 1271020
    },
    {
      "epoch": 412.0259319286872,
      "grad_norm": 1.5205512046813965,
      "learning_rate": 8.819753486863445e-06,
      "loss": 2.4753,
      "step": 1271100
    },
    {
      "epoch": 412.0583468395462,
      "grad_norm": 1.5315979719161987,
      "learning_rate": 8.816509892961402e-06,
      "loss": 2.4733,
      "step": 1271200
    },
    {
      "epoch": 412.0907617504052,
      "grad_norm": 1.7062464952468872,
      "learning_rate": 8.813266299059359e-06,
      "loss": 2.4436,
      "step": 1271300
    },
    {
      "epoch": 412.12317666126415,
      "grad_norm": 1.2734791040420532,
      "learning_rate": 8.810022705157314e-06,
      "loss": 2.4676,
      "step": 1271400
    },
    {
      "epoch": 412.15559157212317,
      "grad_norm": 1.414746880531311,
      "learning_rate": 8.806779111255271e-06,
      "loss": 2.4665,
      "step": 1271500
    },
    {
      "epoch": 412.1880064829822,
      "grad_norm": 1.318833589553833,
      "learning_rate": 8.803535517353227e-06,
      "loss": 2.4865,
      "step": 1271600
    },
    {
      "epoch": 412.22042139384115,
      "grad_norm": 1.5166537761688232,
      "learning_rate": 8.800291923451184e-06,
      "loss": 2.466,
      "step": 1271700
    },
    {
      "epoch": 412.25283630470017,
      "grad_norm": 1.2763214111328125,
      "learning_rate": 8.797048329549141e-06,
      "loss": 2.4983,
      "step": 1271800
    },
    {
      "epoch": 412.2852512155592,
      "grad_norm": 1.4257116317749023,
      "learning_rate": 8.793804735647098e-06,
      "loss": 2.4836,
      "step": 1271900
    },
    {
      "epoch": 412.31766612641815,
      "grad_norm": 1.3852946758270264,
      "learning_rate": 8.790561141745055e-06,
      "loss": 2.4647,
      "step": 1272000
    },
    {
      "epoch": 412.35008103727716,
      "grad_norm": 1.810970425605774,
      "learning_rate": 8.78731754784301e-06,
      "loss": 2.4859,
      "step": 1272100
    },
    {
      "epoch": 412.3824959481361,
      "grad_norm": 1.4083540439605713,
      "learning_rate": 8.784073953940967e-06,
      "loss": 2.5048,
      "step": 1272200
    },
    {
      "epoch": 412.41491085899514,
      "grad_norm": 1.275302529335022,
      "learning_rate": 8.780830360038923e-06,
      "loss": 2.4939,
      "step": 1272300
    },
    {
      "epoch": 412.44732576985416,
      "grad_norm": 1.6105866432189941,
      "learning_rate": 8.77758676613688e-06,
      "loss": 2.4655,
      "step": 1272400
    },
    {
      "epoch": 412.4797406807131,
      "grad_norm": 1.3672070503234863,
      "learning_rate": 8.774343172234837e-06,
      "loss": 2.4745,
      "step": 1272500
    },
    {
      "epoch": 412.51215559157214,
      "grad_norm": 1.4552242755889893,
      "learning_rate": 8.771099578332794e-06,
      "loss": 2.4889,
      "step": 1272600
    },
    {
      "epoch": 412.5445705024311,
      "grad_norm": 1.36043381690979,
      "learning_rate": 8.76785598443075e-06,
      "loss": 2.4747,
      "step": 1272700
    },
    {
      "epoch": 412.5769854132901,
      "grad_norm": 1.5318591594696045,
      "learning_rate": 8.764612390528706e-06,
      "loss": 2.4541,
      "step": 1272800
    },
    {
      "epoch": 412.60940032414914,
      "grad_norm": 1.5401045083999634,
      "learning_rate": 8.761368796626662e-06,
      "loss": 2.4649,
      "step": 1272900
    },
    {
      "epoch": 412.6418152350081,
      "grad_norm": 1.421111822128296,
      "learning_rate": 8.758125202724619e-06,
      "loss": 2.4784,
      "step": 1273000
    },
    {
      "epoch": 412.6742301458671,
      "grad_norm": 1.267102837562561,
      "learning_rate": 8.754881608822576e-06,
      "loss": 2.4775,
      "step": 1273100
    },
    {
      "epoch": 412.7066450567261,
      "grad_norm": 1.502402663230896,
      "learning_rate": 8.751638014920533e-06,
      "loss": 2.4753,
      "step": 1273200
    },
    {
      "epoch": 412.7390599675851,
      "grad_norm": 1.387263298034668,
      "learning_rate": 8.74839442101849e-06,
      "loss": 2.4501,
      "step": 1273300
    },
    {
      "epoch": 412.7714748784441,
      "grad_norm": 1.4814605712890625,
      "learning_rate": 8.745150827116445e-06,
      "loss": 2.4836,
      "step": 1273400
    },
    {
      "epoch": 412.80388978930307,
      "grad_norm": 1.3876709938049316,
      "learning_rate": 8.741939669153421e-06,
      "loss": 2.4784,
      "step": 1273500
    },
    {
      "epoch": 412.8363047001621,
      "grad_norm": 1.2577424049377441,
      "learning_rate": 8.738696075251378e-06,
      "loss": 2.4626,
      "step": 1273600
    },
    {
      "epoch": 412.86871961102105,
      "grad_norm": 1.3396344184875488,
      "learning_rate": 8.735452481349335e-06,
      "loss": 2.4625,
      "step": 1273700
    },
    {
      "epoch": 412.90113452188007,
      "grad_norm": 1.5368298292160034,
      "learning_rate": 8.732208887447292e-06,
      "loss": 2.4664,
      "step": 1273800
    },
    {
      "epoch": 412.9335494327391,
      "grad_norm": 1.4579355716705322,
      "learning_rate": 8.728965293545248e-06,
      "loss": 2.4774,
      "step": 1273900
    },
    {
      "epoch": 412.96596434359805,
      "grad_norm": 1.3800617456436157,
      "learning_rate": 8.725721699643205e-06,
      "loss": 2.459,
      "step": 1274000
    },
    {
      "epoch": 412.99837925445706,
      "grad_norm": 1.568021535873413,
      "learning_rate": 8.722478105741162e-06,
      "loss": 2.4655,
      "step": 1274100
    },
    {
      "epoch": 413.0,
      "eval_bleu": 1.043395788254884,
      "eval_loss": 4.26090145111084,
      "eval_runtime": 4.2194,
      "eval_samples_per_second": 116.604,
      "eval_steps_per_second": 1.896,
      "step": 1274105
    },
    {
      "epoch": 413.030794165316,
      "grad_norm": 1.6682021617889404,
      "learning_rate": 8.719234511839117e-06,
      "loss": 2.4782,
      "step": 1274200
    },
    {
      "epoch": 413.06320907617504,
      "grad_norm": 1.3391426801681519,
      "learning_rate": 8.715990917937076e-06,
      "loss": 2.476,
      "step": 1274300
    },
    {
      "epoch": 413.09562398703406,
      "grad_norm": 1.4654046297073364,
      "learning_rate": 8.712747324035031e-06,
      "loss": 2.44,
      "step": 1274400
    },
    {
      "epoch": 413.128038897893,
      "grad_norm": 1.4551095962524414,
      "learning_rate": 8.709503730132988e-06,
      "loss": 2.4625,
      "step": 1274500
    },
    {
      "epoch": 413.16045380875204,
      "grad_norm": 1.4437100887298584,
      "learning_rate": 8.706260136230944e-06,
      "loss": 2.457,
      "step": 1274600
    },
    {
      "epoch": 413.192868719611,
      "grad_norm": 1.4976223707199097,
      "learning_rate": 8.7030165423289e-06,
      "loss": 2.4846,
      "step": 1274700
    },
    {
      "epoch": 413.22528363047,
      "grad_norm": 1.4912853240966797,
      "learning_rate": 8.699772948426856e-06,
      "loss": 2.4627,
      "step": 1274800
    },
    {
      "epoch": 413.25769854132903,
      "grad_norm": 1.4410396814346313,
      "learning_rate": 8.696529354524815e-06,
      "loss": 2.4971,
      "step": 1274900
    },
    {
      "epoch": 413.290113452188,
      "grad_norm": 1.4150198698043823,
      "learning_rate": 8.69328576062277e-06,
      "loss": 2.4956,
      "step": 1275000
    },
    {
      "epoch": 413.322528363047,
      "grad_norm": 1.5487620830535889,
      "learning_rate": 8.690042166720727e-06,
      "loss": 2.4777,
      "step": 1275100
    },
    {
      "epoch": 413.354943273906,
      "grad_norm": 1.5153965950012207,
      "learning_rate": 8.686798572818683e-06,
      "loss": 2.4545,
      "step": 1275200
    },
    {
      "epoch": 413.387358184765,
      "grad_norm": 1.3875982761383057,
      "learning_rate": 8.68355497891664e-06,
      "loss": 2.5011,
      "step": 1275300
    },
    {
      "epoch": 413.419773095624,
      "grad_norm": 1.4624247550964355,
      "learning_rate": 8.680311385014595e-06,
      "loss": 2.4809,
      "step": 1275400
    },
    {
      "epoch": 413.45218800648297,
      "grad_norm": 1.4330323934555054,
      "learning_rate": 8.677067791112554e-06,
      "loss": 2.4846,
      "step": 1275500
    },
    {
      "epoch": 413.484602917342,
      "grad_norm": 1.3710265159606934,
      "learning_rate": 8.67382419721051e-06,
      "loss": 2.4729,
      "step": 1275600
    },
    {
      "epoch": 413.51701782820095,
      "grad_norm": 1.3440358638763428,
      "learning_rate": 8.670580603308466e-06,
      "loss": 2.4784,
      "step": 1275700
    },
    {
      "epoch": 413.54943273905997,
      "grad_norm": 1.3694195747375488,
      "learning_rate": 8.667337009406423e-06,
      "loss": 2.4831,
      "step": 1275800
    },
    {
      "epoch": 413.581847649919,
      "grad_norm": 1.3911365270614624,
      "learning_rate": 8.664093415504379e-06,
      "loss": 2.4689,
      "step": 1275900
    },
    {
      "epoch": 413.61426256077795,
      "grad_norm": 1.5909299850463867,
      "learning_rate": 8.660849821602336e-06,
      "loss": 2.4577,
      "step": 1276000
    },
    {
      "epoch": 413.64667747163696,
      "grad_norm": 1.3645756244659424,
      "learning_rate": 8.657606227700293e-06,
      "loss": 2.4597,
      "step": 1276100
    },
    {
      "epoch": 413.6790923824959,
      "grad_norm": 1.5729392766952515,
      "learning_rate": 8.65436263379825e-06,
      "loss": 2.4659,
      "step": 1276200
    },
    {
      "epoch": 413.71150729335494,
      "grad_norm": 1.288021445274353,
      "learning_rate": 8.651119039896205e-06,
      "loss": 2.4603,
      "step": 1276300
    },
    {
      "epoch": 413.74392220421396,
      "grad_norm": 1.6305538415908813,
      "learning_rate": 8.647875445994162e-06,
      "loss": 2.4616,
      "step": 1276400
    },
    {
      "epoch": 413.7763371150729,
      "grad_norm": 1.4763745069503784,
      "learning_rate": 8.644631852092118e-06,
      "loss": 2.4612,
      "step": 1276500
    },
    {
      "epoch": 413.80875202593194,
      "grad_norm": 1.4159597158432007,
      "learning_rate": 8.641388258190075e-06,
      "loss": 2.4627,
      "step": 1276600
    },
    {
      "epoch": 413.8411669367909,
      "grad_norm": 1.627378225326538,
      "learning_rate": 8.638144664288032e-06,
      "loss": 2.5,
      "step": 1276700
    },
    {
      "epoch": 413.8735818476499,
      "grad_norm": 1.3544901609420776,
      "learning_rate": 8.634901070385989e-06,
      "loss": 2.4713,
      "step": 1276800
    },
    {
      "epoch": 413.90599675850893,
      "grad_norm": 1.3534374237060547,
      "learning_rate": 8.631657476483946e-06,
      "loss": 2.4657,
      "step": 1276900
    },
    {
      "epoch": 413.9384116693679,
      "grad_norm": 1.466538667678833,
      "learning_rate": 8.628413882581901e-06,
      "loss": 2.4553,
      "step": 1277000
    },
    {
      "epoch": 413.9708265802269,
      "grad_norm": 1.4722360372543335,
      "learning_rate": 8.625170288679858e-06,
      "loss": 2.4586,
      "step": 1277100
    },
    {
      "epoch": 414.0,
      "eval_bleu": 1.051035881572498,
      "eval_loss": 4.262005805969238,
      "eval_runtime": 4.18,
      "eval_samples_per_second": 117.704,
      "eval_steps_per_second": 1.914,
      "step": 1277190
    },
    {
      "epoch": 414.0032414910859,
      "grad_norm": 1.4618924856185913,
      "learning_rate": 8.621926694777813e-06,
      "loss": 2.4765,
      "step": 1277200
    },
    {
      "epoch": 414.0356564019449,
      "grad_norm": 1.3832651376724243,
      "learning_rate": 8.61868310087577e-06,
      "loss": 2.464,
      "step": 1277300
    },
    {
      "epoch": 414.0680713128039,
      "grad_norm": 1.4035815000534058,
      "learning_rate": 8.615439506973728e-06,
      "loss": 2.4933,
      "step": 1277400
    },
    {
      "epoch": 414.10048622366287,
      "grad_norm": 1.6029890775680542,
      "learning_rate": 8.612228349010704e-06,
      "loss": 2.473,
      "step": 1277500
    },
    {
      "epoch": 414.1329011345219,
      "grad_norm": 1.344320297241211,
      "learning_rate": 8.60898475510866e-06,
      "loss": 2.459,
      "step": 1277600
    },
    {
      "epoch": 414.16531604538085,
      "grad_norm": 1.228041648864746,
      "learning_rate": 8.605741161206618e-06,
      "loss": 2.4671,
      "step": 1277700
    },
    {
      "epoch": 414.19773095623987,
      "grad_norm": 1.348909854888916,
      "learning_rate": 8.602497567304573e-06,
      "loss": 2.4665,
      "step": 1277800
    },
    {
      "epoch": 414.2301458670989,
      "grad_norm": 1.534288763999939,
      "learning_rate": 8.59925397340253e-06,
      "loss": 2.4745,
      "step": 1277900
    },
    {
      "epoch": 414.26256077795784,
      "grad_norm": 1.2599577903747559,
      "learning_rate": 8.596010379500487e-06,
      "loss": 2.4778,
      "step": 1278000
    },
    {
      "epoch": 414.29497568881686,
      "grad_norm": 1.7096301317214966,
      "learning_rate": 8.592766785598444e-06,
      "loss": 2.4698,
      "step": 1278100
    },
    {
      "epoch": 414.3273905996758,
      "grad_norm": 1.4799877405166626,
      "learning_rate": 8.5895231916964e-06,
      "loss": 2.4652,
      "step": 1278200
    },
    {
      "epoch": 414.35980551053484,
      "grad_norm": 1.266331434249878,
      "learning_rate": 8.586279597794357e-06,
      "loss": 2.4906,
      "step": 1278300
    },
    {
      "epoch": 414.39222042139386,
      "grad_norm": 1.2647614479064941,
      "learning_rate": 8.583036003892312e-06,
      "loss": 2.4678,
      "step": 1278400
    },
    {
      "epoch": 414.4246353322528,
      "grad_norm": 1.4552879333496094,
      "learning_rate": 8.579792409990269e-06,
      "loss": 2.4831,
      "step": 1278500
    },
    {
      "epoch": 414.45705024311184,
      "grad_norm": 1.6594280004501343,
      "learning_rate": 8.576548816088226e-06,
      "loss": 2.4657,
      "step": 1278600
    },
    {
      "epoch": 414.48946515397085,
      "grad_norm": 1.2628148794174194,
      "learning_rate": 8.573305222186183e-06,
      "loss": 2.4549,
      "step": 1278700
    },
    {
      "epoch": 414.5218800648298,
      "grad_norm": 1.5313847064971924,
      "learning_rate": 8.570061628284139e-06,
      "loss": 2.4621,
      "step": 1278800
    },
    {
      "epoch": 414.55429497568883,
      "grad_norm": 1.396820068359375,
      "learning_rate": 8.566818034382096e-06,
      "loss": 2.4596,
      "step": 1278900
    },
    {
      "epoch": 414.5867098865478,
      "grad_norm": 1.6455169916152954,
      "learning_rate": 8.563574440480051e-06,
      "loss": 2.451,
      "step": 1279000
    },
    {
      "epoch": 414.6191247974068,
      "grad_norm": 1.5530816316604614,
      "learning_rate": 8.56033084657801e-06,
      "loss": 2.4754,
      "step": 1279100
    },
    {
      "epoch": 414.65153970826583,
      "grad_norm": 1.3038781881332397,
      "learning_rate": 8.557087252675967e-06,
      "loss": 2.4725,
      "step": 1279200
    },
    {
      "epoch": 414.6839546191248,
      "grad_norm": 1.423634648323059,
      "learning_rate": 8.553843658773922e-06,
      "loss": 2.479,
      "step": 1279300
    },
    {
      "epoch": 414.7163695299838,
      "grad_norm": 1.3169538974761963,
      "learning_rate": 8.550600064871879e-06,
      "loss": 2.4907,
      "step": 1279400
    },
    {
      "epoch": 414.74878444084277,
      "grad_norm": 1.531488060951233,
      "learning_rate": 8.547356470969834e-06,
      "loss": 2.4588,
      "step": 1279500
    },
    {
      "epoch": 414.7811993517018,
      "grad_norm": 1.5083609819412231,
      "learning_rate": 8.54414531300681e-06,
      "loss": 2.4691,
      "step": 1279600
    },
    {
      "epoch": 414.8136142625608,
      "grad_norm": 1.4339083433151245,
      "learning_rate": 8.54090171910477e-06,
      "loss": 2.4663,
      "step": 1279700
    },
    {
      "epoch": 414.84602917341977,
      "grad_norm": 1.333050012588501,
      "learning_rate": 8.537658125202725e-06,
      "loss": 2.4769,
      "step": 1279800
    },
    {
      "epoch": 414.8784440842788,
      "grad_norm": 1.3666092157363892,
      "learning_rate": 8.534414531300682e-06,
      "loss": 2.4542,
      "step": 1279900
    },
    {
      "epoch": 414.91085899513774,
      "grad_norm": 1.6871658563613892,
      "learning_rate": 8.531170937398639e-06,
      "loss": 2.4944,
      "step": 1280000
    },
    {
      "epoch": 414.94327390599676,
      "grad_norm": 1.4417181015014648,
      "learning_rate": 8.527927343496594e-06,
      "loss": 2.4576,
      "step": 1280100
    },
    {
      "epoch": 414.9756888168558,
      "grad_norm": 1.3086496591567993,
      "learning_rate": 8.524683749594551e-06,
      "loss": 2.4715,
      "step": 1280200
    },
    {
      "epoch": 415.0,
      "eval_bleu": 0.9781003507683561,
      "eval_loss": 4.260599136352539,
      "eval_runtime": 4.3281,
      "eval_samples_per_second": 113.675,
      "eval_steps_per_second": 1.848,
      "step": 1280275
    },
    {
      "epoch": 415.00810372771474,
      "grad_norm": 1.2673274278640747,
      "learning_rate": 8.521440155692508e-06,
      "loss": 2.5011,
      "step": 1280300
    },
    {
      "epoch": 415.04051863857376,
      "grad_norm": 1.4243857860565186,
      "learning_rate": 8.518196561790465e-06,
      "loss": 2.4869,
      "step": 1280400
    },
    {
      "epoch": 415.0729335494327,
      "grad_norm": 1.6261577606201172,
      "learning_rate": 8.51495296788842e-06,
      "loss": 2.4702,
      "step": 1280500
    },
    {
      "epoch": 415.10534846029174,
      "grad_norm": 1.3791389465332031,
      "learning_rate": 8.511709373986378e-06,
      "loss": 2.4434,
      "step": 1280600
    },
    {
      "epoch": 415.13776337115075,
      "grad_norm": 1.6910065412521362,
      "learning_rate": 8.508465780084333e-06,
      "loss": 2.472,
      "step": 1280700
    },
    {
      "epoch": 415.1701782820097,
      "grad_norm": 1.4132815599441528,
      "learning_rate": 8.50522218618229e-06,
      "loss": 2.4699,
      "step": 1280800
    },
    {
      "epoch": 415.20259319286873,
      "grad_norm": 1.5244112014770508,
      "learning_rate": 8.501978592280247e-06,
      "loss": 2.4658,
      "step": 1280900
    },
    {
      "epoch": 415.2350081037277,
      "grad_norm": 1.392663836479187,
      "learning_rate": 8.498734998378204e-06,
      "loss": 2.4536,
      "step": 1281000
    },
    {
      "epoch": 415.2674230145867,
      "grad_norm": 1.367469310760498,
      "learning_rate": 8.49549140447616e-06,
      "loss": 2.4502,
      "step": 1281100
    },
    {
      "epoch": 415.29983792544573,
      "grad_norm": 1.45622980594635,
      "learning_rate": 8.492247810574117e-06,
      "loss": 2.444,
      "step": 1281200
    },
    {
      "epoch": 415.3322528363047,
      "grad_norm": 1.5744004249572754,
      "learning_rate": 8.489004216672074e-06,
      "loss": 2.4917,
      "step": 1281300
    },
    {
      "epoch": 415.3646677471637,
      "grad_norm": 1.2164503335952759,
      "learning_rate": 8.485760622770029e-06,
      "loss": 2.4909,
      "step": 1281400
    },
    {
      "epoch": 415.39708265802267,
      "grad_norm": 1.3123787641525269,
      "learning_rate": 8.482517028867986e-06,
      "loss": 2.4833,
      "step": 1281500
    },
    {
      "epoch": 415.4294975688817,
      "grad_norm": 1.3257982730865479,
      "learning_rate": 8.479273434965943e-06,
      "loss": 2.4742,
      "step": 1281600
    },
    {
      "epoch": 415.4619124797407,
      "grad_norm": 1.2784391641616821,
      "learning_rate": 8.4760298410639e-06,
      "loss": 2.4807,
      "step": 1281700
    },
    {
      "epoch": 415.49432739059966,
      "grad_norm": 1.3965095281600952,
      "learning_rate": 8.472786247161855e-06,
      "loss": 2.4909,
      "step": 1281800
    },
    {
      "epoch": 415.5267423014587,
      "grad_norm": 1.4878653287887573,
      "learning_rate": 8.469542653259813e-06,
      "loss": 2.4621,
      "step": 1281900
    },
    {
      "epoch": 415.55915721231764,
      "grad_norm": 1.4773529767990112,
      "learning_rate": 8.466299059357768e-06,
      "loss": 2.4877,
      "step": 1282000
    },
    {
      "epoch": 415.59157212317666,
      "grad_norm": 1.2796354293823242,
      "learning_rate": 8.463055465455725e-06,
      "loss": 2.477,
      "step": 1282100
    },
    {
      "epoch": 415.6239870340357,
      "grad_norm": 1.474394679069519,
      "learning_rate": 8.459811871553682e-06,
      "loss": 2.4728,
      "step": 1282200
    },
    {
      "epoch": 415.65640194489464,
      "grad_norm": 1.4685630798339844,
      "learning_rate": 8.456568277651639e-06,
      "loss": 2.4775,
      "step": 1282300
    },
    {
      "epoch": 415.68881685575366,
      "grad_norm": 1.5009592771530151,
      "learning_rate": 8.453357119688615e-06,
      "loss": 2.4962,
      "step": 1282400
    },
    {
      "epoch": 415.7212317666126,
      "grad_norm": 1.4372484683990479,
      "learning_rate": 8.450113525786572e-06,
      "loss": 2.4547,
      "step": 1282500
    },
    {
      "epoch": 415.75364667747164,
      "grad_norm": 1.3869051933288574,
      "learning_rate": 8.446869931884528e-06,
      "loss": 2.4774,
      "step": 1282600
    },
    {
      "epoch": 415.78606158833065,
      "grad_norm": 1.4869705438613892,
      "learning_rate": 8.443626337982485e-06,
      "loss": 2.4862,
      "step": 1282700
    },
    {
      "epoch": 415.8184764991896,
      "grad_norm": 1.4714937210083008,
      "learning_rate": 8.440382744080442e-06,
      "loss": 2.4534,
      "step": 1282800
    },
    {
      "epoch": 415.85089141004863,
      "grad_norm": 1.5875941514968872,
      "learning_rate": 8.437139150178399e-06,
      "loss": 2.4525,
      "step": 1282900
    },
    {
      "epoch": 415.8833063209076,
      "grad_norm": 1.4238392114639282,
      "learning_rate": 8.433895556276354e-06,
      "loss": 2.4524,
      "step": 1283000
    },
    {
      "epoch": 415.9157212317666,
      "grad_norm": 1.418162226676941,
      "learning_rate": 8.430651962374311e-06,
      "loss": 2.4722,
      "step": 1283100
    },
    {
      "epoch": 415.94813614262563,
      "grad_norm": 1.3209415674209595,
      "learning_rate": 8.427408368472266e-06,
      "loss": 2.4714,
      "step": 1283200
    },
    {
      "epoch": 415.9805510534846,
      "grad_norm": 1.4933147430419922,
      "learning_rate": 8.424164774570224e-06,
      "loss": 2.4919,
      "step": 1283300
    },
    {
      "epoch": 416.0,
      "eval_bleu": 0.9922592311111817,
      "eval_loss": 4.262331962585449,
      "eval_runtime": 4.3236,
      "eval_samples_per_second": 113.794,
      "eval_steps_per_second": 1.85,
      "step": 1283360
    },
    {
      "epoch": 416.0129659643436,
      "grad_norm": 1.234607219696045,
      "learning_rate": 8.42092118066818e-06,
      "loss": 2.4312,
      "step": 1283400
    },
    {
      "epoch": 416.04538087520257,
      "grad_norm": 1.372880220413208,
      "learning_rate": 8.417677586766138e-06,
      "loss": 2.4785,
      "step": 1283500
    },
    {
      "epoch": 416.0777957860616,
      "grad_norm": 1.3568700551986694,
      "learning_rate": 8.414433992864095e-06,
      "loss": 2.4747,
      "step": 1283600
    },
    {
      "epoch": 416.1102106969206,
      "grad_norm": 1.3556149005889893,
      "learning_rate": 8.41119039896205e-06,
      "loss": 2.4722,
      "step": 1283700
    },
    {
      "epoch": 416.14262560777956,
      "grad_norm": 1.4350279569625854,
      "learning_rate": 8.407946805060007e-06,
      "loss": 2.4518,
      "step": 1283800
    },
    {
      "epoch": 416.1750405186386,
      "grad_norm": 1.4476431608200073,
      "learning_rate": 8.404703211157962e-06,
      "loss": 2.4803,
      "step": 1283900
    },
    {
      "epoch": 416.20745542949754,
      "grad_norm": 1.4594908952713013,
      "learning_rate": 8.401459617255921e-06,
      "loss": 2.4677,
      "step": 1284000
    },
    {
      "epoch": 416.23987034035656,
      "grad_norm": 1.507811427116394,
      "learning_rate": 8.398216023353877e-06,
      "loss": 2.489,
      "step": 1284100
    },
    {
      "epoch": 416.2722852512156,
      "grad_norm": 1.5318114757537842,
      "learning_rate": 8.394972429451834e-06,
      "loss": 2.468,
      "step": 1284200
    },
    {
      "epoch": 416.30470016207454,
      "grad_norm": 1.293404221534729,
      "learning_rate": 8.391728835549789e-06,
      "loss": 2.4723,
      "step": 1284300
    },
    {
      "epoch": 416.33711507293356,
      "grad_norm": 1.610718846321106,
      "learning_rate": 8.388517677586767e-06,
      "loss": 2.4595,
      "step": 1284400
    },
    {
      "epoch": 416.3695299837925,
      "grad_norm": 1.2945629358291626,
      "learning_rate": 8.385274083684724e-06,
      "loss": 2.4805,
      "step": 1284500
    },
    {
      "epoch": 416.40194489465154,
      "grad_norm": 1.4525145292282104,
      "learning_rate": 8.38203048978268e-06,
      "loss": 2.4895,
      "step": 1284600
    },
    {
      "epoch": 416.43435980551055,
      "grad_norm": 1.6487159729003906,
      "learning_rate": 8.378786895880636e-06,
      "loss": 2.4763,
      "step": 1284700
    },
    {
      "epoch": 416.4667747163695,
      "grad_norm": 1.2312300205230713,
      "learning_rate": 8.375543301978593e-06,
      "loss": 2.4866,
      "step": 1284800
    },
    {
      "epoch": 416.49918962722853,
      "grad_norm": 1.394541621208191,
      "learning_rate": 8.372299708076549e-06,
      "loss": 2.4481,
      "step": 1284900
    },
    {
      "epoch": 416.5316045380875,
      "grad_norm": 1.3415024280548096,
      "learning_rate": 8.369056114174506e-06,
      "loss": 2.4772,
      "step": 1285000
    },
    {
      "epoch": 416.5640194489465,
      "grad_norm": 1.3923163414001465,
      "learning_rate": 8.365844956211483e-06,
      "loss": 2.4709,
      "step": 1285100
    },
    {
      "epoch": 416.5964343598055,
      "grad_norm": 1.4434605836868286,
      "learning_rate": 8.36260136230944e-06,
      "loss": 2.465,
      "step": 1285200
    },
    {
      "epoch": 416.6288492706645,
      "grad_norm": 1.7015167474746704,
      "learning_rate": 8.359357768407396e-06,
      "loss": 2.4752,
      "step": 1285300
    },
    {
      "epoch": 416.6612641815235,
      "grad_norm": 1.3023701906204224,
      "learning_rate": 8.356114174505353e-06,
      "loss": 2.4665,
      "step": 1285400
    },
    {
      "epoch": 416.6936790923825,
      "grad_norm": 1.4949764013290405,
      "learning_rate": 8.352870580603308e-06,
      "loss": 2.4859,
      "step": 1285500
    },
    {
      "epoch": 416.7260940032415,
      "grad_norm": 1.3974201679229736,
      "learning_rate": 8.349659422640286e-06,
      "loss": 2.4462,
      "step": 1285600
    },
    {
      "epoch": 416.7585089141005,
      "grad_norm": 1.349635124206543,
      "learning_rate": 8.346415828738243e-06,
      "loss": 2.4887,
      "step": 1285700
    },
    {
      "epoch": 416.79092382495946,
      "grad_norm": 1.2589420080184937,
      "learning_rate": 8.3431722348362e-06,
      "loss": 2.4745,
      "step": 1285800
    },
    {
      "epoch": 416.8233387358185,
      "grad_norm": 1.4273349046707153,
      "learning_rate": 8.339928640934155e-06,
      "loss": 2.4696,
      "step": 1285900
    },
    {
      "epoch": 416.8557536466775,
      "grad_norm": 1.3796651363372803,
      "learning_rate": 8.336685047032113e-06,
      "loss": 2.4684,
      "step": 1286000
    },
    {
      "epoch": 416.88816855753646,
      "grad_norm": 1.5295867919921875,
      "learning_rate": 8.333441453130068e-06,
      "loss": 2.4586,
      "step": 1286100
    },
    {
      "epoch": 416.9205834683955,
      "grad_norm": 1.4284354448318481,
      "learning_rate": 8.330197859228025e-06,
      "loss": 2.4692,
      "step": 1286200
    },
    {
      "epoch": 416.95299837925444,
      "grad_norm": 1.602041244506836,
      "learning_rate": 8.326954265325982e-06,
      "loss": 2.463,
      "step": 1286300
    },
    {
      "epoch": 416.98541329011346,
      "grad_norm": 1.796697735786438,
      "learning_rate": 8.323710671423939e-06,
      "loss": 2.465,
      "step": 1286400
    },
    {
      "epoch": 417.0,
      "eval_bleu": 0.9054527114949261,
      "eval_loss": 4.260032653808594,
      "eval_runtime": 4.5773,
      "eval_samples_per_second": 107.488,
      "eval_steps_per_second": 1.748,
      "step": 1286445
    },
    {
      "epoch": 417.0178282009725,
      "grad_norm": 1.40790855884552,
      "learning_rate": 8.320467077521894e-06,
      "loss": 2.4671,
      "step": 1286500
    },
    {
      "epoch": 417.05024311183143,
      "grad_norm": 1.4361575841903687,
      "learning_rate": 8.317223483619851e-06,
      "loss": 2.4743,
      "step": 1286600
    },
    {
      "epoch": 417.08265802269045,
      "grad_norm": 1.3220123052597046,
      "learning_rate": 8.313979889717807e-06,
      "loss": 2.4512,
      "step": 1286700
    },
    {
      "epoch": 417.1150729335494,
      "grad_norm": 1.3808766603469849,
      "learning_rate": 8.310736295815764e-06,
      "loss": 2.4699,
      "step": 1286800
    },
    {
      "epoch": 417.14748784440843,
      "grad_norm": 1.4066882133483887,
      "learning_rate": 8.307492701913721e-06,
      "loss": 2.4882,
      "step": 1286900
    },
    {
      "epoch": 417.17990275526745,
      "grad_norm": 1.3420588970184326,
      "learning_rate": 8.304249108011678e-06,
      "loss": 2.4761,
      "step": 1287000
    },
    {
      "epoch": 417.2123176661264,
      "grad_norm": 1.2182046175003052,
      "learning_rate": 8.301005514109633e-06,
      "loss": 2.4686,
      "step": 1287100
    },
    {
      "epoch": 417.2447325769854,
      "grad_norm": 1.5641809701919556,
      "learning_rate": 8.29776192020759e-06,
      "loss": 2.4687,
      "step": 1287200
    },
    {
      "epoch": 417.2771474878444,
      "grad_norm": 1.3010468482971191,
      "learning_rate": 8.294518326305547e-06,
      "loss": 2.4475,
      "step": 1287300
    },
    {
      "epoch": 417.3095623987034,
      "grad_norm": 1.501805305480957,
      "learning_rate": 8.291274732403503e-06,
      "loss": 2.4471,
      "step": 1287400
    },
    {
      "epoch": 417.3419773095624,
      "grad_norm": 1.3444348573684692,
      "learning_rate": 8.28803113850146e-06,
      "loss": 2.454,
      "step": 1287500
    },
    {
      "epoch": 417.3743922204214,
      "grad_norm": 1.4843361377716064,
      "learning_rate": 8.284787544599417e-06,
      "loss": 2.4662,
      "step": 1287600
    },
    {
      "epoch": 417.4068071312804,
      "grad_norm": 1.5973377227783203,
      "learning_rate": 8.281543950697374e-06,
      "loss": 2.4747,
      "step": 1287700
    },
    {
      "epoch": 417.43922204213936,
      "grad_norm": 1.5564723014831543,
      "learning_rate": 8.27830035679533e-06,
      "loss": 2.468,
      "step": 1287800
    },
    {
      "epoch": 417.4716369529984,
      "grad_norm": 1.4173475503921509,
      "learning_rate": 8.275056762893286e-06,
      "loss": 2.4532,
      "step": 1287900
    },
    {
      "epoch": 417.5040518638574,
      "grad_norm": 1.4389705657958984,
      "learning_rate": 8.271813168991242e-06,
      "loss": 2.4538,
      "step": 1288000
    },
    {
      "epoch": 417.53646677471636,
      "grad_norm": 1.3751986026763916,
      "learning_rate": 8.268569575089199e-06,
      "loss": 2.4671,
      "step": 1288100
    },
    {
      "epoch": 417.5688816855754,
      "grad_norm": 1.3578680753707886,
      "learning_rate": 8.265325981187156e-06,
      "loss": 2.476,
      "step": 1288200
    },
    {
      "epoch": 417.60129659643434,
      "grad_norm": 1.487407922744751,
      "learning_rate": 8.262082387285113e-06,
      "loss": 2.468,
      "step": 1288300
    },
    {
      "epoch": 417.63371150729336,
      "grad_norm": 1.3747758865356445,
      "learning_rate": 8.258838793383068e-06,
      "loss": 2.4578,
      "step": 1288400
    },
    {
      "epoch": 417.6661264181524,
      "grad_norm": 1.276803731918335,
      "learning_rate": 8.255627635420046e-06,
      "loss": 2.4718,
      "step": 1288500
    },
    {
      "epoch": 417.69854132901133,
      "grad_norm": 1.382780909538269,
      "learning_rate": 8.252384041518001e-06,
      "loss": 2.4837,
      "step": 1288600
    },
    {
      "epoch": 417.73095623987035,
      "grad_norm": 1.3793648481369019,
      "learning_rate": 8.24914044761596e-06,
      "loss": 2.4828,
      "step": 1288700
    },
    {
      "epoch": 417.7633711507293,
      "grad_norm": 1.482279896736145,
      "learning_rate": 8.245896853713915e-06,
      "loss": 2.476,
      "step": 1288800
    },
    {
      "epoch": 417.79578606158833,
      "grad_norm": 1.4261624813079834,
      "learning_rate": 8.242653259811872e-06,
      "loss": 2.4812,
      "step": 1288900
    },
    {
      "epoch": 417.82820097244735,
      "grad_norm": 1.3364765644073486,
      "learning_rate": 8.239409665909828e-06,
      "loss": 2.4745,
      "step": 1289000
    },
    {
      "epoch": 417.8606158833063,
      "grad_norm": 1.3126460313796997,
      "learning_rate": 8.236166072007785e-06,
      "loss": 2.4808,
      "step": 1289100
    },
    {
      "epoch": 417.8930307941653,
      "grad_norm": 1.4418259859085083,
      "learning_rate": 8.23292247810574e-06,
      "loss": 2.4872,
      "step": 1289200
    },
    {
      "epoch": 417.9254457050243,
      "grad_norm": 1.483329176902771,
      "learning_rate": 8.229678884203699e-06,
      "loss": 2.4532,
      "step": 1289300
    },
    {
      "epoch": 417.9578606158833,
      "grad_norm": 1.3608518838882446,
      "learning_rate": 8.226435290301654e-06,
      "loss": 2.4731,
      "step": 1289400
    },
    {
      "epoch": 417.9902755267423,
      "grad_norm": 1.549669861793518,
      "learning_rate": 8.223191696399611e-06,
      "loss": 2.4878,
      "step": 1289500
    },
    {
      "epoch": 418.0,
      "eval_bleu": 1.0380198176297688,
      "eval_loss": 4.260037422180176,
      "eval_runtime": 4.4989,
      "eval_samples_per_second": 109.359,
      "eval_steps_per_second": 1.778,
      "step": 1289530
    },
    {
      "epoch": 418.0226904376013,
      "grad_norm": 1.3561490774154663,
      "learning_rate": 8.219948102497568e-06,
      "loss": 2.4576,
      "step": 1289600
    },
    {
      "epoch": 418.0551053484603,
      "grad_norm": 1.384752631187439,
      "learning_rate": 8.216704508595524e-06,
      "loss": 2.4626,
      "step": 1289700
    },
    {
      "epoch": 418.08752025931926,
      "grad_norm": 1.3568284511566162,
      "learning_rate": 8.21346091469348e-06,
      "loss": 2.4937,
      "step": 1289800
    },
    {
      "epoch": 418.1199351701783,
      "grad_norm": 1.4591001272201538,
      "learning_rate": 8.210217320791438e-06,
      "loss": 2.475,
      "step": 1289900
    },
    {
      "epoch": 418.1523500810373,
      "grad_norm": 1.3307856321334839,
      "learning_rate": 8.206973726889395e-06,
      "loss": 2.4519,
      "step": 1290000
    },
    {
      "epoch": 418.18476499189626,
      "grad_norm": 1.424622654914856,
      "learning_rate": 8.20373013298735e-06,
      "loss": 2.4738,
      "step": 1290100
    },
    {
      "epoch": 418.2171799027553,
      "grad_norm": 1.5910290479660034,
      "learning_rate": 8.200486539085307e-06,
      "loss": 2.4864,
      "step": 1290200
    },
    {
      "epoch": 418.24959481361424,
      "grad_norm": 1.8609663248062134,
      "learning_rate": 8.197242945183263e-06,
      "loss": 2.4472,
      "step": 1290300
    },
    {
      "epoch": 418.28200972447326,
      "grad_norm": 1.4959418773651123,
      "learning_rate": 8.19399935128122e-06,
      "loss": 2.4753,
      "step": 1290400
    },
    {
      "epoch": 418.3144246353323,
      "grad_norm": 1.5303313732147217,
      "learning_rate": 8.190755757379177e-06,
      "loss": 2.4525,
      "step": 1290500
    },
    {
      "epoch": 418.34683954619123,
      "grad_norm": 1.3963044881820679,
      "learning_rate": 8.187512163477134e-06,
      "loss": 2.4408,
      "step": 1290600
    },
    {
      "epoch": 418.37925445705025,
      "grad_norm": 1.5375205278396606,
      "learning_rate": 8.18426856957509e-06,
      "loss": 2.4538,
      "step": 1290700
    },
    {
      "epoch": 418.4116693679092,
      "grad_norm": 1.4267594814300537,
      "learning_rate": 8.181024975673046e-06,
      "loss": 2.4733,
      "step": 1290800
    },
    {
      "epoch": 418.44408427876823,
      "grad_norm": 1.3865067958831787,
      "learning_rate": 8.177781381771003e-06,
      "loss": 2.4806,
      "step": 1290900
    },
    {
      "epoch": 418.47649918962725,
      "grad_norm": 1.3432050943374634,
      "learning_rate": 8.174537787868959e-06,
      "loss": 2.4864,
      "step": 1291000
    },
    {
      "epoch": 418.5089141004862,
      "grad_norm": 1.4379069805145264,
      "learning_rate": 8.171294193966916e-06,
      "loss": 2.4706,
      "step": 1291100
    },
    {
      "epoch": 418.5413290113452,
      "grad_norm": 1.3605711460113525,
      "learning_rate": 8.168050600064873e-06,
      "loss": 2.4619,
      "step": 1291200
    },
    {
      "epoch": 418.5737439222042,
      "grad_norm": 1.4163771867752075,
      "learning_rate": 8.16480700616283e-06,
      "loss": 2.4833,
      "step": 1291300
    },
    {
      "epoch": 418.6061588330632,
      "grad_norm": 1.4412351846694946,
      "learning_rate": 8.161563412260785e-06,
      "loss": 2.4845,
      "step": 1291400
    },
    {
      "epoch": 418.6385737439222,
      "grad_norm": 1.4150646924972534,
      "learning_rate": 8.158319818358742e-06,
      "loss": 2.4704,
      "step": 1291500
    },
    {
      "epoch": 418.6709886547812,
      "grad_norm": 1.424463152885437,
      "learning_rate": 8.155076224456698e-06,
      "loss": 2.4697,
      "step": 1291600
    },
    {
      "epoch": 418.7034035656402,
      "grad_norm": 1.5664105415344238,
      "learning_rate": 8.151832630554655e-06,
      "loss": 2.4894,
      "step": 1291700
    },
    {
      "epoch": 418.73581847649916,
      "grad_norm": 1.2256337404251099,
      "learning_rate": 8.148589036652612e-06,
      "loss": 2.4619,
      "step": 1291800
    },
    {
      "epoch": 418.7682333873582,
      "grad_norm": 1.3810280561447144,
      "learning_rate": 8.145345442750569e-06,
      "loss": 2.4841,
      "step": 1291900
    },
    {
      "epoch": 418.8006482982172,
      "grad_norm": 1.4426264762878418,
      "learning_rate": 8.142101848848524e-06,
      "loss": 2.4793,
      "step": 1292000
    },
    {
      "epoch": 418.83306320907616,
      "grad_norm": 1.4054096937179565,
      "learning_rate": 8.138858254946481e-06,
      "loss": 2.4517,
      "step": 1292100
    },
    {
      "epoch": 418.8654781199352,
      "grad_norm": 1.3659967184066772,
      "learning_rate": 8.135614661044436e-06,
      "loss": 2.478,
      "step": 1292200
    },
    {
      "epoch": 418.8978930307942,
      "grad_norm": 1.3148303031921387,
      "learning_rate": 8.132371067142393e-06,
      "loss": 2.4854,
      "step": 1292300
    },
    {
      "epoch": 418.93030794165315,
      "grad_norm": 1.337517499923706,
      "learning_rate": 8.12912747324035e-06,
      "loss": 2.4681,
      "step": 1292400
    },
    {
      "epoch": 418.9627228525122,
      "grad_norm": 1.4106054306030273,
      "learning_rate": 8.125883879338308e-06,
      "loss": 2.4746,
      "step": 1292500
    },
    {
      "epoch": 418.99513776337113,
      "grad_norm": 1.575177788734436,
      "learning_rate": 8.122640285436265e-06,
      "loss": 2.4504,
      "step": 1292600
    },
    {
      "epoch": 419.0,
      "eval_bleu": 1.0162783636290835,
      "eval_loss": 4.264230728149414,
      "eval_runtime": 4.3546,
      "eval_samples_per_second": 112.985,
      "eval_steps_per_second": 1.837,
      "step": 1292615
    },
    {
      "epoch": 419.02755267423015,
      "grad_norm": 1.5258533954620361,
      "learning_rate": 8.11939669153422e-06,
      "loss": 2.467,
      "step": 1292700
    },
    {
      "epoch": 419.05996758508917,
      "grad_norm": 1.4305521249771118,
      "learning_rate": 8.116153097632177e-06,
      "loss": 2.4673,
      "step": 1292800
    },
    {
      "epoch": 419.09238249594813,
      "grad_norm": 1.433398962020874,
      "learning_rate": 8.112909503730132e-06,
      "loss": 2.4643,
      "step": 1292900
    },
    {
      "epoch": 419.12479740680715,
      "grad_norm": 1.4895775318145752,
      "learning_rate": 8.109665909828091e-06,
      "loss": 2.4606,
      "step": 1293000
    },
    {
      "epoch": 419.1572123176661,
      "grad_norm": 1.403212308883667,
      "learning_rate": 8.106422315926046e-06,
      "loss": 2.4704,
      "step": 1293100
    },
    {
      "epoch": 419.1896272285251,
      "grad_norm": 1.4202595949172974,
      "learning_rate": 8.103178722024004e-06,
      "loss": 2.4695,
      "step": 1293200
    },
    {
      "epoch": 419.22204213938414,
      "grad_norm": 1.4558902978897095,
      "learning_rate": 8.099935128121959e-06,
      "loss": 2.4676,
      "step": 1293300
    },
    {
      "epoch": 419.2544570502431,
      "grad_norm": 1.4861928224563599,
      "learning_rate": 8.096691534219916e-06,
      "loss": 2.4671,
      "step": 1293400
    },
    {
      "epoch": 419.2868719611021,
      "grad_norm": 1.411463975906372,
      "learning_rate": 8.093447940317871e-06,
      "loss": 2.4812,
      "step": 1293500
    },
    {
      "epoch": 419.3192868719611,
      "grad_norm": 1.4494783878326416,
      "learning_rate": 8.09020434641583e-06,
      "loss": 2.4878,
      "step": 1293600
    },
    {
      "epoch": 419.3517017828201,
      "grad_norm": 1.5472593307495117,
      "learning_rate": 8.086960752513785e-06,
      "loss": 2.4721,
      "step": 1293700
    },
    {
      "epoch": 419.3841166936791,
      "grad_norm": 1.3840279579162598,
      "learning_rate": 8.083717158611742e-06,
      "loss": 2.4671,
      "step": 1293800
    },
    {
      "epoch": 419.4165316045381,
      "grad_norm": 1.385667085647583,
      "learning_rate": 8.0804735647097e-06,
      "loss": 2.4889,
      "step": 1293900
    },
    {
      "epoch": 419.4489465153971,
      "grad_norm": 1.2787842750549316,
      "learning_rate": 8.077229970807655e-06,
      "loss": 2.4615,
      "step": 1294000
    },
    {
      "epoch": 419.48136142625606,
      "grad_norm": 1.268447756767273,
      "learning_rate": 8.073986376905612e-06,
      "loss": 2.4791,
      "step": 1294100
    },
    {
      "epoch": 419.5137763371151,
      "grad_norm": 1.3680121898651123,
      "learning_rate": 8.070742783003569e-06,
      "loss": 2.4891,
      "step": 1294200
    },
    {
      "epoch": 419.5461912479741,
      "grad_norm": 1.4639971256256104,
      "learning_rate": 8.067499189101526e-06,
      "loss": 2.4668,
      "step": 1294300
    },
    {
      "epoch": 419.57860615883305,
      "grad_norm": 1.5263358354568481,
      "learning_rate": 8.064255595199481e-06,
      "loss": 2.486,
      "step": 1294400
    },
    {
      "epoch": 419.6110210696921,
      "grad_norm": 1.3764369487762451,
      "learning_rate": 8.061012001297438e-06,
      "loss": 2.4765,
      "step": 1294500
    },
    {
      "epoch": 419.64343598055103,
      "grad_norm": 1.4475468397140503,
      "learning_rate": 8.057768407395394e-06,
      "loss": 2.475,
      "step": 1294600
    },
    {
      "epoch": 419.67585089141005,
      "grad_norm": 1.4867082834243774,
      "learning_rate": 8.05452481349335e-06,
      "loss": 2.4736,
      "step": 1294700
    },
    {
      "epoch": 419.70826580226907,
      "grad_norm": 1.3725948333740234,
      "learning_rate": 8.051281219591308e-06,
      "loss": 2.4679,
      "step": 1294800
    },
    {
      "epoch": 419.74068071312803,
      "grad_norm": 1.291883111000061,
      "learning_rate": 8.048037625689265e-06,
      "loss": 2.4537,
      "step": 1294900
    },
    {
      "epoch": 419.77309562398705,
      "grad_norm": 1.473703145980835,
      "learning_rate": 8.04479403178722e-06,
      "loss": 2.462,
      "step": 1295000
    },
    {
      "epoch": 419.805510534846,
      "grad_norm": 1.4398530721664429,
      "learning_rate": 8.041550437885177e-06,
      "loss": 2.4573,
      "step": 1295100
    },
    {
      "epoch": 419.837925445705,
      "grad_norm": 1.3391135931015015,
      "learning_rate": 8.038306843983134e-06,
      "loss": 2.4616,
      "step": 1295200
    },
    {
      "epoch": 419.87034035656404,
      "grad_norm": 1.4179580211639404,
      "learning_rate": 8.03506325008109e-06,
      "loss": 2.4572,
      "step": 1295300
    },
    {
      "epoch": 419.902755267423,
      "grad_norm": 1.4088268280029297,
      "learning_rate": 8.031819656179047e-06,
      "loss": 2.4778,
      "step": 1295400
    },
    {
      "epoch": 419.935170178282,
      "grad_norm": 1.332491397857666,
      "learning_rate": 8.028576062277004e-06,
      "loss": 2.4489,
      "step": 1295500
    },
    {
      "epoch": 419.967585089141,
      "grad_norm": 1.3860210180282593,
      "learning_rate": 8.02533246837496e-06,
      "loss": 2.4689,
      "step": 1295600
    },
    {
      "epoch": 420.0,
      "grad_norm": 1.4764924049377441,
      "learning_rate": 8.022088874472916e-06,
      "loss": 2.4754,
      "step": 1295700
    },
    {
      "epoch": 420.0,
      "eval_bleu": 1.2010425455654135,
      "eval_loss": 4.2596516609191895,
      "eval_runtime": 4.4087,
      "eval_samples_per_second": 111.597,
      "eval_steps_per_second": 1.815,
      "step": 1295700
    },
    {
      "epoch": 420.032414910859,
      "grad_norm": 1.338868260383606,
      "learning_rate": 8.018845280570873e-06,
      "loss": 2.4656,
      "step": 1295800
    },
    {
      "epoch": 420.064829821718,
      "grad_norm": 1.5763896703720093,
      "learning_rate": 8.015601686668829e-06,
      "loss": 2.4375,
      "step": 1295900
    },
    {
      "epoch": 420.097244732577,
      "grad_norm": 1.4981416463851929,
      "learning_rate": 8.012358092766786e-06,
      "loss": 2.4718,
      "step": 1296000
    },
    {
      "epoch": 420.12965964343596,
      "grad_norm": 1.475157380104065,
      "learning_rate": 8.009114498864743e-06,
      "loss": 2.4632,
      "step": 1296100
    },
    {
      "epoch": 420.162074554295,
      "grad_norm": 1.3013298511505127,
      "learning_rate": 8.00590334090172e-06,
      "loss": 2.4739,
      "step": 1296200
    },
    {
      "epoch": 420.194489465154,
      "grad_norm": 1.487359881401062,
      "learning_rate": 8.002659746999676e-06,
      "loss": 2.4631,
      "step": 1296300
    },
    {
      "epoch": 420.22690437601295,
      "grad_norm": 1.336944341659546,
      "learning_rate": 7.999416153097633e-06,
      "loss": 2.4634,
      "step": 1296400
    },
    {
      "epoch": 420.25931928687197,
      "grad_norm": 1.4932401180267334,
      "learning_rate": 7.996172559195588e-06,
      "loss": 2.4847,
      "step": 1296500
    },
    {
      "epoch": 420.29173419773093,
      "grad_norm": 1.3819767236709595,
      "learning_rate": 7.992928965293545e-06,
      "loss": 2.4749,
      "step": 1296600
    },
    {
      "epoch": 420.32414910858995,
      "grad_norm": 1.4100691080093384,
      "learning_rate": 7.989685371391502e-06,
      "loss": 2.4706,
      "step": 1296700
    },
    {
      "epoch": 420.35656401944897,
      "grad_norm": 1.518831491470337,
      "learning_rate": 7.98644177748946e-06,
      "loss": 2.482,
      "step": 1296800
    },
    {
      "epoch": 420.3889789303079,
      "grad_norm": 1.6381064653396606,
      "learning_rate": 7.983198183587415e-06,
      "loss": 2.4443,
      "step": 1296900
    },
    {
      "epoch": 420.42139384116695,
      "grad_norm": 1.4648468494415283,
      "learning_rate": 7.979954589685372e-06,
      "loss": 2.462,
      "step": 1297000
    },
    {
      "epoch": 420.4538087520259,
      "grad_norm": 1.5103217363357544,
      "learning_rate": 7.976710995783327e-06,
      "loss": 2.461,
      "step": 1297100
    },
    {
      "epoch": 420.4862236628849,
      "grad_norm": 1.53091299533844,
      "learning_rate": 7.973467401881284e-06,
      "loss": 2.457,
      "step": 1297200
    },
    {
      "epoch": 420.51863857374394,
      "grad_norm": 1.6318116188049316,
      "learning_rate": 7.970223807979241e-06,
      "loss": 2.4854,
      "step": 1297300
    },
    {
      "epoch": 420.5510534846029,
      "grad_norm": 1.4028146266937256,
      "learning_rate": 7.966980214077198e-06,
      "loss": 2.4825,
      "step": 1297400
    },
    {
      "epoch": 420.5834683954619,
      "grad_norm": 1.4324536323547363,
      "learning_rate": 7.963736620175155e-06,
      "loss": 2.5068,
      "step": 1297500
    },
    {
      "epoch": 420.6158833063209,
      "grad_norm": 1.4821820259094238,
      "learning_rate": 7.96049302627311e-06,
      "loss": 2.4533,
      "step": 1297600
    },
    {
      "epoch": 420.6482982171799,
      "grad_norm": 1.4766085147857666,
      "learning_rate": 7.957249432371068e-06,
      "loss": 2.4794,
      "step": 1297700
    },
    {
      "epoch": 420.6807131280389,
      "grad_norm": 1.4606657028198242,
      "learning_rate": 7.954005838469023e-06,
      "loss": 2.4598,
      "step": 1297800
    },
    {
      "epoch": 420.7131280388979,
      "grad_norm": 1.2274078130722046,
      "learning_rate": 7.950762244566982e-06,
      "loss": 2.4693,
      "step": 1297900
    },
    {
      "epoch": 420.7455429497569,
      "grad_norm": 1.4779115915298462,
      "learning_rate": 7.947518650664937e-06,
      "loss": 2.468,
      "step": 1298000
    },
    {
      "epoch": 420.77795786061586,
      "grad_norm": 1.6069401502609253,
      "learning_rate": 7.944275056762894e-06,
      "loss": 2.4834,
      "step": 1298100
    },
    {
      "epoch": 420.8103727714749,
      "grad_norm": 1.4675012826919556,
      "learning_rate": 7.94103146286085e-06,
      "loss": 2.4626,
      "step": 1298200
    },
    {
      "epoch": 420.8427876823339,
      "grad_norm": 1.2885451316833496,
      "learning_rate": 7.937787868958807e-06,
      "loss": 2.4797,
      "step": 1298300
    },
    {
      "epoch": 420.87520259319285,
      "grad_norm": 1.404841661453247,
      "learning_rate": 7.934544275056762e-06,
      "loss": 2.4555,
      "step": 1298400
    },
    {
      "epoch": 420.90761750405187,
      "grad_norm": 1.5118626356124878,
      "learning_rate": 7.93130068115472e-06,
      "loss": 2.4914,
      "step": 1298500
    },
    {
      "epoch": 420.94003241491083,
      "grad_norm": 1.3564271926879883,
      "learning_rate": 7.928057087252676e-06,
      "loss": 2.4641,
      "step": 1298600
    },
    {
      "epoch": 420.97244732576985,
      "grad_norm": 1.363691806793213,
      "learning_rate": 7.924813493350633e-06,
      "loss": 2.47,
      "step": 1298700
    },
    {
      "epoch": 421.0,
      "eval_bleu": 1.0195373732951598,
      "eval_loss": 4.261353492736816,
      "eval_runtime": 4.6536,
      "eval_samples_per_second": 105.724,
      "eval_steps_per_second": 1.719,
      "step": 1298785
    },
    {
      "epoch": 421.00486223662887,
      "grad_norm": 1.8393468856811523,
      "learning_rate": 7.921569899448589e-06,
      "loss": 2.4506,
      "step": 1298800
    },
    {
      "epoch": 421.0372771474878,
      "grad_norm": 1.467774510383606,
      "learning_rate": 7.918326305546546e-06,
      "loss": 2.4617,
      "step": 1298900
    },
    {
      "epoch": 421.06969205834685,
      "grad_norm": 1.3992283344268799,
      "learning_rate": 7.915082711644503e-06,
      "loss": 2.4771,
      "step": 1299000
    },
    {
      "epoch": 421.1021069692058,
      "grad_norm": 1.6014783382415771,
      "learning_rate": 7.91183911774246e-06,
      "loss": 2.4704,
      "step": 1299100
    },
    {
      "epoch": 421.1345218800648,
      "grad_norm": 1.5122296810150146,
      "learning_rate": 7.908595523840417e-06,
      "loss": 2.4595,
      "step": 1299200
    },
    {
      "epoch": 421.16693679092384,
      "grad_norm": 1.3567070960998535,
      "learning_rate": 7.905351929938372e-06,
      "loss": 2.4584,
      "step": 1299300
    },
    {
      "epoch": 421.1993517017828,
      "grad_norm": 1.463221549987793,
      "learning_rate": 7.902108336036329e-06,
      "loss": 2.4966,
      "step": 1299400
    },
    {
      "epoch": 421.2317666126418,
      "grad_norm": 1.599306583404541,
      "learning_rate": 7.898864742134284e-06,
      "loss": 2.4835,
      "step": 1299500
    },
    {
      "epoch": 421.26418152350084,
      "grad_norm": 1.4686466455459595,
      "learning_rate": 7.895621148232242e-06,
      "loss": 2.4738,
      "step": 1299600
    },
    {
      "epoch": 421.2965964343598,
      "grad_norm": 1.499961018562317,
      "learning_rate": 7.892377554330199e-06,
      "loss": 2.4691,
      "step": 1299700
    },
    {
      "epoch": 421.3290113452188,
      "grad_norm": 1.7004199028015137,
      "learning_rate": 7.889133960428156e-06,
      "loss": 2.4732,
      "step": 1299800
    },
    {
      "epoch": 421.3614262560778,
      "grad_norm": 1.282275676727295,
      "learning_rate": 7.885890366526111e-06,
      "loss": 2.4643,
      "step": 1299900
    },
    {
      "epoch": 421.3938411669368,
      "grad_norm": 1.4779256582260132,
      "learning_rate": 7.882646772624068e-06,
      "loss": 2.4609,
      "step": 1300000
    },
    {
      "epoch": 421.4262560777958,
      "grad_norm": 1.3412524461746216,
      "learning_rate": 7.879403178722023e-06,
      "loss": 2.4656,
      "step": 1300100
    },
    {
      "epoch": 421.4586709886548,
      "grad_norm": 1.5596297979354858,
      "learning_rate": 7.876192020759001e-06,
      "loss": 2.4679,
      "step": 1300200
    },
    {
      "epoch": 421.4910858995138,
      "grad_norm": 1.3308426141738892,
      "learning_rate": 7.872948426856958e-06,
      "loss": 2.4784,
      "step": 1300300
    },
    {
      "epoch": 421.52350081037275,
      "grad_norm": 1.4355661869049072,
      "learning_rate": 7.869704832954915e-06,
      "loss": 2.4668,
      "step": 1300400
    },
    {
      "epoch": 421.55591572123177,
      "grad_norm": 1.6256581544876099,
      "learning_rate": 7.86646123905287e-06,
      "loss": 2.4669,
      "step": 1300500
    },
    {
      "epoch": 421.5883306320908,
      "grad_norm": 1.314236044883728,
      "learning_rate": 7.863217645150828e-06,
      "loss": 2.4617,
      "step": 1300600
    },
    {
      "epoch": 421.62074554294975,
      "grad_norm": 1.412975549697876,
      "learning_rate": 7.859974051248783e-06,
      "loss": 2.466,
      "step": 1300700
    },
    {
      "epoch": 421.65316045380877,
      "grad_norm": 1.4899672269821167,
      "learning_rate": 7.85673045734674e-06,
      "loss": 2.4773,
      "step": 1300800
    },
    {
      "epoch": 421.6855753646677,
      "grad_norm": 1.5560686588287354,
      "learning_rate": 7.853486863444697e-06,
      "loss": 2.4644,
      "step": 1300900
    },
    {
      "epoch": 421.71799027552674,
      "grad_norm": 1.3518420457839966,
      "learning_rate": 7.850243269542654e-06,
      "loss": 2.4343,
      "step": 1301000
    },
    {
      "epoch": 421.75040518638576,
      "grad_norm": 1.317474365234375,
      "learning_rate": 7.846999675640611e-06,
      "loss": 2.445,
      "step": 1301100
    },
    {
      "epoch": 421.7828200972447,
      "grad_norm": 1.2878237962722778,
      "learning_rate": 7.843756081738567e-06,
      "loss": 2.472,
      "step": 1301200
    },
    {
      "epoch": 421.81523500810374,
      "grad_norm": 1.424438238143921,
      "learning_rate": 7.840512487836524e-06,
      "loss": 2.4838,
      "step": 1301300
    },
    {
      "epoch": 421.8476499189627,
      "grad_norm": 1.5443248748779297,
      "learning_rate": 7.837268893934479e-06,
      "loss": 2.4606,
      "step": 1301400
    },
    {
      "epoch": 421.8800648298217,
      "grad_norm": 1.356279730796814,
      "learning_rate": 7.834025300032436e-06,
      "loss": 2.4843,
      "step": 1301500
    },
    {
      "epoch": 421.91247974068074,
      "grad_norm": 1.3767905235290527,
      "learning_rate": 7.830781706130393e-06,
      "loss": 2.4823,
      "step": 1301600
    },
    {
      "epoch": 421.9448946515397,
      "grad_norm": 1.5781023502349854,
      "learning_rate": 7.82753811222835e-06,
      "loss": 2.4809,
      "step": 1301700
    },
    {
      "epoch": 421.9773095623987,
      "grad_norm": 1.5796406269073486,
      "learning_rate": 7.824294518326305e-06,
      "loss": 2.4533,
      "step": 1301800
    },
    {
      "epoch": 422.0,
      "eval_bleu": 1.238225595976101,
      "eval_loss": 4.2673492431640625,
      "eval_runtime": 4.2354,
      "eval_samples_per_second": 116.163,
      "eval_steps_per_second": 1.889,
      "step": 1301870
    },
    {
      "epoch": 422.0097244732577,
      "grad_norm": 1.277094841003418,
      "learning_rate": 7.821050924424263e-06,
      "loss": 2.4728,
      "step": 1301900
    },
    {
      "epoch": 422.0421393841167,
      "grad_norm": 1.5435066223144531,
      "learning_rate": 7.817807330522218e-06,
      "loss": 2.4603,
      "step": 1302000
    },
    {
      "epoch": 422.0745542949757,
      "grad_norm": 1.294655442237854,
      "learning_rate": 7.814563736620175e-06,
      "loss": 2.4616,
      "step": 1302100
    },
    {
      "epoch": 422.1069692058347,
      "grad_norm": 1.5821360349655151,
      "learning_rate": 7.811352578657153e-06,
      "loss": 2.4853,
      "step": 1302200
    },
    {
      "epoch": 422.1393841166937,
      "grad_norm": 1.2831364870071411,
      "learning_rate": 7.80810898475511e-06,
      "loss": 2.4424,
      "step": 1302300
    },
    {
      "epoch": 422.17179902755265,
      "grad_norm": 1.3016191720962524,
      "learning_rate": 7.804865390853065e-06,
      "loss": 2.4711,
      "step": 1302400
    },
    {
      "epoch": 422.20421393841167,
      "grad_norm": 1.57710862159729,
      "learning_rate": 7.801621796951022e-06,
      "loss": 2.4735,
      "step": 1302500
    },
    {
      "epoch": 422.2366288492707,
      "grad_norm": 1.38560152053833,
      "learning_rate": 7.798378203048978e-06,
      "loss": 2.4584,
      "step": 1302600
    },
    {
      "epoch": 422.26904376012965,
      "grad_norm": 1.254265308380127,
      "learning_rate": 7.795134609146936e-06,
      "loss": 2.4658,
      "step": 1302700
    },
    {
      "epoch": 422.30145867098867,
      "grad_norm": 1.4132769107818604,
      "learning_rate": 7.791891015244892e-06,
      "loss": 2.5004,
      "step": 1302800
    },
    {
      "epoch": 422.3338735818476,
      "grad_norm": 1.567792296409607,
      "learning_rate": 7.788647421342849e-06,
      "loss": 2.4544,
      "step": 1302900
    },
    {
      "epoch": 422.36628849270664,
      "grad_norm": 1.6100711822509766,
      "learning_rate": 7.785403827440804e-06,
      "loss": 2.4716,
      "step": 1303000
    },
    {
      "epoch": 422.39870340356566,
      "grad_norm": 1.4056296348571777,
      "learning_rate": 7.782160233538761e-06,
      "loss": 2.4759,
      "step": 1303100
    },
    {
      "epoch": 422.4311183144246,
      "grad_norm": 1.418903112411499,
      "learning_rate": 7.778916639636716e-06,
      "loss": 2.4555,
      "step": 1303200
    },
    {
      "epoch": 422.46353322528364,
      "grad_norm": 1.3357186317443848,
      "learning_rate": 7.775673045734675e-06,
      "loss": 2.4551,
      "step": 1303300
    },
    {
      "epoch": 422.4959481361426,
      "grad_norm": 1.4920274019241333,
      "learning_rate": 7.772429451832632e-06,
      "loss": 2.4549,
      "step": 1303400
    },
    {
      "epoch": 422.5283630470016,
      "grad_norm": 1.4984813928604126,
      "learning_rate": 7.769185857930588e-06,
      "loss": 2.4744,
      "step": 1303500
    },
    {
      "epoch": 422.56077795786064,
      "grad_norm": 1.3895450830459595,
      "learning_rate": 7.765942264028545e-06,
      "loss": 2.4615,
      "step": 1303600
    },
    {
      "epoch": 422.5931928687196,
      "grad_norm": 1.3566056489944458,
      "learning_rate": 7.76273110606552e-06,
      "loss": 2.4592,
      "step": 1303700
    },
    {
      "epoch": 422.6256077795786,
      "grad_norm": 1.7203609943389893,
      "learning_rate": 7.759487512163476e-06,
      "loss": 2.4759,
      "step": 1303800
    },
    {
      "epoch": 422.6580226904376,
      "grad_norm": 1.5446687936782837,
      "learning_rate": 7.756243918261435e-06,
      "loss": 2.4653,
      "step": 1303900
    },
    {
      "epoch": 422.6904376012966,
      "grad_norm": 1.4512814283370972,
      "learning_rate": 7.75300032435939e-06,
      "loss": 2.4517,
      "step": 1304000
    },
    {
      "epoch": 422.7228525121556,
      "grad_norm": 1.3818811178207397,
      "learning_rate": 7.749756730457347e-06,
      "loss": 2.4748,
      "step": 1304100
    },
    {
      "epoch": 422.7552674230146,
      "grad_norm": 1.216374397277832,
      "learning_rate": 7.746513136555304e-06,
      "loss": 2.4831,
      "step": 1304200
    },
    {
      "epoch": 422.7876823338736,
      "grad_norm": 1.3803653717041016,
      "learning_rate": 7.74326954265326e-06,
      "loss": 2.4636,
      "step": 1304300
    },
    {
      "epoch": 422.82009724473255,
      "grad_norm": 1.3832992315292358,
      "learning_rate": 7.740025948751217e-06,
      "loss": 2.4783,
      "step": 1304400
    },
    {
      "epoch": 422.85251215559157,
      "grad_norm": 1.3687989711761475,
      "learning_rate": 7.736782354849174e-06,
      "loss": 2.4812,
      "step": 1304500
    },
    {
      "epoch": 422.8849270664506,
      "grad_norm": 1.4230084419250488,
      "learning_rate": 7.73353876094713e-06,
      "loss": 2.4868,
      "step": 1304600
    },
    {
      "epoch": 422.91734197730955,
      "grad_norm": 1.2949554920196533,
      "learning_rate": 7.730295167045086e-06,
      "loss": 2.4636,
      "step": 1304700
    },
    {
      "epoch": 422.94975688816857,
      "grad_norm": 1.4207003116607666,
      "learning_rate": 7.727051573143043e-06,
      "loss": 2.4788,
      "step": 1304800
    },
    {
      "epoch": 422.9821717990275,
      "grad_norm": 1.2690057754516602,
      "learning_rate": 7.723807979240999e-06,
      "loss": 2.4526,
      "step": 1304900
    },
    {
      "epoch": 423.0,
      "eval_bleu": 0.9956482245051496,
      "eval_loss": 4.268625259399414,
      "eval_runtime": 4.5247,
      "eval_samples_per_second": 108.738,
      "eval_steps_per_second": 1.768,
      "step": 1304955
    },
    {
      "epoch": 423.01458670988654,
      "grad_norm": 1.4786841869354248,
      "learning_rate": 7.720564385338956e-06,
      "loss": 2.4816,
      "step": 1305000
    },
    {
      "epoch": 423.04700162074556,
      "grad_norm": 1.3787403106689453,
      "learning_rate": 7.717320791436913e-06,
      "loss": 2.4729,
      "step": 1305100
    },
    {
      "epoch": 423.0794165316045,
      "grad_norm": 1.7461861371994019,
      "learning_rate": 7.71407719753487e-06,
      "loss": 2.4713,
      "step": 1305200
    },
    {
      "epoch": 423.11183144246354,
      "grad_norm": 1.5216140747070312,
      "learning_rate": 7.710833603632825e-06,
      "loss": 2.4499,
      "step": 1305300
    },
    {
      "epoch": 423.1442463533225,
      "grad_norm": 1.430981993675232,
      "learning_rate": 7.707590009730782e-06,
      "loss": 2.467,
      "step": 1305400
    },
    {
      "epoch": 423.1766612641815,
      "grad_norm": 1.5306018590927124,
      "learning_rate": 7.704346415828739e-06,
      "loss": 2.4665,
      "step": 1305500
    },
    {
      "epoch": 423.20907617504054,
      "grad_norm": 1.4286807775497437,
      "learning_rate": 7.701102821926695e-06,
      "loss": 2.4709,
      "step": 1305600
    },
    {
      "epoch": 423.2414910858995,
      "grad_norm": 1.3432077169418335,
      "learning_rate": 7.697859228024652e-06,
      "loss": 2.4611,
      "step": 1305700
    },
    {
      "epoch": 423.2739059967585,
      "grad_norm": 1.559921145439148,
      "learning_rate": 7.694615634122609e-06,
      "loss": 2.4641,
      "step": 1305800
    },
    {
      "epoch": 423.3063209076175,
      "grad_norm": 1.3986326456069946,
      "learning_rate": 7.691372040220566e-06,
      "loss": 2.4639,
      "step": 1305900
    },
    {
      "epoch": 423.3387358184765,
      "grad_norm": 1.353849172592163,
      "learning_rate": 7.688128446318521e-06,
      "loss": 2.4311,
      "step": 1306000
    },
    {
      "epoch": 423.3711507293355,
      "grad_norm": 1.7321484088897705,
      "learning_rate": 7.684884852416478e-06,
      "loss": 2.4691,
      "step": 1306100
    },
    {
      "epoch": 423.4035656401945,
      "grad_norm": 1.3714624643325806,
      "learning_rate": 7.681641258514433e-06,
      "loss": 2.459,
      "step": 1306200
    },
    {
      "epoch": 423.4359805510535,
      "grad_norm": 1.520966649055481,
      "learning_rate": 7.67839766461239e-06,
      "loss": 2.4645,
      "step": 1306300
    },
    {
      "epoch": 423.4683954619125,
      "grad_norm": 1.514414668083191,
      "learning_rate": 7.675154070710348e-06,
      "loss": 2.4628,
      "step": 1306400
    },
    {
      "epoch": 423.50081037277147,
      "grad_norm": 1.204646110534668,
      "learning_rate": 7.671910476808305e-06,
      "loss": 2.4676,
      "step": 1306500
    },
    {
      "epoch": 423.5332252836305,
      "grad_norm": 1.3643819093704224,
      "learning_rate": 7.66866688290626e-06,
      "loss": 2.4649,
      "step": 1306600
    },
    {
      "epoch": 423.56564019448945,
      "grad_norm": 1.610653281211853,
      "learning_rate": 7.665423289004217e-06,
      "loss": 2.456,
      "step": 1306700
    },
    {
      "epoch": 423.59805510534846,
      "grad_norm": 1.407662272453308,
      "learning_rate": 7.662179695102172e-06,
      "loss": 2.4756,
      "step": 1306800
    },
    {
      "epoch": 423.6304700162075,
      "grad_norm": 1.2953511476516724,
      "learning_rate": 7.65893610120013e-06,
      "loss": 2.4802,
      "step": 1306900
    },
    {
      "epoch": 423.66288492706644,
      "grad_norm": 1.5164122581481934,
      "learning_rate": 7.655724943237107e-06,
      "loss": 2.4884,
      "step": 1307000
    },
    {
      "epoch": 423.69529983792546,
      "grad_norm": 1.3709338903427124,
      "learning_rate": 7.652481349335064e-06,
      "loss": 2.4841,
      "step": 1307100
    },
    {
      "epoch": 423.7277147487844,
      "grad_norm": 1.4464914798736572,
      "learning_rate": 7.64923775543302e-06,
      "loss": 2.4695,
      "step": 1307200
    },
    {
      "epoch": 423.76012965964344,
      "grad_norm": 1.6368571519851685,
      "learning_rate": 7.645994161530977e-06,
      "loss": 2.4526,
      "step": 1307300
    },
    {
      "epoch": 423.79254457050246,
      "grad_norm": 1.681671380996704,
      "learning_rate": 7.642750567628932e-06,
      "loss": 2.494,
      "step": 1307400
    },
    {
      "epoch": 423.8249594813614,
      "grad_norm": 1.4285520315170288,
      "learning_rate": 7.639506973726889e-06,
      "loss": 2.4646,
      "step": 1307500
    },
    {
      "epoch": 423.85737439222044,
      "grad_norm": 1.2843910455703735,
      "learning_rate": 7.636263379824846e-06,
      "loss": 2.4736,
      "step": 1307600
    },
    {
      "epoch": 423.8897893030794,
      "grad_norm": 1.3725076913833618,
      "learning_rate": 7.633019785922803e-06,
      "loss": 2.4433,
      "step": 1307700
    },
    {
      "epoch": 423.9222042139384,
      "grad_norm": 1.4855445623397827,
      "learning_rate": 7.62977619202076e-06,
      "loss": 2.4743,
      "step": 1307800
    },
    {
      "epoch": 423.95461912479743,
      "grad_norm": 1.3568602800369263,
      "learning_rate": 7.6265325981187155e-06,
      "loss": 2.472,
      "step": 1307900
    },
    {
      "epoch": 423.9870340356564,
      "grad_norm": 1.5645592212677002,
      "learning_rate": 7.6232890042166726e-06,
      "loss": 2.4747,
      "step": 1308000
    },
    {
      "epoch": 424.0,
      "eval_bleu": 1.1237239411427904,
      "eval_loss": 4.267836093902588,
      "eval_runtime": 4.6256,
      "eval_samples_per_second": 106.365,
      "eval_steps_per_second": 1.73,
      "step": 1308040
    },
    {
      "epoch": 424.0194489465154,
      "grad_norm": 1.26707923412323,
      "learning_rate": 7.620045410314629e-06,
      "loss": 2.4536,
      "step": 1308100
    },
    {
      "epoch": 424.05186385737437,
      "grad_norm": 1.4340733289718628,
      "learning_rate": 7.616801816412586e-06,
      "loss": 2.4384,
      "step": 1308200
    },
    {
      "epoch": 424.0842787682334,
      "grad_norm": 1.5340656042099,
      "learning_rate": 7.613558222510542e-06,
      "loss": 2.4503,
      "step": 1308300
    },
    {
      "epoch": 424.1166936790924,
      "grad_norm": 1.4711254835128784,
      "learning_rate": 7.610314628608499e-06,
      "loss": 2.469,
      "step": 1308400
    },
    {
      "epoch": 424.14910858995137,
      "grad_norm": 1.2777870893478394,
      "learning_rate": 7.6070710347064544e-06,
      "loss": 2.4766,
      "step": 1308500
    },
    {
      "epoch": 424.1815235008104,
      "grad_norm": 1.3438133001327515,
      "learning_rate": 7.6038274408044115e-06,
      "loss": 2.4696,
      "step": 1308600
    },
    {
      "epoch": 424.21393841166935,
      "grad_norm": 1.4777345657348633,
      "learning_rate": 7.600583846902368e-06,
      "loss": 2.4712,
      "step": 1308700
    },
    {
      "epoch": 424.24635332252836,
      "grad_norm": 1.5064603090286255,
      "learning_rate": 7.597340253000325e-06,
      "loss": 2.4769,
      "step": 1308800
    },
    {
      "epoch": 424.2787682333874,
      "grad_norm": 1.4540295600891113,
      "learning_rate": 7.594096659098281e-06,
      "loss": 2.4681,
      "step": 1308900
    },
    {
      "epoch": 424.31118314424634,
      "grad_norm": 1.4176386594772339,
      "learning_rate": 7.590853065196238e-06,
      "loss": 2.4598,
      "step": 1309000
    },
    {
      "epoch": 424.34359805510536,
      "grad_norm": 1.3764971494674683,
      "learning_rate": 7.587609471294193e-06,
      "loss": 2.472,
      "step": 1309100
    },
    {
      "epoch": 424.3760129659643,
      "grad_norm": 1.3509738445281982,
      "learning_rate": 7.584365877392151e-06,
      "loss": 2.4696,
      "step": 1309200
    },
    {
      "epoch": 424.40842787682334,
      "grad_norm": 1.4576609134674072,
      "learning_rate": 7.581122283490108e-06,
      "loss": 2.4848,
      "step": 1309300
    },
    {
      "epoch": 424.44084278768236,
      "grad_norm": 1.2050940990447998,
      "learning_rate": 7.577878689588064e-06,
      "loss": 2.4715,
      "step": 1309400
    },
    {
      "epoch": 424.4732576985413,
      "grad_norm": 1.6163434982299805,
      "learning_rate": 7.574635095686021e-06,
      "loss": 2.4395,
      "step": 1309500
    },
    {
      "epoch": 424.50567260940034,
      "grad_norm": 1.4987938404083252,
      "learning_rate": 7.571391501783977e-06,
      "loss": 2.4478,
      "step": 1309600
    },
    {
      "epoch": 424.5380875202593,
      "grad_norm": 1.2552436590194702,
      "learning_rate": 7.568147907881934e-06,
      "loss": 2.4602,
      "step": 1309700
    },
    {
      "epoch": 424.5705024311183,
      "grad_norm": 1.5870766639709473,
      "learning_rate": 7.56490431397989e-06,
      "loss": 2.4381,
      "step": 1309800
    },
    {
      "epoch": 424.60291734197733,
      "grad_norm": 1.557579517364502,
      "learning_rate": 7.561660720077847e-06,
      "loss": 2.4731,
      "step": 1309900
    },
    {
      "epoch": 424.6353322528363,
      "grad_norm": 1.40682053565979,
      "learning_rate": 7.5584171261758025e-06,
      "loss": 2.4679,
      "step": 1310000
    },
    {
      "epoch": 424.6677471636953,
      "grad_norm": 1.499574899673462,
      "learning_rate": 7.55517353227376e-06,
      "loss": 2.4516,
      "step": 1310100
    },
    {
      "epoch": 424.70016207455427,
      "grad_norm": 1.4711871147155762,
      "learning_rate": 7.551929938371716e-06,
      "loss": 2.4662,
      "step": 1310200
    },
    {
      "epoch": 424.7325769854133,
      "grad_norm": 1.2719467878341675,
      "learning_rate": 7.548686344469673e-06,
      "loss": 2.4683,
      "step": 1310300
    },
    {
      "epoch": 424.7649918962723,
      "grad_norm": 1.3216713666915894,
      "learning_rate": 7.545442750567629e-06,
      "loss": 2.4965,
      "step": 1310400
    },
    {
      "epoch": 424.79740680713127,
      "grad_norm": 1.3807792663574219,
      "learning_rate": 7.542199156665586e-06,
      "loss": 2.4671,
      "step": 1310500
    },
    {
      "epoch": 424.8298217179903,
      "grad_norm": 1.3318992853164673,
      "learning_rate": 7.538955562763543e-06,
      "loss": 2.4719,
      "step": 1310600
    },
    {
      "epoch": 424.86223662884925,
      "grad_norm": 1.588918924331665,
      "learning_rate": 7.5357119688614985e-06,
      "loss": 2.4786,
      "step": 1310700
    },
    {
      "epoch": 424.89465153970826,
      "grad_norm": 1.207470417022705,
      "learning_rate": 7.532468374959456e-06,
      "loss": 2.484,
      "step": 1310800
    },
    {
      "epoch": 424.9270664505673,
      "grad_norm": 1.6267598867416382,
      "learning_rate": 7.529224781057412e-06,
      "loss": 2.4704,
      "step": 1310900
    },
    {
      "epoch": 424.95948136142624,
      "grad_norm": 1.7072535753250122,
      "learning_rate": 7.526013623094389e-06,
      "loss": 2.4606,
      "step": 1311000
    },
    {
      "epoch": 424.99189627228526,
      "grad_norm": 1.6114697456359863,
      "learning_rate": 7.522770029192346e-06,
      "loss": 2.4604,
      "step": 1311100
    },
    {
      "epoch": 425.0,
      "eval_bleu": 0.9614041629571172,
      "eval_loss": 4.266318321228027,
      "eval_runtime": 4.3747,
      "eval_samples_per_second": 112.465,
      "eval_steps_per_second": 1.829,
      "step": 1311125
    },
    {
      "epoch": 425.0243111831442,
      "grad_norm": 1.385143756866455,
      "learning_rate": 7.519526435290301e-06,
      "loss": 2.4684,
      "step": 1311200
    },
    {
      "epoch": 425.05672609400324,
      "grad_norm": 1.418748140335083,
      "learning_rate": 7.516282841388258e-06,
      "loss": 2.4688,
      "step": 1311300
    },
    {
      "epoch": 425.08914100486226,
      "grad_norm": 1.3935726881027222,
      "learning_rate": 7.513039247486216e-06,
      "loss": 2.4552,
      "step": 1311400
    },
    {
      "epoch": 425.1215559157212,
      "grad_norm": 1.245523452758789,
      "learning_rate": 7.509795653584171e-06,
      "loss": 2.4792,
      "step": 1311500
    },
    {
      "epoch": 425.15397082658023,
      "grad_norm": 1.4918384552001953,
      "learning_rate": 7.5065520596821284e-06,
      "loss": 2.4918,
      "step": 1311600
    },
    {
      "epoch": 425.1863857374392,
      "grad_norm": 1.3422324657440186,
      "learning_rate": 7.503308465780085e-06,
      "loss": 2.4773,
      "step": 1311700
    },
    {
      "epoch": 425.2188006482982,
      "grad_norm": 1.5178556442260742,
      "learning_rate": 7.500064871878042e-06,
      "loss": 2.4766,
      "step": 1311800
    },
    {
      "epoch": 425.25121555915723,
      "grad_norm": 1.5010604858398438,
      "learning_rate": 7.496821277975998e-06,
      "loss": 2.4673,
      "step": 1311900
    },
    {
      "epoch": 425.2836304700162,
      "grad_norm": 1.3382105827331543,
      "learning_rate": 7.493577684073955e-06,
      "loss": 2.4757,
      "step": 1312000
    },
    {
      "epoch": 425.3160453808752,
      "grad_norm": 1.5721625089645386,
      "learning_rate": 7.490366526110931e-06,
      "loss": 2.4467,
      "step": 1312100
    },
    {
      "epoch": 425.34846029173417,
      "grad_norm": 1.3056650161743164,
      "learning_rate": 7.487122932208888e-06,
      "loss": 2.4554,
      "step": 1312200
    },
    {
      "epoch": 425.3808752025932,
      "grad_norm": 1.4818371534347534,
      "learning_rate": 7.483879338306844e-06,
      "loss": 2.4854,
      "step": 1312300
    },
    {
      "epoch": 425.4132901134522,
      "grad_norm": 1.3999463319778442,
      "learning_rate": 7.480635744404801e-06,
      "loss": 2.4663,
      "step": 1312400
    },
    {
      "epoch": 425.44570502431117,
      "grad_norm": 1.381045937538147,
      "learning_rate": 7.4773921505027576e-06,
      "loss": 2.4626,
      "step": 1312500
    },
    {
      "epoch": 425.4781199351702,
      "grad_norm": 1.5072287321090698,
      "learning_rate": 7.474148556600715e-06,
      "loss": 2.4715,
      "step": 1312600
    },
    {
      "epoch": 425.51053484602915,
      "grad_norm": 1.388759970664978,
      "learning_rate": 7.47090496269867e-06,
      "loss": 2.4752,
      "step": 1312700
    },
    {
      "epoch": 425.54294975688816,
      "grad_norm": 1.5260721445083618,
      "learning_rate": 7.467661368796627e-06,
      "loss": 2.4555,
      "step": 1312800
    },
    {
      "epoch": 425.5753646677472,
      "grad_norm": 1.3902994394302368,
      "learning_rate": 7.464417774894583e-06,
      "loss": 2.451,
      "step": 1312900
    },
    {
      "epoch": 425.60777957860614,
      "grad_norm": 1.414237380027771,
      "learning_rate": 7.46117418099254e-06,
      "loss": 2.4445,
      "step": 1313000
    },
    {
      "epoch": 425.64019448946516,
      "grad_norm": 1.3782309293746948,
      "learning_rate": 7.4579305870904965e-06,
      "loss": 2.4706,
      "step": 1313100
    },
    {
      "epoch": 425.6726094003242,
      "grad_norm": 1.5320165157318115,
      "learning_rate": 7.4546869931884535e-06,
      "loss": 2.4702,
      "step": 1313200
    },
    {
      "epoch": 425.70502431118314,
      "grad_norm": 1.4050687551498413,
      "learning_rate": 7.451443399286409e-06,
      "loss": 2.4574,
      "step": 1313300
    },
    {
      "epoch": 425.73743922204216,
      "grad_norm": 1.4964491128921509,
      "learning_rate": 7.448199805384366e-06,
      "loss": 2.4957,
      "step": 1313400
    },
    {
      "epoch": 425.7698541329011,
      "grad_norm": 1.544007420539856,
      "learning_rate": 7.444956211482322e-06,
      "loss": 2.4642,
      "step": 1313500
    },
    {
      "epoch": 425.80226904376013,
      "grad_norm": 1.4568324089050293,
      "learning_rate": 7.441712617580279e-06,
      "loss": 2.4657,
      "step": 1313600
    },
    {
      "epoch": 425.83468395461915,
      "grad_norm": 1.5283809900283813,
      "learning_rate": 7.438469023678236e-06,
      "loss": 2.4774,
      "step": 1313700
    },
    {
      "epoch": 425.8670988654781,
      "grad_norm": 1.3593120574951172,
      "learning_rate": 7.435225429776192e-06,
      "loss": 2.4736,
      "step": 1313800
    },
    {
      "epoch": 425.89951377633713,
      "grad_norm": 1.3485084772109985,
      "learning_rate": 7.4319818358741495e-06,
      "loss": 2.467,
      "step": 1313900
    },
    {
      "epoch": 425.9319286871961,
      "grad_norm": 1.5964114665985107,
      "learning_rate": 7.428738241972105e-06,
      "loss": 2.4464,
      "step": 1314000
    },
    {
      "epoch": 425.9643435980551,
      "grad_norm": 1.4802794456481934,
      "learning_rate": 7.425494648070063e-06,
      "loss": 2.4648,
      "step": 1314100
    },
    {
      "epoch": 425.9967585089141,
      "grad_norm": 1.4282124042510986,
      "learning_rate": 7.422251054168018e-06,
      "loss": 2.4438,
      "step": 1314200
    },
    {
      "epoch": 426.0,
      "eval_bleu": 0.9823557298560512,
      "eval_loss": 4.2694196701049805,
      "eval_runtime": 4.0594,
      "eval_samples_per_second": 121.2,
      "eval_steps_per_second": 1.971,
      "step": 1314210
    },
    {
      "epoch": 426.0291734197731,
      "grad_norm": 1.2871274948120117,
      "learning_rate": 7.419007460265975e-06,
      "loss": 2.4594,
      "step": 1314300
    },
    {
      "epoch": 426.0615883306321,
      "grad_norm": 1.6088311672210693,
      "learning_rate": 7.415763866363931e-06,
      "loss": 2.4575,
      "step": 1314400
    },
    {
      "epoch": 426.09400324149107,
      "grad_norm": 1.3173432350158691,
      "learning_rate": 7.412520272461888e-06,
      "loss": 2.4637,
      "step": 1314500
    },
    {
      "epoch": 426.1264181523501,
      "grad_norm": 1.2968186140060425,
      "learning_rate": 7.4092766785598446e-06,
      "loss": 2.4727,
      "step": 1314600
    },
    {
      "epoch": 426.1588330632091,
      "grad_norm": 1.3738396167755127,
      "learning_rate": 7.406033084657802e-06,
      "loss": 2.4613,
      "step": 1314700
    },
    {
      "epoch": 426.19124797406806,
      "grad_norm": 1.419933795928955,
      "learning_rate": 7.402789490755757e-06,
      "loss": 2.4748,
      "step": 1314800
    },
    {
      "epoch": 426.2236628849271,
      "grad_norm": 1.716193675994873,
      "learning_rate": 7.399545896853714e-06,
      "loss": 2.475,
      "step": 1314900
    },
    {
      "epoch": 426.25607779578604,
      "grad_norm": 1.2465523481369019,
      "learning_rate": 7.396302302951671e-06,
      "loss": 2.4631,
      "step": 1315000
    },
    {
      "epoch": 426.28849270664506,
      "grad_norm": 1.5871562957763672,
      "learning_rate": 7.393058709049627e-06,
      "loss": 2.4718,
      "step": 1315100
    },
    {
      "epoch": 426.3209076175041,
      "grad_norm": 1.5325448513031006,
      "learning_rate": 7.389815115147584e-06,
      "loss": 2.4832,
      "step": 1315200
    },
    {
      "epoch": 426.35332252836304,
      "grad_norm": 1.5278429985046387,
      "learning_rate": 7.3865715212455405e-06,
      "loss": 2.4512,
      "step": 1315300
    },
    {
      "epoch": 426.38573743922205,
      "grad_norm": 1.7034705877304077,
      "learning_rate": 7.3833279273434976e-06,
      "loss": 2.4441,
      "step": 1315400
    },
    {
      "epoch": 426.418152350081,
      "grad_norm": 1.4482297897338867,
      "learning_rate": 7.380084333441453e-06,
      "loss": 2.4703,
      "step": 1315500
    },
    {
      "epoch": 426.45056726094003,
      "grad_norm": 1.309561848640442,
      "learning_rate": 7.376840739539411e-06,
      "loss": 2.4697,
      "step": 1315600
    },
    {
      "epoch": 426.48298217179905,
      "grad_norm": 1.518286943435669,
      "learning_rate": 7.373597145637366e-06,
      "loss": 2.4624,
      "step": 1315700
    },
    {
      "epoch": 426.515397082658,
      "grad_norm": 1.3458917140960693,
      "learning_rate": 7.370353551735323e-06,
      "loss": 2.4707,
      "step": 1315800
    },
    {
      "epoch": 426.54781199351703,
      "grad_norm": 1.5753697156906128,
      "learning_rate": 7.367109957833279e-06,
      "loss": 2.4675,
      "step": 1315900
    },
    {
      "epoch": 426.580226904376,
      "grad_norm": 1.246131420135498,
      "learning_rate": 7.3638663639312365e-06,
      "loss": 2.483,
      "step": 1316000
    },
    {
      "epoch": 426.612641815235,
      "grad_norm": 1.2988486289978027,
      "learning_rate": 7.360622770029192e-06,
      "loss": 2.4639,
      "step": 1316100
    },
    {
      "epoch": 426.645056726094,
      "grad_norm": 1.2103934288024902,
      "learning_rate": 7.3574116120661705e-06,
      "loss": 2.471,
      "step": 1316200
    },
    {
      "epoch": 426.677471636953,
      "grad_norm": 1.3706705570220947,
      "learning_rate": 7.354168018164126e-06,
      "loss": 2.4854,
      "step": 1316300
    },
    {
      "epoch": 426.709886547812,
      "grad_norm": 1.2257730960845947,
      "learning_rate": 7.350924424262083e-06,
      "loss": 2.4608,
      "step": 1316400
    },
    {
      "epoch": 426.74230145867097,
      "grad_norm": 1.4262903928756714,
      "learning_rate": 7.347680830360039e-06,
      "loss": 2.4468,
      "step": 1316500
    },
    {
      "epoch": 426.77471636953,
      "grad_norm": 1.4264642000198364,
      "learning_rate": 7.344437236457996e-06,
      "loss": 2.4814,
      "step": 1316600
    },
    {
      "epoch": 426.807131280389,
      "grad_norm": 1.510872721672058,
      "learning_rate": 7.3411936425559515e-06,
      "loss": 2.4608,
      "step": 1316700
    },
    {
      "epoch": 426.83954619124796,
      "grad_norm": 1.4923278093338013,
      "learning_rate": 7.337950048653909e-06,
      "loss": 2.4679,
      "step": 1316800
    },
    {
      "epoch": 426.871961102107,
      "grad_norm": 1.4266164302825928,
      "learning_rate": 7.334706454751865e-06,
      "loss": 2.4809,
      "step": 1316900
    },
    {
      "epoch": 426.90437601296594,
      "grad_norm": 1.3530610799789429,
      "learning_rate": 7.331462860849822e-06,
      "loss": 2.4643,
      "step": 1317000
    },
    {
      "epoch": 426.93679092382496,
      "grad_norm": 1.4014439582824707,
      "learning_rate": 7.328219266947778e-06,
      "loss": 2.4714,
      "step": 1317100
    },
    {
      "epoch": 426.969205834684,
      "grad_norm": 1.251092791557312,
      "learning_rate": 7.324975673045735e-06,
      "loss": 2.4499,
      "step": 1317200
    },
    {
      "epoch": 427.0,
      "eval_bleu": 1.1562940371119061,
      "eval_loss": 4.267209053039551,
      "eval_runtime": 4.4513,
      "eval_samples_per_second": 110.529,
      "eval_steps_per_second": 1.797,
      "step": 1317295
    },
    {
      "epoch": 427.00162074554294,
      "grad_norm": 1.3383302688598633,
      "learning_rate": 7.321732079143692e-06,
      "loss": 2.4705,
      "step": 1317300
    },
    {
      "epoch": 427.03403565640195,
      "grad_norm": 1.5676376819610596,
      "learning_rate": 7.318488485241648e-06,
      "loss": 2.4515,
      "step": 1317400
    },
    {
      "epoch": 427.0664505672609,
      "grad_norm": 1.2790513038635254,
      "learning_rate": 7.315244891339605e-06,
      "loss": 2.4667,
      "step": 1317500
    },
    {
      "epoch": 427.09886547811993,
      "grad_norm": 1.4673879146575928,
      "learning_rate": 7.312001297437561e-06,
      "loss": 2.4733,
      "step": 1317600
    },
    {
      "epoch": 427.13128038897895,
      "grad_norm": 1.5645556449890137,
      "learning_rate": 7.308757703535518e-06,
      "loss": 2.4672,
      "step": 1317700
    },
    {
      "epoch": 427.1636952998379,
      "grad_norm": 1.518940806388855,
      "learning_rate": 7.305514109633474e-06,
      "loss": 2.4511,
      "step": 1317800
    },
    {
      "epoch": 427.19611021069693,
      "grad_norm": 1.3428417444229126,
      "learning_rate": 7.302270515731431e-06,
      "loss": 2.45,
      "step": 1317900
    },
    {
      "epoch": 427.2285251215559,
      "grad_norm": 1.4397732019424438,
      "learning_rate": 7.299026921829387e-06,
      "loss": 2.4698,
      "step": 1318000
    },
    {
      "epoch": 427.2609400324149,
      "grad_norm": 1.3841558694839478,
      "learning_rate": 7.295783327927344e-06,
      "loss": 2.442,
      "step": 1318100
    },
    {
      "epoch": 427.2933549432739,
      "grad_norm": 1.476702094078064,
      "learning_rate": 7.29257216996432e-06,
      "loss": 2.4585,
      "step": 1318200
    },
    {
      "epoch": 427.3257698541329,
      "grad_norm": 1.5093729496002197,
      "learning_rate": 7.289328576062278e-06,
      "loss": 2.4529,
      "step": 1318300
    },
    {
      "epoch": 427.3581847649919,
      "grad_norm": 1.2949761152267456,
      "learning_rate": 7.286084982160234e-06,
      "loss": 2.4587,
      "step": 1318400
    },
    {
      "epoch": 427.39059967585086,
      "grad_norm": 1.4473625421524048,
      "learning_rate": 7.282841388258191e-06,
      "loss": 2.487,
      "step": 1318500
    },
    {
      "epoch": 427.4230145867099,
      "grad_norm": 1.3792723417282104,
      "learning_rate": 7.279597794356147e-06,
      "loss": 2.4562,
      "step": 1318600
    },
    {
      "epoch": 427.4554294975689,
      "grad_norm": 1.3774712085723877,
      "learning_rate": 7.276354200454104e-06,
      "loss": 2.4311,
      "step": 1318700
    },
    {
      "epoch": 427.48784440842786,
      "grad_norm": 1.4095008373260498,
      "learning_rate": 7.273110606552059e-06,
      "loss": 2.4626,
      "step": 1318800
    },
    {
      "epoch": 427.5202593192869,
      "grad_norm": 1.5169061422348022,
      "learning_rate": 7.269867012650017e-06,
      "loss": 2.4686,
      "step": 1318900
    },
    {
      "epoch": 427.55267423014584,
      "grad_norm": 1.416090965270996,
      "learning_rate": 7.2666234187479725e-06,
      "loss": 2.4589,
      "step": 1319000
    },
    {
      "epoch": 427.58508914100486,
      "grad_norm": 1.4905076026916504,
      "learning_rate": 7.2633798248459295e-06,
      "loss": 2.496,
      "step": 1319100
    },
    {
      "epoch": 427.6175040518639,
      "grad_norm": 1.5139117240905762,
      "learning_rate": 7.260136230943886e-06,
      "loss": 2.4645,
      "step": 1319200
    },
    {
      "epoch": 427.64991896272284,
      "grad_norm": 1.3282811641693115,
      "learning_rate": 7.256892637041843e-06,
      "loss": 2.4863,
      "step": 1319300
    },
    {
      "epoch": 427.68233387358185,
      "grad_norm": 1.447518229484558,
      "learning_rate": 7.2536490431398e-06,
      "loss": 2.4976,
      "step": 1319400
    },
    {
      "epoch": 427.7147487844408,
      "grad_norm": 1.381097435951233,
      "learning_rate": 7.250437885176777e-06,
      "loss": 2.4918,
      "step": 1319500
    },
    {
      "epoch": 427.74716369529983,
      "grad_norm": 1.4036308526992798,
      "learning_rate": 7.247194291274732e-06,
      "loss": 2.4732,
      "step": 1319600
    },
    {
      "epoch": 427.77957860615885,
      "grad_norm": 1.3432705402374268,
      "learning_rate": 7.243950697372689e-06,
      "loss": 2.467,
      "step": 1319700
    },
    {
      "epoch": 427.8119935170178,
      "grad_norm": 1.414519190788269,
      "learning_rate": 7.240707103470645e-06,
      "loss": 2.4455,
      "step": 1319800
    },
    {
      "epoch": 427.84440842787683,
      "grad_norm": 1.3480064868927002,
      "learning_rate": 7.2374635095686025e-06,
      "loss": 2.4489,
      "step": 1319900
    },
    {
      "epoch": 427.87682333873585,
      "grad_norm": 1.3507672548294067,
      "learning_rate": 7.234219915666559e-06,
      "loss": 2.4895,
      "step": 1320000
    },
    {
      "epoch": 427.9092382495948,
      "grad_norm": 1.3215417861938477,
      "learning_rate": 7.230976321764516e-06,
      "loss": 2.4643,
      "step": 1320100
    },
    {
      "epoch": 427.9416531604538,
      "grad_norm": 1.6764016151428223,
      "learning_rate": 7.227732727862473e-06,
      "loss": 2.4677,
      "step": 1320200
    },
    {
      "epoch": 427.9740680713128,
      "grad_norm": 1.522920846939087,
      "learning_rate": 7.224489133960428e-06,
      "loss": 2.4735,
      "step": 1320300
    },
    {
      "epoch": 428.0,
      "eval_bleu": 1.0784086788447074,
      "eval_loss": 4.265738487243652,
      "eval_runtime": 4.3378,
      "eval_samples_per_second": 113.421,
      "eval_steps_per_second": 1.844,
      "step": 1320380
    },
    {
      "epoch": 428.0064829821718,
      "grad_norm": 1.400728464126587,
      "learning_rate": 7.221245540058386e-06,
      "loss": 2.4603,
      "step": 1320400
    },
    {
      "epoch": 428.0388978930308,
      "grad_norm": 1.3415648937225342,
      "learning_rate": 7.218001946156341e-06,
      "loss": 2.4731,
      "step": 1320500
    },
    {
      "epoch": 428.0713128038898,
      "grad_norm": 1.4888243675231934,
      "learning_rate": 7.214758352254298e-06,
      "loss": 2.4942,
      "step": 1320600
    },
    {
      "epoch": 428.1037277147488,
      "grad_norm": 1.3345893621444702,
      "learning_rate": 7.211514758352255e-06,
      "loss": 2.4643,
      "step": 1320700
    },
    {
      "epoch": 428.13614262560776,
      "grad_norm": 1.394403338432312,
      "learning_rate": 7.208271164450212e-06,
      "loss": 2.4622,
      "step": 1320800
    },
    {
      "epoch": 428.1685575364668,
      "grad_norm": 1.3701235055923462,
      "learning_rate": 7.205027570548167e-06,
      "loss": 2.46,
      "step": 1320900
    },
    {
      "epoch": 428.2009724473258,
      "grad_norm": 1.4936950206756592,
      "learning_rate": 7.201783976646125e-06,
      "loss": 2.4587,
      "step": 1321000
    },
    {
      "epoch": 428.23338735818476,
      "grad_norm": 1.4699177742004395,
      "learning_rate": 7.19854038274408e-06,
      "loss": 2.4596,
      "step": 1321100
    },
    {
      "epoch": 428.2658022690438,
      "grad_norm": 1.5357414484024048,
      "learning_rate": 7.195296788842037e-06,
      "loss": 2.4714,
      "step": 1321200
    },
    {
      "epoch": 428.29821717990274,
      "grad_norm": 1.3438923358917236,
      "learning_rate": 7.1920531949399935e-06,
      "loss": 2.4694,
      "step": 1321300
    },
    {
      "epoch": 428.33063209076175,
      "grad_norm": 1.2605828046798706,
      "learning_rate": 7.1888096010379506e-06,
      "loss": 2.4547,
      "step": 1321400
    },
    {
      "epoch": 428.36304700162077,
      "grad_norm": 1.4876784086227417,
      "learning_rate": 7.185566007135906e-06,
      "loss": 2.4529,
      "step": 1321500
    },
    {
      "epoch": 428.39546191247973,
      "grad_norm": 1.2904387712478638,
      "learning_rate": 7.182322413233864e-06,
      "loss": 2.4873,
      "step": 1321600
    },
    {
      "epoch": 428.42787682333875,
      "grad_norm": 1.3478578329086304,
      "learning_rate": 7.179078819331821e-06,
      "loss": 2.4737,
      "step": 1321700
    },
    {
      "epoch": 428.4602917341977,
      "grad_norm": 1.594177484512329,
      "learning_rate": 7.175835225429776e-06,
      "loss": 2.4597,
      "step": 1321800
    },
    {
      "epoch": 428.4927066450567,
      "grad_norm": 1.237408995628357,
      "learning_rate": 7.172624067466753e-06,
      "loss": 2.4639,
      "step": 1321900
    },
    {
      "epoch": 428.52512155591575,
      "grad_norm": 1.361846685409546,
      "learning_rate": 7.16938047356471e-06,
      "loss": 2.4453,
      "step": 1322000
    },
    {
      "epoch": 428.5575364667747,
      "grad_norm": 1.2344434261322021,
      "learning_rate": 7.166136879662666e-06,
      "loss": 2.4472,
      "step": 1322100
    },
    {
      "epoch": 428.5899513776337,
      "grad_norm": 1.3420040607452393,
      "learning_rate": 7.1628932857606235e-06,
      "loss": 2.47,
      "step": 1322200
    },
    {
      "epoch": 428.6223662884927,
      "grad_norm": 1.960338830947876,
      "learning_rate": 7.159649691858579e-06,
      "loss": 2.4528,
      "step": 1322300
    },
    {
      "epoch": 428.6547811993517,
      "grad_norm": 1.4024379253387451,
      "learning_rate": 7.156406097956536e-06,
      "loss": 2.4697,
      "step": 1322400
    },
    {
      "epoch": 428.6871961102107,
      "grad_norm": 1.524705410003662,
      "learning_rate": 7.153162504054493e-06,
      "loss": 2.4614,
      "step": 1322500
    },
    {
      "epoch": 428.7196110210697,
      "grad_norm": 1.3601504564285278,
      "learning_rate": 7.149918910152449e-06,
      "loss": 2.4769,
      "step": 1322600
    },
    {
      "epoch": 428.7520259319287,
      "grad_norm": 1.3667484521865845,
      "learning_rate": 7.146675316250406e-06,
      "loss": 2.4571,
      "step": 1322700
    },
    {
      "epoch": 428.78444084278766,
      "grad_norm": 1.3956788778305054,
      "learning_rate": 7.143431722348362e-06,
      "loss": 2.4769,
      "step": 1322800
    },
    {
      "epoch": 428.8168557536467,
      "grad_norm": 1.4366931915283203,
      "learning_rate": 7.140188128446319e-06,
      "loss": 2.4551,
      "step": 1322900
    },
    {
      "epoch": 428.8492706645057,
      "grad_norm": 1.5449334383010864,
      "learning_rate": 7.136944534544275e-06,
      "loss": 2.4718,
      "step": 1323000
    },
    {
      "epoch": 428.88168557536466,
      "grad_norm": 1.3637988567352295,
      "learning_rate": 7.133700940642233e-06,
      "loss": 2.4567,
      "step": 1323100
    },
    {
      "epoch": 428.9141004862237,
      "grad_norm": 1.2517284154891968,
      "learning_rate": 7.130457346740188e-06,
      "loss": 2.4788,
      "step": 1323200
    },
    {
      "epoch": 428.94651539708263,
      "grad_norm": 1.4504343271255493,
      "learning_rate": 7.127213752838145e-06,
      "loss": 2.4719,
      "step": 1323300
    },
    {
      "epoch": 428.97893030794165,
      "grad_norm": 1.3231723308563232,
      "learning_rate": 7.123970158936101e-06,
      "loss": 2.4633,
      "step": 1323400
    },
    {
      "epoch": 429.0,
      "eval_bleu": 1.1817417858872792,
      "eval_loss": 4.271410942077637,
      "eval_runtime": 4.2598,
      "eval_samples_per_second": 115.499,
      "eval_steps_per_second": 1.878,
      "step": 1323465
    },
    {
      "epoch": 429.01134521880067,
      "grad_norm": 1.3272621631622314,
      "learning_rate": 7.120726565034058e-06,
      "loss": 2.4512,
      "step": 1323500
    },
    {
      "epoch": 429.04376012965963,
      "grad_norm": 1.4380534887313843,
      "learning_rate": 7.117482971132014e-06,
      "loss": 2.4663,
      "step": 1323600
    },
    {
      "epoch": 429.07617504051865,
      "grad_norm": 1.466831922531128,
      "learning_rate": 7.1142393772299716e-06,
      "loss": 2.4616,
      "step": 1323700
    },
    {
      "epoch": 429.1085899513776,
      "grad_norm": 1.4902437925338745,
      "learning_rate": 7.110995783327929e-06,
      "loss": 2.463,
      "step": 1323800
    },
    {
      "epoch": 429.1410048622366,
      "grad_norm": 1.5659205913543701,
      "learning_rate": 7.107752189425884e-06,
      "loss": 2.4808,
      "step": 1323900
    },
    {
      "epoch": 429.17341977309565,
      "grad_norm": 1.3710858821868896,
      "learning_rate": 7.104508595523841e-06,
      "loss": 2.4318,
      "step": 1324000
    },
    {
      "epoch": 429.2058346839546,
      "grad_norm": 1.336814284324646,
      "learning_rate": 7.101265001621797e-06,
      "loss": 2.4526,
      "step": 1324100
    },
    {
      "epoch": 429.2382495948136,
      "grad_norm": 1.4129984378814697,
      "learning_rate": 7.098021407719754e-06,
      "loss": 2.4734,
      "step": 1324200
    },
    {
      "epoch": 429.2706645056726,
      "grad_norm": 1.5426126718521118,
      "learning_rate": 7.0947778138177105e-06,
      "loss": 2.4737,
      "step": 1324300
    },
    {
      "epoch": 429.3030794165316,
      "grad_norm": 1.3636891841888428,
      "learning_rate": 7.0915342199156675e-06,
      "loss": 2.4503,
      "step": 1324400
    },
    {
      "epoch": 429.3354943273906,
      "grad_norm": 1.3989871740341187,
      "learning_rate": 7.088290626013623e-06,
      "loss": 2.4601,
      "step": 1324500
    },
    {
      "epoch": 429.3679092382496,
      "grad_norm": 1.6043546199798584,
      "learning_rate": 7.08504703211158e-06,
      "loss": 2.4853,
      "step": 1324600
    },
    {
      "epoch": 429.4003241491086,
      "grad_norm": 1.2880356311798096,
      "learning_rate": 7.081803438209536e-06,
      "loss": 2.4645,
      "step": 1324700
    },
    {
      "epoch": 429.43273905996756,
      "grad_norm": 1.3798167705535889,
      "learning_rate": 7.078559844307493e-06,
      "loss": 2.4732,
      "step": 1324800
    },
    {
      "epoch": 429.4651539708266,
      "grad_norm": 1.3811286687850952,
      "learning_rate": 7.075316250405449e-06,
      "loss": 2.4807,
      "step": 1324900
    },
    {
      "epoch": 429.4975688816856,
      "grad_norm": 1.3379582166671753,
      "learning_rate": 7.0720726565034064e-06,
      "loss": 2.4644,
      "step": 1325000
    },
    {
      "epoch": 429.52998379254456,
      "grad_norm": 1.4025954008102417,
      "learning_rate": 7.068829062601362e-06,
      "loss": 2.4659,
      "step": 1325100
    },
    {
      "epoch": 429.5623987034036,
      "grad_norm": 1.5735665559768677,
      "learning_rate": 7.065585468699319e-06,
      "loss": 2.4737,
      "step": 1325200
    },
    {
      "epoch": 429.59481361426253,
      "grad_norm": 1.397937297821045,
      "learning_rate": 7.062341874797277e-06,
      "loss": 2.4634,
      "step": 1325300
    },
    {
      "epoch": 429.62722852512155,
      "grad_norm": 1.2742011547088623,
      "learning_rate": 7.059098280895232e-06,
      "loss": 2.4671,
      "step": 1325400
    },
    {
      "epoch": 429.65964343598057,
      "grad_norm": 1.4322853088378906,
      "learning_rate": 7.055854686993189e-06,
      "loss": 2.4796,
      "step": 1325500
    },
    {
      "epoch": 429.69205834683953,
      "grad_norm": 1.7493308782577515,
      "learning_rate": 7.052611093091145e-06,
      "loss": 2.4532,
      "step": 1325600
    },
    {
      "epoch": 429.72447325769855,
      "grad_norm": 1.528591513633728,
      "learning_rate": 7.049367499189102e-06,
      "loss": 2.4573,
      "step": 1325700
    },
    {
      "epoch": 429.7568881685575,
      "grad_norm": 1.351473093032837,
      "learning_rate": 7.046123905287058e-06,
      "loss": 2.4647,
      "step": 1325800
    },
    {
      "epoch": 429.7893030794165,
      "grad_norm": 1.3130990266799927,
      "learning_rate": 7.042912747324035e-06,
      "loss": 2.469,
      "step": 1325900
    },
    {
      "epoch": 429.82171799027554,
      "grad_norm": 1.492860198020935,
      "learning_rate": 7.039669153421992e-06,
      "loss": 2.4504,
      "step": 1326000
    },
    {
      "epoch": 429.8541329011345,
      "grad_norm": 1.5074305534362793,
      "learning_rate": 7.036425559519949e-06,
      "loss": 2.4619,
      "step": 1326100
    },
    {
      "epoch": 429.8865478119935,
      "grad_norm": 1.4544845819473267,
      "learning_rate": 7.033181965617905e-06,
      "loss": 2.465,
      "step": 1326200
    },
    {
      "epoch": 429.9189627228525,
      "grad_norm": 1.2920010089874268,
      "learning_rate": 7.029938371715862e-06,
      "loss": 2.465,
      "step": 1326300
    },
    {
      "epoch": 429.9513776337115,
      "grad_norm": 1.3820898532867432,
      "learning_rate": 7.026727213752839e-06,
      "loss": 2.4504,
      "step": 1326400
    },
    {
      "epoch": 429.9837925445705,
      "grad_norm": 1.5967341661453247,
      "learning_rate": 7.023483619850794e-06,
      "loss": 2.4623,
      "step": 1326500
    },
    {
      "epoch": 430.0,
      "eval_bleu": 1.0584841362674056,
      "eval_loss": 4.270573616027832,
      "eval_runtime": 4.4884,
      "eval_samples_per_second": 109.615,
      "eval_steps_per_second": 1.782,
      "step": 1326550
    },
    {
      "epoch": 430.0162074554295,
      "grad_norm": 1.2408453226089478,
      "learning_rate": 7.020240025948751e-06,
      "loss": 2.4687,
      "step": 1326600
    },
    {
      "epoch": 430.0486223662885,
      "grad_norm": 1.380922555923462,
      "learning_rate": 7.016996432046708e-06,
      "loss": 2.4504,
      "step": 1326700
    },
    {
      "epoch": 430.0810372771475,
      "grad_norm": 1.300574541091919,
      "learning_rate": 7.013752838144665e-06,
      "loss": 2.4596,
      "step": 1326800
    },
    {
      "epoch": 430.1134521880065,
      "grad_norm": 1.2840940952301025,
      "learning_rate": 7.010509244242622e-06,
      "loss": 2.4632,
      "step": 1326900
    },
    {
      "epoch": 430.1458670988655,
      "grad_norm": 1.5082472562789917,
      "learning_rate": 7.007265650340578e-06,
      "loss": 2.4549,
      "step": 1327000
    },
    {
      "epoch": 430.17828200972446,
      "grad_norm": 1.500305414199829,
      "learning_rate": 7.004022056438535e-06,
      "loss": 2.4724,
      "step": 1327100
    },
    {
      "epoch": 430.2106969205835,
      "grad_norm": 1.3435760736465454,
      "learning_rate": 7.00077846253649e-06,
      "loss": 2.4468,
      "step": 1327200
    },
    {
      "epoch": 430.2431118314425,
      "grad_norm": 1.2438942193984985,
      "learning_rate": 6.997534868634447e-06,
      "loss": 2.4734,
      "step": 1327300
    },
    {
      "epoch": 430.27552674230145,
      "grad_norm": 1.5208953619003296,
      "learning_rate": 6.9942912747324036e-06,
      "loss": 2.4627,
      "step": 1327400
    },
    {
      "epoch": 430.30794165316047,
      "grad_norm": 1.407698154449463,
      "learning_rate": 6.991047680830361e-06,
      "loss": 2.4749,
      "step": 1327500
    },
    {
      "epoch": 430.34035656401943,
      "grad_norm": 1.2496381998062134,
      "learning_rate": 6.987804086928317e-06,
      "loss": 2.4756,
      "step": 1327600
    },
    {
      "epoch": 430.37277147487845,
      "grad_norm": 1.3190538883209229,
      "learning_rate": 6.984560493026274e-06,
      "loss": 2.462,
      "step": 1327700
    },
    {
      "epoch": 430.40518638573747,
      "grad_norm": 1.4353814125061035,
      "learning_rate": 6.981316899124229e-06,
      "loss": 2.4529,
      "step": 1327800
    },
    {
      "epoch": 430.4376012965964,
      "grad_norm": 1.340118646621704,
      "learning_rate": 6.978073305222186e-06,
      "loss": 2.4726,
      "step": 1327900
    },
    {
      "epoch": 430.47001620745544,
      "grad_norm": 1.5328351259231567,
      "learning_rate": 6.9748297113201425e-06,
      "loss": 2.4779,
      "step": 1328000
    },
    {
      "epoch": 430.5024311183144,
      "grad_norm": 1.5722838640213013,
      "learning_rate": 6.9715861174180995e-06,
      "loss": 2.4678,
      "step": 1328100
    },
    {
      "epoch": 430.5348460291734,
      "grad_norm": 1.5387996435165405,
      "learning_rate": 6.968342523516056e-06,
      "loss": 2.4756,
      "step": 1328200
    },
    {
      "epoch": 430.56726094003244,
      "grad_norm": 1.384701132774353,
      "learning_rate": 6.965098929614013e-06,
      "loss": 2.4549,
      "step": 1328300
    },
    {
      "epoch": 430.5996758508914,
      "grad_norm": 1.696739673614502,
      "learning_rate": 6.96185533571197e-06,
      "loss": 2.4511,
      "step": 1328400
    },
    {
      "epoch": 430.6320907617504,
      "grad_norm": 1.3088208436965942,
      "learning_rate": 6.958611741809926e-06,
      "loss": 2.434,
      "step": 1328500
    },
    {
      "epoch": 430.6645056726094,
      "grad_norm": 1.395552158355713,
      "learning_rate": 6.955368147907883e-06,
      "loss": 2.4593,
      "step": 1328600
    },
    {
      "epoch": 430.6969205834684,
      "grad_norm": 1.3752318620681763,
      "learning_rate": 6.952156989944859e-06,
      "loss": 2.4658,
      "step": 1328700
    },
    {
      "epoch": 430.7293354943274,
      "grad_norm": 1.3829920291900635,
      "learning_rate": 6.948913396042815e-06,
      "loss": 2.4561,
      "step": 1328800
    },
    {
      "epoch": 430.7617504051864,
      "grad_norm": 1.534347414970398,
      "learning_rate": 6.945669802140772e-06,
      "loss": 2.4647,
      "step": 1328900
    },
    {
      "epoch": 430.7941653160454,
      "grad_norm": 1.3599088191986084,
      "learning_rate": 6.9424262082387295e-06,
      "loss": 2.4534,
      "step": 1329000
    },
    {
      "epoch": 430.82658022690435,
      "grad_norm": 1.4419941902160645,
      "learning_rate": 6.939182614336686e-06,
      "loss": 2.4959,
      "step": 1329100
    },
    {
      "epoch": 430.8589951377634,
      "grad_norm": 1.3165243864059448,
      "learning_rate": 6.935939020434643e-06,
      "loss": 2.4703,
      "step": 1329200
    },
    {
      "epoch": 430.8914100486224,
      "grad_norm": 1.534520149230957,
      "learning_rate": 6.932695426532598e-06,
      "loss": 2.4907,
      "step": 1329300
    },
    {
      "epoch": 430.92382495948135,
      "grad_norm": 1.4977772235870361,
      "learning_rate": 6.929451832630555e-06,
      "loss": 2.4685,
      "step": 1329400
    },
    {
      "epoch": 430.95623987034037,
      "grad_norm": 1.4568610191345215,
      "learning_rate": 6.926208238728511e-06,
      "loss": 2.4665,
      "step": 1329500
    },
    {
      "epoch": 430.98865478119933,
      "grad_norm": 1.4093520641326904,
      "learning_rate": 6.922964644826468e-06,
      "loss": 2.4499,
      "step": 1329600
    },
    {
      "epoch": 431.0,
      "eval_bleu": 1.0348211903704445,
      "eval_loss": 4.268459320068359,
      "eval_runtime": 4.5706,
      "eval_samples_per_second": 107.645,
      "eval_steps_per_second": 1.75,
      "step": 1329635
    },
    {
      "epoch": 431.02106969205835,
      "grad_norm": 1.518129825592041,
      "learning_rate": 6.9197210509244246e-06,
      "loss": 2.4475,
      "step": 1329700
    },
    {
      "epoch": 431.05348460291737,
      "grad_norm": 1.48862624168396,
      "learning_rate": 6.916477457022382e-06,
      "loss": 2.4704,
      "step": 1329800
    },
    {
      "epoch": 431.0858995137763,
      "grad_norm": 1.4637798070907593,
      "learning_rate": 6.913233863120337e-06,
      "loss": 2.4561,
      "step": 1329900
    },
    {
      "epoch": 431.11831442463534,
      "grad_norm": 1.5993812084197998,
      "learning_rate": 6.909990269218294e-06,
      "loss": 2.4575,
      "step": 1330000
    },
    {
      "epoch": 431.1507293354943,
      "grad_norm": 1.5473963022232056,
      "learning_rate": 6.90674667531625e-06,
      "loss": 2.4692,
      "step": 1330100
    },
    {
      "epoch": 431.1831442463533,
      "grad_norm": 1.7650039196014404,
      "learning_rate": 6.903503081414207e-06,
      "loss": 2.4457,
      "step": 1330200
    },
    {
      "epoch": 431.21555915721234,
      "grad_norm": 1.417442798614502,
      "learning_rate": 6.9002594875121635e-06,
      "loss": 2.4625,
      "step": 1330300
    },
    {
      "epoch": 431.2479740680713,
      "grad_norm": 1.4611785411834717,
      "learning_rate": 6.8970158936101205e-06,
      "loss": 2.4715,
      "step": 1330400
    },
    {
      "epoch": 431.2803889789303,
      "grad_norm": 1.4151370525360107,
      "learning_rate": 6.8937722997080776e-06,
      "loss": 2.4521,
      "step": 1330500
    },
    {
      "epoch": 431.3128038897893,
      "grad_norm": 1.5453135967254639,
      "learning_rate": 6.890528705806033e-06,
      "loss": 2.4567,
      "step": 1330600
    },
    {
      "epoch": 431.3452188006483,
      "grad_norm": 1.3919706344604492,
      "learning_rate": 6.887285111903991e-06,
      "loss": 2.4768,
      "step": 1330700
    },
    {
      "epoch": 431.3776337115073,
      "grad_norm": 1.402592420578003,
      "learning_rate": 6.884041518001946e-06,
      "loss": 2.457,
      "step": 1330800
    },
    {
      "epoch": 431.4100486223663,
      "grad_norm": 1.8079304695129395,
      "learning_rate": 6.880797924099903e-06,
      "loss": 2.4584,
      "step": 1330900
    },
    {
      "epoch": 431.4424635332253,
      "grad_norm": 1.2571507692337036,
      "learning_rate": 6.8775543301978594e-06,
      "loss": 2.4662,
      "step": 1331000
    },
    {
      "epoch": 431.47487844408425,
      "grad_norm": 1.397584319114685,
      "learning_rate": 6.8743107362958165e-06,
      "loss": 2.4832,
      "step": 1331100
    },
    {
      "epoch": 431.5072933549433,
      "grad_norm": 1.3631465435028076,
      "learning_rate": 6.871067142393772e-06,
      "loss": 2.4413,
      "step": 1331200
    },
    {
      "epoch": 431.5397082658023,
      "grad_norm": 1.4604167938232422,
      "learning_rate": 6.86782354849173e-06,
      "loss": 2.4571,
      "step": 1331300
    },
    {
      "epoch": 431.57212317666125,
      "grad_norm": 1.4687602519989014,
      "learning_rate": 6.864579954589685e-06,
      "loss": 2.4499,
      "step": 1331400
    },
    {
      "epoch": 431.60453808752027,
      "grad_norm": 1.3840465545654297,
      "learning_rate": 6.861336360687642e-06,
      "loss": 2.4646,
      "step": 1331500
    },
    {
      "epoch": 431.63695299837923,
      "grad_norm": 1.3516793251037598,
      "learning_rate": 6.858092766785598e-06,
      "loss": 2.471,
      "step": 1331600
    },
    {
      "epoch": 431.66936790923825,
      "grad_norm": 1.389054298400879,
      "learning_rate": 6.854849172883555e-06,
      "loss": 2.4612,
      "step": 1331700
    },
    {
      "epoch": 431.70178282009726,
      "grad_norm": 1.3605492115020752,
      "learning_rate": 6.851605578981512e-06,
      "loss": 2.4744,
      "step": 1331800
    },
    {
      "epoch": 431.7341977309562,
      "grad_norm": 1.489820957183838,
      "learning_rate": 6.848361985079469e-06,
      "loss": 2.4388,
      "step": 1331900
    },
    {
      "epoch": 431.76661264181524,
      "grad_norm": 1.3595362901687622,
      "learning_rate": 6.845118391177426e-06,
      "loss": 2.4733,
      "step": 1332000
    },
    {
      "epoch": 431.7990275526742,
      "grad_norm": 1.5145472288131714,
      "learning_rate": 6.841874797275381e-06,
      "loss": 2.4664,
      "step": 1332100
    },
    {
      "epoch": 431.8314424635332,
      "grad_norm": 1.555733323097229,
      "learning_rate": 6.838631203373339e-06,
      "loss": 2.4659,
      "step": 1332200
    },
    {
      "epoch": 431.86385737439224,
      "grad_norm": 1.4646109342575073,
      "learning_rate": 6.835387609471294e-06,
      "loss": 2.4829,
      "step": 1332300
    },
    {
      "epoch": 431.8962722852512,
      "grad_norm": 1.2335420846939087,
      "learning_rate": 6.832144015569251e-06,
      "loss": 2.4616,
      "step": 1332400
    },
    {
      "epoch": 431.9286871961102,
      "grad_norm": 1.382932186126709,
      "learning_rate": 6.8289004216672075e-06,
      "loss": 2.4687,
      "step": 1332500
    },
    {
      "epoch": 431.9611021069692,
      "grad_norm": 1.2555421590805054,
      "learning_rate": 6.825656827765165e-06,
      "loss": 2.4675,
      "step": 1332600
    },
    {
      "epoch": 431.9935170178282,
      "grad_norm": 1.5076706409454346,
      "learning_rate": 6.822445669802141e-06,
      "loss": 2.5068,
      "step": 1332700
    },
    {
      "epoch": 432.0,
      "eval_bleu": 1.1186415519310955,
      "eval_loss": 4.269844055175781,
      "eval_runtime": 4.7874,
      "eval_samples_per_second": 102.77,
      "eval_steps_per_second": 1.671,
      "step": 1332720
    },
    {
      "epoch": 432.0259319286872,
      "grad_norm": 1.4542572498321533,
      "learning_rate": 6.819202075900099e-06,
      "loss": 2.4613,
      "step": 1332800
    },
    {
      "epoch": 432.0583468395462,
      "grad_norm": 1.4639405012130737,
      "learning_rate": 6.815958481998054e-06,
      "loss": 2.4596,
      "step": 1332900
    },
    {
      "epoch": 432.0907617504052,
      "grad_norm": 1.5697225332260132,
      "learning_rate": 6.812714888096011e-06,
      "loss": 2.449,
      "step": 1333000
    },
    {
      "epoch": 432.12317666126415,
      "grad_norm": 1.2715864181518555,
      "learning_rate": 6.809471294193967e-06,
      "loss": 2.4638,
      "step": 1333100
    },
    {
      "epoch": 432.15559157212317,
      "grad_norm": 1.4596095085144043,
      "learning_rate": 6.806227700291924e-06,
      "loss": 2.4865,
      "step": 1333200
    },
    {
      "epoch": 432.1880064829822,
      "grad_norm": 1.4615250825881958,
      "learning_rate": 6.80298410638988e-06,
      "loss": 2.4732,
      "step": 1333300
    },
    {
      "epoch": 432.22042139384115,
      "grad_norm": 1.5322208404541016,
      "learning_rate": 6.7997405124878375e-06,
      "loss": 2.4589,
      "step": 1333400
    },
    {
      "epoch": 432.25283630470017,
      "grad_norm": 1.5868197679519653,
      "learning_rate": 6.796496918585793e-06,
      "loss": 2.4651,
      "step": 1333500
    },
    {
      "epoch": 432.2852512155592,
      "grad_norm": 1.4386444091796875,
      "learning_rate": 6.79325332468375e-06,
      "loss": 2.4503,
      "step": 1333600
    },
    {
      "epoch": 432.31766612641815,
      "grad_norm": 1.3504871129989624,
      "learning_rate": 6.790009730781706e-06,
      "loss": 2.4693,
      "step": 1333700
    },
    {
      "epoch": 432.35008103727716,
      "grad_norm": 1.4965866804122925,
      "learning_rate": 6.786766136879663e-06,
      "loss": 2.4686,
      "step": 1333800
    },
    {
      "epoch": 432.3824959481361,
      "grad_norm": 1.5056790113449097,
      "learning_rate": 6.7835225429776185e-06,
      "loss": 2.4461,
      "step": 1333900
    },
    {
      "epoch": 432.41491085899514,
      "grad_norm": 1.376819133758545,
      "learning_rate": 6.780278949075576e-06,
      "loss": 2.4752,
      "step": 1334000
    },
    {
      "epoch": 432.44732576985416,
      "grad_norm": 1.449933648109436,
      "learning_rate": 6.7770677911125525e-06,
      "loss": 2.4748,
      "step": 1334100
    },
    {
      "epoch": 432.4797406807131,
      "grad_norm": 1.5734751224517822,
      "learning_rate": 6.7738241972105096e-06,
      "loss": 2.4694,
      "step": 1334200
    },
    {
      "epoch": 432.51215559157214,
      "grad_norm": 1.4444632530212402,
      "learning_rate": 6.770580603308466e-06,
      "loss": 2.4611,
      "step": 1334300
    },
    {
      "epoch": 432.5445705024311,
      "grad_norm": 1.521655797958374,
      "learning_rate": 6.767337009406423e-06,
      "loss": 2.461,
      "step": 1334400
    },
    {
      "epoch": 432.5769854132901,
      "grad_norm": 1.3648418188095093,
      "learning_rate": 6.764093415504379e-06,
      "loss": 2.4721,
      "step": 1334500
    },
    {
      "epoch": 432.60940032414914,
      "grad_norm": 1.4596858024597168,
      "learning_rate": 6.760849821602336e-06,
      "loss": 2.461,
      "step": 1334600
    },
    {
      "epoch": 432.6418152350081,
      "grad_norm": 1.4504939317703247,
      "learning_rate": 6.757606227700291e-06,
      "loss": 2.476,
      "step": 1334700
    },
    {
      "epoch": 432.6742301458671,
      "grad_norm": 1.6919405460357666,
      "learning_rate": 6.7543626337982485e-06,
      "loss": 2.4517,
      "step": 1334800
    },
    {
      "epoch": 432.7066450567261,
      "grad_norm": 1.5655925273895264,
      "learning_rate": 6.751119039896206e-06,
      "loss": 2.4913,
      "step": 1334900
    },
    {
      "epoch": 432.7390599675851,
      "grad_norm": 1.401616096496582,
      "learning_rate": 6.747875445994162e-06,
      "loss": 2.4387,
      "step": 1335000
    },
    {
      "epoch": 432.7714748784441,
      "grad_norm": 1.519574761390686,
      "learning_rate": 6.744631852092119e-06,
      "loss": 2.4619,
      "step": 1335100
    },
    {
      "epoch": 432.80388978930307,
      "grad_norm": 1.4678105115890503,
      "learning_rate": 6.741388258190075e-06,
      "loss": 2.4598,
      "step": 1335200
    },
    {
      "epoch": 432.8363047001621,
      "grad_norm": 1.6473551988601685,
      "learning_rate": 6.738144664288032e-06,
      "loss": 2.4423,
      "step": 1335300
    },
    {
      "epoch": 432.86871961102105,
      "grad_norm": 1.5483756065368652,
      "learning_rate": 6.734933506325008e-06,
      "loss": 2.4547,
      "step": 1335400
    },
    {
      "epoch": 432.90113452188007,
      "grad_norm": 1.3611022233963013,
      "learning_rate": 6.731689912422964e-06,
      "loss": 2.4661,
      "step": 1335500
    },
    {
      "epoch": 432.9335494327391,
      "grad_norm": 1.4706077575683594,
      "learning_rate": 6.728446318520921e-06,
      "loss": 2.4805,
      "step": 1335600
    },
    {
      "epoch": 432.96596434359805,
      "grad_norm": 1.5685380697250366,
      "learning_rate": 6.725202724618878e-06,
      "loss": 2.4717,
      "step": 1335700
    },
    {
      "epoch": 432.99837925445706,
      "grad_norm": 1.4427279233932495,
      "learning_rate": 6.721959130716835e-06,
      "loss": 2.4516,
      "step": 1335800
    },
    {
      "epoch": 433.0,
      "eval_bleu": 0.9948015558804085,
      "eval_loss": 4.270742416381836,
      "eval_runtime": 4.4719,
      "eval_samples_per_second": 110.02,
      "eval_steps_per_second": 1.789,
      "step": 1335805
    },
    {
      "epoch": 433.030794165316,
      "grad_norm": 1.3870898485183716,
      "learning_rate": 6.718715536814792e-06,
      "loss": 2.462,
      "step": 1335900
    },
    {
      "epoch": 433.06320907617504,
      "grad_norm": 1.3602484464645386,
      "learning_rate": 6.715471942912748e-06,
      "loss": 2.4614,
      "step": 1336000
    },
    {
      "epoch": 433.09562398703406,
      "grad_norm": 1.6488522291183472,
      "learning_rate": 6.712228349010705e-06,
      "loss": 2.4538,
      "step": 1336100
    },
    {
      "epoch": 433.128038897893,
      "grad_norm": 1.4426038265228271,
      "learning_rate": 6.70898475510866e-06,
      "loss": 2.4654,
      "step": 1336200
    },
    {
      "epoch": 433.16045380875204,
      "grad_norm": 1.5475428104400635,
      "learning_rate": 6.705741161206617e-06,
      "loss": 2.4741,
      "step": 1336300
    },
    {
      "epoch": 433.192868719611,
      "grad_norm": 1.5178635120391846,
      "learning_rate": 6.7024975673045735e-06,
      "loss": 2.4677,
      "step": 1336400
    },
    {
      "epoch": 433.22528363047,
      "grad_norm": 1.3375838994979858,
      "learning_rate": 6.6992539734025306e-06,
      "loss": 2.4589,
      "step": 1336500
    },
    {
      "epoch": 433.25769854132903,
      "grad_norm": 1.399864912033081,
      "learning_rate": 6.696010379500487e-06,
      "loss": 2.4535,
      "step": 1336600
    },
    {
      "epoch": 433.290113452188,
      "grad_norm": 1.403013825416565,
      "learning_rate": 6.692766785598444e-06,
      "loss": 2.4548,
      "step": 1336700
    },
    {
      "epoch": 433.322528363047,
      "grad_norm": 1.227324366569519,
      "learning_rate": 6.689523191696399e-06,
      "loss": 2.4597,
      "step": 1336800
    },
    {
      "epoch": 433.354943273906,
      "grad_norm": 1.5412858724594116,
      "learning_rate": 6.686279597794356e-06,
      "loss": 2.4361,
      "step": 1336900
    },
    {
      "epoch": 433.387358184765,
      "grad_norm": 1.4396699666976929,
      "learning_rate": 6.683036003892312e-06,
      "loss": 2.4727,
      "step": 1337000
    },
    {
      "epoch": 433.419773095624,
      "grad_norm": 1.5070133209228516,
      "learning_rate": 6.6797924099902695e-06,
      "loss": 2.4698,
      "step": 1337100
    },
    {
      "epoch": 433.45218800648297,
      "grad_norm": 1.5532432794570923,
      "learning_rate": 6.6765488160882265e-06,
      "loss": 2.4569,
      "step": 1337200
    },
    {
      "epoch": 433.484602917342,
      "grad_norm": 1.3581902980804443,
      "learning_rate": 6.673305222186183e-06,
      "loss": 2.4406,
      "step": 1337300
    },
    {
      "epoch": 433.51701782820095,
      "grad_norm": 1.4569834470748901,
      "learning_rate": 6.67006162828414e-06,
      "loss": 2.4544,
      "step": 1337400
    },
    {
      "epoch": 433.54943273905997,
      "grad_norm": 1.425665259361267,
      "learning_rate": 6.666818034382095e-06,
      "loss": 2.4629,
      "step": 1337500
    },
    {
      "epoch": 433.581847649919,
      "grad_norm": 1.3712053298950195,
      "learning_rate": 6.663574440480053e-06,
      "loss": 2.4645,
      "step": 1337600
    },
    {
      "epoch": 433.61426256077795,
      "grad_norm": 1.4899235963821411,
      "learning_rate": 6.660330846578008e-06,
      "loss": 2.475,
      "step": 1337700
    },
    {
      "epoch": 433.64667747163696,
      "grad_norm": 1.2366981506347656,
      "learning_rate": 6.6570872526759654e-06,
      "loss": 2.4709,
      "step": 1337800
    },
    {
      "epoch": 433.6790923824959,
      "grad_norm": 1.3051135540008545,
      "learning_rate": 6.653843658773922e-06,
      "loss": 2.4694,
      "step": 1337900
    },
    {
      "epoch": 433.71150729335494,
      "grad_norm": 1.5572082996368408,
      "learning_rate": 6.650600064871879e-06,
      "loss": 2.4729,
      "step": 1338000
    },
    {
      "epoch": 433.74392220421396,
      "grad_norm": 1.5120470523834229,
      "learning_rate": 6.647356470969834e-06,
      "loss": 2.4664,
      "step": 1338100
    },
    {
      "epoch": 433.7763371150729,
      "grad_norm": 1.343607783317566,
      "learning_rate": 6.644112877067792e-06,
      "loss": 2.4693,
      "step": 1338200
    },
    {
      "epoch": 433.80875202593194,
      "grad_norm": 1.5037822723388672,
      "learning_rate": 6.640869283165747e-06,
      "loss": 2.4473,
      "step": 1338300
    },
    {
      "epoch": 433.8411669367909,
      "grad_norm": 1.53544282913208,
      "learning_rate": 6.637625689263704e-06,
      "loss": 2.4622,
      "step": 1338400
    },
    {
      "epoch": 433.8735818476499,
      "grad_norm": 1.4838824272155762,
      "learning_rate": 6.634382095361661e-06,
      "loss": 2.4673,
      "step": 1338500
    },
    {
      "epoch": 433.90599675850893,
      "grad_norm": 1.4695388078689575,
      "learning_rate": 6.631138501459618e-06,
      "loss": 2.4689,
      "step": 1338600
    },
    {
      "epoch": 433.9384116693679,
      "grad_norm": 1.425165057182312,
      "learning_rate": 6.627894907557575e-06,
      "loss": 2.4921,
      "step": 1338700
    },
    {
      "epoch": 433.9708265802269,
      "grad_norm": 1.3340038061141968,
      "learning_rate": 6.624651313655531e-06,
      "loss": 2.4828,
      "step": 1338800
    },
    {
      "epoch": 434.0,
      "eval_bleu": 1.208765607023118,
      "eval_loss": 4.268754005432129,
      "eval_runtime": 4.4002,
      "eval_samples_per_second": 111.814,
      "eval_steps_per_second": 1.818,
      "step": 1338890
    },
    {
      "epoch": 434.0032414910859,
      "grad_norm": 1.335157871246338,
      "learning_rate": 6.621407719753488e-06,
      "loss": 2.4592,
      "step": 1338900
    },
    {
      "epoch": 434.0356564019449,
      "grad_norm": 1.3301587104797363,
      "learning_rate": 6.618164125851443e-06,
      "loss": 2.4716,
      "step": 1339000
    },
    {
      "epoch": 434.0680713128039,
      "grad_norm": 1.353570580482483,
      "learning_rate": 6.6149205319494e-06,
      "loss": 2.4608,
      "step": 1339100
    },
    {
      "epoch": 434.10048622366287,
      "grad_norm": 1.2963531017303467,
      "learning_rate": 6.6116769380473565e-06,
      "loss": 2.4516,
      "step": 1339200
    },
    {
      "epoch": 434.1329011345219,
      "grad_norm": 1.6489907503128052,
      "learning_rate": 6.6084333441453135e-06,
      "loss": 2.4704,
      "step": 1339300
    },
    {
      "epoch": 434.16531604538085,
      "grad_norm": 1.4176359176635742,
      "learning_rate": 6.60518975024327e-06,
      "loss": 2.4616,
      "step": 1339400
    },
    {
      "epoch": 434.19773095623987,
      "grad_norm": 1.5132249593734741,
      "learning_rate": 6.601946156341227e-06,
      "loss": 2.4521,
      "step": 1339500
    },
    {
      "epoch": 434.2301458670989,
      "grad_norm": 1.5539977550506592,
      "learning_rate": 6.598702562439182e-06,
      "loss": 2.4608,
      "step": 1339600
    },
    {
      "epoch": 434.26256077795784,
      "grad_norm": 1.5323504209518433,
      "learning_rate": 6.595458968537139e-06,
      "loss": 2.464,
      "step": 1339700
    },
    {
      "epoch": 434.29497568881686,
      "grad_norm": 1.303683876991272,
      "learning_rate": 6.592215374635095e-06,
      "loss": 2.4612,
      "step": 1339800
    },
    {
      "epoch": 434.3273905996758,
      "grad_norm": 1.5244592428207397,
      "learning_rate": 6.5889717807330524e-06,
      "loss": 2.4721,
      "step": 1339900
    },
    {
      "epoch": 434.35980551053484,
      "grad_norm": 1.3594104051589966,
      "learning_rate": 6.5857281868310095e-06,
      "loss": 2.4478,
      "step": 1340000
    },
    {
      "epoch": 434.39222042139386,
      "grad_norm": 1.608876347541809,
      "learning_rate": 6.582484592928966e-06,
      "loss": 2.4451,
      "step": 1340100
    },
    {
      "epoch": 434.4246353322528,
      "grad_norm": 1.3365554809570312,
      "learning_rate": 6.579240999026923e-06,
      "loss": 2.4749,
      "step": 1340200
    },
    {
      "epoch": 434.45705024311184,
      "grad_norm": 1.609732747077942,
      "learning_rate": 6.575997405124878e-06,
      "loss": 2.4644,
      "step": 1340300
    },
    {
      "epoch": 434.48946515397085,
      "grad_norm": 1.25289785861969,
      "learning_rate": 6.572753811222836e-06,
      "loss": 2.4655,
      "step": 1340400
    },
    {
      "epoch": 434.5218800648298,
      "grad_norm": 1.5488622188568115,
      "learning_rate": 6.569510217320791e-06,
      "loss": 2.4423,
      "step": 1340500
    },
    {
      "epoch": 434.55429497568883,
      "grad_norm": 1.3091059923171997,
      "learning_rate": 6.566266623418748e-06,
      "loss": 2.4803,
      "step": 1340600
    },
    {
      "epoch": 434.5867098865478,
      "grad_norm": 1.3706700801849365,
      "learning_rate": 6.563023029516705e-06,
      "loss": 2.4407,
      "step": 1340700
    },
    {
      "epoch": 434.6191247974068,
      "grad_norm": 1.3548458814620972,
      "learning_rate": 6.559779435614662e-06,
      "loss": 2.4485,
      "step": 1340800
    },
    {
      "epoch": 434.65153970826583,
      "grad_norm": 1.7392512559890747,
      "learning_rate": 6.556535841712618e-06,
      "loss": 2.4631,
      "step": 1340900
    },
    {
      "epoch": 434.6839546191248,
      "grad_norm": 1.3760926723480225,
      "learning_rate": 6.553292247810575e-06,
      "loss": 2.4616,
      "step": 1341000
    },
    {
      "epoch": 434.7163695299838,
      "grad_norm": 1.5682681798934937,
      "learning_rate": 6.55004865390853e-06,
      "loss": 2.4737,
      "step": 1341100
    },
    {
      "epoch": 434.74878444084277,
      "grad_norm": 1.3188947439193726,
      "learning_rate": 6.546805060006487e-06,
      "loss": 2.4816,
      "step": 1341200
    },
    {
      "epoch": 434.7811993517018,
      "grad_norm": 1.5031870603561401,
      "learning_rate": 6.5435614661044435e-06,
      "loss": 2.4436,
      "step": 1341300
    },
    {
      "epoch": 434.8136142625608,
      "grad_norm": 1.6173062324523926,
      "learning_rate": 6.540350308141421e-06,
      "loss": 2.4569,
      "step": 1341400
    },
    {
      "epoch": 434.84602917341977,
      "grad_norm": 1.3745489120483398,
      "learning_rate": 6.5371067142393775e-06,
      "loss": 2.4886,
      "step": 1341500
    },
    {
      "epoch": 434.8784440842788,
      "grad_norm": 1.7156153917312622,
      "learning_rate": 6.5338631203373345e-06,
      "loss": 2.476,
      "step": 1341600
    },
    {
      "epoch": 434.91085899513774,
      "grad_norm": 1.4883580207824707,
      "learning_rate": 6.53061952643529e-06,
      "loss": 2.4672,
      "step": 1341700
    },
    {
      "epoch": 434.94327390599676,
      "grad_norm": 1.4211477041244507,
      "learning_rate": 6.527375932533247e-06,
      "loss": 2.4504,
      "step": 1341800
    },
    {
      "epoch": 434.9756888168558,
      "grad_norm": 1.3782895803451538,
      "learning_rate": 6.524132338631203e-06,
      "loss": 2.4643,
      "step": 1341900
    },
    {
      "epoch": 435.0,
      "eval_bleu": 1.2571251047824947,
      "eval_loss": 4.273865699768066,
      "eval_runtime": 4.2787,
      "eval_samples_per_second": 114.989,
      "eval_steps_per_second": 1.87,
      "step": 1341975
    },
    {
      "epoch": 435.00810372771474,
      "grad_norm": 1.3338240385055542,
      "learning_rate": 6.52088874472916e-06,
      "loss": 2.4679,
      "step": 1342000
    },
    {
      "epoch": 435.04051863857376,
      "grad_norm": 1.4690250158309937,
      "learning_rate": 6.517645150827116e-06,
      "loss": 2.4789,
      "step": 1342100
    },
    {
      "epoch": 435.0729335494327,
      "grad_norm": 1.3290600776672363,
      "learning_rate": 6.5144015569250735e-06,
      "loss": 2.459,
      "step": 1342200
    },
    {
      "epoch": 435.10534846029174,
      "grad_norm": 1.4059572219848633,
      "learning_rate": 6.5111579630230305e-06,
      "loss": 2.4702,
      "step": 1342300
    },
    {
      "epoch": 435.13776337115075,
      "grad_norm": 1.4415881633758545,
      "learning_rate": 6.507914369120986e-06,
      "loss": 2.4641,
      "step": 1342400
    },
    {
      "epoch": 435.1701782820097,
      "grad_norm": 1.403672456741333,
      "learning_rate": 6.504670775218944e-06,
      "loss": 2.4614,
      "step": 1342500
    },
    {
      "epoch": 435.20259319286873,
      "grad_norm": 1.512609601020813,
      "learning_rate": 6.501427181316899e-06,
      "loss": 2.4778,
      "step": 1342600
    },
    {
      "epoch": 435.2350081037277,
      "grad_norm": 1.5074044466018677,
      "learning_rate": 6.498183587414856e-06,
      "loss": 2.4594,
      "step": 1342700
    },
    {
      "epoch": 435.2674230145867,
      "grad_norm": 1.563359260559082,
      "learning_rate": 6.494939993512812e-06,
      "loss": 2.4544,
      "step": 1342800
    },
    {
      "epoch": 435.29983792544573,
      "grad_norm": 1.521247386932373,
      "learning_rate": 6.491696399610769e-06,
      "loss": 2.4553,
      "step": 1342900
    },
    {
      "epoch": 435.3322528363047,
      "grad_norm": 1.4307085275650024,
      "learning_rate": 6.488452805708725e-06,
      "loss": 2.4801,
      "step": 1343000
    },
    {
      "epoch": 435.3646677471637,
      "grad_norm": 1.7148303985595703,
      "learning_rate": 6.485241647745703e-06,
      "loss": 2.4734,
      "step": 1343100
    },
    {
      "epoch": 435.39708265802267,
      "grad_norm": 1.4008108377456665,
      "learning_rate": 6.481998053843659e-06,
      "loss": 2.448,
      "step": 1343200
    },
    {
      "epoch": 435.4294975688817,
      "grad_norm": 1.470229983329773,
      "learning_rate": 6.478754459941616e-06,
      "loss": 2.4752,
      "step": 1343300
    },
    {
      "epoch": 435.4619124797407,
      "grad_norm": 1.568272352218628,
      "learning_rate": 6.475510866039572e-06,
      "loss": 2.4605,
      "step": 1343400
    },
    {
      "epoch": 435.49432739059966,
      "grad_norm": 1.3956948518753052,
      "learning_rate": 6.472267272137529e-06,
      "loss": 2.4502,
      "step": 1343500
    },
    {
      "epoch": 435.5267423014587,
      "grad_norm": 1.4577513933181763,
      "learning_rate": 6.469023678235485e-06,
      "loss": 2.471,
      "step": 1343600
    },
    {
      "epoch": 435.55915721231764,
      "grad_norm": 1.3911104202270508,
      "learning_rate": 6.465780084333442e-06,
      "loss": 2.4658,
      "step": 1343700
    },
    {
      "epoch": 435.59157212317666,
      "grad_norm": 1.4977455139160156,
      "learning_rate": 6.462536490431398e-06,
      "loss": 2.4485,
      "step": 1343800
    },
    {
      "epoch": 435.6239870340357,
      "grad_norm": 1.330367088317871,
      "learning_rate": 6.459292896529355e-06,
      "loss": 2.4692,
      "step": 1343900
    },
    {
      "epoch": 435.65640194489464,
      "grad_norm": 1.4032666683197021,
      "learning_rate": 6.456049302627311e-06,
      "loss": 2.4694,
      "step": 1344000
    },
    {
      "epoch": 435.68881685575366,
      "grad_norm": 1.2801214456558228,
      "learning_rate": 6.452805708725268e-06,
      "loss": 2.4591,
      "step": 1344100
    },
    {
      "epoch": 435.7212317666126,
      "grad_norm": 1.3258843421936035,
      "learning_rate": 6.449562114823224e-06,
      "loss": 2.4547,
      "step": 1344200
    },
    {
      "epoch": 435.75364667747164,
      "grad_norm": 1.2512425184249878,
      "learning_rate": 6.446318520921181e-06,
      "loss": 2.4672,
      "step": 1344300
    },
    {
      "epoch": 435.78606158833065,
      "grad_norm": 1.5690524578094482,
      "learning_rate": 6.443074927019138e-06,
      "loss": 2.4612,
      "step": 1344400
    },
    {
      "epoch": 435.8184764991896,
      "grad_norm": 1.315354347229004,
      "learning_rate": 6.439831333117094e-06,
      "loss": 2.4582,
      "step": 1344500
    },
    {
      "epoch": 435.85089141004863,
      "grad_norm": 1.6325607299804688,
      "learning_rate": 6.4365877392150515e-06,
      "loss": 2.4609,
      "step": 1344600
    },
    {
      "epoch": 435.8833063209076,
      "grad_norm": 1.5506962537765503,
      "learning_rate": 6.433344145313007e-06,
      "loss": 2.4598,
      "step": 1344700
    },
    {
      "epoch": 435.9157212317666,
      "grad_norm": 1.4569350481033325,
      "learning_rate": 6.430100551410964e-06,
      "loss": 2.4492,
      "step": 1344800
    },
    {
      "epoch": 435.94813614262563,
      "grad_norm": 1.2944926023483276,
      "learning_rate": 6.42685695750892e-06,
      "loss": 2.4597,
      "step": 1344900
    },
    {
      "epoch": 435.9805510534846,
      "grad_norm": 1.52564537525177,
      "learning_rate": 6.423613363606877e-06,
      "loss": 2.4618,
      "step": 1345000
    },
    {
      "epoch": 436.0,
      "eval_bleu": 1.2803644670200145,
      "eval_loss": 4.275569915771484,
      "eval_runtime": 4.4859,
      "eval_samples_per_second": 109.676,
      "eval_steps_per_second": 1.783,
      "step": 1345060
    },
    {
      "epoch": 436.0129659643436,
      "grad_norm": 1.5514476299285889,
      "learning_rate": 6.4203697697048325e-06,
      "loss": 2.4568,
      "step": 1345100
    },
    {
      "epoch": 436.04538087520257,
      "grad_norm": 1.4069701433181763,
      "learning_rate": 6.41712617580279e-06,
      "loss": 2.4373,
      "step": 1345200
    },
    {
      "epoch": 436.0777957860616,
      "grad_norm": 1.5047554969787598,
      "learning_rate": 6.413882581900746e-06,
      "loss": 2.442,
      "step": 1345300
    },
    {
      "epoch": 436.1102106969206,
      "grad_norm": 1.6254534721374512,
      "learning_rate": 6.410638987998703e-06,
      "loss": 2.4725,
      "step": 1345400
    },
    {
      "epoch": 436.14262560777956,
      "grad_norm": 1.5454410314559937,
      "learning_rate": 6.407395394096659e-06,
      "loss": 2.4659,
      "step": 1345500
    },
    {
      "epoch": 436.1750405186386,
      "grad_norm": 1.280631184577942,
      "learning_rate": 6.404151800194616e-06,
      "loss": 2.4703,
      "step": 1345600
    },
    {
      "epoch": 436.20745542949754,
      "grad_norm": 1.5254671573638916,
      "learning_rate": 6.4009082062925714e-06,
      "loss": 2.455,
      "step": 1345700
    },
    {
      "epoch": 436.23987034035656,
      "grad_norm": 1.4015014171600342,
      "learning_rate": 6.397664612390529e-06,
      "loss": 2.4638,
      "step": 1345800
    },
    {
      "epoch": 436.2722852512156,
      "grad_norm": 1.3704383373260498,
      "learning_rate": 6.394421018488486e-06,
      "loss": 2.4562,
      "step": 1345900
    },
    {
      "epoch": 436.30470016207454,
      "grad_norm": 1.1705305576324463,
      "learning_rate": 6.391177424586442e-06,
      "loss": 2.4831,
      "step": 1346000
    },
    {
      "epoch": 436.33711507293356,
      "grad_norm": 1.6159015893936157,
      "learning_rate": 6.387933830684399e-06,
      "loss": 2.4229,
      "step": 1346100
    },
    {
      "epoch": 436.3695299837925,
      "grad_norm": 1.5438884496688843,
      "learning_rate": 6.384690236782355e-06,
      "loss": 2.4627,
      "step": 1346200
    },
    {
      "epoch": 436.40194489465154,
      "grad_norm": 1.2253283262252808,
      "learning_rate": 6.381446642880312e-06,
      "loss": 2.4548,
      "step": 1346300
    },
    {
      "epoch": 436.43435980551055,
      "grad_norm": 1.425491452217102,
      "learning_rate": 6.378203048978268e-06,
      "loss": 2.5122,
      "step": 1346400
    },
    {
      "epoch": 436.4667747163695,
      "grad_norm": 1.5328775644302368,
      "learning_rate": 6.374959455076225e-06,
      "loss": 2.4482,
      "step": 1346500
    },
    {
      "epoch": 436.49918962722853,
      "grad_norm": 1.3266526460647583,
      "learning_rate": 6.371715861174181e-06,
      "loss": 2.4563,
      "step": 1346600
    },
    {
      "epoch": 436.5316045380875,
      "grad_norm": 1.417981743812561,
      "learning_rate": 6.368504703211159e-06,
      "loss": 2.4882,
      "step": 1346700
    },
    {
      "epoch": 436.5640194489465,
      "grad_norm": 1.4145663976669312,
      "learning_rate": 6.365261109309115e-06,
      "loss": 2.4656,
      "step": 1346800
    },
    {
      "epoch": 436.5964343598055,
      "grad_norm": 1.549392819404602,
      "learning_rate": 6.362017515407072e-06,
      "loss": 2.4776,
      "step": 1346900
    },
    {
      "epoch": 436.6288492706645,
      "grad_norm": 1.5205538272857666,
      "learning_rate": 6.358773921505028e-06,
      "loss": 2.4522,
      "step": 1347000
    },
    {
      "epoch": 436.6612641815235,
      "grad_norm": 1.2632015943527222,
      "learning_rate": 6.355530327602985e-06,
      "loss": 2.4629,
      "step": 1347100
    },
    {
      "epoch": 436.6936790923825,
      "grad_norm": 1.2496047019958496,
      "learning_rate": 6.35228673370094e-06,
      "loss": 2.4588,
      "step": 1347200
    },
    {
      "epoch": 436.7260940032415,
      "grad_norm": 1.5683611631393433,
      "learning_rate": 6.349043139798898e-06,
      "loss": 2.4566,
      "step": 1347300
    },
    {
      "epoch": 436.7585089141005,
      "grad_norm": 1.7118480205535889,
      "learning_rate": 6.3457995458968535e-06,
      "loss": 2.4544,
      "step": 1347400
    },
    {
      "epoch": 436.79092382495946,
      "grad_norm": 1.421337366104126,
      "learning_rate": 6.342555951994811e-06,
      "loss": 2.4557,
      "step": 1347500
    },
    {
      "epoch": 436.8233387358185,
      "grad_norm": 1.529062032699585,
      "learning_rate": 6.339312358092767e-06,
      "loss": 2.46,
      "step": 1347600
    },
    {
      "epoch": 436.8557536466775,
      "grad_norm": 1.4394022226333618,
      "learning_rate": 6.336068764190724e-06,
      "loss": 2.4665,
      "step": 1347700
    },
    {
      "epoch": 436.88816855753646,
      "grad_norm": 1.344852328300476,
      "learning_rate": 6.332825170288679e-06,
      "loss": 2.4629,
      "step": 1347800
    },
    {
      "epoch": 436.9205834683955,
      "grad_norm": 1.3756331205368042,
      "learning_rate": 6.329581576386637e-06,
      "loss": 2.4727,
      "step": 1347900
    },
    {
      "epoch": 436.95299837925444,
      "grad_norm": 1.4098714590072632,
      "learning_rate": 6.326337982484594e-06,
      "loss": 2.4654,
      "step": 1348000
    },
    {
      "epoch": 436.98541329011346,
      "grad_norm": 1.6222230195999146,
      "learning_rate": 6.3230943885825495e-06,
      "loss": 2.4428,
      "step": 1348100
    },
    {
      "epoch": 437.0,
      "eval_bleu": 1.1573239171606688,
      "eval_loss": 4.2737650871276855,
      "eval_runtime": 4.2851,
      "eval_samples_per_second": 114.816,
      "eval_steps_per_second": 1.867,
      "step": 1348145
    },
    {
      "epoch": 437.0178282009725,
      "grad_norm": 1.496333122253418,
      "learning_rate": 6.3198507946805065e-06,
      "loss": 2.4448,
      "step": 1348200
    },
    {
      "epoch": 437.05024311183143,
      "grad_norm": 1.5893666744232178,
      "learning_rate": 6.316607200778463e-06,
      "loss": 2.4722,
      "step": 1348300
    },
    {
      "epoch": 437.08265802269045,
      "grad_norm": 1.4430652856826782,
      "learning_rate": 6.31336360687642e-06,
      "loss": 2.459,
      "step": 1348400
    },
    {
      "epoch": 437.1150729335494,
      "grad_norm": 1.341248869895935,
      "learning_rate": 6.310120012974376e-06,
      "loss": 2.446,
      "step": 1348500
    },
    {
      "epoch": 437.14748784440843,
      "grad_norm": 1.9781841039657593,
      "learning_rate": 6.306876419072333e-06,
      "loss": 2.4366,
      "step": 1348600
    },
    {
      "epoch": 437.17990275526745,
      "grad_norm": 1.3428161144256592,
      "learning_rate": 6.303632825170288e-06,
      "loss": 2.4599,
      "step": 1348700
    },
    {
      "epoch": 437.2123176661264,
      "grad_norm": 1.319553256034851,
      "learning_rate": 6.3003892312682454e-06,
      "loss": 2.4737,
      "step": 1348800
    },
    {
      "epoch": 437.2447325769854,
      "grad_norm": 1.466808795928955,
      "learning_rate": 6.297145637366202e-06,
      "loss": 2.4876,
      "step": 1348900
    },
    {
      "epoch": 437.2771474878444,
      "grad_norm": 1.4726037979125977,
      "learning_rate": 6.293902043464159e-06,
      "loss": 2.4479,
      "step": 1349000
    },
    {
      "epoch": 437.3095623987034,
      "grad_norm": 1.448775053024292,
      "learning_rate": 6.290658449562115e-06,
      "loss": 2.4572,
      "step": 1349100
    },
    {
      "epoch": 437.3419773095624,
      "grad_norm": 1.353452444076538,
      "learning_rate": 6.287414855660072e-06,
      "loss": 2.4604,
      "step": 1349200
    },
    {
      "epoch": 437.3743922204214,
      "grad_norm": 1.3159469366073608,
      "learning_rate": 6.284171261758027e-06,
      "loss": 2.4465,
      "step": 1349300
    },
    {
      "epoch": 437.4068071312804,
      "grad_norm": 1.3321844339370728,
      "learning_rate": 6.280927667855984e-06,
      "loss": 2.4493,
      "step": 1349400
    },
    {
      "epoch": 437.43922204213936,
      "grad_norm": 1.2617803812026978,
      "learning_rate": 6.277684073953942e-06,
      "loss": 2.4498,
      "step": 1349500
    },
    {
      "epoch": 437.4716369529984,
      "grad_norm": 1.4502865076065063,
      "learning_rate": 6.274440480051898e-06,
      "loss": 2.433,
      "step": 1349600
    },
    {
      "epoch": 437.5040518638574,
      "grad_norm": 1.2509502172470093,
      "learning_rate": 6.271196886149855e-06,
      "loss": 2.4529,
      "step": 1349700
    },
    {
      "epoch": 437.53646677471636,
      "grad_norm": 1.3215079307556152,
      "learning_rate": 6.267953292247811e-06,
      "loss": 2.4658,
      "step": 1349800
    },
    {
      "epoch": 437.5688816855754,
      "grad_norm": 1.3248437643051147,
      "learning_rate": 6.264709698345768e-06,
      "loss": 2.4797,
      "step": 1349900
    },
    {
      "epoch": 437.60129659643434,
      "grad_norm": 1.4544353485107422,
      "learning_rate": 6.261466104443723e-06,
      "loss": 2.4755,
      "step": 1350000
    },
    {
      "epoch": 437.63371150729336,
      "grad_norm": 1.2417582273483276,
      "learning_rate": 6.258222510541681e-06,
      "loss": 2.4682,
      "step": 1350100
    },
    {
      "epoch": 437.6661264181524,
      "grad_norm": 1.3274061679840088,
      "learning_rate": 6.2549789166396365e-06,
      "loss": 2.4676,
      "step": 1350200
    },
    {
      "epoch": 437.69854132901133,
      "grad_norm": 1.5156842470169067,
      "learning_rate": 6.2517353227375936e-06,
      "loss": 2.4527,
      "step": 1350300
    },
    {
      "epoch": 437.73095623987035,
      "grad_norm": 1.4659497737884521,
      "learning_rate": 6.248491728835551e-06,
      "loss": 2.4563,
      "step": 1350400
    },
    {
      "epoch": 437.7633711507293,
      "grad_norm": 1.3470301628112793,
      "learning_rate": 6.245248134933507e-06,
      "loss": 2.4759,
      "step": 1350500
    },
    {
      "epoch": 437.79578606158833,
      "grad_norm": 1.5354481935501099,
      "learning_rate": 6.242004541031463e-06,
      "loss": 2.4522,
      "step": 1350600
    },
    {
      "epoch": 437.82820097244735,
      "grad_norm": 1.5345481634140015,
      "learning_rate": 6.23879338306844e-06,
      "loss": 2.459,
      "step": 1350700
    },
    {
      "epoch": 437.8606158833063,
      "grad_norm": 1.5556172132492065,
      "learning_rate": 6.235549789166396e-06,
      "loss": 2.4736,
      "step": 1350800
    },
    {
      "epoch": 437.8930307941653,
      "grad_norm": 1.6142629384994507,
      "learning_rate": 6.232306195264353e-06,
      "loss": 2.4725,
      "step": 1350900
    },
    {
      "epoch": 437.9254457050243,
      "grad_norm": 1.2998524904251099,
      "learning_rate": 6.22906260136231e-06,
      "loss": 2.4753,
      "step": 1351000
    },
    {
      "epoch": 437.9578606158833,
      "grad_norm": 1.4819883108139038,
      "learning_rate": 6.2258190074602665e-06,
      "loss": 2.4645,
      "step": 1351100
    },
    {
      "epoch": 437.9902755267423,
      "grad_norm": 1.619184970855713,
      "learning_rate": 6.222575413558223e-06,
      "loss": 2.4791,
      "step": 1351200
    },
    {
      "epoch": 438.0,
      "eval_bleu": 0.9695571260683304,
      "eval_loss": 4.274360179901123,
      "eval_runtime": 4.6351,
      "eval_samples_per_second": 106.147,
      "eval_steps_per_second": 1.726,
      "step": 1351230
    },
    {
      "epoch": 438.0226904376013,
      "grad_norm": 1.2761696577072144,
      "learning_rate": 6.21933181965618e-06,
      "loss": 2.4563,
      "step": 1351300
    },
    {
      "epoch": 438.0551053484603,
      "grad_norm": 1.6559109687805176,
      "learning_rate": 6.216088225754136e-06,
      "loss": 2.4494,
      "step": 1351400
    },
    {
      "epoch": 438.08752025931926,
      "grad_norm": 1.4857815504074097,
      "learning_rate": 6.212844631852092e-06,
      "loss": 2.4813,
      "step": 1351500
    },
    {
      "epoch": 438.1199351701783,
      "grad_norm": 1.8012504577636719,
      "learning_rate": 6.209601037950049e-06,
      "loss": 2.4442,
      "step": 1351600
    },
    {
      "epoch": 438.1523500810373,
      "grad_norm": 1.6188048124313354,
      "learning_rate": 6.206357444048005e-06,
      "loss": 2.4651,
      "step": 1351700
    },
    {
      "epoch": 438.18476499189626,
      "grad_norm": 1.3057316541671753,
      "learning_rate": 6.2031138501459616e-06,
      "loss": 2.4549,
      "step": 1351800
    },
    {
      "epoch": 438.2171799027553,
      "grad_norm": 1.2264890670776367,
      "learning_rate": 6.199870256243919e-06,
      "loss": 2.4818,
      "step": 1351900
    },
    {
      "epoch": 438.24959481361424,
      "grad_norm": 1.3997700214385986,
      "learning_rate": 6.196626662341875e-06,
      "loss": 2.443,
      "step": 1352000
    },
    {
      "epoch": 438.28200972447326,
      "grad_norm": 1.3056564331054688,
      "learning_rate": 6.193383068439831e-06,
      "loss": 2.4713,
      "step": 1352100
    },
    {
      "epoch": 438.3144246353323,
      "grad_norm": 1.5171769857406616,
      "learning_rate": 6.190139474537788e-06,
      "loss": 2.4638,
      "step": 1352200
    },
    {
      "epoch": 438.34683954619123,
      "grad_norm": 1.3815921545028687,
      "learning_rate": 6.186895880635744e-06,
      "loss": 2.4634,
      "step": 1352300
    },
    {
      "epoch": 438.37925445705025,
      "grad_norm": 1.6253103017807007,
      "learning_rate": 6.183652286733701e-06,
      "loss": 2.493,
      "step": 1352400
    },
    {
      "epoch": 438.4116693679092,
      "grad_norm": 1.4957354068756104,
      "learning_rate": 6.180408692831658e-06,
      "loss": 2.4754,
      "step": 1352500
    },
    {
      "epoch": 438.44408427876823,
      "grad_norm": 1.4671635627746582,
      "learning_rate": 6.1771650989296146e-06,
      "loss": 2.4756,
      "step": 1352600
    },
    {
      "epoch": 438.47649918962725,
      "grad_norm": 1.3183512687683105,
      "learning_rate": 6.1739539409665915e-06,
      "loss": 2.4642,
      "step": 1352700
    },
    {
      "epoch": 438.5089141004862,
      "grad_norm": 1.6354798078536987,
      "learning_rate": 6.170710347064548e-06,
      "loss": 2.4574,
      "step": 1352800
    },
    {
      "epoch": 438.5413290113452,
      "grad_norm": 1.262386441230774,
      "learning_rate": 6.167466753162504e-06,
      "loss": 2.4683,
      "step": 1352900
    },
    {
      "epoch": 438.5737439222042,
      "grad_norm": 1.5004026889801025,
      "learning_rate": 6.164223159260461e-06,
      "loss": 2.4373,
      "step": 1353000
    },
    {
      "epoch": 438.6061588330632,
      "grad_norm": 1.4483911991119385,
      "learning_rate": 6.160979565358417e-06,
      "loss": 2.4715,
      "step": 1353100
    },
    {
      "epoch": 438.6385737439222,
      "grad_norm": 1.456550121307373,
      "learning_rate": 6.157735971456374e-06,
      "loss": 2.4448,
      "step": 1353200
    },
    {
      "epoch": 438.6709886547812,
      "grad_norm": 1.3437665700912476,
      "learning_rate": 6.1544923775543304e-06,
      "loss": 2.4594,
      "step": 1353300
    },
    {
      "epoch": 438.7034035656402,
      "grad_norm": 1.5451900959014893,
      "learning_rate": 6.1512487836522875e-06,
      "loss": 2.4525,
      "step": 1353400
    },
    {
      "epoch": 438.73581847649916,
      "grad_norm": 1.4843883514404297,
      "learning_rate": 6.148005189750244e-06,
      "loss": 2.4683,
      "step": 1353500
    },
    {
      "epoch": 438.7682333873582,
      "grad_norm": 1.2573647499084473,
      "learning_rate": 6.1447615958482e-06,
      "loss": 2.4595,
      "step": 1353600
    },
    {
      "epoch": 438.8006482982172,
      "grad_norm": 1.3509867191314697,
      "learning_rate": 6.141518001946157e-06,
      "loss": 2.4685,
      "step": 1353700
    },
    {
      "epoch": 438.83306320907616,
      "grad_norm": 1.4617151021957397,
      "learning_rate": 6.138306843983133e-06,
      "loss": 2.4551,
      "step": 1353800
    },
    {
      "epoch": 438.8654781199352,
      "grad_norm": 1.414230227470398,
      "learning_rate": 6.13506325008109e-06,
      "loss": 2.4635,
      "step": 1353900
    },
    {
      "epoch": 438.8978930307942,
      "grad_norm": 1.2229267358779907,
      "learning_rate": 6.131819656179047e-06,
      "loss": 2.4512,
      "step": 1354000
    },
    {
      "epoch": 438.93030794165315,
      "grad_norm": 1.326993465423584,
      "learning_rate": 6.128576062277003e-06,
      "loss": 2.47,
      "step": 1354100
    },
    {
      "epoch": 438.9627228525122,
      "grad_norm": 1.5504647493362427,
      "learning_rate": 6.1253324683749595e-06,
      "loss": 2.4551,
      "step": 1354200
    },
    {
      "epoch": 438.99513776337113,
      "grad_norm": 1.6444733142852783,
      "learning_rate": 6.122088874472917e-06,
      "loss": 2.4471,
      "step": 1354300
    },
    {
      "epoch": 439.0,
      "eval_bleu": 1.0535230704323502,
      "eval_loss": 4.273537635803223,
      "eval_runtime": 4.4963,
      "eval_samples_per_second": 109.423,
      "eval_steps_per_second": 1.779,
      "step": 1354315
    },
    {
      "epoch": 439.02755267423015,
      "grad_norm": 1.5966496467590332,
      "learning_rate": 6.118845280570873e-06,
      "loss": 2.4433,
      "step": 1354400
    },
    {
      "epoch": 439.05996758508917,
      "grad_norm": 1.6114847660064697,
      "learning_rate": 6.11563412260785e-06,
      "loss": 2.4638,
      "step": 1354500
    },
    {
      "epoch": 439.09238249594813,
      "grad_norm": 1.5579267740249634,
      "learning_rate": 6.112390528705806e-06,
      "loss": 2.4614,
      "step": 1354600
    },
    {
      "epoch": 439.12479740680715,
      "grad_norm": 1.6065349578857422,
      "learning_rate": 6.109146934803763e-06,
      "loss": 2.4481,
      "step": 1354700
    },
    {
      "epoch": 439.1572123176661,
      "grad_norm": 1.3897947072982788,
      "learning_rate": 6.10590334090172e-06,
      "loss": 2.4382,
      "step": 1354800
    },
    {
      "epoch": 439.1896272285251,
      "grad_norm": 1.7231038808822632,
      "learning_rate": 6.102659746999676e-06,
      "loss": 2.4754,
      "step": 1354900
    },
    {
      "epoch": 439.22204213938414,
      "grad_norm": 1.3755919933319092,
      "learning_rate": 6.0994161530976324e-06,
      "loss": 2.4666,
      "step": 1355000
    },
    {
      "epoch": 439.2544570502431,
      "grad_norm": 1.3351415395736694,
      "learning_rate": 6.0961725591955895e-06,
      "loss": 2.4463,
      "step": 1355100
    },
    {
      "epoch": 439.2868719611021,
      "grad_norm": 1.3604923486709595,
      "learning_rate": 6.092928965293546e-06,
      "loss": 2.4581,
      "step": 1355200
    },
    {
      "epoch": 439.3192868719611,
      "grad_norm": 1.305005669593811,
      "learning_rate": 6.089685371391502e-06,
      "loss": 2.4526,
      "step": 1355300
    },
    {
      "epoch": 439.3517017828201,
      "grad_norm": 1.3336275815963745,
      "learning_rate": 6.086441777489459e-06,
      "loss": 2.4382,
      "step": 1355400
    },
    {
      "epoch": 439.3841166936791,
      "grad_norm": 1.2992745637893677,
      "learning_rate": 6.083198183587415e-06,
      "loss": 2.4509,
      "step": 1355500
    },
    {
      "epoch": 439.4165316045381,
      "grad_norm": 1.5191285610198975,
      "learning_rate": 6.079954589685371e-06,
      "loss": 2.4446,
      "step": 1355600
    },
    {
      "epoch": 439.4489465153971,
      "grad_norm": 1.451621174812317,
      "learning_rate": 6.076710995783328e-06,
      "loss": 2.4394,
      "step": 1355700
    },
    {
      "epoch": 439.48136142625606,
      "grad_norm": 1.428134560585022,
      "learning_rate": 6.073467401881285e-06,
      "loss": 2.4725,
      "step": 1355800
    },
    {
      "epoch": 439.5137763371151,
      "grad_norm": 1.4569694995880127,
      "learning_rate": 6.070223807979241e-06,
      "loss": 2.4556,
      "step": 1355900
    },
    {
      "epoch": 439.5461912479741,
      "grad_norm": 1.4741184711456299,
      "learning_rate": 6.066980214077198e-06,
      "loss": 2.4628,
      "step": 1356000
    },
    {
      "epoch": 439.57860615883305,
      "grad_norm": 1.4680490493774414,
      "learning_rate": 6.063736620175155e-06,
      "loss": 2.4831,
      "step": 1356100
    },
    {
      "epoch": 439.6110210696921,
      "grad_norm": 1.5138664245605469,
      "learning_rate": 6.060493026273111e-06,
      "loss": 2.4839,
      "step": 1356200
    },
    {
      "epoch": 439.64343598055103,
      "grad_norm": 1.471713900566101,
      "learning_rate": 6.057249432371067e-06,
      "loss": 2.4615,
      "step": 1356300
    },
    {
      "epoch": 439.67585089141005,
      "grad_norm": 1.3446606397628784,
      "learning_rate": 6.054005838469024e-06,
      "loss": 2.4662,
      "step": 1356400
    },
    {
      "epoch": 439.70826580226907,
      "grad_norm": 1.5921034812927246,
      "learning_rate": 6.0507622445669806e-06,
      "loss": 2.473,
      "step": 1356500
    },
    {
      "epoch": 439.74068071312803,
      "grad_norm": 1.5219199657440186,
      "learning_rate": 6.047518650664937e-06,
      "loss": 2.4596,
      "step": 1356600
    },
    {
      "epoch": 439.77309562398705,
      "grad_norm": 1.4854201078414917,
      "learning_rate": 6.044275056762894e-06,
      "loss": 2.4668,
      "step": 1356700
    },
    {
      "epoch": 439.805510534846,
      "grad_norm": 1.3668404817581177,
      "learning_rate": 6.04103146286085e-06,
      "loss": 2.457,
      "step": 1356800
    },
    {
      "epoch": 439.837925445705,
      "grad_norm": 1.5335588455200195,
      "learning_rate": 6.037787868958806e-06,
      "loss": 2.4559,
      "step": 1356900
    },
    {
      "epoch": 439.87034035656404,
      "grad_norm": 1.405449628829956,
      "learning_rate": 6.034544275056763e-06,
      "loss": 2.4657,
      "step": 1357000
    },
    {
      "epoch": 439.902755267423,
      "grad_norm": 1.4216922521591187,
      "learning_rate": 6.0313006811547195e-06,
      "loss": 2.4818,
      "step": 1357100
    },
    {
      "epoch": 439.935170178282,
      "grad_norm": 1.3148914575576782,
      "learning_rate": 6.028057087252676e-06,
      "loss": 2.467,
      "step": 1357200
    },
    {
      "epoch": 439.967585089141,
      "grad_norm": 1.3957408666610718,
      "learning_rate": 6.024813493350633e-06,
      "loss": 2.4642,
      "step": 1357300
    },
    {
      "epoch": 440.0,
      "grad_norm": 1.4002667665481567,
      "learning_rate": 6.021569899448589e-06,
      "loss": 2.4612,
      "step": 1357400
    },
    {
      "epoch": 440.0,
      "eval_bleu": 1.026068885299033,
      "eval_loss": 4.269482612609863,
      "eval_runtime": 4.4975,
      "eval_samples_per_second": 109.395,
      "eval_steps_per_second": 1.779,
      "step": 1357400
    },
    {
      "epoch": 440.032414910859,
      "grad_norm": 1.3687057495117188,
      "learning_rate": 6.018326305546545e-06,
      "loss": 2.4547,
      "step": 1357500
    },
    {
      "epoch": 440.064829821718,
      "grad_norm": 1.4528149366378784,
      "learning_rate": 6.015082711644503e-06,
      "loss": 2.4655,
      "step": 1357600
    },
    {
      "epoch": 440.097244732577,
      "grad_norm": 1.4464011192321777,
      "learning_rate": 6.011839117742459e-06,
      "loss": 2.47,
      "step": 1357700
    },
    {
      "epoch": 440.12965964343596,
      "grad_norm": 1.3889480829238892,
      "learning_rate": 6.008595523840415e-06,
      "loss": 2.4492,
      "step": 1357800
    },
    {
      "epoch": 440.162074554295,
      "grad_norm": 1.3549273014068604,
      "learning_rate": 6.0053519299383725e-06,
      "loss": 2.4685,
      "step": 1357900
    },
    {
      "epoch": 440.194489465154,
      "grad_norm": 1.5014145374298096,
      "learning_rate": 6.002108336036329e-06,
      "loss": 2.4546,
      "step": 1358000
    },
    {
      "epoch": 440.22690437601295,
      "grad_norm": 1.5035237073898315,
      "learning_rate": 5.998864742134285e-06,
      "loss": 2.4364,
      "step": 1358100
    },
    {
      "epoch": 440.25931928687197,
      "grad_norm": 1.5587760210037231,
      "learning_rate": 5.995621148232242e-06,
      "loss": 2.4591,
      "step": 1358200
    },
    {
      "epoch": 440.29173419773093,
      "grad_norm": 1.3158671855926514,
      "learning_rate": 5.992377554330198e-06,
      "loss": 2.4696,
      "step": 1358300
    },
    {
      "epoch": 440.32414910858995,
      "grad_norm": 1.480700969696045,
      "learning_rate": 5.989133960428154e-06,
      "loss": 2.4605,
      "step": 1358400
    },
    {
      "epoch": 440.35656401944897,
      "grad_norm": 1.42558753490448,
      "learning_rate": 5.985890366526111e-06,
      "loss": 2.4623,
      "step": 1358500
    },
    {
      "epoch": 440.3889789303079,
      "grad_norm": 1.421069622039795,
      "learning_rate": 5.9826467726240676e-06,
      "loss": 2.4483,
      "step": 1358600
    },
    {
      "epoch": 440.42139384116695,
      "grad_norm": 1.5974879264831543,
      "learning_rate": 5.979403178722024e-06,
      "loss": 2.4855,
      "step": 1358700
    },
    {
      "epoch": 440.4538087520259,
      "grad_norm": 1.4869974851608276,
      "learning_rate": 5.976159584819981e-06,
      "loss": 2.4534,
      "step": 1358800
    },
    {
      "epoch": 440.4862236628849,
      "grad_norm": 1.3922438621520996,
      "learning_rate": 5.972915990917937e-06,
      "loss": 2.4518,
      "step": 1358900
    },
    {
      "epoch": 440.51863857374394,
      "grad_norm": 1.5067805051803589,
      "learning_rate": 5.969672397015894e-06,
      "loss": 2.4712,
      "step": 1359000
    },
    {
      "epoch": 440.5510534846029,
      "grad_norm": 1.5067532062530518,
      "learning_rate": 5.966428803113851e-06,
      "loss": 2.4654,
      "step": 1359100
    },
    {
      "epoch": 440.5834683954619,
      "grad_norm": 1.7511520385742188,
      "learning_rate": 5.963185209211807e-06,
      "loss": 2.4498,
      "step": 1359200
    },
    {
      "epoch": 440.6158833063209,
      "grad_norm": 1.3127495050430298,
      "learning_rate": 5.9599416153097635e-06,
      "loss": 2.4796,
      "step": 1359300
    },
    {
      "epoch": 440.6482982171799,
      "grad_norm": 1.6328119039535522,
      "learning_rate": 5.9566980214077206e-06,
      "loss": 2.4503,
      "step": 1359400
    },
    {
      "epoch": 440.6807131280389,
      "grad_norm": 1.368005633354187,
      "learning_rate": 5.953454427505677e-06,
      "loss": 2.4726,
      "step": 1359500
    },
    {
      "epoch": 440.7131280388979,
      "grad_norm": 1.4233160018920898,
      "learning_rate": 5.950210833603633e-06,
      "loss": 2.4631,
      "step": 1359600
    },
    {
      "epoch": 440.7455429497569,
      "grad_norm": 1.4120509624481201,
      "learning_rate": 5.94696723970159e-06,
      "loss": 2.4793,
      "step": 1359700
    },
    {
      "epoch": 440.77795786061586,
      "grad_norm": 1.2453601360321045,
      "learning_rate": 5.943723645799546e-06,
      "loss": 2.4721,
      "step": 1359800
    },
    {
      "epoch": 440.8103727714749,
      "grad_norm": 1.302274227142334,
      "learning_rate": 5.9404800518975024e-06,
      "loss": 2.4637,
      "step": 1359900
    },
    {
      "epoch": 440.8427876823339,
      "grad_norm": 1.3514525890350342,
      "learning_rate": 5.9372364579954595e-06,
      "loss": 2.4577,
      "step": 1360000
    },
    {
      "epoch": 440.87520259319285,
      "grad_norm": 1.4474544525146484,
      "learning_rate": 5.933992864093416e-06,
      "loss": 2.4554,
      "step": 1360100
    },
    {
      "epoch": 440.90761750405187,
      "grad_norm": 1.6476181745529175,
      "learning_rate": 5.930749270191372e-06,
      "loss": 2.4423,
      "step": 1360200
    },
    {
      "epoch": 440.94003241491083,
      "grad_norm": 1.4387389421463013,
      "learning_rate": 5.927505676289329e-06,
      "loss": 2.4271,
      "step": 1360300
    },
    {
      "epoch": 440.97244732576985,
      "grad_norm": 1.6069608926773071,
      "learning_rate": 5.924262082387285e-06,
      "loss": 2.4482,
      "step": 1360400
    },
    {
      "epoch": 441.0,
      "eval_bleu": 1.0310447718808748,
      "eval_loss": 4.273784637451172,
      "eval_runtime": 4.1192,
      "eval_samples_per_second": 119.442,
      "eval_steps_per_second": 1.942,
      "step": 1360485
    },
    {
      "epoch": 441.00486223662887,
      "grad_norm": 1.3631770610809326,
      "learning_rate": 5.921050924424262e-06,
      "loss": 2.4705,
      "step": 1360500
    },
    {
      "epoch": 441.0372771474878,
      "grad_norm": 1.451589584350586,
      "learning_rate": 5.917807330522219e-06,
      "loss": 2.4686,
      "step": 1360600
    },
    {
      "epoch": 441.06969205834685,
      "grad_norm": 1.5082178115844727,
      "learning_rate": 5.914563736620175e-06,
      "loss": 2.4704,
      "step": 1360700
    },
    {
      "epoch": 441.1021069692058,
      "grad_norm": 1.3553364276885986,
      "learning_rate": 5.9113201427181315e-06,
      "loss": 2.4308,
      "step": 1360800
    },
    {
      "epoch": 441.1345218800648,
      "grad_norm": 1.4786443710327148,
      "learning_rate": 5.908076548816089e-06,
      "loss": 2.4635,
      "step": 1360900
    },
    {
      "epoch": 441.16693679092384,
      "grad_norm": 1.3997584581375122,
      "learning_rate": 5.904832954914045e-06,
      "loss": 2.4577,
      "step": 1361000
    },
    {
      "epoch": 441.1993517017828,
      "grad_norm": 1.5275661945343018,
      "learning_rate": 5.901589361012001e-06,
      "loss": 2.4577,
      "step": 1361100
    },
    {
      "epoch": 441.2317666126418,
      "grad_norm": 1.358812928199768,
      "learning_rate": 5.898345767109958e-06,
      "loss": 2.4507,
      "step": 1361200
    },
    {
      "epoch": 441.26418152350084,
      "grad_norm": 1.3322638273239136,
      "learning_rate": 5.895102173207915e-06,
      "loss": 2.467,
      "step": 1361300
    },
    {
      "epoch": 441.2965964343598,
      "grad_norm": 1.5125758647918701,
      "learning_rate": 5.891858579305871e-06,
      "loss": 2.4492,
      "step": 1361400
    },
    {
      "epoch": 441.3290113452188,
      "grad_norm": 1.5301778316497803,
      "learning_rate": 5.888614985403828e-06,
      "loss": 2.4873,
      "step": 1361500
    },
    {
      "epoch": 441.3614262560778,
      "grad_norm": 1.6008602380752563,
      "learning_rate": 5.8853713915017845e-06,
      "loss": 2.4471,
      "step": 1361600
    },
    {
      "epoch": 441.3938411669368,
      "grad_norm": 1.5789086818695068,
      "learning_rate": 5.882127797599741e-06,
      "loss": 2.4491,
      "step": 1361700
    },
    {
      "epoch": 441.4262560777958,
      "grad_norm": 1.4107706546783447,
      "learning_rate": 5.878884203697698e-06,
      "loss": 2.4636,
      "step": 1361800
    },
    {
      "epoch": 441.4586709886548,
      "grad_norm": 1.303341269493103,
      "learning_rate": 5.875640609795654e-06,
      "loss": 2.4509,
      "step": 1361900
    },
    {
      "epoch": 441.4910858995138,
      "grad_norm": 1.7040650844573975,
      "learning_rate": 5.87239701589361e-06,
      "loss": 2.464,
      "step": 1362000
    },
    {
      "epoch": 441.52350081037275,
      "grad_norm": 1.4760832786560059,
      "learning_rate": 5.869153421991567e-06,
      "loss": 2.4587,
      "step": 1362100
    },
    {
      "epoch": 441.55591572123177,
      "grad_norm": 1.537343144416809,
      "learning_rate": 5.8659098280895234e-06,
      "loss": 2.4504,
      "step": 1362200
    },
    {
      "epoch": 441.5883306320908,
      "grad_norm": 1.3724706172943115,
      "learning_rate": 5.86266623418748e-06,
      "loss": 2.4675,
      "step": 1362300
    },
    {
      "epoch": 441.62074554294975,
      "grad_norm": 1.4389067888259888,
      "learning_rate": 5.859422640285437e-06,
      "loss": 2.468,
      "step": 1362400
    },
    {
      "epoch": 441.65316045380877,
      "grad_norm": 1.432079792022705,
      "learning_rate": 5.856179046383393e-06,
      "loss": 2.4554,
      "step": 1362500
    },
    {
      "epoch": 441.6855753646677,
      "grad_norm": 1.5117731094360352,
      "learning_rate": 5.85296788842037e-06,
      "loss": 2.4461,
      "step": 1362600
    },
    {
      "epoch": 441.71799027552674,
      "grad_norm": 1.44942307472229,
      "learning_rate": 5.849724294518327e-06,
      "loss": 2.4862,
      "step": 1362700
    },
    {
      "epoch": 441.75040518638576,
      "grad_norm": 1.3922126293182373,
      "learning_rate": 5.846480700616283e-06,
      "loss": 2.4499,
      "step": 1362800
    },
    {
      "epoch": 441.7828200972447,
      "grad_norm": 1.2958842515945435,
      "learning_rate": 5.843237106714239e-06,
      "loss": 2.4595,
      "step": 1362900
    },
    {
      "epoch": 441.81523500810374,
      "grad_norm": 1.5432391166687012,
      "learning_rate": 5.839993512812196e-06,
      "loss": 2.4624,
      "step": 1363000
    },
    {
      "epoch": 441.8476499189627,
      "grad_norm": 1.9785611629486084,
      "learning_rate": 5.8367499189101525e-06,
      "loss": 2.4578,
      "step": 1363100
    },
    {
      "epoch": 441.8800648298217,
      "grad_norm": 1.510264277458191,
      "learning_rate": 5.833506325008109e-06,
      "loss": 2.4715,
      "step": 1363200
    },
    {
      "epoch": 441.91247974068074,
      "grad_norm": 1.3257122039794922,
      "learning_rate": 5.830262731106066e-06,
      "loss": 2.4636,
      "step": 1363300
    },
    {
      "epoch": 441.9448946515397,
      "grad_norm": 1.5697726011276245,
      "learning_rate": 5.827019137204023e-06,
      "loss": 2.4473,
      "step": 1363400
    },
    {
      "epoch": 441.9773095623987,
      "grad_norm": 1.3035612106323242,
      "learning_rate": 5.823775543301979e-06,
      "loss": 2.4621,
      "step": 1363500
    },
    {
      "epoch": 442.0,
      "eval_bleu": 1.044566623370207,
      "eval_loss": 4.273426055908203,
      "eval_runtime": 4.4208,
      "eval_samples_per_second": 111.291,
      "eval_steps_per_second": 1.81,
      "step": 1363570
    },
    {
      "epoch": 442.0097244732577,
      "grad_norm": 1.19855535030365,
      "learning_rate": 5.820531949399935e-06,
      "loss": 2.4663,
      "step": 1363600
    },
    {
      "epoch": 442.0421393841167,
      "grad_norm": 1.480460286140442,
      "learning_rate": 5.817288355497892e-06,
      "loss": 2.4459,
      "step": 1363700
    },
    {
      "epoch": 442.0745542949757,
      "grad_norm": 1.387479305267334,
      "learning_rate": 5.8140447615958485e-06,
      "loss": 2.4736,
      "step": 1363800
    },
    {
      "epoch": 442.1069692058347,
      "grad_norm": 1.2998743057250977,
      "learning_rate": 5.810801167693805e-06,
      "loss": 2.4615,
      "step": 1363900
    },
    {
      "epoch": 442.1393841166937,
      "grad_norm": 1.3027043342590332,
      "learning_rate": 5.807557573791762e-06,
      "loss": 2.4495,
      "step": 1364000
    },
    {
      "epoch": 442.17179902755265,
      "grad_norm": 1.4359917640686035,
      "learning_rate": 5.804313979889718e-06,
      "loss": 2.4639,
      "step": 1364100
    },
    {
      "epoch": 442.20421393841167,
      "grad_norm": 1.269673228263855,
      "learning_rate": 5.801070385987674e-06,
      "loss": 2.4565,
      "step": 1364200
    },
    {
      "epoch": 442.2366288492707,
      "grad_norm": 1.2996418476104736,
      "learning_rate": 5.797826792085631e-06,
      "loss": 2.4413,
      "step": 1364300
    },
    {
      "epoch": 442.26904376012965,
      "grad_norm": 1.4270622730255127,
      "learning_rate": 5.794583198183587e-06,
      "loss": 2.45,
      "step": 1364400
    },
    {
      "epoch": 442.30145867098867,
      "grad_norm": 1.3221430778503418,
      "learning_rate": 5.7913396042815445e-06,
      "loss": 2.449,
      "step": 1364500
    },
    {
      "epoch": 442.3338735818476,
      "grad_norm": 1.384891390800476,
      "learning_rate": 5.788128446318521e-06,
      "loss": 2.4738,
      "step": 1364600
    },
    {
      "epoch": 442.36628849270664,
      "grad_norm": 1.3516548871994019,
      "learning_rate": 5.784884852416478e-06,
      "loss": 2.4731,
      "step": 1364700
    },
    {
      "epoch": 442.39870340356566,
      "grad_norm": 1.4690850973129272,
      "learning_rate": 5.781641258514435e-06,
      "loss": 2.4778,
      "step": 1364800
    },
    {
      "epoch": 442.4311183144246,
      "grad_norm": 1.4832868576049805,
      "learning_rate": 5.778397664612391e-06,
      "loss": 2.4716,
      "step": 1364900
    },
    {
      "epoch": 442.46353322528364,
      "grad_norm": 1.3018834590911865,
      "learning_rate": 5.775154070710347e-06,
      "loss": 2.4528,
      "step": 1365000
    },
    {
      "epoch": 442.4959481361426,
      "grad_norm": 1.279255747795105,
      "learning_rate": 5.771910476808304e-06,
      "loss": 2.4742,
      "step": 1365100
    },
    {
      "epoch": 442.5283630470016,
      "grad_norm": 1.3638544082641602,
      "learning_rate": 5.76866688290626e-06,
      "loss": 2.4806,
      "step": 1365200
    },
    {
      "epoch": 442.56077795786064,
      "grad_norm": 1.3554329872131348,
      "learning_rate": 5.7654232890042165e-06,
      "loss": 2.458,
      "step": 1365300
    },
    {
      "epoch": 442.5931928687196,
      "grad_norm": 1.488726019859314,
      "learning_rate": 5.7621796951021736e-06,
      "loss": 2.4538,
      "step": 1365400
    },
    {
      "epoch": 442.6256077795786,
      "grad_norm": 1.4275670051574707,
      "learning_rate": 5.75893610120013e-06,
      "loss": 2.4647,
      "step": 1365500
    },
    {
      "epoch": 442.6580226904376,
      "grad_norm": 1.4912326335906982,
      "learning_rate": 5.755692507298087e-06,
      "loss": 2.4601,
      "step": 1365600
    },
    {
      "epoch": 442.6904376012966,
      "grad_norm": 1.5051298141479492,
      "learning_rate": 5.752448913396043e-06,
      "loss": 2.4643,
      "step": 1365700
    },
    {
      "epoch": 442.7228525121556,
      "grad_norm": 1.3906970024108887,
      "learning_rate": 5.749205319494e-06,
      "loss": 2.4475,
      "step": 1365800
    },
    {
      "epoch": 442.7552674230146,
      "grad_norm": 1.399420142173767,
      "learning_rate": 5.745961725591956e-06,
      "loss": 2.4751,
      "step": 1365900
    },
    {
      "epoch": 442.7876823338736,
      "grad_norm": 1.5839372873306274,
      "learning_rate": 5.7427181316899125e-06,
      "loss": 2.4612,
      "step": 1366000
    },
    {
      "epoch": 442.82009724473255,
      "grad_norm": 1.3567149639129639,
      "learning_rate": 5.7394745377878695e-06,
      "loss": 2.4544,
      "step": 1366100
    },
    {
      "epoch": 442.85251215559157,
      "grad_norm": 1.3644967079162598,
      "learning_rate": 5.736230943885826e-06,
      "loss": 2.4415,
      "step": 1366200
    },
    {
      "epoch": 442.8849270664506,
      "grad_norm": 1.4712616205215454,
      "learning_rate": 5.733019785922803e-06,
      "loss": 2.4646,
      "step": 1366300
    },
    {
      "epoch": 442.91734197730955,
      "grad_norm": 1.330992341041565,
      "learning_rate": 5.72977619202076e-06,
      "loss": 2.4569,
      "step": 1366400
    },
    {
      "epoch": 442.94975688816857,
      "grad_norm": 1.3411883115768433,
      "learning_rate": 5.726532598118716e-06,
      "loss": 2.4327,
      "step": 1366500
    },
    {
      "epoch": 442.9821717990275,
      "grad_norm": 1.5022774934768677,
      "learning_rate": 5.723289004216673e-06,
      "loss": 2.4563,
      "step": 1366600
    },
    {
      "epoch": 443.0,
      "eval_bleu": 1.041858073609369,
      "eval_loss": 4.276449203491211,
      "eval_runtime": 4.1903,
      "eval_samples_per_second": 117.414,
      "eval_steps_per_second": 1.909,
      "step": 1366655
    },
    {
      "epoch": 443.01458670988654,
      "grad_norm": 1.4982465505599976,
      "learning_rate": 5.720045410314629e-06,
      "loss": 2.4695,
      "step": 1366700
    },
    {
      "epoch": 443.04700162074556,
      "grad_norm": 1.4216798543930054,
      "learning_rate": 5.716801816412585e-06,
      "loss": 2.4625,
      "step": 1366800
    },
    {
      "epoch": 443.0794165316045,
      "grad_norm": 1.3166072368621826,
      "learning_rate": 5.7135582225105424e-06,
      "loss": 2.449,
      "step": 1366900
    },
    {
      "epoch": 443.11183144246354,
      "grad_norm": 1.4700638055801392,
      "learning_rate": 5.710314628608499e-06,
      "loss": 2.4529,
      "step": 1367000
    },
    {
      "epoch": 443.1442463533225,
      "grad_norm": 1.4163614511489868,
      "learning_rate": 5.707071034706455e-06,
      "loss": 2.4323,
      "step": 1367100
    },
    {
      "epoch": 443.1766612641815,
      "grad_norm": 1.6031438112258911,
      "learning_rate": 5.703827440804412e-06,
      "loss": 2.4645,
      "step": 1367200
    },
    {
      "epoch": 443.20907617504054,
      "grad_norm": 1.4908685684204102,
      "learning_rate": 5.700583846902368e-06,
      "loss": 2.4831,
      "step": 1367300
    },
    {
      "epoch": 443.2414910858995,
      "grad_norm": 1.5141409635543823,
      "learning_rate": 5.697340253000324e-06,
      "loss": 2.4714,
      "step": 1367400
    },
    {
      "epoch": 443.2739059967585,
      "grad_norm": 1.513938069343567,
      "learning_rate": 5.694096659098281e-06,
      "loss": 2.4461,
      "step": 1367500
    },
    {
      "epoch": 443.3063209076175,
      "grad_norm": 1.4044091701507568,
      "learning_rate": 5.6908530651962375e-06,
      "loss": 2.448,
      "step": 1367600
    },
    {
      "epoch": 443.3387358184765,
      "grad_norm": 1.56148362159729,
      "learning_rate": 5.687609471294194e-06,
      "loss": 2.4338,
      "step": 1367700
    },
    {
      "epoch": 443.3711507293355,
      "grad_norm": 1.5241713523864746,
      "learning_rate": 5.684365877392151e-06,
      "loss": 2.4469,
      "step": 1367800
    },
    {
      "epoch": 443.4035656401945,
      "grad_norm": 1.611446499824524,
      "learning_rate": 5.681122283490108e-06,
      "loss": 2.4661,
      "step": 1367900
    },
    {
      "epoch": 443.4359805510535,
      "grad_norm": 1.3286023139953613,
      "learning_rate": 5.677878689588064e-06,
      "loss": 2.4696,
      "step": 1368000
    },
    {
      "epoch": 443.4683954619125,
      "grad_norm": 1.5625325441360474,
      "learning_rate": 5.67463509568602e-06,
      "loss": 2.4749,
      "step": 1368100
    },
    {
      "epoch": 443.50081037277147,
      "grad_norm": 1.308890461921692,
      "learning_rate": 5.671391501783977e-06,
      "loss": 2.4579,
      "step": 1368200
    },
    {
      "epoch": 443.5332252836305,
      "grad_norm": 1.438550591468811,
      "learning_rate": 5.6681479078819335e-06,
      "loss": 2.4699,
      "step": 1368300
    },
    {
      "epoch": 443.56564019448945,
      "grad_norm": 1.661206603050232,
      "learning_rate": 5.66490431397989e-06,
      "loss": 2.4426,
      "step": 1368400
    },
    {
      "epoch": 443.59805510534846,
      "grad_norm": 1.5143547058105469,
      "learning_rate": 5.661660720077847e-06,
      "loss": 2.4771,
      "step": 1368500
    },
    {
      "epoch": 443.6304700162075,
      "grad_norm": 1.5143078565597534,
      "learning_rate": 5.658417126175803e-06,
      "loss": 2.4505,
      "step": 1368600
    },
    {
      "epoch": 443.66288492706644,
      "grad_norm": 1.6023777723312378,
      "learning_rate": 5.655173532273759e-06,
      "loss": 2.4788,
      "step": 1368700
    },
    {
      "epoch": 443.69529983792546,
      "grad_norm": 1.3538776636123657,
      "learning_rate": 5.651929938371716e-06,
      "loss": 2.4647,
      "step": 1368800
    },
    {
      "epoch": 443.7277147487844,
      "grad_norm": 1.3683958053588867,
      "learning_rate": 5.648686344469672e-06,
      "loss": 2.4615,
      "step": 1368900
    },
    {
      "epoch": 443.76012965964344,
      "grad_norm": 1.6727931499481201,
      "learning_rate": 5.645442750567629e-06,
      "loss": 2.4626,
      "step": 1369000
    },
    {
      "epoch": 443.79254457050246,
      "grad_norm": 1.4233767986297607,
      "learning_rate": 5.642199156665586e-06,
      "loss": 2.4442,
      "step": 1369100
    },
    {
      "epoch": 443.8249594813614,
      "grad_norm": 1.3219685554504395,
      "learning_rate": 5.638955562763542e-06,
      "loss": 2.4591,
      "step": 1369200
    },
    {
      "epoch": 443.85737439222044,
      "grad_norm": 1.231782078742981,
      "learning_rate": 5.635711968861499e-06,
      "loss": 2.4593,
      "step": 1369300
    },
    {
      "epoch": 443.8897893030794,
      "grad_norm": 1.446709156036377,
      "learning_rate": 5.632468374959456e-06,
      "loss": 2.4559,
      "step": 1369400
    },
    {
      "epoch": 443.9222042139384,
      "grad_norm": 1.509286880493164,
      "learning_rate": 5.629224781057412e-06,
      "loss": 2.472,
      "step": 1369500
    },
    {
      "epoch": 443.95461912479743,
      "grad_norm": 1.5411590337753296,
      "learning_rate": 5.625981187155368e-06,
      "loss": 2.4591,
      "step": 1369600
    },
    {
      "epoch": 443.9870340356564,
      "grad_norm": 1.526794195175171,
      "learning_rate": 5.622737593253325e-06,
      "loss": 2.4605,
      "step": 1369700
    },
    {
      "epoch": 444.0,
      "eval_bleu": 0.9737583106176417,
      "eval_loss": 4.2755937576293945,
      "eval_runtime": 4.4292,
      "eval_samples_per_second": 111.082,
      "eval_steps_per_second": 1.806,
      "step": 1369740
    },
    {
      "epoch": 444.0194489465154,
      "grad_norm": 1.4566619396209717,
      "learning_rate": 5.619493999351282e-06,
      "loss": 2.4679,
      "step": 1369800
    },
    {
      "epoch": 444.05186385737437,
      "grad_norm": 1.3856582641601562,
      "learning_rate": 5.616250405449238e-06,
      "loss": 2.474,
      "step": 1369900
    },
    {
      "epoch": 444.0842787682334,
      "grad_norm": 1.4767616987228394,
      "learning_rate": 5.613006811547195e-06,
      "loss": 2.4427,
      "step": 1370000
    },
    {
      "epoch": 444.1166936790924,
      "grad_norm": 1.4272620677947998,
      "learning_rate": 5.609763217645151e-06,
      "loss": 2.4578,
      "step": 1370100
    },
    {
      "epoch": 444.14910858995137,
      "grad_norm": 1.4664802551269531,
      "learning_rate": 5.606519623743107e-06,
      "loss": 2.4705,
      "step": 1370200
    },
    {
      "epoch": 444.1815235008104,
      "grad_norm": 1.3124067783355713,
      "learning_rate": 5.603308465780085e-06,
      "loss": 2.473,
      "step": 1370300
    },
    {
      "epoch": 444.21393841166935,
      "grad_norm": 1.529266357421875,
      "learning_rate": 5.600064871878041e-06,
      "loss": 2.4657,
      "step": 1370400
    },
    {
      "epoch": 444.24635332252836,
      "grad_norm": 1.5160183906555176,
      "learning_rate": 5.5968212779759975e-06,
      "loss": 2.4609,
      "step": 1370500
    },
    {
      "epoch": 444.2787682333874,
      "grad_norm": 1.4365392923355103,
      "learning_rate": 5.5935776840739545e-06,
      "loss": 2.4448,
      "step": 1370600
    },
    {
      "epoch": 444.31118314424634,
      "grad_norm": 1.3876053094863892,
      "learning_rate": 5.590334090171911e-06,
      "loss": 2.4703,
      "step": 1370700
    },
    {
      "epoch": 444.34359805510536,
      "grad_norm": 1.761475920677185,
      "learning_rate": 5.587090496269867e-06,
      "loss": 2.4628,
      "step": 1370800
    },
    {
      "epoch": 444.3760129659643,
      "grad_norm": 1.504892349243164,
      "learning_rate": 5.583846902367824e-06,
      "loss": 2.4577,
      "step": 1370900
    },
    {
      "epoch": 444.40842787682334,
      "grad_norm": 1.7837258577346802,
      "learning_rate": 5.58060330846578e-06,
      "loss": 2.4379,
      "step": 1371000
    },
    {
      "epoch": 444.44084278768236,
      "grad_norm": 1.3770363330841064,
      "learning_rate": 5.577359714563736e-06,
      "loss": 2.464,
      "step": 1371100
    },
    {
      "epoch": 444.4732576985413,
      "grad_norm": 1.3482321500778198,
      "learning_rate": 5.574116120661693e-06,
      "loss": 2.4998,
      "step": 1371200
    },
    {
      "epoch": 444.50567260940034,
      "grad_norm": 1.300629734992981,
      "learning_rate": 5.57087252675965e-06,
      "loss": 2.4645,
      "step": 1371300
    },
    {
      "epoch": 444.5380875202593,
      "grad_norm": 1.6019257307052612,
      "learning_rate": 5.567628932857606e-06,
      "loss": 2.459,
      "step": 1371400
    },
    {
      "epoch": 444.5705024311183,
      "grad_norm": 1.420226812362671,
      "learning_rate": 5.564385338955564e-06,
      "loss": 2.4547,
      "step": 1371500
    },
    {
      "epoch": 444.60291734197733,
      "grad_norm": 1.308335542678833,
      "learning_rate": 5.56114174505352e-06,
      "loss": 2.4756,
      "step": 1371600
    },
    {
      "epoch": 444.6353322528363,
      "grad_norm": 1.4890233278274536,
      "learning_rate": 5.557898151151476e-06,
      "loss": 2.4439,
      "step": 1371700
    },
    {
      "epoch": 444.6677471636953,
      "grad_norm": 1.3842644691467285,
      "learning_rate": 5.554654557249433e-06,
      "loss": 2.4658,
      "step": 1371800
    },
    {
      "epoch": 444.70016207455427,
      "grad_norm": 1.4408437013626099,
      "learning_rate": 5.551410963347389e-06,
      "loss": 2.458,
      "step": 1371900
    },
    {
      "epoch": 444.7325769854133,
      "grad_norm": 1.3263157606124878,
      "learning_rate": 5.548199805384366e-06,
      "loss": 2.431,
      "step": 1372000
    },
    {
      "epoch": 444.7649918962723,
      "grad_norm": 1.4896994829177856,
      "learning_rate": 5.5449562114823225e-06,
      "loss": 2.4541,
      "step": 1372100
    },
    {
      "epoch": 444.79740680713127,
      "grad_norm": 1.6308060884475708,
      "learning_rate": 5.541712617580279e-06,
      "loss": 2.468,
      "step": 1372200
    },
    {
      "epoch": 444.8298217179903,
      "grad_norm": 1.4074453115463257,
      "learning_rate": 5.538469023678236e-06,
      "loss": 2.4657,
      "step": 1372300
    },
    {
      "epoch": 444.86223662884925,
      "grad_norm": 1.3158552646636963,
      "learning_rate": 5.535225429776193e-06,
      "loss": 2.4524,
      "step": 1372400
    },
    {
      "epoch": 444.89465153970826,
      "grad_norm": 1.4769457578659058,
      "learning_rate": 5.531981835874149e-06,
      "loss": 2.4555,
      "step": 1372500
    },
    {
      "epoch": 444.9270664505673,
      "grad_norm": 1.422845721244812,
      "learning_rate": 5.528738241972105e-06,
      "loss": 2.4476,
      "step": 1372600
    },
    {
      "epoch": 444.95948136142624,
      "grad_norm": 1.4458391666412354,
      "learning_rate": 5.525494648070062e-06,
      "loss": 2.4457,
      "step": 1372700
    },
    {
      "epoch": 444.99189627228526,
      "grad_norm": 1.749709129333496,
      "learning_rate": 5.5222510541680185e-06,
      "loss": 2.4543,
      "step": 1372800
    },
    {
      "epoch": 445.0,
      "eval_bleu": 1.1501036756074698,
      "eval_loss": 4.275124549865723,
      "eval_runtime": 4.7144,
      "eval_samples_per_second": 104.361,
      "eval_steps_per_second": 1.697,
      "step": 1372825
    },
    {
      "epoch": 445.0243111831442,
      "grad_norm": 1.4971033334732056,
      "learning_rate": 5.519007460265975e-06,
      "loss": 2.4624,
      "step": 1372900
    },
    {
      "epoch": 445.05672609400324,
      "grad_norm": 1.4347501993179321,
      "learning_rate": 5.515763866363932e-06,
      "loss": 2.4433,
      "step": 1373000
    },
    {
      "epoch": 445.08914100486226,
      "grad_norm": 1.454301357269287,
      "learning_rate": 5.512520272461888e-06,
      "loss": 2.4528,
      "step": 1373100
    },
    {
      "epoch": 445.1215559157212,
      "grad_norm": 1.4180262088775635,
      "learning_rate": 5.509276678559844e-06,
      "loss": 2.4648,
      "step": 1373200
    },
    {
      "epoch": 445.15397082658023,
      "grad_norm": 1.357354760169983,
      "learning_rate": 5.506033084657801e-06,
      "loss": 2.4737,
      "step": 1373300
    },
    {
      "epoch": 445.1863857374392,
      "grad_norm": 1.5633023977279663,
      "learning_rate": 5.502789490755757e-06,
      "loss": 2.4261,
      "step": 1373400
    },
    {
      "epoch": 445.2188006482982,
      "grad_norm": 1.548823595046997,
      "learning_rate": 5.4995458968537136e-06,
      "loss": 2.4595,
      "step": 1373500
    },
    {
      "epoch": 445.25121555915723,
      "grad_norm": 1.6842386722564697,
      "learning_rate": 5.496302302951671e-06,
      "loss": 2.4413,
      "step": 1373600
    },
    {
      "epoch": 445.2836304700162,
      "grad_norm": 1.4272291660308838,
      "learning_rate": 5.493058709049628e-06,
      "loss": 2.4606,
      "step": 1373700
    },
    {
      "epoch": 445.3160453808752,
      "grad_norm": 1.5185993909835815,
      "learning_rate": 5.489815115147584e-06,
      "loss": 2.4673,
      "step": 1373800
    },
    {
      "epoch": 445.34846029173417,
      "grad_norm": 1.333326816558838,
      "learning_rate": 5.486571521245541e-06,
      "loss": 2.4651,
      "step": 1373900
    },
    {
      "epoch": 445.3808752025932,
      "grad_norm": 1.451303482055664,
      "learning_rate": 5.483327927343497e-06,
      "loss": 2.4477,
      "step": 1374000
    },
    {
      "epoch": 445.4132901134522,
      "grad_norm": 1.4829448461532593,
      "learning_rate": 5.480084333441453e-06,
      "loss": 2.4684,
      "step": 1374100
    },
    {
      "epoch": 445.44570502431117,
      "grad_norm": 1.5127230882644653,
      "learning_rate": 5.47684073953941e-06,
      "loss": 2.4655,
      "step": 1374200
    },
    {
      "epoch": 445.4781199351702,
      "grad_norm": 1.5807229280471802,
      "learning_rate": 5.4736295815763865e-06,
      "loss": 2.4805,
      "step": 1374300
    },
    {
      "epoch": 445.51053484602915,
      "grad_norm": 1.620919942855835,
      "learning_rate": 5.470385987674343e-06,
      "loss": 2.4682,
      "step": 1374400
    },
    {
      "epoch": 445.54294975688816,
      "grad_norm": 1.3632689714431763,
      "learning_rate": 5.467142393772301e-06,
      "loss": 2.4496,
      "step": 1374500
    },
    {
      "epoch": 445.5753646677472,
      "grad_norm": 1.4089521169662476,
      "learning_rate": 5.463898799870257e-06,
      "loss": 2.4376,
      "step": 1374600
    },
    {
      "epoch": 445.60777957860614,
      "grad_norm": 1.4298853874206543,
      "learning_rate": 5.460655205968213e-06,
      "loss": 2.4466,
      "step": 1374700
    },
    {
      "epoch": 445.64019448946516,
      "grad_norm": 1.4219282865524292,
      "learning_rate": 5.45741161206617e-06,
      "loss": 2.4586,
      "step": 1374800
    },
    {
      "epoch": 445.6726094003242,
      "grad_norm": 1.3538730144500732,
      "learning_rate": 5.454168018164126e-06,
      "loss": 2.4661,
      "step": 1374900
    },
    {
      "epoch": 445.70502431118314,
      "grad_norm": 1.498734951019287,
      "learning_rate": 5.4509244242620824e-06,
      "loss": 2.4332,
      "step": 1375000
    },
    {
      "epoch": 445.73743922204216,
      "grad_norm": 1.4349702596664429,
      "learning_rate": 5.4476808303600395e-06,
      "loss": 2.4829,
      "step": 1375100
    },
    {
      "epoch": 445.7698541329011,
      "grad_norm": 1.616604208946228,
      "learning_rate": 5.444437236457996e-06,
      "loss": 2.4788,
      "step": 1375200
    },
    {
      "epoch": 445.80226904376013,
      "grad_norm": 1.5902280807495117,
      "learning_rate": 5.441193642555952e-06,
      "loss": 2.475,
      "step": 1375300
    },
    {
      "epoch": 445.83468395461915,
      "grad_norm": 1.5415977239608765,
      "learning_rate": 5.437950048653909e-06,
      "loss": 2.4398,
      "step": 1375400
    },
    {
      "epoch": 445.8670988654781,
      "grad_norm": 1.3980627059936523,
      "learning_rate": 5.434706454751865e-06,
      "loss": 2.4346,
      "step": 1375500
    },
    {
      "epoch": 445.89951377633713,
      "grad_norm": 1.3643962144851685,
      "learning_rate": 5.431462860849821e-06,
      "loss": 2.4816,
      "step": 1375600
    },
    {
      "epoch": 445.9319286871961,
      "grad_norm": 1.7159638404846191,
      "learning_rate": 5.428219266947778e-06,
      "loss": 2.4557,
      "step": 1375700
    },
    {
      "epoch": 445.9643435980551,
      "grad_norm": 1.3021583557128906,
      "learning_rate": 5.424975673045735e-06,
      "loss": 2.4814,
      "step": 1375800
    },
    {
      "epoch": 445.9967585089141,
      "grad_norm": 1.4369091987609863,
      "learning_rate": 5.421732079143692e-06,
      "loss": 2.4551,
      "step": 1375900
    },
    {
      "epoch": 446.0,
      "eval_bleu": 1.2114340578908647,
      "eval_loss": 4.275586128234863,
      "eval_runtime": 4.2052,
      "eval_samples_per_second": 116.998,
      "eval_steps_per_second": 1.902,
      "step": 1375910
    },
    {
      "epoch": 446.0291734197731,
      "grad_norm": 1.411246657371521,
      "learning_rate": 5.418488485241649e-06,
      "loss": 2.466,
      "step": 1376000
    },
    {
      "epoch": 446.0615883306321,
      "grad_norm": 1.311263918876648,
      "learning_rate": 5.415244891339605e-06,
      "loss": 2.4618,
      "step": 1376100
    },
    {
      "epoch": 446.09400324149107,
      "grad_norm": 1.3940706253051758,
      "learning_rate": 5.412001297437561e-06,
      "loss": 2.4701,
      "step": 1376200
    },
    {
      "epoch": 446.1264181523501,
      "grad_norm": 1.4204120635986328,
      "learning_rate": 5.408757703535518e-06,
      "loss": 2.4582,
      "step": 1376300
    },
    {
      "epoch": 446.1588330632091,
      "grad_norm": 1.4323906898498535,
      "learning_rate": 5.405514109633474e-06,
      "loss": 2.4729,
      "step": 1376400
    },
    {
      "epoch": 446.19124797406806,
      "grad_norm": 1.3023840188980103,
      "learning_rate": 5.4022705157314305e-06,
      "loss": 2.4476,
      "step": 1376500
    },
    {
      "epoch": 446.2236628849271,
      "grad_norm": 1.4388903379440308,
      "learning_rate": 5.399026921829388e-06,
      "loss": 2.4598,
      "step": 1376600
    },
    {
      "epoch": 446.25607779578604,
      "grad_norm": 1.5255255699157715,
      "learning_rate": 5.395783327927344e-06,
      "loss": 2.457,
      "step": 1376700
    },
    {
      "epoch": 446.28849270664506,
      "grad_norm": 1.370840072631836,
      "learning_rate": 5.3925397340253e-06,
      "loss": 2.4653,
      "step": 1376800
    },
    {
      "epoch": 446.3209076175041,
      "grad_norm": 1.5009099245071411,
      "learning_rate": 5.389296140123257e-06,
      "loss": 2.4383,
      "step": 1376900
    },
    {
      "epoch": 446.35332252836304,
      "grad_norm": 1.3079771995544434,
      "learning_rate": 5.386052546221213e-06,
      "loss": 2.4577,
      "step": 1377000
    },
    {
      "epoch": 446.38573743922205,
      "grad_norm": 1.2780605554580688,
      "learning_rate": 5.3828089523191694e-06,
      "loss": 2.4546,
      "step": 1377100
    },
    {
      "epoch": 446.418152350081,
      "grad_norm": 1.46353280544281,
      "learning_rate": 5.3795653584171265e-06,
      "loss": 2.448,
      "step": 1377200
    },
    {
      "epoch": 446.45056726094003,
      "grad_norm": 1.521364450454712,
      "learning_rate": 5.376321764515083e-06,
      "loss": 2.4471,
      "step": 1377300
    },
    {
      "epoch": 446.48298217179905,
      "grad_norm": 1.5549894571304321,
      "learning_rate": 5.37307817061304e-06,
      "loss": 2.4409,
      "step": 1377400
    },
    {
      "epoch": 446.515397082658,
      "grad_norm": 1.5732485055923462,
      "learning_rate": 5.369834576710996e-06,
      "loss": 2.434,
      "step": 1377500
    },
    {
      "epoch": 446.54781199351703,
      "grad_norm": 1.6764423847198486,
      "learning_rate": 5.366590982808953e-06,
      "loss": 2.4228,
      "step": 1377600
    },
    {
      "epoch": 446.580226904376,
      "grad_norm": 1.655761480331421,
      "learning_rate": 5.363347388906909e-06,
      "loss": 2.4556,
      "step": 1377700
    },
    {
      "epoch": 446.612641815235,
      "grad_norm": 1.323278546333313,
      "learning_rate": 5.360136230943886e-06,
      "loss": 2.4411,
      "step": 1377800
    },
    {
      "epoch": 446.645056726094,
      "grad_norm": 1.3976484537124634,
      "learning_rate": 5.356892637041842e-06,
      "loss": 2.4694,
      "step": 1377900
    },
    {
      "epoch": 446.677471636953,
      "grad_norm": 1.3330522775650024,
      "learning_rate": 5.3536490431397986e-06,
      "loss": 2.4685,
      "step": 1378000
    },
    {
      "epoch": 446.709886547812,
      "grad_norm": 1.3139970302581787,
      "learning_rate": 5.350405449237756e-06,
      "loss": 2.4916,
      "step": 1378100
    },
    {
      "epoch": 446.74230145867097,
      "grad_norm": 1.3682204484939575,
      "learning_rate": 5.347161855335713e-06,
      "loss": 2.479,
      "step": 1378200
    },
    {
      "epoch": 446.77471636953,
      "grad_norm": 1.2791427373886108,
      "learning_rate": 5.343918261433669e-06,
      "loss": 2.4531,
      "step": 1378300
    },
    {
      "epoch": 446.807131280389,
      "grad_norm": 1.565930962562561,
      "learning_rate": 5.340674667531626e-06,
      "loss": 2.4445,
      "step": 1378400
    },
    {
      "epoch": 446.83954619124796,
      "grad_norm": 1.5415384769439697,
      "learning_rate": 5.337431073629582e-06,
      "loss": 2.4605,
      "step": 1378500
    },
    {
      "epoch": 446.871961102107,
      "grad_norm": 1.4621559381484985,
      "learning_rate": 5.334187479727538e-06,
      "loss": 2.4746,
      "step": 1378600
    },
    {
      "epoch": 446.90437601296594,
      "grad_norm": 1.4791587591171265,
      "learning_rate": 5.330943885825495e-06,
      "loss": 2.4563,
      "step": 1378700
    },
    {
      "epoch": 446.93679092382496,
      "grad_norm": 1.4187650680541992,
      "learning_rate": 5.3277002919234516e-06,
      "loss": 2.4597,
      "step": 1378800
    },
    {
      "epoch": 446.969205834684,
      "grad_norm": 1.350358247756958,
      "learning_rate": 5.324456698021408e-06,
      "loss": 2.4634,
      "step": 1378900
    },
    {
      "epoch": 447.0,
      "eval_bleu": 1.1441233629537573,
      "eval_loss": 4.278284072875977,
      "eval_runtime": 4.6903,
      "eval_samples_per_second": 104.897,
      "eval_steps_per_second": 1.706,
      "step": 1378995
    },
    {
      "epoch": 447.00162074554294,
      "grad_norm": 1.3729865550994873,
      "learning_rate": 5.321213104119365e-06,
      "loss": 2.4757,
      "step": 1379000
    },
    {
      "epoch": 447.03403565640195,
      "grad_norm": 1.275160551071167,
      "learning_rate": 5.317969510217321e-06,
      "loss": 2.4484,
      "step": 1379100
    },
    {
      "epoch": 447.0664505672609,
      "grad_norm": 1.6123636960983276,
      "learning_rate": 5.314725916315277e-06,
      "loss": 2.4551,
      "step": 1379200
    },
    {
      "epoch": 447.09886547811993,
      "grad_norm": 1.2898956537246704,
      "learning_rate": 5.311482322413234e-06,
      "loss": 2.4558,
      "step": 1379300
    },
    {
      "epoch": 447.13128038897895,
      "grad_norm": 1.3647462129592896,
      "learning_rate": 5.3082387285111905e-06,
      "loss": 2.459,
      "step": 1379400
    },
    {
      "epoch": 447.1636952998379,
      "grad_norm": 1.3002678155899048,
      "learning_rate": 5.304995134609147e-06,
      "loss": 2.4345,
      "step": 1379500
    },
    {
      "epoch": 447.19611021069693,
      "grad_norm": 1.5647586584091187,
      "learning_rate": 5.301751540707104e-06,
      "loss": 2.4476,
      "step": 1379600
    },
    {
      "epoch": 447.2285251215559,
      "grad_norm": 1.3995805978775024,
      "learning_rate": 5.298507946805061e-06,
      "loss": 2.4504,
      "step": 1379700
    },
    {
      "epoch": 447.2609400324149,
      "grad_norm": 1.3129098415374756,
      "learning_rate": 5.295264352903017e-06,
      "loss": 2.4763,
      "step": 1379800
    },
    {
      "epoch": 447.2933549432739,
      "grad_norm": 1.4506146907806396,
      "learning_rate": 5.292020759000973e-06,
      "loss": 2.448,
      "step": 1379900
    },
    {
      "epoch": 447.3257698541329,
      "grad_norm": 1.543727159500122,
      "learning_rate": 5.28877716509893e-06,
      "loss": 2.4698,
      "step": 1380000
    },
    {
      "epoch": 447.3581847649919,
      "grad_norm": 1.5001411437988281,
      "learning_rate": 5.285533571196886e-06,
      "loss": 2.457,
      "step": 1380100
    },
    {
      "epoch": 447.39059967585086,
      "grad_norm": 1.4768052101135254,
      "learning_rate": 5.282289977294843e-06,
      "loss": 2.4605,
      "step": 1380200
    },
    {
      "epoch": 447.4230145867099,
      "grad_norm": 1.5923023223876953,
      "learning_rate": 5.2790463833928e-06,
      "loss": 2.4654,
      "step": 1380300
    },
    {
      "epoch": 447.4554294975689,
      "grad_norm": 1.342397928237915,
      "learning_rate": 5.275802789490756e-06,
      "loss": 2.4566,
      "step": 1380400
    },
    {
      "epoch": 447.48784440842786,
      "grad_norm": 1.2717798948287964,
      "learning_rate": 5.272559195588712e-06,
      "loss": 2.4501,
      "step": 1380500
    },
    {
      "epoch": 447.5202593192869,
      "grad_norm": 1.5172648429870605,
      "learning_rate": 5.269315601686669e-06,
      "loss": 2.4361,
      "step": 1380600
    },
    {
      "epoch": 447.55267423014584,
      "grad_norm": 1.5009106397628784,
      "learning_rate": 5.266072007784625e-06,
      "loss": 2.4751,
      "step": 1380700
    },
    {
      "epoch": 447.58508914100486,
      "grad_norm": 1.3199495077133179,
      "learning_rate": 5.2628284138825815e-06,
      "loss": 2.4342,
      "step": 1380800
    },
    {
      "epoch": 447.6175040518639,
      "grad_norm": 1.5691876411437988,
      "learning_rate": 5.2595848199805386e-06,
      "loss": 2.4705,
      "step": 1380900
    },
    {
      "epoch": 447.64991896272284,
      "grad_norm": 1.2303396463394165,
      "learning_rate": 5.256341226078496e-06,
      "loss": 2.4438,
      "step": 1381000
    },
    {
      "epoch": 447.68233387358185,
      "grad_norm": 1.332322597503662,
      "learning_rate": 5.253097632176452e-06,
      "loss": 2.4305,
      "step": 1381100
    },
    {
      "epoch": 447.7147487844408,
      "grad_norm": 1.2604073286056519,
      "learning_rate": 5.249854038274409e-06,
      "loss": 2.4566,
      "step": 1381200
    },
    {
      "epoch": 447.74716369529983,
      "grad_norm": 1.4346997737884521,
      "learning_rate": 5.246642880311385e-06,
      "loss": 2.4834,
      "step": 1381300
    },
    {
      "epoch": 447.77957860615885,
      "grad_norm": 1.4197320938110352,
      "learning_rate": 5.243431722348363e-06,
      "loss": 2.4838,
      "step": 1381400
    },
    {
      "epoch": 447.8119935170178,
      "grad_norm": 1.3112298250198364,
      "learning_rate": 5.240188128446319e-06,
      "loss": 2.4506,
      "step": 1381500
    },
    {
      "epoch": 447.84440842787683,
      "grad_norm": 1.3945140838623047,
      "learning_rate": 5.236944534544275e-06,
      "loss": 2.4552,
      "step": 1381600
    },
    {
      "epoch": 447.87682333873585,
      "grad_norm": 1.3861336708068848,
      "learning_rate": 5.233700940642232e-06,
      "loss": 2.4729,
      "step": 1381700
    },
    {
      "epoch": 447.9092382495948,
      "grad_norm": 1.3433791399002075,
      "learning_rate": 5.2304573467401884e-06,
      "loss": 2.4742,
      "step": 1381800
    },
    {
      "epoch": 447.9416531604538,
      "grad_norm": 1.2365553379058838,
      "learning_rate": 5.227213752838145e-06,
      "loss": 2.4688,
      "step": 1381900
    },
    {
      "epoch": 447.9740680713128,
      "grad_norm": 1.560233235359192,
      "learning_rate": 5.223970158936102e-06,
      "loss": 2.4631,
      "step": 1382000
    },
    {
      "epoch": 448.0,
      "eval_bleu": 1.301322975759629,
      "eval_loss": 4.273812294006348,
      "eval_runtime": 4.4444,
      "eval_samples_per_second": 110.701,
      "eval_steps_per_second": 1.8,
      "step": 1382080
    },
    {
      "epoch": 448.0064829821718,
      "grad_norm": 1.3345125913619995,
      "learning_rate": 5.220726565034058e-06,
      "loss": 2.4676,
      "step": 1382100
    },
    {
      "epoch": 448.0388978930308,
      "grad_norm": 1.2997844219207764,
      "learning_rate": 5.217482971132014e-06,
      "loss": 2.449,
      "step": 1382200
    },
    {
      "epoch": 448.0713128038898,
      "grad_norm": 1.3227206468582153,
      "learning_rate": 5.214239377229971e-06,
      "loss": 2.4705,
      "step": 1382300
    },
    {
      "epoch": 448.1037277147488,
      "grad_norm": 1.5571852922439575,
      "learning_rate": 5.210995783327927e-06,
      "loss": 2.4601,
      "step": 1382400
    },
    {
      "epoch": 448.13614262560776,
      "grad_norm": 1.295448660850525,
      "learning_rate": 5.207752189425884e-06,
      "loss": 2.4545,
      "step": 1382500
    },
    {
      "epoch": 448.1685575364668,
      "grad_norm": 1.4426848888397217,
      "learning_rate": 5.204508595523841e-06,
      "loss": 2.469,
      "step": 1382600
    },
    {
      "epoch": 448.2009724473258,
      "grad_norm": 1.502653956413269,
      "learning_rate": 5.201265001621798e-06,
      "loss": 2.462,
      "step": 1382700
    },
    {
      "epoch": 448.23338735818476,
      "grad_norm": 1.3710843324661255,
      "learning_rate": 5.198021407719754e-06,
      "loss": 2.4659,
      "step": 1382800
    },
    {
      "epoch": 448.2658022690438,
      "grad_norm": 1.4078059196472168,
      "learning_rate": 5.19477781381771e-06,
      "loss": 2.4382,
      "step": 1382900
    },
    {
      "epoch": 448.29821717990274,
      "grad_norm": 1.5312480926513672,
      "learning_rate": 5.191534219915667e-06,
      "loss": 2.432,
      "step": 1383000
    },
    {
      "epoch": 448.33063209076175,
      "grad_norm": 1.5743083953857422,
      "learning_rate": 5.188290626013623e-06,
      "loss": 2.4947,
      "step": 1383100
    },
    {
      "epoch": 448.36304700162077,
      "grad_norm": 1.4192934036254883,
      "learning_rate": 5.1850470321115795e-06,
      "loss": 2.4484,
      "step": 1383200
    },
    {
      "epoch": 448.39546191247973,
      "grad_norm": 1.5367571115493774,
      "learning_rate": 5.1818034382095365e-06,
      "loss": 2.4422,
      "step": 1383300
    },
    {
      "epoch": 448.42787682333875,
      "grad_norm": 1.3885807991027832,
      "learning_rate": 5.178559844307493e-06,
      "loss": 2.4495,
      "step": 1383400
    },
    {
      "epoch": 448.4602917341977,
      "grad_norm": 1.5545462369918823,
      "learning_rate": 5.175316250405449e-06,
      "loss": 2.4502,
      "step": 1383500
    },
    {
      "epoch": 448.4927066450567,
      "grad_norm": 1.4027295112609863,
      "learning_rate": 5.172072656503406e-06,
      "loss": 2.4501,
      "step": 1383600
    },
    {
      "epoch": 448.52512155591575,
      "grad_norm": 1.420304775238037,
      "learning_rate": 5.168829062601362e-06,
      "loss": 2.4728,
      "step": 1383700
    },
    {
      "epoch": 448.5575364667747,
      "grad_norm": 1.5425268411636353,
      "learning_rate": 5.165585468699318e-06,
      "loss": 2.4485,
      "step": 1383800
    },
    {
      "epoch": 448.5899513776337,
      "grad_norm": 1.5365931987762451,
      "learning_rate": 5.1623418747972754e-06,
      "loss": 2.4494,
      "step": 1383900
    },
    {
      "epoch": 448.6223662884927,
      "grad_norm": 1.3071997165679932,
      "learning_rate": 5.1590982808952325e-06,
      "loss": 2.4491,
      "step": 1384000
    },
    {
      "epoch": 448.6547811993517,
      "grad_norm": 1.6683193445205688,
      "learning_rate": 5.155854686993189e-06,
      "loss": 2.4514,
      "step": 1384100
    },
    {
      "epoch": 448.6871961102107,
      "grad_norm": 1.5609139204025269,
      "learning_rate": 5.152611093091146e-06,
      "loss": 2.4477,
      "step": 1384200
    },
    {
      "epoch": 448.7196110210697,
      "grad_norm": 1.2917085886001587,
      "learning_rate": 5.149367499189102e-06,
      "loss": 2.4658,
      "step": 1384300
    },
    {
      "epoch": 448.7520259319287,
      "grad_norm": 1.3728126287460327,
      "learning_rate": 5.146123905287058e-06,
      "loss": 2.4636,
      "step": 1384400
    },
    {
      "epoch": 448.78444084278766,
      "grad_norm": 1.4385782480239868,
      "learning_rate": 5.142880311385015e-06,
      "loss": 2.4935,
      "step": 1384500
    },
    {
      "epoch": 448.8168557536467,
      "grad_norm": 1.3344478607177734,
      "learning_rate": 5.139636717482971e-06,
      "loss": 2.4566,
      "step": 1384600
    },
    {
      "epoch": 448.8492706645057,
      "grad_norm": 1.3189277648925781,
      "learning_rate": 5.136393123580928e-06,
      "loss": 2.4695,
      "step": 1384700
    },
    {
      "epoch": 448.88168557536466,
      "grad_norm": 1.581465721130371,
      "learning_rate": 5.133149529678885e-06,
      "loss": 2.4768,
      "step": 1384800
    },
    {
      "epoch": 448.9141004862237,
      "grad_norm": 1.380131483078003,
      "learning_rate": 5.129905935776841e-06,
      "loss": 2.4508,
      "step": 1384900
    },
    {
      "epoch": 448.94651539708263,
      "grad_norm": 1.4088222980499268,
      "learning_rate": 5.126662341874797e-06,
      "loss": 2.4598,
      "step": 1385000
    },
    {
      "epoch": 448.97893030794165,
      "grad_norm": 1.3891263008117676,
      "learning_rate": 5.123418747972754e-06,
      "loss": 2.4564,
      "step": 1385100
    },
    {
      "epoch": 449.0,
      "eval_bleu": 1.2687677356810674,
      "eval_loss": 4.274731159210205,
      "eval_runtime": 4.4998,
      "eval_samples_per_second": 109.338,
      "eval_steps_per_second": 1.778,
      "step": 1385165
    },
    {
      "epoch": 449.01134521880067,
      "grad_norm": 1.5630682706832886,
      "learning_rate": 5.12017515407071e-06,
      "loss": 2.472,
      "step": 1385200
    },
    {
      "epoch": 449.04376012965963,
      "grad_norm": 1.8665516376495361,
      "learning_rate": 5.1169315601686665e-06,
      "loss": 2.4563,
      "step": 1385300
    },
    {
      "epoch": 449.07617504051865,
      "grad_norm": 1.4801820516586304,
      "learning_rate": 5.113687966266624e-06,
      "loss": 2.4472,
      "step": 1385400
    },
    {
      "epoch": 449.1085899513776,
      "grad_norm": 1.45797860622406,
      "learning_rate": 5.110444372364581e-06,
      "loss": 2.4552,
      "step": 1385500
    },
    {
      "epoch": 449.1410048622366,
      "grad_norm": 1.3653877973556519,
      "learning_rate": 5.107200778462537e-06,
      "loss": 2.4381,
      "step": 1385600
    },
    {
      "epoch": 449.17341977309565,
      "grad_norm": 1.6012424230575562,
      "learning_rate": 5.103957184560494e-06,
      "loss": 2.4533,
      "step": 1385700
    },
    {
      "epoch": 449.2058346839546,
      "grad_norm": 1.570082426071167,
      "learning_rate": 5.10071359065845e-06,
      "loss": 2.4241,
      "step": 1385800
    },
    {
      "epoch": 449.2382495948136,
      "grad_norm": 1.41560959815979,
      "learning_rate": 5.097469996756406e-06,
      "loss": 2.449,
      "step": 1385900
    },
    {
      "epoch": 449.2706645056726,
      "grad_norm": 1.3146209716796875,
      "learning_rate": 5.094226402854363e-06,
      "loss": 2.4543,
      "step": 1386000
    },
    {
      "epoch": 449.3030794165316,
      "grad_norm": 1.4722949266433716,
      "learning_rate": 5.0909828089523195e-06,
      "loss": 2.4684,
      "step": 1386100
    },
    {
      "epoch": 449.3354943273906,
      "grad_norm": 1.3762050867080688,
      "learning_rate": 5.087739215050276e-06,
      "loss": 2.4564,
      "step": 1386200
    },
    {
      "epoch": 449.3679092382496,
      "grad_norm": 1.3795013427734375,
      "learning_rate": 5.084495621148233e-06,
      "loss": 2.4564,
      "step": 1386300
    },
    {
      "epoch": 449.4003241491086,
      "grad_norm": 1.4470701217651367,
      "learning_rate": 5.081252027246189e-06,
      "loss": 2.4587,
      "step": 1386400
    },
    {
      "epoch": 449.43273905996756,
      "grad_norm": 1.4509230852127075,
      "learning_rate": 5.078008433344145e-06,
      "loss": 2.4233,
      "step": 1386500
    },
    {
      "epoch": 449.4651539708266,
      "grad_norm": 1.5562633275985718,
      "learning_rate": 5.074764839442102e-06,
      "loss": 2.4682,
      "step": 1386600
    },
    {
      "epoch": 449.4975688816856,
      "grad_norm": 1.4320191144943237,
      "learning_rate": 5.071521245540058e-06,
      "loss": 2.4682,
      "step": 1386700
    },
    {
      "epoch": 449.52998379254456,
      "grad_norm": 1.5719479322433472,
      "learning_rate": 5.068277651638015e-06,
      "loss": 2.451,
      "step": 1386800
    },
    {
      "epoch": 449.5623987034036,
      "grad_norm": 1.377646565437317,
      "learning_rate": 5.065034057735972e-06,
      "loss": 2.4599,
      "step": 1386900
    },
    {
      "epoch": 449.59481361426253,
      "grad_norm": 1.464397668838501,
      "learning_rate": 5.061790463833929e-06,
      "loss": 2.4776,
      "step": 1387000
    },
    {
      "epoch": 449.62722852512155,
      "grad_norm": 1.6962720155715942,
      "learning_rate": 5.058546869931885e-06,
      "loss": 2.4581,
      "step": 1387100
    },
    {
      "epoch": 449.65964343598057,
      "grad_norm": 1.4314974546432495,
      "learning_rate": 5.055335711968862e-06,
      "loss": 2.4447,
      "step": 1387200
    },
    {
      "epoch": 449.69205834683953,
      "grad_norm": 1.2272781133651733,
      "learning_rate": 5.052092118066818e-06,
      "loss": 2.4744,
      "step": 1387300
    },
    {
      "epoch": 449.72447325769855,
      "grad_norm": 1.6006397008895874,
      "learning_rate": 5.048848524164774e-06,
      "loss": 2.4803,
      "step": 1387400
    },
    {
      "epoch": 449.7568881685575,
      "grad_norm": 1.3458545207977295,
      "learning_rate": 5.045604930262731e-06,
      "loss": 2.4521,
      "step": 1387500
    },
    {
      "epoch": 449.7893030794165,
      "grad_norm": 1.3513816595077515,
      "learning_rate": 5.042361336360688e-06,
      "loss": 2.4758,
      "step": 1387600
    },
    {
      "epoch": 449.82171799027554,
      "grad_norm": 1.5180058479309082,
      "learning_rate": 5.0391177424586446e-06,
      "loss": 2.4667,
      "step": 1387700
    },
    {
      "epoch": 449.8541329011345,
      "grad_norm": 1.4806559085845947,
      "learning_rate": 5.035874148556602e-06,
      "loss": 2.4508,
      "step": 1387800
    },
    {
      "epoch": 449.8865478119935,
      "grad_norm": 1.2393044233322144,
      "learning_rate": 5.032630554654558e-06,
      "loss": 2.4718,
      "step": 1387900
    },
    {
      "epoch": 449.9189627228525,
      "grad_norm": 1.4851198196411133,
      "learning_rate": 5.029386960752514e-06,
      "loss": 2.4623,
      "step": 1388000
    },
    {
      "epoch": 449.9513776337115,
      "grad_norm": 1.454352617263794,
      "learning_rate": 5.026143366850471e-06,
      "loss": 2.4524,
      "step": 1388100
    },
    {
      "epoch": 449.9837925445705,
      "grad_norm": 1.3666138648986816,
      "learning_rate": 5.022899772948427e-06,
      "loss": 2.4438,
      "step": 1388200
    },
    {
      "epoch": 450.0,
      "eval_bleu": 1.0472169771072095,
      "eval_loss": 4.2769012451171875,
      "eval_runtime": 4.5706,
      "eval_samples_per_second": 107.646,
      "eval_steps_per_second": 1.75,
      "step": 1388250
    },
    {
      "epoch": 450.0162074554295,
      "grad_norm": 1.4147231578826904,
      "learning_rate": 5.0196561790463835e-06,
      "loss": 2.4631,
      "step": 1388300
    },
    {
      "epoch": 450.0486223662885,
      "grad_norm": 1.3248854875564575,
      "learning_rate": 5.0164125851443405e-06,
      "loss": 2.4759,
      "step": 1388400
    },
    {
      "epoch": 450.0810372771475,
      "grad_norm": 1.4646737575531006,
      "learning_rate": 5.013168991242297e-06,
      "loss": 2.4662,
      "step": 1388500
    },
    {
      "epoch": 450.1134521880065,
      "grad_norm": 1.3705841302871704,
      "learning_rate": 5.009925397340253e-06,
      "loss": 2.4517,
      "step": 1388600
    },
    {
      "epoch": 450.1458670988655,
      "grad_norm": 1.5063387155532837,
      "learning_rate": 5.00668180343821e-06,
      "loss": 2.4756,
      "step": 1388700
    },
    {
      "epoch": 450.17828200972446,
      "grad_norm": 1.3701094388961792,
      "learning_rate": 5.003438209536166e-06,
      "loss": 2.4511,
      "step": 1388800
    },
    {
      "epoch": 450.2106969205835,
      "grad_norm": 1.3012006282806396,
      "learning_rate": 5.000194615634122e-06,
      "loss": 2.4469,
      "step": 1388900
    },
    {
      "epoch": 450.2431118314425,
      "grad_norm": 1.4763339757919312,
      "learning_rate": 4.9969510217320794e-06,
      "loss": 2.4706,
      "step": 1389000
    },
    {
      "epoch": 450.27552674230145,
      "grad_norm": 1.3802541494369507,
      "learning_rate": 4.9937074278300365e-06,
      "loss": 2.4643,
      "step": 1389100
    },
    {
      "epoch": 450.30794165316047,
      "grad_norm": 1.370739221572876,
      "learning_rate": 4.990463833927993e-06,
      "loss": 2.453,
      "step": 1389200
    },
    {
      "epoch": 450.34035656401943,
      "grad_norm": 1.3678070306777954,
      "learning_rate": 4.987220240025949e-06,
      "loss": 2.4522,
      "step": 1389300
    },
    {
      "epoch": 450.37277147487845,
      "grad_norm": 1.4770134687423706,
      "learning_rate": 4.983976646123906e-06,
      "loss": 2.4521,
      "step": 1389400
    },
    {
      "epoch": 450.40518638573747,
      "grad_norm": 1.3704109191894531,
      "learning_rate": 4.980733052221862e-06,
      "loss": 2.4632,
      "step": 1389500
    },
    {
      "epoch": 450.4376012965964,
      "grad_norm": 1.5124846696853638,
      "learning_rate": 4.977521894258839e-06,
      "loss": 2.4715,
      "step": 1389600
    },
    {
      "epoch": 450.47001620745544,
      "grad_norm": 1.4346833229064941,
      "learning_rate": 4.974278300356795e-06,
      "loss": 2.456,
      "step": 1389700
    },
    {
      "epoch": 450.5024311183144,
      "grad_norm": 1.3701450824737549,
      "learning_rate": 4.971034706454752e-06,
      "loss": 2.4504,
      "step": 1389800
    },
    {
      "epoch": 450.5348460291734,
      "grad_norm": 1.681988000869751,
      "learning_rate": 4.9677911125527085e-06,
      "loss": 2.4487,
      "step": 1389900
    },
    {
      "epoch": 450.56726094003244,
      "grad_norm": 1.4302822351455688,
      "learning_rate": 4.964547518650666e-06,
      "loss": 2.4592,
      "step": 1390000
    },
    {
      "epoch": 450.5996758508914,
      "grad_norm": 1.361626386642456,
      "learning_rate": 4.961303924748622e-06,
      "loss": 2.4626,
      "step": 1390100
    },
    {
      "epoch": 450.6320907617504,
      "grad_norm": 1.4878705739974976,
      "learning_rate": 4.958060330846578e-06,
      "loss": 2.4605,
      "step": 1390200
    },
    {
      "epoch": 450.6645056726094,
      "grad_norm": 1.5223618745803833,
      "learning_rate": 4.954816736944535e-06,
      "loss": 2.4566,
      "step": 1390300
    },
    {
      "epoch": 450.6969205834684,
      "grad_norm": 1.5704196691513062,
      "learning_rate": 4.951573143042491e-06,
      "loss": 2.444,
      "step": 1390400
    },
    {
      "epoch": 450.7293354943274,
      "grad_norm": 1.5787805318832397,
      "learning_rate": 4.9483295491404474e-06,
      "loss": 2.4514,
      "step": 1390500
    },
    {
      "epoch": 450.7617504051864,
      "grad_norm": 1.250065803527832,
      "learning_rate": 4.9450859552384045e-06,
      "loss": 2.4617,
      "step": 1390600
    },
    {
      "epoch": 450.7941653160454,
      "grad_norm": 1.5639808177947998,
      "learning_rate": 4.941842361336361e-06,
      "loss": 2.4347,
      "step": 1390700
    },
    {
      "epoch": 450.82658022690435,
      "grad_norm": 1.3729126453399658,
      "learning_rate": 4.938598767434318e-06,
      "loss": 2.4734,
      "step": 1390800
    },
    {
      "epoch": 450.8589951377634,
      "grad_norm": 1.3479390144348145,
      "learning_rate": 4.935355173532274e-06,
      "loss": 2.4404,
      "step": 1390900
    },
    {
      "epoch": 450.8914100486224,
      "grad_norm": 1.8183059692382812,
      "learning_rate": 4.93211157963023e-06,
      "loss": 2.4382,
      "step": 1391000
    },
    {
      "epoch": 450.92382495948135,
      "grad_norm": 1.5243257284164429,
      "learning_rate": 4.928867985728187e-06,
      "loss": 2.451,
      "step": 1391100
    },
    {
      "epoch": 450.95623987034037,
      "grad_norm": 1.6110285520553589,
      "learning_rate": 4.925624391826143e-06,
      "loss": 2.4735,
      "step": 1391200
    },
    {
      "epoch": 450.98865478119933,
      "grad_norm": 1.3545156717300415,
      "learning_rate": 4.9223807979241004e-06,
      "loss": 2.4509,
      "step": 1391300
    },
    {
      "epoch": 451.0,
      "eval_bleu": 1.281505059613121,
      "eval_loss": 4.276974201202393,
      "eval_runtime": 4.5902,
      "eval_samples_per_second": 107.186,
      "eval_steps_per_second": 1.743,
      "step": 1391335
    },
    {
      "epoch": 451.02106969205835,
      "grad_norm": 1.1659433841705322,
      "learning_rate": 4.919137204022057e-06,
      "loss": 2.4889,
      "step": 1391400
    },
    {
      "epoch": 451.05348460291737,
      "grad_norm": 1.342202067375183,
      "learning_rate": 4.915893610120014e-06,
      "loss": 2.4647,
      "step": 1391500
    },
    {
      "epoch": 451.0858995137763,
      "grad_norm": 1.431169867515564,
      "learning_rate": 4.91265001621797e-06,
      "loss": 2.4514,
      "step": 1391600
    },
    {
      "epoch": 451.11831442463534,
      "grad_norm": 1.4309285879135132,
      "learning_rate": 4.909406422315926e-06,
      "loss": 2.449,
      "step": 1391700
    },
    {
      "epoch": 451.1507293354943,
      "grad_norm": 1.5898510217666626,
      "learning_rate": 4.906162828413883e-06,
      "loss": 2.4895,
      "step": 1391800
    },
    {
      "epoch": 451.1831442463533,
      "grad_norm": 1.6132540702819824,
      "learning_rate": 4.902919234511839e-06,
      "loss": 2.4254,
      "step": 1391900
    },
    {
      "epoch": 451.21555915721234,
      "grad_norm": 1.3439457416534424,
      "learning_rate": 4.8996756406097955e-06,
      "loss": 2.4486,
      "step": 1392000
    },
    {
      "epoch": 451.2479740680713,
      "grad_norm": 1.4135003089904785,
      "learning_rate": 4.896432046707753e-06,
      "loss": 2.4671,
      "step": 1392100
    },
    {
      "epoch": 451.2803889789303,
      "grad_norm": 1.354030966758728,
      "learning_rate": 4.893188452805709e-06,
      "loss": 2.4489,
      "step": 1392200
    },
    {
      "epoch": 451.3128038897893,
      "grad_norm": 1.377909779548645,
      "learning_rate": 4.889944858903665e-06,
      "loss": 2.424,
      "step": 1392300
    },
    {
      "epoch": 451.3452188006483,
      "grad_norm": 1.361562967300415,
      "learning_rate": 4.886701265001622e-06,
      "loss": 2.429,
      "step": 1392400
    },
    {
      "epoch": 451.3776337115073,
      "grad_norm": 1.4120508432388306,
      "learning_rate": 4.883457671099578e-06,
      "loss": 2.4638,
      "step": 1392500
    },
    {
      "epoch": 451.4100486223663,
      "grad_norm": 1.4243260622024536,
      "learning_rate": 4.8802140771975345e-06,
      "loss": 2.4477,
      "step": 1392600
    },
    {
      "epoch": 451.4424635332253,
      "grad_norm": 1.4903154373168945,
      "learning_rate": 4.876970483295492e-06,
      "loss": 2.4877,
      "step": 1392700
    },
    {
      "epoch": 451.47487844408425,
      "grad_norm": 1.6337709426879883,
      "learning_rate": 4.8737268893934485e-06,
      "loss": 2.4741,
      "step": 1392800
    },
    {
      "epoch": 451.5072933549433,
      "grad_norm": 1.2255241870880127,
      "learning_rate": 4.870483295491405e-06,
      "loss": 2.4529,
      "step": 1392900
    },
    {
      "epoch": 451.5397082658023,
      "grad_norm": 1.3571453094482422,
      "learning_rate": 4.867239701589362e-06,
      "loss": 2.4407,
      "step": 1393000
    },
    {
      "epoch": 451.57212317666125,
      "grad_norm": 1.5279432535171509,
      "learning_rate": 4.863996107687318e-06,
      "loss": 2.4578,
      "step": 1393100
    },
    {
      "epoch": 451.60453808752027,
      "grad_norm": 1.3551344871520996,
      "learning_rate": 4.860752513785274e-06,
      "loss": 2.4715,
      "step": 1393200
    },
    {
      "epoch": 451.63695299837923,
      "grad_norm": 1.691519856452942,
      "learning_rate": 4.857508919883231e-06,
      "loss": 2.4564,
      "step": 1393300
    },
    {
      "epoch": 451.66936790923825,
      "grad_norm": 1.364625334739685,
      "learning_rate": 4.8542653259811875e-06,
      "loss": 2.4736,
      "step": 1393400
    },
    {
      "epoch": 451.70178282009726,
      "grad_norm": 1.2192838191986084,
      "learning_rate": 4.851021732079144e-06,
      "loss": 2.4593,
      "step": 1393500
    },
    {
      "epoch": 451.7341977309562,
      "grad_norm": 1.3733612298965454,
      "learning_rate": 4.8478105741161215e-06,
      "loss": 2.4729,
      "step": 1393600
    },
    {
      "epoch": 451.76661264181524,
      "grad_norm": 1.5129973888397217,
      "learning_rate": 4.8445994161530976e-06,
      "loss": 2.4686,
      "step": 1393700
    },
    {
      "epoch": 451.7990275526742,
      "grad_norm": 1.4845057725906372,
      "learning_rate": 4.841355822251055e-06,
      "loss": 2.4575,
      "step": 1393800
    },
    {
      "epoch": 451.8314424635332,
      "grad_norm": 1.4429274797439575,
      "learning_rate": 4.838112228349011e-06,
      "loss": 2.4538,
      "step": 1393900
    },
    {
      "epoch": 451.86385737439224,
      "grad_norm": 1.288513422012329,
      "learning_rate": 4.834868634446967e-06,
      "loss": 2.4623,
      "step": 1394000
    },
    {
      "epoch": 451.8962722852512,
      "grad_norm": 1.469395399093628,
      "learning_rate": 4.831625040544924e-06,
      "loss": 2.4481,
      "step": 1394100
    },
    {
      "epoch": 451.9286871961102,
      "grad_norm": 1.487390160560608,
      "learning_rate": 4.82838144664288e-06,
      "loss": 2.4749,
      "step": 1394200
    },
    {
      "epoch": 451.9611021069692,
      "grad_norm": 1.7272663116455078,
      "learning_rate": 4.825137852740837e-06,
      "loss": 2.4439,
      "step": 1394300
    },
    {
      "epoch": 451.9935170178282,
      "grad_norm": 1.583342432975769,
      "learning_rate": 4.8218942588387935e-06,
      "loss": 2.4451,
      "step": 1394400
    },
    {
      "epoch": 452.0,
      "eval_bleu": 1.189050079377705,
      "eval_loss": 4.2768144607543945,
      "eval_runtime": 4.3327,
      "eval_samples_per_second": 113.556,
      "eval_steps_per_second": 1.846,
      "step": 1394420
    },
    {
      "epoch": 452.0259319286872,
      "grad_norm": 1.4839882850646973,
      "learning_rate": 4.8186506649367506e-06,
      "loss": 2.451,
      "step": 1394500
    },
    {
      "epoch": 452.0583468395462,
      "grad_norm": 1.4234691858291626,
      "learning_rate": 4.815407071034707e-06,
      "loss": 2.4437,
      "step": 1394600
    },
    {
      "epoch": 452.0907617504052,
      "grad_norm": 1.4927102327346802,
      "learning_rate": 4.812163477132663e-06,
      "loss": 2.4384,
      "step": 1394700
    },
    {
      "epoch": 452.12317666126415,
      "grad_norm": 1.4867724180221558,
      "learning_rate": 4.80891988323062e-06,
      "loss": 2.4695,
      "step": 1394800
    },
    {
      "epoch": 452.15559157212317,
      "grad_norm": 1.3457945585250854,
      "learning_rate": 4.805676289328576e-06,
      "loss": 2.4622,
      "step": 1394900
    },
    {
      "epoch": 452.1880064829822,
      "grad_norm": 1.6367284059524536,
      "learning_rate": 4.802432695426532e-06,
      "loss": 2.4596,
      "step": 1395000
    },
    {
      "epoch": 452.22042139384115,
      "grad_norm": 1.4910027980804443,
      "learning_rate": 4.7991891015244895e-06,
      "loss": 2.4475,
      "step": 1395100
    },
    {
      "epoch": 452.25283630470017,
      "grad_norm": 1.4605847597122192,
      "learning_rate": 4.795945507622446e-06,
      "loss": 2.4504,
      "step": 1395200
    },
    {
      "epoch": 452.2852512155592,
      "grad_norm": 1.8114019632339478,
      "learning_rate": 4.792701913720402e-06,
      "loss": 2.4775,
      "step": 1395300
    },
    {
      "epoch": 452.31766612641815,
      "grad_norm": 1.2473902702331543,
      "learning_rate": 4.789458319818359e-06,
      "loss": 2.4515,
      "step": 1395400
    },
    {
      "epoch": 452.35008103727716,
      "grad_norm": 1.3800095319747925,
      "learning_rate": 4.786214725916315e-06,
      "loss": 2.4434,
      "step": 1395500
    },
    {
      "epoch": 452.3824959481361,
      "grad_norm": 1.4567111730575562,
      "learning_rate": 4.782971132014271e-06,
      "loss": 2.4638,
      "step": 1395600
    },
    {
      "epoch": 452.41491085899514,
      "grad_norm": 1.475933313369751,
      "learning_rate": 4.779727538112229e-06,
      "loss": 2.4508,
      "step": 1395700
    },
    {
      "epoch": 452.44732576985416,
      "grad_norm": 1.624072551727295,
      "learning_rate": 4.776483944210185e-06,
      "loss": 2.4661,
      "step": 1395800
    },
    {
      "epoch": 452.4797406807131,
      "grad_norm": 1.2759687900543213,
      "learning_rate": 4.773240350308142e-06,
      "loss": 2.4481,
      "step": 1395900
    },
    {
      "epoch": 452.51215559157214,
      "grad_norm": 1.4711458683013916,
      "learning_rate": 4.769996756406099e-06,
      "loss": 2.4581,
      "step": 1396000
    },
    {
      "epoch": 452.5445705024311,
      "grad_norm": 1.3789883852005005,
      "learning_rate": 4.766753162504055e-06,
      "loss": 2.4697,
      "step": 1396100
    },
    {
      "epoch": 452.5769854132901,
      "grad_norm": 1.256333351135254,
      "learning_rate": 4.763509568602011e-06,
      "loss": 2.4298,
      "step": 1396200
    },
    {
      "epoch": 452.60940032414914,
      "grad_norm": 1.560694694519043,
      "learning_rate": 4.760265974699968e-06,
      "loss": 2.4533,
      "step": 1396300
    },
    {
      "epoch": 452.6418152350081,
      "grad_norm": 1.5057016611099243,
      "learning_rate": 4.757022380797924e-06,
      "loss": 2.446,
      "step": 1396400
    },
    {
      "epoch": 452.6742301458671,
      "grad_norm": 1.4415266513824463,
      "learning_rate": 4.7537787868958805e-06,
      "loss": 2.457,
      "step": 1396500
    },
    {
      "epoch": 452.7066450567261,
      "grad_norm": 1.3924388885498047,
      "learning_rate": 4.750535192993838e-06,
      "loss": 2.4438,
      "step": 1396600
    },
    {
      "epoch": 452.7390599675851,
      "grad_norm": 1.6167187690734863,
      "learning_rate": 4.747291599091794e-06,
      "loss": 2.4629,
      "step": 1396700
    },
    {
      "epoch": 452.7714748784441,
      "grad_norm": 1.3703423738479614,
      "learning_rate": 4.74404800518975e-06,
      "loss": 2.4601,
      "step": 1396800
    },
    {
      "epoch": 452.80388978930307,
      "grad_norm": 1.4630506038665771,
      "learning_rate": 4.740804411287707e-06,
      "loss": 2.4783,
      "step": 1396900
    },
    {
      "epoch": 452.8363047001621,
      "grad_norm": 1.4058424234390259,
      "learning_rate": 4.737560817385663e-06,
      "loss": 2.4557,
      "step": 1397000
    },
    {
      "epoch": 452.86871961102105,
      "grad_norm": 1.4128893613815308,
      "learning_rate": 4.73431722348362e-06,
      "loss": 2.4677,
      "step": 1397100
    },
    {
      "epoch": 452.90113452188007,
      "grad_norm": 1.3728662729263306,
      "learning_rate": 4.7310736295815765e-06,
      "loss": 2.469,
      "step": 1397200
    },
    {
      "epoch": 452.9335494327391,
      "grad_norm": 1.5864263772964478,
      "learning_rate": 4.7278300356795335e-06,
      "loss": 2.4407,
      "step": 1397300
    },
    {
      "epoch": 452.96596434359805,
      "grad_norm": 1.6052616834640503,
      "learning_rate": 4.72458644177749e-06,
      "loss": 2.4671,
      "step": 1397400
    },
    {
      "epoch": 452.99837925445706,
      "grad_norm": 1.284254550933838,
      "learning_rate": 4.721342847875447e-06,
      "loss": 2.4669,
      "step": 1397500
    },
    {
      "epoch": 453.0,
      "eval_bleu": 1.1633720682836282,
      "eval_loss": 4.277168273925781,
      "eval_runtime": 4.4504,
      "eval_samples_per_second": 110.551,
      "eval_steps_per_second": 1.798,
      "step": 1397505
    },
    {
      "epoch": 453.030794165316,
      "grad_norm": 1.3548181056976318,
      "learning_rate": 4.718099253973403e-06,
      "loss": 2.4648,
      "step": 1397600
    },
    {
      "epoch": 453.06320907617504,
      "grad_norm": 1.503633975982666,
      "learning_rate": 4.714888096010379e-06,
      "loss": 2.4701,
      "step": 1397700
    },
    {
      "epoch": 453.09562398703406,
      "grad_norm": 1.492688775062561,
      "learning_rate": 4.711644502108336e-06,
      "loss": 2.4667,
      "step": 1397800
    },
    {
      "epoch": 453.128038897893,
      "grad_norm": 1.5857080221176147,
      "learning_rate": 4.708400908206293e-06,
      "loss": 2.4495,
      "step": 1397900
    },
    {
      "epoch": 453.16045380875204,
      "grad_norm": 1.3700878620147705,
      "learning_rate": 4.705157314304249e-06,
      "loss": 2.4812,
      "step": 1398000
    },
    {
      "epoch": 453.192868719611,
      "grad_norm": 1.5328270196914673,
      "learning_rate": 4.7019137204022064e-06,
      "loss": 2.4732,
      "step": 1398100
    },
    {
      "epoch": 453.22528363047,
      "grad_norm": 1.4393961429595947,
      "learning_rate": 4.698670126500163e-06,
      "loss": 2.4493,
      "step": 1398200
    },
    {
      "epoch": 453.25769854132903,
      "grad_norm": 1.5389162302017212,
      "learning_rate": 4.695426532598119e-06,
      "loss": 2.4496,
      "step": 1398300
    },
    {
      "epoch": 453.290113452188,
      "grad_norm": 1.3897922039031982,
      "learning_rate": 4.692182938696076e-06,
      "loss": 2.4701,
      "step": 1398400
    },
    {
      "epoch": 453.322528363047,
      "grad_norm": 1.3356884717941284,
      "learning_rate": 4.688939344794032e-06,
      "loss": 2.4472,
      "step": 1398500
    },
    {
      "epoch": 453.354943273906,
      "grad_norm": 1.5261170864105225,
      "learning_rate": 4.685695750891988e-06,
      "loss": 2.4602,
      "step": 1398600
    },
    {
      "epoch": 453.387358184765,
      "grad_norm": 1.5360684394836426,
      "learning_rate": 4.682452156989945e-06,
      "loss": 2.4389,
      "step": 1398700
    },
    {
      "epoch": 453.419773095624,
      "grad_norm": 1.3661469221115112,
      "learning_rate": 4.6792085630879015e-06,
      "loss": 2.441,
      "step": 1398800
    },
    {
      "epoch": 453.45218800648297,
      "grad_norm": 1.2869778871536255,
      "learning_rate": 4.675964969185858e-06,
      "loss": 2.4258,
      "step": 1398900
    },
    {
      "epoch": 453.484602917342,
      "grad_norm": 1.4124456644058228,
      "learning_rate": 4.672721375283815e-06,
      "loss": 2.4434,
      "step": 1399000
    },
    {
      "epoch": 453.51701782820095,
      "grad_norm": 1.452396035194397,
      "learning_rate": 4.669477781381771e-06,
      "loss": 2.4735,
      "step": 1399100
    },
    {
      "epoch": 453.54943273905997,
      "grad_norm": 1.6079678535461426,
      "learning_rate": 4.666234187479727e-06,
      "loss": 2.4633,
      "step": 1399200
    },
    {
      "epoch": 453.581847649919,
      "grad_norm": 1.384911060333252,
      "learning_rate": 4.662990593577684e-06,
      "loss": 2.4708,
      "step": 1399300
    },
    {
      "epoch": 453.61426256077795,
      "grad_norm": 1.3987330198287964,
      "learning_rate": 4.659746999675641e-06,
      "loss": 2.4427,
      "step": 1399400
    },
    {
      "epoch": 453.64667747163696,
      "grad_norm": 1.4934135675430298,
      "learning_rate": 4.6565034057735975e-06,
      "loss": 2.4562,
      "step": 1399500
    },
    {
      "epoch": 453.6790923824959,
      "grad_norm": 1.4632450342178345,
      "learning_rate": 4.653259811871554e-06,
      "loss": 2.4434,
      "step": 1399600
    },
    {
      "epoch": 453.71150729335494,
      "grad_norm": 1.3628844022750854,
      "learning_rate": 4.650048653908531e-06,
      "loss": 2.4581,
      "step": 1399700
    },
    {
      "epoch": 453.74392220421396,
      "grad_norm": 1.313637614250183,
      "learning_rate": 4.646805060006487e-06,
      "loss": 2.4246,
      "step": 1399800
    },
    {
      "epoch": 453.7763371150729,
      "grad_norm": 1.427477240562439,
      "learning_rate": 4.643561466104444e-06,
      "loss": 2.4647,
      "step": 1399900
    },
    {
      "epoch": 453.80875202593194,
      "grad_norm": 1.2581145763397217,
      "learning_rate": 4.6403178722024e-06,
      "loss": 2.4804,
      "step": 1400000
    },
    {
      "epoch": 453.8411669367909,
      "grad_norm": 1.4902918338775635,
      "learning_rate": 4.637074278300357e-06,
      "loss": 2.46,
      "step": 1400100
    },
    {
      "epoch": 453.8735818476499,
      "grad_norm": 1.4011611938476562,
      "learning_rate": 4.633830684398314e-06,
      "loss": 2.4397,
      "step": 1400200
    },
    {
      "epoch": 453.90599675850893,
      "grad_norm": 1.5010325908660889,
      "learning_rate": 4.63058709049627e-06,
      "loss": 2.4417,
      "step": 1400300
    },
    {
      "epoch": 453.9384116693679,
      "grad_norm": 1.397863507270813,
      "learning_rate": 4.627343496594227e-06,
      "loss": 2.4635,
      "step": 1400400
    },
    {
      "epoch": 453.9708265802269,
      "grad_norm": 1.3681859970092773,
      "learning_rate": 4.624099902692184e-06,
      "loss": 2.4668,
      "step": 1400500
    },
    {
      "epoch": 454.0,
      "eval_bleu": 1.1062143239969693,
      "eval_loss": 4.276951789855957,
      "eval_runtime": 4.3609,
      "eval_samples_per_second": 112.822,
      "eval_steps_per_second": 1.834,
      "step": 1400590
    },
    {
      "epoch": 454.0032414910859,
      "grad_norm": 1.4524741172790527,
      "learning_rate": 4.62085630879014e-06,
      "loss": 2.4726,
      "step": 1400600
    },
    {
      "epoch": 454.0356564019449,
      "grad_norm": 1.915386438369751,
      "learning_rate": 4.617612714888096e-06,
      "loss": 2.4548,
      "step": 1400700
    },
    {
      "epoch": 454.0680713128039,
      "grad_norm": 1.7975051403045654,
      "learning_rate": 4.614369120986053e-06,
      "loss": 2.4661,
      "step": 1400800
    },
    {
      "epoch": 454.10048622366287,
      "grad_norm": 1.5074564218521118,
      "learning_rate": 4.611125527084009e-06,
      "loss": 2.4685,
      "step": 1400900
    },
    {
      "epoch": 454.1329011345219,
      "grad_norm": 1.6168136596679688,
      "learning_rate": 4.6078819331819655e-06,
      "loss": 2.4821,
      "step": 1401000
    },
    {
      "epoch": 454.16531604538085,
      "grad_norm": 1.4649343490600586,
      "learning_rate": 4.6046383392799226e-06,
      "loss": 2.4556,
      "step": 1401100
    },
    {
      "epoch": 454.19773095623987,
      "grad_norm": 1.4752774238586426,
      "learning_rate": 4.601394745377879e-06,
      "loss": 2.4681,
      "step": 1401200
    },
    {
      "epoch": 454.2301458670989,
      "grad_norm": 1.3615858554840088,
      "learning_rate": 4.598151151475835e-06,
      "loss": 2.4723,
      "step": 1401300
    },
    {
      "epoch": 454.26256077795784,
      "grad_norm": 1.3204022645950317,
      "learning_rate": 4.594907557573792e-06,
      "loss": 2.4423,
      "step": 1401400
    },
    {
      "epoch": 454.29497568881686,
      "grad_norm": 1.306904911994934,
      "learning_rate": 4.591663963671749e-06,
      "loss": 2.456,
      "step": 1401500
    },
    {
      "epoch": 454.3273905996758,
      "grad_norm": 1.3322888612747192,
      "learning_rate": 4.588420369769705e-06,
      "loss": 2.4335,
      "step": 1401600
    },
    {
      "epoch": 454.35980551053484,
      "grad_norm": 1.5449612140655518,
      "learning_rate": 4.585209211806682e-06,
      "loss": 2.4305,
      "step": 1401700
    },
    {
      "epoch": 454.39222042139386,
      "grad_norm": 1.3668169975280762,
      "learning_rate": 4.581965617904638e-06,
      "loss": 2.451,
      "step": 1401800
    },
    {
      "epoch": 454.4246353322528,
      "grad_norm": 1.485511064529419,
      "learning_rate": 4.578722024002595e-06,
      "loss": 2.4479,
      "step": 1401900
    },
    {
      "epoch": 454.45705024311184,
      "grad_norm": 1.2445392608642578,
      "learning_rate": 4.575478430100552e-06,
      "loss": 2.4354,
      "step": 1402000
    },
    {
      "epoch": 454.48946515397085,
      "grad_norm": 1.4365851879119873,
      "learning_rate": 4.572234836198508e-06,
      "loss": 2.4335,
      "step": 1402100
    },
    {
      "epoch": 454.5218800648298,
      "grad_norm": 1.6277778148651123,
      "learning_rate": 4.568991242296464e-06,
      "loss": 2.4622,
      "step": 1402200
    },
    {
      "epoch": 454.55429497568883,
      "grad_norm": 1.2459142208099365,
      "learning_rate": 4.565747648394422e-06,
      "loss": 2.4474,
      "step": 1402300
    },
    {
      "epoch": 454.5867098865478,
      "grad_norm": 1.4735461473464966,
      "learning_rate": 4.562504054492378e-06,
      "loss": 2.4455,
      "step": 1402400
    },
    {
      "epoch": 454.6191247974068,
      "grad_norm": 1.399374008178711,
      "learning_rate": 4.559260460590334e-06,
      "loss": 2.4588,
      "step": 1402500
    },
    {
      "epoch": 454.65153970826583,
      "grad_norm": 1.2934544086456299,
      "learning_rate": 4.556016866688291e-06,
      "loss": 2.4488,
      "step": 1402600
    },
    {
      "epoch": 454.6839546191248,
      "grad_norm": 1.494681477546692,
      "learning_rate": 4.552773272786248e-06,
      "loss": 2.4635,
      "step": 1402700
    },
    {
      "epoch": 454.7163695299838,
      "grad_norm": 1.557991862297058,
      "learning_rate": 4.549529678884204e-06,
      "loss": 2.4534,
      "step": 1402800
    },
    {
      "epoch": 454.74878444084277,
      "grad_norm": 1.5747523307800293,
      "learning_rate": 4.546286084982161e-06,
      "loss": 2.4737,
      "step": 1402900
    },
    {
      "epoch": 454.7811993517018,
      "grad_norm": 1.4291387796401978,
      "learning_rate": 4.543042491080117e-06,
      "loss": 2.4287,
      "step": 1403000
    },
    {
      "epoch": 454.8136142625608,
      "grad_norm": 1.3595582246780396,
      "learning_rate": 4.539798897178073e-06,
      "loss": 2.4687,
      "step": 1403100
    },
    {
      "epoch": 454.84602917341977,
      "grad_norm": 1.4348535537719727,
      "learning_rate": 4.53655530327603e-06,
      "loss": 2.4771,
      "step": 1403200
    },
    {
      "epoch": 454.8784440842788,
      "grad_norm": 1.4799796342849731,
      "learning_rate": 4.533344145313007e-06,
      "loss": 2.4543,
      "step": 1403300
    },
    {
      "epoch": 454.91085899513774,
      "grad_norm": 1.446561574935913,
      "learning_rate": 4.5301005514109635e-06,
      "loss": 2.4476,
      "step": 1403400
    },
    {
      "epoch": 454.94327390599676,
      "grad_norm": 1.4133154153823853,
      "learning_rate": 4.5268569575089205e-06,
      "loss": 2.4664,
      "step": 1403500
    },
    {
      "epoch": 454.9756888168558,
      "grad_norm": 1.454217553138733,
      "learning_rate": 4.523613363606877e-06,
      "loss": 2.4687,
      "step": 1403600
    },
    {
      "epoch": 455.0,
      "eval_bleu": 1.1305552350617822,
      "eval_loss": 4.276638984680176,
      "eval_runtime": 4.6139,
      "eval_samples_per_second": 106.635,
      "eval_steps_per_second": 1.734,
      "step": 1403675
    },
    {
      "epoch": 455.00810372771474,
      "grad_norm": 1.428599238395691,
      "learning_rate": 4.520369769704833e-06,
      "loss": 2.4595,
      "step": 1403700
    },
    {
      "epoch": 455.04051863857376,
      "grad_norm": 1.745545744895935,
      "learning_rate": 4.51712617580279e-06,
      "loss": 2.4634,
      "step": 1403800
    },
    {
      "epoch": 455.0729335494327,
      "grad_norm": 1.3220754861831665,
      "learning_rate": 4.513882581900746e-06,
      "loss": 2.4457,
      "step": 1403900
    },
    {
      "epoch": 455.10534846029174,
      "grad_norm": 1.2304878234863281,
      "learning_rate": 4.510638987998702e-06,
      "loss": 2.4463,
      "step": 1404000
    },
    {
      "epoch": 455.13776337115075,
      "grad_norm": 1.6546634435653687,
      "learning_rate": 4.5073953940966594e-06,
      "loss": 2.4622,
      "step": 1404100
    },
    {
      "epoch": 455.1701782820097,
      "grad_norm": 1.662108063697815,
      "learning_rate": 4.504151800194616e-06,
      "loss": 2.4589,
      "step": 1404200
    },
    {
      "epoch": 455.20259319286873,
      "grad_norm": 1.4866876602172852,
      "learning_rate": 4.500908206292572e-06,
      "loss": 2.4193,
      "step": 1404300
    },
    {
      "epoch": 455.2350081037277,
      "grad_norm": 1.4429304599761963,
      "learning_rate": 4.497664612390529e-06,
      "loss": 2.4481,
      "step": 1404400
    },
    {
      "epoch": 455.2674230145867,
      "grad_norm": 1.2699435949325562,
      "learning_rate": 4.494421018488486e-06,
      "loss": 2.4482,
      "step": 1404500
    },
    {
      "epoch": 455.29983792544573,
      "grad_norm": 1.4146480560302734,
      "learning_rate": 4.491177424586442e-06,
      "loss": 2.447,
      "step": 1404600
    },
    {
      "epoch": 455.3322528363047,
      "grad_norm": 1.3885568380355835,
      "learning_rate": 4.487933830684399e-06,
      "loss": 2.4805,
      "step": 1404700
    },
    {
      "epoch": 455.3646677471637,
      "grad_norm": 1.3442726135253906,
      "learning_rate": 4.484690236782355e-06,
      "loss": 2.4325,
      "step": 1404800
    },
    {
      "epoch": 455.39708265802267,
      "grad_norm": 1.3748122453689575,
      "learning_rate": 4.481446642880312e-06,
      "loss": 2.4751,
      "step": 1404900
    },
    {
      "epoch": 455.4294975688817,
      "grad_norm": 1.3891717195510864,
      "learning_rate": 4.478203048978269e-06,
      "loss": 2.4564,
      "step": 1405000
    },
    {
      "epoch": 455.4619124797407,
      "grad_norm": 1.5262370109558105,
      "learning_rate": 4.474959455076225e-06,
      "loss": 2.4605,
      "step": 1405100
    },
    {
      "epoch": 455.49432739059966,
      "grad_norm": 1.3697813749313354,
      "learning_rate": 4.471715861174181e-06,
      "loss": 2.4633,
      "step": 1405200
    },
    {
      "epoch": 455.5267423014587,
      "grad_norm": 1.5683395862579346,
      "learning_rate": 4.468472267272138e-06,
      "loss": 2.4722,
      "step": 1405300
    },
    {
      "epoch": 455.55915721231764,
      "grad_norm": 1.4113081693649292,
      "learning_rate": 4.465228673370094e-06,
      "loss": 2.4555,
      "step": 1405400
    },
    {
      "epoch": 455.59157212317666,
      "grad_norm": 1.5705418586730957,
      "learning_rate": 4.4619850794680505e-06,
      "loss": 2.4617,
      "step": 1405500
    },
    {
      "epoch": 455.6239870340357,
      "grad_norm": 1.4852194786071777,
      "learning_rate": 4.4587414855660075e-06,
      "loss": 2.4398,
      "step": 1405600
    },
    {
      "epoch": 455.65640194489464,
      "grad_norm": 1.3191026449203491,
      "learning_rate": 4.455497891663964e-06,
      "loss": 2.4822,
      "step": 1405700
    },
    {
      "epoch": 455.68881685575366,
      "grad_norm": 1.4757816791534424,
      "learning_rate": 4.45225429776192e-06,
      "loss": 2.4478,
      "step": 1405800
    },
    {
      "epoch": 455.7212317666126,
      "grad_norm": 1.4447495937347412,
      "learning_rate": 4.449010703859877e-06,
      "loss": 2.458,
      "step": 1405900
    },
    {
      "epoch": 455.75364667747164,
      "grad_norm": 1.5312716960906982,
      "learning_rate": 4.445767109957834e-06,
      "loss": 2.4635,
      "step": 1406000
    },
    {
      "epoch": 455.78606158833065,
      "grad_norm": 1.4146946668624878,
      "learning_rate": 4.44252351605579e-06,
      "loss": 2.4269,
      "step": 1406100
    },
    {
      "epoch": 455.8184764991896,
      "grad_norm": 1.613957405090332,
      "learning_rate": 4.4392799221537464e-06,
      "loss": 2.4614,
      "step": 1406200
    },
    {
      "epoch": 455.85089141004863,
      "grad_norm": 1.4605340957641602,
      "learning_rate": 4.4360363282517035e-06,
      "loss": 2.4523,
      "step": 1406300
    },
    {
      "epoch": 455.8833063209076,
      "grad_norm": 1.3198965787887573,
      "learning_rate": 4.43279273434966e-06,
      "loss": 2.4648,
      "step": 1406400
    },
    {
      "epoch": 455.9157212317666,
      "grad_norm": 1.3857170343399048,
      "learning_rate": 4.429549140447616e-06,
      "loss": 2.4526,
      "step": 1406500
    },
    {
      "epoch": 455.94813614262563,
      "grad_norm": 1.4594203233718872,
      "learning_rate": 4.426305546545573e-06,
      "loss": 2.4609,
      "step": 1406600
    },
    {
      "epoch": 455.9805510534846,
      "grad_norm": 1.6158040761947632,
      "learning_rate": 4.423061952643529e-06,
      "loss": 2.4672,
      "step": 1406700
    },
    {
      "epoch": 456.0,
      "eval_bleu": 1.1076216234096172,
      "eval_loss": 4.275479793548584,
      "eval_runtime": 4.4386,
      "eval_samples_per_second": 110.845,
      "eval_steps_per_second": 1.802,
      "step": 1406760
    },
    {
      "epoch": 456.0129659643436,
      "grad_norm": 1.9863409996032715,
      "learning_rate": 4.419818358741485e-06,
      "loss": 2.4373,
      "step": 1406800
    },
    {
      "epoch": 456.04538087520257,
      "grad_norm": 1.4180091619491577,
      "learning_rate": 4.416574764839442e-06,
      "loss": 2.4573,
      "step": 1406900
    },
    {
      "epoch": 456.0777957860616,
      "grad_norm": 1.4096565246582031,
      "learning_rate": 4.413363606876419e-06,
      "loss": 2.4484,
      "step": 1407000
    },
    {
      "epoch": 456.1102106969206,
      "grad_norm": 1.3247345685958862,
      "learning_rate": 4.4101200129743756e-06,
      "loss": 2.4559,
      "step": 1407100
    },
    {
      "epoch": 456.14262560777956,
      "grad_norm": 1.348490834236145,
      "learning_rate": 4.406876419072333e-06,
      "loss": 2.4619,
      "step": 1407200
    },
    {
      "epoch": 456.1750405186386,
      "grad_norm": 1.3035067319869995,
      "learning_rate": 4.403632825170289e-06,
      "loss": 2.4442,
      "step": 1407300
    },
    {
      "epoch": 456.20745542949754,
      "grad_norm": 1.4934031963348389,
      "learning_rate": 4.400389231268245e-06,
      "loss": 2.4558,
      "step": 1407400
    },
    {
      "epoch": 456.23987034035656,
      "grad_norm": 1.3351683616638184,
      "learning_rate": 4.397145637366202e-06,
      "loss": 2.4662,
      "step": 1407500
    },
    {
      "epoch": 456.2722852512156,
      "grad_norm": 1.408425211906433,
      "learning_rate": 4.393902043464158e-06,
      "loss": 2.4867,
      "step": 1407600
    },
    {
      "epoch": 456.30470016207454,
      "grad_norm": 1.4256900548934937,
      "learning_rate": 4.390658449562115e-06,
      "loss": 2.4476,
      "step": 1407700
    },
    {
      "epoch": 456.33711507293356,
      "grad_norm": 1.3852225542068481,
      "learning_rate": 4.3874148556600715e-06,
      "loss": 2.427,
      "step": 1407800
    },
    {
      "epoch": 456.3695299837925,
      "grad_norm": 1.5099588632583618,
      "learning_rate": 4.384171261758028e-06,
      "loss": 2.4393,
      "step": 1407900
    },
    {
      "epoch": 456.40194489465154,
      "grad_norm": 1.2845467329025269,
      "learning_rate": 4.380927667855985e-06,
      "loss": 2.4458,
      "step": 1408000
    },
    {
      "epoch": 456.43435980551055,
      "grad_norm": 1.427781581878662,
      "learning_rate": 4.377684073953941e-06,
      "loss": 2.4515,
      "step": 1408100
    },
    {
      "epoch": 456.4667747163695,
      "grad_norm": 1.4333807229995728,
      "learning_rate": 4.374440480051898e-06,
      "loss": 2.4575,
      "step": 1408200
    },
    {
      "epoch": 456.49918962722853,
      "grad_norm": 1.557732343673706,
      "learning_rate": 4.371229322088875e-06,
      "loss": 2.4644,
      "step": 1408300
    },
    {
      "epoch": 456.5316045380875,
      "grad_norm": 1.44526207447052,
      "learning_rate": 4.367985728186831e-06,
      "loss": 2.4586,
      "step": 1408400
    },
    {
      "epoch": 456.5640194489465,
      "grad_norm": 1.4439680576324463,
      "learning_rate": 4.364742134284787e-06,
      "loss": 2.4301,
      "step": 1408500
    },
    {
      "epoch": 456.5964343598055,
      "grad_norm": 1.33045494556427,
      "learning_rate": 4.361498540382744e-06,
      "loss": 2.4665,
      "step": 1408600
    },
    {
      "epoch": 456.6288492706645,
      "grad_norm": 1.4154767990112305,
      "learning_rate": 4.358254946480701e-06,
      "loss": 2.4542,
      "step": 1408700
    },
    {
      "epoch": 456.6612641815235,
      "grad_norm": 1.2993085384368896,
      "learning_rate": 4.355011352578657e-06,
      "loss": 2.4592,
      "step": 1408800
    },
    {
      "epoch": 456.6936790923825,
      "grad_norm": 1.4653209447860718,
      "learning_rate": 4.351767758676614e-06,
      "loss": 2.4653,
      "step": 1408900
    },
    {
      "epoch": 456.7260940032415,
      "grad_norm": 1.486811876296997,
      "learning_rate": 4.348524164774571e-06,
      "loss": 2.4619,
      "step": 1409000
    },
    {
      "epoch": 456.7585089141005,
      "grad_norm": 1.5767208337783813,
      "learning_rate": 4.345280570872527e-06,
      "loss": 2.4662,
      "step": 1409100
    },
    {
      "epoch": 456.79092382495946,
      "grad_norm": 1.386175274848938,
      "learning_rate": 4.342036976970483e-06,
      "loss": 2.4719,
      "step": 1409200
    },
    {
      "epoch": 456.8233387358185,
      "grad_norm": 1.4783358573913574,
      "learning_rate": 4.33879338306844e-06,
      "loss": 2.4619,
      "step": 1409300
    },
    {
      "epoch": 456.8557536466775,
      "grad_norm": 1.2971266508102417,
      "learning_rate": 4.3355497891663966e-06,
      "loss": 2.451,
      "step": 1409400
    },
    {
      "epoch": 456.88816855753646,
      "grad_norm": 1.4774763584136963,
      "learning_rate": 4.332306195264353e-06,
      "loss": 2.4533,
      "step": 1409500
    },
    {
      "epoch": 456.9205834683955,
      "grad_norm": 1.4645582437515259,
      "learning_rate": 4.32906260136231e-06,
      "loss": 2.4575,
      "step": 1409600
    },
    {
      "epoch": 456.95299837925444,
      "grad_norm": 1.653273582458496,
      "learning_rate": 4.325819007460266e-06,
      "loss": 2.4364,
      "step": 1409700
    },
    {
      "epoch": 456.98541329011346,
      "grad_norm": 1.6363472938537598,
      "learning_rate": 4.322575413558222e-06,
      "loss": 2.4666,
      "step": 1409800
    },
    {
      "epoch": 457.0,
      "eval_bleu": 0.9502618740065658,
      "eval_loss": 4.274448871612549,
      "eval_runtime": 4.4576,
      "eval_samples_per_second": 110.374,
      "eval_steps_per_second": 1.795,
      "step": 1409845
    },
    {
      "epoch": 457.0178282009725,
      "grad_norm": 1.4687057733535767,
      "learning_rate": 4.319331819656179e-06,
      "loss": 2.4567,
      "step": 1409900
    },
    {
      "epoch": 457.05024311183143,
      "grad_norm": 1.3508415222167969,
      "learning_rate": 4.3160882257541355e-06,
      "loss": 2.4361,
      "step": 1410000
    },
    {
      "epoch": 457.08265802269045,
      "grad_norm": 1.5657321214675903,
      "learning_rate": 4.312844631852092e-06,
      "loss": 2.4357,
      "step": 1410100
    },
    {
      "epoch": 457.1150729335494,
      "grad_norm": 1.342975378036499,
      "learning_rate": 4.309601037950049e-06,
      "loss": 2.4664,
      "step": 1410200
    },
    {
      "epoch": 457.14748784440843,
      "grad_norm": 1.3209749460220337,
      "learning_rate": 4.306357444048005e-06,
      "loss": 2.4417,
      "step": 1410300
    },
    {
      "epoch": 457.17990275526745,
      "grad_norm": 1.623998999595642,
      "learning_rate": 4.303113850145962e-06,
      "loss": 2.48,
      "step": 1410400
    },
    {
      "epoch": 457.2123176661264,
      "grad_norm": 1.3962044715881348,
      "learning_rate": 4.299870256243919e-06,
      "loss": 2.4496,
      "step": 1410500
    },
    {
      "epoch": 457.2447325769854,
      "grad_norm": 1.3476155996322632,
      "learning_rate": 4.296659098280895e-06,
      "loss": 2.4734,
      "step": 1410600
    },
    {
      "epoch": 457.2771474878444,
      "grad_norm": 1.5221400260925293,
      "learning_rate": 4.293415504378852e-06,
      "loss": 2.4315,
      "step": 1410700
    },
    {
      "epoch": 457.3095623987034,
      "grad_norm": 1.402079463005066,
      "learning_rate": 4.290204346415829e-06,
      "loss": 2.4651,
      "step": 1410800
    },
    {
      "epoch": 457.3419773095624,
      "grad_norm": 1.28223717212677,
      "learning_rate": 4.286960752513785e-06,
      "loss": 2.4704,
      "step": 1410900
    },
    {
      "epoch": 457.3743922204214,
      "grad_norm": 1.5330027341842651,
      "learning_rate": 4.283717158611742e-06,
      "loss": 2.4353,
      "step": 1411000
    },
    {
      "epoch": 457.4068071312804,
      "grad_norm": 1.2639992237091064,
      "learning_rate": 4.280473564709699e-06,
      "loss": 2.4535,
      "step": 1411100
    },
    {
      "epoch": 457.43922204213936,
      "grad_norm": 1.5573824644088745,
      "learning_rate": 4.277229970807655e-06,
      "loss": 2.4314,
      "step": 1411200
    },
    {
      "epoch": 457.4716369529984,
      "grad_norm": 1.3781888484954834,
      "learning_rate": 4.273986376905612e-06,
      "loss": 2.4451,
      "step": 1411300
    },
    {
      "epoch": 457.5040518638574,
      "grad_norm": 1.614511489868164,
      "learning_rate": 4.270742783003568e-06,
      "loss": 2.445,
      "step": 1411400
    },
    {
      "epoch": 457.53646677471636,
      "grad_norm": 1.417288064956665,
      "learning_rate": 4.267499189101524e-06,
      "loss": 2.4444,
      "step": 1411500
    },
    {
      "epoch": 457.5688816855754,
      "grad_norm": 1.4097585678100586,
      "learning_rate": 4.264255595199481e-06,
      "loss": 2.4552,
      "step": 1411600
    },
    {
      "epoch": 457.60129659643434,
      "grad_norm": 1.2913312911987305,
      "learning_rate": 4.2610120012974375e-06,
      "loss": 2.4607,
      "step": 1411700
    },
    {
      "epoch": 457.63371150729336,
      "grad_norm": 1.4099407196044922,
      "learning_rate": 4.257768407395394e-06,
      "loss": 2.4589,
      "step": 1411800
    },
    {
      "epoch": 457.6661264181524,
      "grad_norm": 1.2989507913589478,
      "learning_rate": 4.254524813493352e-06,
      "loss": 2.4578,
      "step": 1411900
    },
    {
      "epoch": 457.69854132901133,
      "grad_norm": 1.5122424364089966,
      "learning_rate": 4.251281219591308e-06,
      "loss": 2.4573,
      "step": 1412000
    },
    {
      "epoch": 457.73095623987035,
      "grad_norm": 1.3350193500518799,
      "learning_rate": 4.248037625689264e-06,
      "loss": 2.4622,
      "step": 1412100
    },
    {
      "epoch": 457.7633711507293,
      "grad_norm": 1.7329543828964233,
      "learning_rate": 4.244794031787221e-06,
      "loss": 2.455,
      "step": 1412200
    },
    {
      "epoch": 457.79578606158833,
      "grad_norm": 1.5453779697418213,
      "learning_rate": 4.241550437885177e-06,
      "loss": 2.452,
      "step": 1412300
    },
    {
      "epoch": 457.82820097244735,
      "grad_norm": 1.4803909063339233,
      "learning_rate": 4.2383068439831334e-06,
      "loss": 2.4468,
      "step": 1412400
    },
    {
      "epoch": 457.8606158833063,
      "grad_norm": 1.657198429107666,
      "learning_rate": 4.2350632500810905e-06,
      "loss": 2.4625,
      "step": 1412500
    },
    {
      "epoch": 457.8930307941653,
      "grad_norm": 1.593738079071045,
      "learning_rate": 4.231819656179047e-06,
      "loss": 2.481,
      "step": 1412600
    },
    {
      "epoch": 457.9254457050243,
      "grad_norm": 1.4189201593399048,
      "learning_rate": 4.228576062277003e-06,
      "loss": 2.4754,
      "step": 1412700
    },
    {
      "epoch": 457.9578606158833,
      "grad_norm": 1.7950754165649414,
      "learning_rate": 4.22533246837496e-06,
      "loss": 2.4413,
      "step": 1412800
    },
    {
      "epoch": 457.9902755267423,
      "grad_norm": 1.408851981163025,
      "learning_rate": 4.222088874472916e-06,
      "loss": 2.4521,
      "step": 1412900
    },
    {
      "epoch": 458.0,
      "eval_bleu": 0.929922937770613,
      "eval_loss": 4.279746055603027,
      "eval_runtime": 4.398,
      "eval_samples_per_second": 111.869,
      "eval_steps_per_second": 1.819,
      "step": 1412930
    },
    {
      "epoch": 458.0226904376013,
      "grad_norm": 1.4464784860610962,
      "learning_rate": 4.218845280570872e-06,
      "loss": 2.4505,
      "step": 1413000
    },
    {
      "epoch": 458.0551053484603,
      "grad_norm": 1.3339494466781616,
      "learning_rate": 4.215601686668829e-06,
      "loss": 2.4437,
      "step": 1413100
    },
    {
      "epoch": 458.08752025931926,
      "grad_norm": 1.4526678323745728,
      "learning_rate": 4.212358092766786e-06,
      "loss": 2.4616,
      "step": 1413200
    },
    {
      "epoch": 458.1199351701783,
      "grad_norm": 1.5023952722549438,
      "learning_rate": 4.209114498864742e-06,
      "loss": 2.4478,
      "step": 1413300
    },
    {
      "epoch": 458.1523500810373,
      "grad_norm": 1.6008682250976562,
      "learning_rate": 4.205870904962699e-06,
      "loss": 2.4476,
      "step": 1413400
    },
    {
      "epoch": 458.18476499189626,
      "grad_norm": 1.7098678350448608,
      "learning_rate": 4.202627311060656e-06,
      "loss": 2.4552,
      "step": 1413500
    },
    {
      "epoch": 458.2171799027553,
      "grad_norm": 1.4504555463790894,
      "learning_rate": 4.199383717158612e-06,
      "loss": 2.4481,
      "step": 1413600
    },
    {
      "epoch": 458.24959481361424,
      "grad_norm": 1.505990982055664,
      "learning_rate": 4.196140123256568e-06,
      "loss": 2.4614,
      "step": 1413700
    },
    {
      "epoch": 458.28200972447326,
      "grad_norm": 1.544478178024292,
      "learning_rate": 4.192896529354525e-06,
      "loss": 2.4725,
      "step": 1413800
    },
    {
      "epoch": 458.3144246353323,
      "grad_norm": 1.4886629581451416,
      "learning_rate": 4.1896529354524816e-06,
      "loss": 2.4579,
      "step": 1413900
    },
    {
      "epoch": 458.34683954619123,
      "grad_norm": 1.5106083154678345,
      "learning_rate": 4.186409341550438e-06,
      "loss": 2.4555,
      "step": 1414000
    },
    {
      "epoch": 458.37925445705025,
      "grad_norm": 1.3262699842453003,
      "learning_rate": 4.183165747648395e-06,
      "loss": 2.4522,
      "step": 1414100
    },
    {
      "epoch": 458.4116693679092,
      "grad_norm": 1.5210399627685547,
      "learning_rate": 4.179922153746351e-06,
      "loss": 2.4606,
      "step": 1414200
    },
    {
      "epoch": 458.44408427876823,
      "grad_norm": 1.5186911821365356,
      "learning_rate": 4.176678559844307e-06,
      "loss": 2.459,
      "step": 1414300
    },
    {
      "epoch": 458.47649918962725,
      "grad_norm": 1.3664605617523193,
      "learning_rate": 4.173434965942264e-06,
      "loss": 2.4537,
      "step": 1414400
    },
    {
      "epoch": 458.5089141004862,
      "grad_norm": 1.3833225965499878,
      "learning_rate": 4.1701913720402205e-06,
      "loss": 2.452,
      "step": 1414500
    },
    {
      "epoch": 458.5413290113452,
      "grad_norm": 1.5192269086837769,
      "learning_rate": 4.166947778138177e-06,
      "loss": 2.4384,
      "step": 1414600
    },
    {
      "epoch": 458.5737439222042,
      "grad_norm": 1.3581957817077637,
      "learning_rate": 4.163704184236134e-06,
      "loss": 2.4395,
      "step": 1414700
    },
    {
      "epoch": 458.6061588330632,
      "grad_norm": 1.501138687133789,
      "learning_rate": 4.160460590334091e-06,
      "loss": 2.4424,
      "step": 1414800
    },
    {
      "epoch": 458.6385737439222,
      "grad_norm": 1.538074254989624,
      "learning_rate": 4.157216996432047e-06,
      "loss": 2.4587,
      "step": 1414900
    },
    {
      "epoch": 458.6709886547812,
      "grad_norm": 1.5170563459396362,
      "learning_rate": 4.153973402530004e-06,
      "loss": 2.4616,
      "step": 1415000
    },
    {
      "epoch": 458.7034035656402,
      "grad_norm": 1.4727671146392822,
      "learning_rate": 4.15072980862796e-06,
      "loss": 2.4755,
      "step": 1415100
    },
    {
      "epoch": 458.73581847649916,
      "grad_norm": 1.3391436338424683,
      "learning_rate": 4.147486214725916e-06,
      "loss": 2.4526,
      "step": 1415200
    },
    {
      "epoch": 458.7682333873582,
      "grad_norm": 1.2958176136016846,
      "learning_rate": 4.1442426208238735e-06,
      "loss": 2.4619,
      "step": 1415300
    },
    {
      "epoch": 458.8006482982172,
      "grad_norm": 1.4303245544433594,
      "learning_rate": 4.14099902692183e-06,
      "loss": 2.4499,
      "step": 1415400
    },
    {
      "epoch": 458.83306320907616,
      "grad_norm": 1.4746901988983154,
      "learning_rate": 4.137755433019786e-06,
      "loss": 2.4784,
      "step": 1415500
    },
    {
      "epoch": 458.8654781199352,
      "grad_norm": 1.5788896083831787,
      "learning_rate": 4.134511839117743e-06,
      "loss": 2.4605,
      "step": 1415600
    },
    {
      "epoch": 458.8978930307942,
      "grad_norm": 1.39192533493042,
      "learning_rate": 4.131268245215699e-06,
      "loss": 2.4534,
      "step": 1415700
    },
    {
      "epoch": 458.93030794165315,
      "grad_norm": 1.3206883668899536,
      "learning_rate": 4.128024651313655e-06,
      "loss": 2.4502,
      "step": 1415800
    },
    {
      "epoch": 458.9627228525122,
      "grad_norm": 1.6312789916992188,
      "learning_rate": 4.124781057411612e-06,
      "loss": 2.452,
      "step": 1415900
    },
    {
      "epoch": 458.99513776337113,
      "grad_norm": 1.6160848140716553,
      "learning_rate": 4.1215374635095686e-06,
      "loss": 2.4505,
      "step": 1416000
    },
    {
      "epoch": 459.0,
      "eval_bleu": 1.1057050635067758,
      "eval_loss": 4.278891563415527,
      "eval_runtime": 4.1735,
      "eval_samples_per_second": 117.886,
      "eval_steps_per_second": 1.917,
      "step": 1416015
    },
    {
      "epoch": 459.02755267423015,
      "grad_norm": 1.33670175075531,
      "learning_rate": 4.118293869607525e-06,
      "loss": 2.4499,
      "step": 1416100
    },
    {
      "epoch": 459.05996758508917,
      "grad_norm": 1.9458969831466675,
      "learning_rate": 4.115050275705482e-06,
      "loss": 2.4674,
      "step": 1416200
    },
    {
      "epoch": 459.09238249594813,
      "grad_norm": 1.2849339246749878,
      "learning_rate": 4.111806681803439e-06,
      "loss": 2.4565,
      "step": 1416300
    },
    {
      "epoch": 459.12479740680715,
      "grad_norm": 1.4387400150299072,
      "learning_rate": 4.108563087901395e-06,
      "loss": 2.4646,
      "step": 1416400
    },
    {
      "epoch": 459.1572123176661,
      "grad_norm": 1.3493293523788452,
      "learning_rate": 4.105319493999351e-06,
      "loss": 2.4644,
      "step": 1416500
    },
    {
      "epoch": 459.1896272285251,
      "grad_norm": 1.5632948875427246,
      "learning_rate": 4.102075900097308e-06,
      "loss": 2.4631,
      "step": 1416600
    },
    {
      "epoch": 459.22204213938414,
      "grad_norm": 1.4510304927825928,
      "learning_rate": 4.0988323061952645e-06,
      "loss": 2.4292,
      "step": 1416700
    },
    {
      "epoch": 459.2544570502431,
      "grad_norm": 1.317996621131897,
      "learning_rate": 4.095588712293221e-06,
      "loss": 2.4365,
      "step": 1416800
    },
    {
      "epoch": 459.2868719611021,
      "grad_norm": 1.4904797077178955,
      "learning_rate": 4.092345118391178e-06,
      "loss": 2.4401,
      "step": 1416900
    },
    {
      "epoch": 459.3192868719611,
      "grad_norm": 1.3428544998168945,
      "learning_rate": 4.089101524489134e-06,
      "loss": 2.4756,
      "step": 1417000
    },
    {
      "epoch": 459.3517017828201,
      "grad_norm": 1.4189856052398682,
      "learning_rate": 4.085857930587091e-06,
      "loss": 2.4441,
      "step": 1417100
    },
    {
      "epoch": 459.3841166936791,
      "grad_norm": 1.1748521327972412,
      "learning_rate": 4.082614336685047e-06,
      "loss": 2.4687,
      "step": 1417200
    },
    {
      "epoch": 459.4165316045381,
      "grad_norm": 1.3764890432357788,
      "learning_rate": 4.079370742783003e-06,
      "loss": 2.4544,
      "step": 1417300
    },
    {
      "epoch": 459.4489465153971,
      "grad_norm": 1.521866798400879,
      "learning_rate": 4.0761271488809605e-06,
      "loss": 2.4467,
      "step": 1417400
    },
    {
      "epoch": 459.48136142625606,
      "grad_norm": 1.4583110809326172,
      "learning_rate": 4.072883554978917e-06,
      "loss": 2.4516,
      "step": 1417500
    },
    {
      "epoch": 459.5137763371151,
      "grad_norm": 1.3200647830963135,
      "learning_rate": 4.069639961076873e-06,
      "loss": 2.4585,
      "step": 1417600
    },
    {
      "epoch": 459.5461912479741,
      "grad_norm": 1.2545183897018433,
      "learning_rate": 4.06639636717483e-06,
      "loss": 2.445,
      "step": 1417700
    },
    {
      "epoch": 459.57860615883305,
      "grad_norm": 1.6548558473587036,
      "learning_rate": 4.063152773272787e-06,
      "loss": 2.459,
      "step": 1417800
    },
    {
      "epoch": 459.6110210696921,
      "grad_norm": 1.5393444299697876,
      "learning_rate": 4.059909179370743e-06,
      "loss": 2.4635,
      "step": 1417900
    },
    {
      "epoch": 459.64343598055103,
      "grad_norm": 1.5574700832366943,
      "learning_rate": 4.056665585468699e-06,
      "loss": 2.4478,
      "step": 1418000
    },
    {
      "epoch": 459.67585089141005,
      "grad_norm": 1.3233213424682617,
      "learning_rate": 4.0534219915666564e-06,
      "loss": 2.4494,
      "step": 1418100
    },
    {
      "epoch": 459.70826580226907,
      "grad_norm": 1.3891996145248413,
      "learning_rate": 4.050178397664613e-06,
      "loss": 2.4766,
      "step": 1418200
    },
    {
      "epoch": 459.74068071312803,
      "grad_norm": 1.4316271543502808,
      "learning_rate": 4.046934803762569e-06,
      "loss": 2.4297,
      "step": 1418300
    },
    {
      "epoch": 459.77309562398705,
      "grad_norm": 1.315492033958435,
      "learning_rate": 4.043691209860526e-06,
      "loss": 2.4503,
      "step": 1418400
    },
    {
      "epoch": 459.805510534846,
      "grad_norm": 1.3254879713058472,
      "learning_rate": 4.040447615958482e-06,
      "loss": 2.4602,
      "step": 1418500
    },
    {
      "epoch": 459.837925445705,
      "grad_norm": 1.6409721374511719,
      "learning_rate": 4.037204022056438e-06,
      "loss": 2.4569,
      "step": 1418600
    },
    {
      "epoch": 459.87034035656404,
      "grad_norm": 1.4890027046203613,
      "learning_rate": 4.033960428154395e-06,
      "loss": 2.4486,
      "step": 1418700
    },
    {
      "epoch": 459.902755267423,
      "grad_norm": 1.6636000871658325,
      "learning_rate": 4.0307168342523515e-06,
      "loss": 2.4601,
      "step": 1418800
    },
    {
      "epoch": 459.935170178282,
      "grad_norm": 1.5134844779968262,
      "learning_rate": 4.0275056762893285e-06,
      "loss": 2.4573,
      "step": 1418900
    },
    {
      "epoch": 459.967585089141,
      "grad_norm": 1.406217336654663,
      "learning_rate": 4.0242620823872855e-06,
      "loss": 2.4501,
      "step": 1419000
    },
    {
      "epoch": 460.0,
      "grad_norm": 1.7395060062408447,
      "learning_rate": 4.021018488485242e-06,
      "loss": 2.4662,
      "step": 1419100
    },
    {
      "epoch": 460.0,
      "eval_bleu": 0.9868804834739031,
      "eval_loss": 4.278120994567871,
      "eval_runtime": 4.1482,
      "eval_samples_per_second": 118.605,
      "eval_steps_per_second": 1.929,
      "step": 1419100
    },
    {
      "epoch": 460.032414910859,
      "grad_norm": 1.3709867000579834,
      "learning_rate": 4.017774894583198e-06,
      "loss": 2.4548,
      "step": 1419200
    },
    {
      "epoch": 460.064829821718,
      "grad_norm": 1.4472579956054688,
      "learning_rate": 4.014531300681155e-06,
      "loss": 2.4684,
      "step": 1419300
    },
    {
      "epoch": 460.097244732577,
      "grad_norm": 1.3300615549087524,
      "learning_rate": 4.011287706779111e-06,
      "loss": 2.4591,
      "step": 1419400
    },
    {
      "epoch": 460.12965964343596,
      "grad_norm": 1.2418040037155151,
      "learning_rate": 4.008044112877067e-06,
      "loss": 2.4553,
      "step": 1419500
    },
    {
      "epoch": 460.162074554295,
      "grad_norm": 1.512094497680664,
      "learning_rate": 4.0048005189750244e-06,
      "loss": 2.462,
      "step": 1419600
    },
    {
      "epoch": 460.194489465154,
      "grad_norm": 1.3924442529678345,
      "learning_rate": 4.001556925072981e-06,
      "loss": 2.4506,
      "step": 1419700
    },
    {
      "epoch": 460.22690437601295,
      "grad_norm": 1.351487398147583,
      "learning_rate": 3.998313331170937e-06,
      "loss": 2.4493,
      "step": 1419800
    },
    {
      "epoch": 460.25931928687197,
      "grad_norm": 1.3376483917236328,
      "learning_rate": 3.995069737268895e-06,
      "loss": 2.4421,
      "step": 1419900
    },
    {
      "epoch": 460.29173419773093,
      "grad_norm": 1.5372354984283447,
      "learning_rate": 3.991826143366851e-06,
      "loss": 2.4336,
      "step": 1420000
    },
    {
      "epoch": 460.32414910858995,
      "grad_norm": 1.340621829032898,
      "learning_rate": 3.988582549464807e-06,
      "loss": 2.4675,
      "step": 1420100
    },
    {
      "epoch": 460.35656401944897,
      "grad_norm": 1.486527442932129,
      "learning_rate": 3.985338955562764e-06,
      "loss": 2.4475,
      "step": 1420200
    },
    {
      "epoch": 460.3889789303079,
      "grad_norm": 1.3727890253067017,
      "learning_rate": 3.98209536166072e-06,
      "loss": 2.4541,
      "step": 1420300
    },
    {
      "epoch": 460.42139384116695,
      "grad_norm": 1.5362972021102905,
      "learning_rate": 3.978851767758677e-06,
      "loss": 2.477,
      "step": 1420400
    },
    {
      "epoch": 460.4538087520259,
      "grad_norm": 1.4595595598220825,
      "learning_rate": 3.975608173856634e-06,
      "loss": 2.4629,
      "step": 1420500
    },
    {
      "epoch": 460.4862236628849,
      "grad_norm": 1.5268341302871704,
      "learning_rate": 3.97236457995459e-06,
      "loss": 2.4445,
      "step": 1420600
    },
    {
      "epoch": 460.51863857374394,
      "grad_norm": 1.4589135646820068,
      "learning_rate": 3.969120986052546e-06,
      "loss": 2.4444,
      "step": 1420700
    },
    {
      "epoch": 460.5510534846029,
      "grad_norm": 1.5740400552749634,
      "learning_rate": 3.965877392150503e-06,
      "loss": 2.4626,
      "step": 1420800
    },
    {
      "epoch": 460.5834683954619,
      "grad_norm": 1.6007654666900635,
      "learning_rate": 3.96266623418748e-06,
      "loss": 2.4479,
      "step": 1420900
    },
    {
      "epoch": 460.6158833063209,
      "grad_norm": 1.4460194110870361,
      "learning_rate": 3.959422640285436e-06,
      "loss": 2.4584,
      "step": 1421000
    },
    {
      "epoch": 460.6482982171799,
      "grad_norm": 1.6138451099395752,
      "learning_rate": 3.956179046383393e-06,
      "loss": 2.4705,
      "step": 1421100
    },
    {
      "epoch": 460.6807131280389,
      "grad_norm": 1.3479303121566772,
      "learning_rate": 3.9529354524813495e-06,
      "loss": 2.479,
      "step": 1421200
    },
    {
      "epoch": 460.7131280388979,
      "grad_norm": 1.3150134086608887,
      "learning_rate": 3.949691858579306e-06,
      "loss": 2.4508,
      "step": 1421300
    },
    {
      "epoch": 460.7455429497569,
      "grad_norm": 1.3252675533294678,
      "learning_rate": 3.946448264677263e-06,
      "loss": 2.4446,
      "step": 1421400
    },
    {
      "epoch": 460.77795786061586,
      "grad_norm": 1.6575757265090942,
      "learning_rate": 3.943204670775219e-06,
      "loss": 2.4534,
      "step": 1421500
    },
    {
      "epoch": 460.8103727714749,
      "grad_norm": 1.383325219154358,
      "learning_rate": 3.939961076873175e-06,
      "loss": 2.4464,
      "step": 1421600
    },
    {
      "epoch": 460.8427876823339,
      "grad_norm": 1.624692440032959,
      "learning_rate": 3.936717482971132e-06,
      "loss": 2.4535,
      "step": 1421700
    },
    {
      "epoch": 460.87520259319285,
      "grad_norm": 1.3834093809127808,
      "learning_rate": 3.933473889069088e-06,
      "loss": 2.4348,
      "step": 1421800
    },
    {
      "epoch": 460.90761750405187,
      "grad_norm": 1.468605399131775,
      "learning_rate": 3.930230295167045e-06,
      "loss": 2.4534,
      "step": 1421900
    },
    {
      "epoch": 460.94003241491083,
      "grad_norm": 1.4110901355743408,
      "learning_rate": 3.926986701265002e-06,
      "loss": 2.4194,
      "step": 1422000
    },
    {
      "epoch": 460.97244732576985,
      "grad_norm": 1.3207955360412598,
      "learning_rate": 3.923743107362959e-06,
      "loss": 2.4564,
      "step": 1422100
    },
    {
      "epoch": 461.0,
      "eval_bleu": 1.161720557233797,
      "eval_loss": 4.281291961669922,
      "eval_runtime": 4.2948,
      "eval_samples_per_second": 114.557,
      "eval_steps_per_second": 1.863,
      "step": 1422185
    },
    {
      "epoch": 461.00486223662887,
      "grad_norm": 1.7903492450714111,
      "learning_rate": 3.920499513460915e-06,
      "loss": 2.4545,
      "step": 1422200
    },
    {
      "epoch": 461.0372771474878,
      "grad_norm": 1.4645248651504517,
      "learning_rate": 3.917255919558872e-06,
      "loss": 2.4696,
      "step": 1422300
    },
    {
      "epoch": 461.06969205834685,
      "grad_norm": 1.2987055778503418,
      "learning_rate": 3.914012325656828e-06,
      "loss": 2.4449,
      "step": 1422400
    },
    {
      "epoch": 461.1021069692058,
      "grad_norm": 1.438826560974121,
      "learning_rate": 3.910768731754784e-06,
      "loss": 2.4742,
      "step": 1422500
    },
    {
      "epoch": 461.1345218800648,
      "grad_norm": 1.356665849685669,
      "learning_rate": 3.907525137852741e-06,
      "loss": 2.4522,
      "step": 1422600
    },
    {
      "epoch": 461.16693679092384,
      "grad_norm": 1.595280408859253,
      "learning_rate": 3.904281543950698e-06,
      "loss": 2.4429,
      "step": 1422700
    },
    {
      "epoch": 461.1993517017828,
      "grad_norm": 1.3096263408660889,
      "learning_rate": 3.901037950048654e-06,
      "loss": 2.4236,
      "step": 1422800
    },
    {
      "epoch": 461.2317666126418,
      "grad_norm": 1.4375554323196411,
      "learning_rate": 3.897826792085632e-06,
      "loss": 2.4511,
      "step": 1422900
    },
    {
      "epoch": 461.26418152350084,
      "grad_norm": 1.555627703666687,
      "learning_rate": 3.894583198183588e-06,
      "loss": 2.4492,
      "step": 1423000
    },
    {
      "epoch": 461.2965964343598,
      "grad_norm": 1.4617425203323364,
      "learning_rate": 3.891339604281544e-06,
      "loss": 2.4644,
      "step": 1423100
    },
    {
      "epoch": 461.3290113452188,
      "grad_norm": 1.3752365112304688,
      "learning_rate": 3.888096010379501e-06,
      "loss": 2.4515,
      "step": 1423200
    },
    {
      "epoch": 461.3614262560778,
      "grad_norm": 1.3924288749694824,
      "learning_rate": 3.884852416477457e-06,
      "loss": 2.4509,
      "step": 1423300
    },
    {
      "epoch": 461.3938411669368,
      "grad_norm": 1.346561312675476,
      "learning_rate": 3.881641258514434e-06,
      "loss": 2.4363,
      "step": 1423400
    },
    {
      "epoch": 461.4262560777958,
      "grad_norm": 1.5023328065872192,
      "learning_rate": 3.87839766461239e-06,
      "loss": 2.442,
      "step": 1423500
    },
    {
      "epoch": 461.4586709886548,
      "grad_norm": 1.4417753219604492,
      "learning_rate": 3.8751540707103475e-06,
      "loss": 2.4473,
      "step": 1423600
    },
    {
      "epoch": 461.4910858995138,
      "grad_norm": 1.4562132358551025,
      "learning_rate": 3.871910476808304e-06,
      "loss": 2.46,
      "step": 1423700
    },
    {
      "epoch": 461.52350081037275,
      "grad_norm": 1.4273947477340698,
      "learning_rate": 3.868666882906261e-06,
      "loss": 2.4695,
      "step": 1423800
    },
    {
      "epoch": 461.55591572123177,
      "grad_norm": 1.4667689800262451,
      "learning_rate": 3.865423289004217e-06,
      "loss": 2.4353,
      "step": 1423900
    },
    {
      "epoch": 461.5883306320908,
      "grad_norm": 1.4950478076934814,
      "learning_rate": 3.862212131041194e-06,
      "loss": 2.4351,
      "step": 1424000
    },
    {
      "epoch": 461.62074554294975,
      "grad_norm": 1.611462950706482,
      "learning_rate": 3.85896853713915e-06,
      "loss": 2.471,
      "step": 1424100
    },
    {
      "epoch": 461.65316045380877,
      "grad_norm": 1.590836763381958,
      "learning_rate": 3.855724943237106e-06,
      "loss": 2.4562,
      "step": 1424200
    },
    {
      "epoch": 461.6855753646677,
      "grad_norm": 1.6812403202056885,
      "learning_rate": 3.852481349335063e-06,
      "loss": 2.4671,
      "step": 1424300
    },
    {
      "epoch": 461.71799027552674,
      "grad_norm": 1.5962601900100708,
      "learning_rate": 3.84923775543302e-06,
      "loss": 2.4682,
      "step": 1424400
    },
    {
      "epoch": 461.75040518638576,
      "grad_norm": 1.382643222808838,
      "learning_rate": 3.8459941615309766e-06,
      "loss": 2.4604,
      "step": 1424500
    },
    {
      "epoch": 461.7828200972447,
      "grad_norm": 1.3745909929275513,
      "learning_rate": 3.842750567628934e-06,
      "loss": 2.4586,
      "step": 1424600
    },
    {
      "epoch": 461.81523500810374,
      "grad_norm": 1.4525057077407837,
      "learning_rate": 3.83950697372689e-06,
      "loss": 2.4649,
      "step": 1424700
    },
    {
      "epoch": 461.8476499189627,
      "grad_norm": 1.365926742553711,
      "learning_rate": 3.836263379824846e-06,
      "loss": 2.463,
      "step": 1424800
    },
    {
      "epoch": 461.8800648298217,
      "grad_norm": 1.4606395959854126,
      "learning_rate": 3.833019785922803e-06,
      "loss": 2.4539,
      "step": 1424900
    },
    {
      "epoch": 461.91247974068074,
      "grad_norm": 1.6506799459457397,
      "learning_rate": 3.829776192020759e-06,
      "loss": 2.4505,
      "step": 1425000
    },
    {
      "epoch": 461.9448946515397,
      "grad_norm": 1.2486722469329834,
      "learning_rate": 3.8265325981187155e-06,
      "loss": 2.4273,
      "step": 1425100
    },
    {
      "epoch": 461.9773095623987,
      "grad_norm": 1.60995352268219,
      "learning_rate": 3.8232890042166725e-06,
      "loss": 2.4632,
      "step": 1425200
    },
    {
      "epoch": 462.0,
      "eval_bleu": 1.1812182501727968,
      "eval_loss": 4.282537460327148,
      "eval_runtime": 4.4847,
      "eval_samples_per_second": 109.705,
      "eval_steps_per_second": 1.784,
      "step": 1425270
    },
    {
      "epoch": 462.0097244732577,
      "grad_norm": 1.6059198379516602,
      "learning_rate": 3.820045410314629e-06,
      "loss": 2.4654,
      "step": 1425300
    },
    {
      "epoch": 462.0421393841167,
      "grad_norm": 1.4596625566482544,
      "learning_rate": 3.816801816412585e-06,
      "loss": 2.4485,
      "step": 1425400
    },
    {
      "epoch": 462.0745542949757,
      "grad_norm": 1.587185025215149,
      "learning_rate": 3.8135582225105416e-06,
      "loss": 2.4537,
      "step": 1425500
    },
    {
      "epoch": 462.1069692058347,
      "grad_norm": 1.4763942956924438,
      "learning_rate": 3.810314628608498e-06,
      "loss": 2.4621,
      "step": 1425600
    },
    {
      "epoch": 462.1393841166937,
      "grad_norm": 1.2626121044158936,
      "learning_rate": 3.807071034706455e-06,
      "loss": 2.4565,
      "step": 1425700
    },
    {
      "epoch": 462.17179902755265,
      "grad_norm": 1.6132088899612427,
      "learning_rate": 3.803827440804412e-06,
      "loss": 2.4544,
      "step": 1425800
    },
    {
      "epoch": 462.20421393841167,
      "grad_norm": 1.3944239616394043,
      "learning_rate": 3.8005838469023685e-06,
      "loss": 2.4445,
      "step": 1425900
    },
    {
      "epoch": 462.2366288492707,
      "grad_norm": 1.4705867767333984,
      "learning_rate": 3.7973402530003247e-06,
      "loss": 2.4599,
      "step": 1426000
    },
    {
      "epoch": 462.26904376012965,
      "grad_norm": 1.3933379650115967,
      "learning_rate": 3.7940966590982813e-06,
      "loss": 2.4584,
      "step": 1426100
    },
    {
      "epoch": 462.30145867098867,
      "grad_norm": 1.4093888998031616,
      "learning_rate": 3.790853065196238e-06,
      "loss": 2.4601,
      "step": 1426200
    },
    {
      "epoch": 462.3338735818476,
      "grad_norm": 1.4455727338790894,
      "learning_rate": 3.787609471294194e-06,
      "loss": 2.4647,
      "step": 1426300
    },
    {
      "epoch": 462.36628849270664,
      "grad_norm": 2.079583168029785,
      "learning_rate": 3.7843658773921508e-06,
      "loss": 2.4502,
      "step": 1426400
    },
    {
      "epoch": 462.39870340356566,
      "grad_norm": 1.6526252031326294,
      "learning_rate": 3.7811222834901074e-06,
      "loss": 2.4577,
      "step": 1426500
    },
    {
      "epoch": 462.4311183144246,
      "grad_norm": 1.6657058000564575,
      "learning_rate": 3.7778786895880636e-06,
      "loss": 2.46,
      "step": 1426600
    },
    {
      "epoch": 462.46353322528364,
      "grad_norm": 1.5523779392242432,
      "learning_rate": 3.7746350956860202e-06,
      "loss": 2.4636,
      "step": 1426700
    },
    {
      "epoch": 462.4959481361426,
      "grad_norm": 1.6379157304763794,
      "learning_rate": 3.771391501783977e-06,
      "loss": 2.4632,
      "step": 1426800
    },
    {
      "epoch": 462.5283630470016,
      "grad_norm": 1.6243724822998047,
      "learning_rate": 3.768147907881933e-06,
      "loss": 2.4319,
      "step": 1426900
    },
    {
      "epoch": 462.56077795786064,
      "grad_norm": 1.5206109285354614,
      "learning_rate": 3.7649043139798897e-06,
      "loss": 2.4621,
      "step": 1427000
    },
    {
      "epoch": 462.5931928687196,
      "grad_norm": 1.6785444021224976,
      "learning_rate": 3.7616607200778463e-06,
      "loss": 2.4591,
      "step": 1427100
    },
    {
      "epoch": 462.6256077795786,
      "grad_norm": 1.228979468345642,
      "learning_rate": 3.7584171261758025e-06,
      "loss": 2.4556,
      "step": 1427200
    },
    {
      "epoch": 462.6580226904376,
      "grad_norm": 1.528476357460022,
      "learning_rate": 3.75517353227376e-06,
      "loss": 2.4372,
      "step": 1427300
    },
    {
      "epoch": 462.6904376012966,
      "grad_norm": 1.3469716310501099,
      "learning_rate": 3.751929938371716e-06,
      "loss": 2.4423,
      "step": 1427400
    },
    {
      "epoch": 462.7228525121556,
      "grad_norm": 1.5733156204223633,
      "learning_rate": 3.748686344469673e-06,
      "loss": 2.4497,
      "step": 1427500
    },
    {
      "epoch": 462.7552674230146,
      "grad_norm": 1.3376833200454712,
      "learning_rate": 3.7454427505676294e-06,
      "loss": 2.4493,
      "step": 1427600
    },
    {
      "epoch": 462.7876823338736,
      "grad_norm": 1.4714957475662231,
      "learning_rate": 3.7421991566655856e-06,
      "loss": 2.4721,
      "step": 1427700
    },
    {
      "epoch": 462.82009724473255,
      "grad_norm": 1.4900096654891968,
      "learning_rate": 3.7389555627635422e-06,
      "loss": 2.4501,
      "step": 1427800
    },
    {
      "epoch": 462.85251215559157,
      "grad_norm": 1.4394958019256592,
      "learning_rate": 3.735711968861499e-06,
      "loss": 2.4428,
      "step": 1427900
    },
    {
      "epoch": 462.8849270664506,
      "grad_norm": 1.5429474115371704,
      "learning_rate": 3.732468374959455e-06,
      "loss": 2.438,
      "step": 1428000
    },
    {
      "epoch": 462.91734197730955,
      "grad_norm": 1.7424099445343018,
      "learning_rate": 3.7292247810574117e-06,
      "loss": 2.4327,
      "step": 1428100
    },
    {
      "epoch": 462.94975688816857,
      "grad_norm": 1.481918454170227,
      "learning_rate": 3.7259811871553683e-06,
      "loss": 2.4485,
      "step": 1428200
    },
    {
      "epoch": 462.9821717990275,
      "grad_norm": 1.233577013015747,
      "learning_rate": 3.7227375932533245e-06,
      "loss": 2.4347,
      "step": 1428300
    },
    {
      "epoch": 463.0,
      "eval_bleu": 1.1716371114774013,
      "eval_loss": 4.284170150756836,
      "eval_runtime": 4.3211,
      "eval_samples_per_second": 113.859,
      "eval_steps_per_second": 1.851,
      "step": 1428355
    },
    {
      "epoch": 463.01458670988654,
      "grad_norm": 1.4594616889953613,
      "learning_rate": 3.719493999351281e-06,
      "loss": 2.4529,
      "step": 1428400
    },
    {
      "epoch": 463.04700162074556,
      "grad_norm": 1.405595302581787,
      "learning_rate": 3.7162504054492378e-06,
      "loss": 2.461,
      "step": 1428500
    },
    {
      "epoch": 463.0794165316045,
      "grad_norm": 1.750102162361145,
      "learning_rate": 3.713006811547194e-06,
      "loss": 2.4416,
      "step": 1428600
    },
    {
      "epoch": 463.11183144246354,
      "grad_norm": 1.5618208646774292,
      "learning_rate": 3.7097632176451514e-06,
      "loss": 2.4362,
      "step": 1428700
    },
    {
      "epoch": 463.1442463533225,
      "grad_norm": 1.1904468536376953,
      "learning_rate": 3.7065196237431077e-06,
      "loss": 2.4638,
      "step": 1428800
    },
    {
      "epoch": 463.1766612641815,
      "grad_norm": 1.4877912998199463,
      "learning_rate": 3.7032760298410643e-06,
      "loss": 2.457,
      "step": 1428900
    },
    {
      "epoch": 463.20907617504054,
      "grad_norm": 1.3603495359420776,
      "learning_rate": 3.700032435939021e-06,
      "loss": 2.4772,
      "step": 1429000
    },
    {
      "epoch": 463.2414910858995,
      "grad_norm": 1.651313304901123,
      "learning_rate": 3.696788842036977e-06,
      "loss": 2.4481,
      "step": 1429100
    },
    {
      "epoch": 463.2739059967585,
      "grad_norm": 1.653699278831482,
      "learning_rate": 3.6935452481349337e-06,
      "loss": 2.4411,
      "step": 1429200
    },
    {
      "epoch": 463.3063209076175,
      "grad_norm": 1.526121973991394,
      "learning_rate": 3.6903016542328904e-06,
      "loss": 2.4466,
      "step": 1429300
    },
    {
      "epoch": 463.3387358184765,
      "grad_norm": 1.4201816320419312,
      "learning_rate": 3.6870580603308466e-06,
      "loss": 2.4486,
      "step": 1429400
    },
    {
      "epoch": 463.3711507293355,
      "grad_norm": 1.536715030670166,
      "learning_rate": 3.683814466428803e-06,
      "loss": 2.4575,
      "step": 1429500
    },
    {
      "epoch": 463.4035656401945,
      "grad_norm": 1.4103031158447266,
      "learning_rate": 3.68057087252676e-06,
      "loss": 2.4392,
      "step": 1429600
    },
    {
      "epoch": 463.4359805510535,
      "grad_norm": 1.538159966468811,
      "learning_rate": 3.677327278624716e-06,
      "loss": 2.4646,
      "step": 1429700
    },
    {
      "epoch": 463.4683954619125,
      "grad_norm": 1.4276386499404907,
      "learning_rate": 3.6740836847226726e-06,
      "loss": 2.4508,
      "step": 1429800
    },
    {
      "epoch": 463.50081037277147,
      "grad_norm": 1.41179358959198,
      "learning_rate": 3.6708400908206293e-06,
      "loss": 2.4673,
      "step": 1429900
    },
    {
      "epoch": 463.5332252836305,
      "grad_norm": 1.4224236011505127,
      "learning_rate": 3.6676289328576066e-06,
      "loss": 2.442,
      "step": 1430000
    },
    {
      "epoch": 463.56564019448945,
      "grad_norm": 1.427654504776001,
      "learning_rate": 3.664385338955563e-06,
      "loss": 2.4428,
      "step": 1430100
    },
    {
      "epoch": 463.59805510534846,
      "grad_norm": 1.5402926206588745,
      "learning_rate": 3.6611417450535195e-06,
      "loss": 2.4607,
      "step": 1430200
    },
    {
      "epoch": 463.6304700162075,
      "grad_norm": 1.5313445329666138,
      "learning_rate": 3.657898151151476e-06,
      "loss": 2.4502,
      "step": 1430300
    },
    {
      "epoch": 463.66288492706644,
      "grad_norm": 1.3976716995239258,
      "learning_rate": 3.6546545572494323e-06,
      "loss": 2.4611,
      "step": 1430400
    },
    {
      "epoch": 463.69529983792546,
      "grad_norm": 1.511778712272644,
      "learning_rate": 3.651410963347389e-06,
      "loss": 2.4723,
      "step": 1430500
    },
    {
      "epoch": 463.7277147487844,
      "grad_norm": 1.536562442779541,
      "learning_rate": 3.6481673694453455e-06,
      "loss": 2.4402,
      "step": 1430600
    },
    {
      "epoch": 463.76012965964344,
      "grad_norm": 1.4504908323287964,
      "learning_rate": 3.6449237755433017e-06,
      "loss": 2.4405,
      "step": 1430700
    },
    {
      "epoch": 463.79254457050246,
      "grad_norm": 1.3255008459091187,
      "learning_rate": 3.6416801816412584e-06,
      "loss": 2.4471,
      "step": 1430800
    },
    {
      "epoch": 463.8249594813614,
      "grad_norm": 1.429430603981018,
      "learning_rate": 3.6384365877392154e-06,
      "loss": 2.4652,
      "step": 1430900
    },
    {
      "epoch": 463.85737439222044,
      "grad_norm": 1.2279167175292969,
      "learning_rate": 3.635192993837172e-06,
      "loss": 2.4642,
      "step": 1431000
    },
    {
      "epoch": 463.8897893030794,
      "grad_norm": 1.7775659561157227,
      "learning_rate": 3.6319493999351287e-06,
      "loss": 2.4437,
      "step": 1431100
    },
    {
      "epoch": 463.9222042139384,
      "grad_norm": 1.4763630628585815,
      "learning_rate": 3.628705806033085e-06,
      "loss": 2.4453,
      "step": 1431200
    },
    {
      "epoch": 463.95461912479743,
      "grad_norm": 1.444075584411621,
      "learning_rate": 3.6254622121310415e-06,
      "loss": 2.4587,
      "step": 1431300
    },
    {
      "epoch": 463.9870340356564,
      "grad_norm": 1.4061400890350342,
      "learning_rate": 3.622218618228998e-06,
      "loss": 2.4334,
      "step": 1431400
    },
    {
      "epoch": 464.0,
      "eval_bleu": 1.0552737762142272,
      "eval_loss": 4.280850410461426,
      "eval_runtime": 4.4703,
      "eval_samples_per_second": 110.06,
      "eval_steps_per_second": 1.79,
      "step": 1431440
    },
    {
      "epoch": 464.0194489465154,
      "grad_norm": 1.5251652002334595,
      "learning_rate": 3.6189750243269543e-06,
      "loss": 2.4684,
      "step": 1431500
    },
    {
      "epoch": 464.05186385737437,
      "grad_norm": 1.443969964981079,
      "learning_rate": 3.615731430424911e-06,
      "loss": 2.4476,
      "step": 1431600
    },
    {
      "epoch": 464.0842787682334,
      "grad_norm": 1.5197925567626953,
      "learning_rate": 3.6124878365228676e-06,
      "loss": 2.4577,
      "step": 1431700
    },
    {
      "epoch": 464.1166936790924,
      "grad_norm": 1.545315146446228,
      "learning_rate": 3.6092442426208238e-06,
      "loss": 2.4579,
      "step": 1431800
    },
    {
      "epoch": 464.14910858995137,
      "grad_norm": 1.5554821491241455,
      "learning_rate": 3.6060006487187804e-06,
      "loss": 2.45,
      "step": 1431900
    },
    {
      "epoch": 464.1815235008104,
      "grad_norm": 1.3213882446289062,
      "learning_rate": 3.6027894907557578e-06,
      "loss": 2.4246,
      "step": 1432000
    },
    {
      "epoch": 464.21393841166935,
      "grad_norm": 1.4844770431518555,
      "learning_rate": 3.5995458968537144e-06,
      "loss": 2.4592,
      "step": 1432100
    },
    {
      "epoch": 464.24635332252836,
      "grad_norm": 1.319384217262268,
      "learning_rate": 3.5963023029516706e-06,
      "loss": 2.4512,
      "step": 1432200
    },
    {
      "epoch": 464.2787682333874,
      "grad_norm": 1.5087285041809082,
      "learning_rate": 3.5930587090496272e-06,
      "loss": 2.4458,
      "step": 1432300
    },
    {
      "epoch": 464.31118314424634,
      "grad_norm": 1.3410587310791016,
      "learning_rate": 3.589815115147584e-06,
      "loss": 2.4609,
      "step": 1432400
    },
    {
      "epoch": 464.34359805510536,
      "grad_norm": 1.411297082901001,
      "learning_rate": 3.58657152124554e-06,
      "loss": 2.4343,
      "step": 1432500
    },
    {
      "epoch": 464.3760129659643,
      "grad_norm": 1.3355344533920288,
      "learning_rate": 3.5833279273434967e-06,
      "loss": 2.4511,
      "step": 1432600
    },
    {
      "epoch": 464.40842787682334,
      "grad_norm": 1.278757095336914,
      "learning_rate": 3.5800843334414533e-06,
      "loss": 2.4544,
      "step": 1432700
    },
    {
      "epoch": 464.44084278768236,
      "grad_norm": 1.3866829872131348,
      "learning_rate": 3.5768407395394095e-06,
      "loss": 2.4548,
      "step": 1432800
    },
    {
      "epoch": 464.4732576985413,
      "grad_norm": 1.378801941871643,
      "learning_rate": 3.573597145637366e-06,
      "loss": 2.4459,
      "step": 1432900
    },
    {
      "epoch": 464.50567260940034,
      "grad_norm": 1.296964168548584,
      "learning_rate": 3.5703535517353228e-06,
      "loss": 2.4572,
      "step": 1433000
    },
    {
      "epoch": 464.5380875202593,
      "grad_norm": 1.6280497312545776,
      "learning_rate": 3.56710995783328e-06,
      "loss": 2.4761,
      "step": 1433100
    },
    {
      "epoch": 464.5705024311183,
      "grad_norm": 1.4821193218231201,
      "learning_rate": 3.5638663639312364e-06,
      "loss": 2.4632,
      "step": 1433200
    },
    {
      "epoch": 464.60291734197733,
      "grad_norm": 1.4938316345214844,
      "learning_rate": 3.5606227700291926e-06,
      "loss": 2.4454,
      "step": 1433300
    },
    {
      "epoch": 464.6353322528363,
      "grad_norm": 1.472171664237976,
      "learning_rate": 3.5573791761271493e-06,
      "loss": 2.4391,
      "step": 1433400
    },
    {
      "epoch": 464.6677471636953,
      "grad_norm": 1.494828224182129,
      "learning_rate": 3.554135582225106e-06,
      "loss": 2.4557,
      "step": 1433500
    },
    {
      "epoch": 464.70016207455427,
      "grad_norm": 1.611580729484558,
      "learning_rate": 3.550891988323062e-06,
      "loss": 2.4479,
      "step": 1433600
    },
    {
      "epoch": 464.7325769854133,
      "grad_norm": 1.4715216159820557,
      "learning_rate": 3.5476483944210187e-06,
      "loss": 2.4607,
      "step": 1433700
    },
    {
      "epoch": 464.7649918962723,
      "grad_norm": 1.6184855699539185,
      "learning_rate": 3.5444048005189753e-06,
      "loss": 2.4563,
      "step": 1433800
    },
    {
      "epoch": 464.79740680713127,
      "grad_norm": 1.4678380489349365,
      "learning_rate": 3.5411612066169315e-06,
      "loss": 2.4604,
      "step": 1433900
    },
    {
      "epoch": 464.8298217179903,
      "grad_norm": 1.3134759664535522,
      "learning_rate": 3.537917612714888e-06,
      "loss": 2.4613,
      "step": 1434000
    },
    {
      "epoch": 464.86223662884925,
      "grad_norm": 1.4745444059371948,
      "learning_rate": 3.5347064547518655e-06,
      "loss": 2.447,
      "step": 1434100
    },
    {
      "epoch": 464.89465153970826,
      "grad_norm": 1.4980056285858154,
      "learning_rate": 3.5314628608498217e-06,
      "loss": 2.4748,
      "step": 1434200
    },
    {
      "epoch": 464.9270664505673,
      "grad_norm": 1.709742546081543,
      "learning_rate": 3.5282192669477784e-06,
      "loss": 2.4316,
      "step": 1434300
    },
    {
      "epoch": 464.95948136142624,
      "grad_norm": 1.7885912656784058,
      "learning_rate": 3.524975673045735e-06,
      "loss": 2.4682,
      "step": 1434400
    },
    {
      "epoch": 464.99189627228526,
      "grad_norm": 1.6027321815490723,
      "learning_rate": 3.5217320791436916e-06,
      "loss": 2.4545,
      "step": 1434500
    },
    {
      "epoch": 465.0,
      "eval_bleu": 1.204945280635923,
      "eval_loss": 4.282196044921875,
      "eval_runtime": 4.3306,
      "eval_samples_per_second": 113.61,
      "eval_steps_per_second": 1.847,
      "step": 1434525
    },
    {
      "epoch": 465.0243111831442,
      "grad_norm": 1.4064898490905762,
      "learning_rate": 3.518488485241648e-06,
      "loss": 2.467,
      "step": 1434600
    },
    {
      "epoch": 465.05672609400324,
      "grad_norm": 1.44315505027771,
      "learning_rate": 3.5152448913396044e-06,
      "loss": 2.4719,
      "step": 1434700
    },
    {
      "epoch": 465.08914100486226,
      "grad_norm": 1.335646629333496,
      "learning_rate": 3.512001297437561e-06,
      "loss": 2.4642,
      "step": 1434800
    },
    {
      "epoch": 465.1215559157212,
      "grad_norm": 1.364524245262146,
      "learning_rate": 3.5087577035355173e-06,
      "loss": 2.4714,
      "step": 1434900
    },
    {
      "epoch": 465.15397082658023,
      "grad_norm": 1.2815585136413574,
      "learning_rate": 3.505514109633474e-06,
      "loss": 2.4608,
      "step": 1435000
    },
    {
      "epoch": 465.1863857374392,
      "grad_norm": 1.3810980319976807,
      "learning_rate": 3.5022705157314305e-06,
      "loss": 2.4412,
      "step": 1435100
    },
    {
      "epoch": 465.2188006482982,
      "grad_norm": 1.635290503501892,
      "learning_rate": 3.4990269218293867e-06,
      "loss": 2.4432,
      "step": 1435200
    },
    {
      "epoch": 465.25121555915723,
      "grad_norm": 1.3018945455551147,
      "learning_rate": 3.495783327927344e-06,
      "loss": 2.46,
      "step": 1435300
    },
    {
      "epoch": 465.2836304700162,
      "grad_norm": 1.5292503833770752,
      "learning_rate": 3.4925397340253004e-06,
      "loss": 2.4609,
      "step": 1435400
    },
    {
      "epoch": 465.3160453808752,
      "grad_norm": 1.5614838600158691,
      "learning_rate": 3.489296140123257e-06,
      "loss": 2.4391,
      "step": 1435500
    },
    {
      "epoch": 465.34846029173417,
      "grad_norm": 1.5051339864730835,
      "learning_rate": 3.4860525462212136e-06,
      "loss": 2.4375,
      "step": 1435600
    },
    {
      "epoch": 465.3808752025932,
      "grad_norm": 1.4479502439498901,
      "learning_rate": 3.48280895231917e-06,
      "loss": 2.4486,
      "step": 1435700
    },
    {
      "epoch": 465.4132901134522,
      "grad_norm": 1.6735368967056274,
      "learning_rate": 3.4795653584171265e-06,
      "loss": 2.4443,
      "step": 1435800
    },
    {
      "epoch": 465.44570502431117,
      "grad_norm": 1.4260343313217163,
      "learning_rate": 3.476321764515083e-06,
      "loss": 2.4358,
      "step": 1435900
    },
    {
      "epoch": 465.4781199351702,
      "grad_norm": 1.4251399040222168,
      "learning_rate": 3.4730781706130393e-06,
      "loss": 2.4416,
      "step": 1436000
    },
    {
      "epoch": 465.51053484602915,
      "grad_norm": 1.5793181657791138,
      "learning_rate": 3.4698670126500167e-06,
      "loss": 2.4791,
      "step": 1436100
    },
    {
      "epoch": 465.54294975688816,
      "grad_norm": 1.593532919883728,
      "learning_rate": 3.4666234187479733e-06,
      "loss": 2.4365,
      "step": 1436200
    },
    {
      "epoch": 465.5753646677472,
      "grad_norm": 1.3728159666061401,
      "learning_rate": 3.4633798248459295e-06,
      "loss": 2.4409,
      "step": 1436300
    },
    {
      "epoch": 465.60777957860614,
      "grad_norm": 1.5609031915664673,
      "learning_rate": 3.4601686668829065e-06,
      "loss": 2.4476,
      "step": 1436400
    },
    {
      "epoch": 465.64019448946516,
      "grad_norm": 1.4189894199371338,
      "learning_rate": 3.4569250729808627e-06,
      "loss": 2.4416,
      "step": 1436500
    },
    {
      "epoch": 465.6726094003242,
      "grad_norm": 1.4087913036346436,
      "learning_rate": 3.4536814790788193e-06,
      "loss": 2.4404,
      "step": 1436600
    },
    {
      "epoch": 465.70502431118314,
      "grad_norm": 1.323387861251831,
      "learning_rate": 3.450437885176776e-06,
      "loss": 2.4638,
      "step": 1436700
    },
    {
      "epoch": 465.73743922204216,
      "grad_norm": 1.5767422914505005,
      "learning_rate": 3.447194291274732e-06,
      "loss": 2.4501,
      "step": 1436800
    },
    {
      "epoch": 465.7698541329011,
      "grad_norm": 1.3579541444778442,
      "learning_rate": 3.4439506973726896e-06,
      "loss": 2.4596,
      "step": 1436900
    },
    {
      "epoch": 465.80226904376013,
      "grad_norm": 1.4802420139312744,
      "learning_rate": 3.4407071034706458e-06,
      "loss": 2.4653,
      "step": 1437000
    },
    {
      "epoch": 465.83468395461915,
      "grad_norm": 1.4587466716766357,
      "learning_rate": 3.4374635095686024e-06,
      "loss": 2.4561,
      "step": 1437100
    },
    {
      "epoch": 465.8670988654781,
      "grad_norm": 1.786521553993225,
      "learning_rate": 3.434219915666559e-06,
      "loss": 2.4449,
      "step": 1437200
    },
    {
      "epoch": 465.89951377633713,
      "grad_norm": 1.4703748226165771,
      "learning_rate": 3.4309763217645152e-06,
      "loss": 2.4592,
      "step": 1437300
    },
    {
      "epoch": 465.9319286871961,
      "grad_norm": 1.4869030714035034,
      "learning_rate": 3.427732727862472e-06,
      "loss": 2.44,
      "step": 1437400
    },
    {
      "epoch": 465.9643435980551,
      "grad_norm": 1.3824740648269653,
      "learning_rate": 3.4244891339604285e-06,
      "loss": 2.4832,
      "step": 1437500
    },
    {
      "epoch": 465.9967585089141,
      "grad_norm": 1.464106559753418,
      "learning_rate": 3.4212455400583847e-06,
      "loss": 2.4459,
      "step": 1437600
    },
    {
      "epoch": 466.0,
      "eval_bleu": 1.143620273335996,
      "eval_loss": 4.282437801361084,
      "eval_runtime": 4.4433,
      "eval_samples_per_second": 110.729,
      "eval_steps_per_second": 1.8,
      "step": 1437610
    },
    {
      "epoch": 466.0291734197731,
      "grad_norm": 1.4117931127548218,
      "learning_rate": 3.4180019461563413e-06,
      "loss": 2.479,
      "step": 1437700
    },
    {
      "epoch": 466.0615883306321,
      "grad_norm": 1.4401332139968872,
      "learning_rate": 3.414758352254298e-06,
      "loss": 2.4542,
      "step": 1437800
    },
    {
      "epoch": 466.09400324149107,
      "grad_norm": 1.3737564086914062,
      "learning_rate": 3.411514758352254e-06,
      "loss": 2.4469,
      "step": 1437900
    },
    {
      "epoch": 466.1264181523501,
      "grad_norm": 1.5575942993164062,
      "learning_rate": 3.4082711644502108e-06,
      "loss": 2.4629,
      "step": 1438000
    },
    {
      "epoch": 466.1588330632091,
      "grad_norm": 1.442079782485962,
      "learning_rate": 3.4050275705481674e-06,
      "loss": 2.4787,
      "step": 1438100
    },
    {
      "epoch": 466.19124797406806,
      "grad_norm": 1.4071916341781616,
      "learning_rate": 3.4017839766461236e-06,
      "loss": 2.4438,
      "step": 1438200
    },
    {
      "epoch": 466.2236628849271,
      "grad_norm": 1.5110564231872559,
      "learning_rate": 3.398540382744081e-06,
      "loss": 2.4556,
      "step": 1438300
    },
    {
      "epoch": 466.25607779578604,
      "grad_norm": 1.38840913772583,
      "learning_rate": 3.3952967888420373e-06,
      "loss": 2.4641,
      "step": 1438400
    },
    {
      "epoch": 466.28849270664506,
      "grad_norm": 1.2896809577941895,
      "learning_rate": 3.392053194939994e-06,
      "loss": 2.4441,
      "step": 1438500
    },
    {
      "epoch": 466.3209076175041,
      "grad_norm": 1.3731261491775513,
      "learning_rate": 3.3888096010379505e-06,
      "loss": 2.4548,
      "step": 1438600
    },
    {
      "epoch": 466.35332252836304,
      "grad_norm": 1.3849804401397705,
      "learning_rate": 3.3855660071359067e-06,
      "loss": 2.4402,
      "step": 1438700
    },
    {
      "epoch": 466.38573743922205,
      "grad_norm": 1.4645644426345825,
      "learning_rate": 3.3823224132338633e-06,
      "loss": 2.4202,
      "step": 1438800
    },
    {
      "epoch": 466.418152350081,
      "grad_norm": 1.477808952331543,
      "learning_rate": 3.37907881933182e-06,
      "loss": 2.4527,
      "step": 1438900
    },
    {
      "epoch": 466.45056726094003,
      "grad_norm": 1.512628436088562,
      "learning_rate": 3.375835225429776e-06,
      "loss": 2.4674,
      "step": 1439000
    },
    {
      "epoch": 466.48298217179905,
      "grad_norm": 1.3727657794952393,
      "learning_rate": 3.372591631527733e-06,
      "loss": 2.4258,
      "step": 1439100
    },
    {
      "epoch": 466.515397082658,
      "grad_norm": 1.424459457397461,
      "learning_rate": 3.3693480376256894e-06,
      "loss": 2.4466,
      "step": 1439200
    },
    {
      "epoch": 466.54781199351703,
      "grad_norm": 1.2360520362854004,
      "learning_rate": 3.3661044437236456e-06,
      "loss": 2.4595,
      "step": 1439300
    },
    {
      "epoch": 466.580226904376,
      "grad_norm": 1.531018614768982,
      "learning_rate": 3.3628608498216023e-06,
      "loss": 2.4518,
      "step": 1439400
    },
    {
      "epoch": 466.612641815235,
      "grad_norm": 1.7947542667388916,
      "learning_rate": 3.359617255919559e-06,
      "loss": 2.4641,
      "step": 1439500
    },
    {
      "epoch": 466.645056726094,
      "grad_norm": 1.506263017654419,
      "learning_rate": 3.356373662017515e-06,
      "loss": 2.4497,
      "step": 1439600
    },
    {
      "epoch": 466.677471636953,
      "grad_norm": 1.6504426002502441,
      "learning_rate": 3.3531300681154726e-06,
      "loss": 2.4608,
      "step": 1439700
    },
    {
      "epoch": 466.709886547812,
      "grad_norm": 1.4314824342727661,
      "learning_rate": 3.3498864742134288e-06,
      "loss": 2.4562,
      "step": 1439800
    },
    {
      "epoch": 466.74230145867097,
      "grad_norm": 1.268364667892456,
      "learning_rate": 3.3466428803113854e-06,
      "loss": 2.457,
      "step": 1439900
    },
    {
      "epoch": 466.77471636953,
      "grad_norm": 1.5467287302017212,
      "learning_rate": 3.343399286409342e-06,
      "loss": 2.455,
      "step": 1440000
    },
    {
      "epoch": 466.807131280389,
      "grad_norm": 1.5910271406173706,
      "learning_rate": 3.340155692507298e-06,
      "loss": 2.4345,
      "step": 1440100
    },
    {
      "epoch": 466.83954619124796,
      "grad_norm": 1.6458981037139893,
      "learning_rate": 3.336912098605255e-06,
      "loss": 2.4564,
      "step": 1440200
    },
    {
      "epoch": 466.871961102107,
      "grad_norm": 1.4979965686798096,
      "learning_rate": 3.3336685047032115e-06,
      "loss": 2.4524,
      "step": 1440300
    },
    {
      "epoch": 466.90437601296594,
      "grad_norm": 1.4193836450576782,
      "learning_rate": 3.330457346740188e-06,
      "loss": 2.4675,
      "step": 1440400
    },
    {
      "epoch": 466.93679092382496,
      "grad_norm": 1.3739880323410034,
      "learning_rate": 3.327213752838145e-06,
      "loss": 2.445,
      "step": 1440500
    },
    {
      "epoch": 466.969205834684,
      "grad_norm": 1.2090787887573242,
      "learning_rate": 3.3239701589361017e-06,
      "loss": 2.4384,
      "step": 1440600
    },
    {
      "epoch": 467.0,
      "eval_bleu": 1.1669988828698195,
      "eval_loss": 4.283037185668945,
      "eval_runtime": 4.3983,
      "eval_samples_per_second": 111.86,
      "eval_steps_per_second": 1.819,
      "step": 1440695
    },
    {
      "epoch": 467.00162074554294,
      "grad_norm": 1.4893110990524292,
      "learning_rate": 3.3207265650340583e-06,
      "loss": 2.4579,
      "step": 1440700
    },
    {
      "epoch": 467.03403565640195,
      "grad_norm": 1.5057729482650757,
      "learning_rate": 3.3174829711320145e-06,
      "loss": 2.4239,
      "step": 1440800
    },
    {
      "epoch": 467.0664505672609,
      "grad_norm": 1.6359273195266724,
      "learning_rate": 3.314239377229971e-06,
      "loss": 2.4567,
      "step": 1440900
    },
    {
      "epoch": 467.09886547811993,
      "grad_norm": 1.489410638809204,
      "learning_rate": 3.3109957833279277e-06,
      "loss": 2.441,
      "step": 1441000
    },
    {
      "epoch": 467.13128038897895,
      "grad_norm": 1.5880275964736938,
      "learning_rate": 3.307752189425884e-06,
      "loss": 2.4298,
      "step": 1441100
    },
    {
      "epoch": 467.1636952998379,
      "grad_norm": 1.5059887170791626,
      "learning_rate": 3.3045085955238406e-06,
      "loss": 2.4648,
      "step": 1441200
    },
    {
      "epoch": 467.19611021069693,
      "grad_norm": 1.366735816001892,
      "learning_rate": 3.301265001621797e-06,
      "loss": 2.4552,
      "step": 1441300
    },
    {
      "epoch": 467.2285251215559,
      "grad_norm": 1.4446178674697876,
      "learning_rate": 3.2980214077197534e-06,
      "loss": 2.4405,
      "step": 1441400
    },
    {
      "epoch": 467.2609400324149,
      "grad_norm": 1.5837886333465576,
      "learning_rate": 3.29477781381771e-06,
      "loss": 2.445,
      "step": 1441500
    },
    {
      "epoch": 467.2933549432739,
      "grad_norm": 1.4061189889907837,
      "learning_rate": 3.2915342199156666e-06,
      "loss": 2.4546,
      "step": 1441600
    },
    {
      "epoch": 467.3257698541329,
      "grad_norm": 1.6651510000228882,
      "learning_rate": 3.288290626013623e-06,
      "loss": 2.4381,
      "step": 1441700
    },
    {
      "epoch": 467.3581847649919,
      "grad_norm": 1.3547487258911133,
      "learning_rate": 3.2850470321115795e-06,
      "loss": 2.4595,
      "step": 1441800
    },
    {
      "epoch": 467.39059967585086,
      "grad_norm": 1.4519572257995605,
      "learning_rate": 3.281803438209536e-06,
      "loss": 2.4531,
      "step": 1441900
    },
    {
      "epoch": 467.4230145867099,
      "grad_norm": 1.3823602199554443,
      "learning_rate": 3.2785922802465135e-06,
      "loss": 2.4522,
      "step": 1442000
    },
    {
      "epoch": 467.4554294975689,
      "grad_norm": 1.469130039215088,
      "learning_rate": 3.2753486863444697e-06,
      "loss": 2.4719,
      "step": 1442100
    },
    {
      "epoch": 467.48784440842786,
      "grad_norm": 1.5440795421600342,
      "learning_rate": 3.2721050924424263e-06,
      "loss": 2.4809,
      "step": 1442200
    },
    {
      "epoch": 467.5202593192869,
      "grad_norm": 1.427151083946228,
      "learning_rate": 3.268861498540383e-06,
      "loss": 2.4836,
      "step": 1442300
    },
    {
      "epoch": 467.55267423014584,
      "grad_norm": 1.4322365522384644,
      "learning_rate": 3.265617904638339e-06,
      "loss": 2.4552,
      "step": 1442400
    },
    {
      "epoch": 467.58508914100486,
      "grad_norm": 1.6479326486587524,
      "learning_rate": 3.2623743107362958e-06,
      "loss": 2.4618,
      "step": 1442500
    },
    {
      "epoch": 467.6175040518639,
      "grad_norm": 1.4476590156555176,
      "learning_rate": 3.2591307168342524e-06,
      "loss": 2.4422,
      "step": 1442600
    },
    {
      "epoch": 467.64991896272284,
      "grad_norm": 1.5363080501556396,
      "learning_rate": 3.2558871229322094e-06,
      "loss": 2.4529,
      "step": 1442700
    },
    {
      "epoch": 467.68233387358185,
      "grad_norm": 1.2897945642471313,
      "learning_rate": 3.252643529030166e-06,
      "loss": 2.4467,
      "step": 1442800
    },
    {
      "epoch": 467.7147487844408,
      "grad_norm": 1.4462989568710327,
      "learning_rate": 3.2493999351281223e-06,
      "loss": 2.4509,
      "step": 1442900
    },
    {
      "epoch": 467.74716369529983,
      "grad_norm": 1.357210397720337,
      "learning_rate": 3.246156341226079e-06,
      "loss": 2.4496,
      "step": 1443000
    },
    {
      "epoch": 467.77957860615885,
      "grad_norm": 1.4441312551498413,
      "learning_rate": 3.2429127473240355e-06,
      "loss": 2.4537,
      "step": 1443100
    },
    {
      "epoch": 467.8119935170178,
      "grad_norm": 1.3435863256454468,
      "learning_rate": 3.2396691534219917e-06,
      "loss": 2.4476,
      "step": 1443200
    },
    {
      "epoch": 467.84440842787683,
      "grad_norm": 1.2900569438934326,
      "learning_rate": 3.2364255595199483e-06,
      "loss": 2.4595,
      "step": 1443300
    },
    {
      "epoch": 467.87682333873585,
      "grad_norm": 1.371069073677063,
      "learning_rate": 3.233181965617905e-06,
      "loss": 2.4642,
      "step": 1443400
    },
    {
      "epoch": 467.9092382495948,
      "grad_norm": 1.346145749092102,
      "learning_rate": 3.229938371715861e-06,
      "loss": 2.4325,
      "step": 1443500
    },
    {
      "epoch": 467.9416531604538,
      "grad_norm": 1.4913851022720337,
      "learning_rate": 3.2266947778138178e-06,
      "loss": 2.4474,
      "step": 1443600
    },
    {
      "epoch": 467.9740680713128,
      "grad_norm": 1.4326856136322021,
      "learning_rate": 3.2234511839117744e-06,
      "loss": 2.4655,
      "step": 1443700
    },
    {
      "epoch": 468.0,
      "eval_bleu": 1.2432856910299783,
      "eval_loss": 4.281932830810547,
      "eval_runtime": 4.4362,
      "eval_samples_per_second": 110.906,
      "eval_steps_per_second": 1.803,
      "step": 1443780
    },
    {
      "epoch": 468.0064829821718,
      "grad_norm": 1.5130571126937866,
      "learning_rate": 3.2202075900097306e-06,
      "loss": 2.4506,
      "step": 1443800
    },
    {
      "epoch": 468.0388978930308,
      "grad_norm": 1.5916261672973633,
      "learning_rate": 3.2169639961076872e-06,
      "loss": 2.4551,
      "step": 1443900
    },
    {
      "epoch": 468.0713128038898,
      "grad_norm": 1.3953899145126343,
      "learning_rate": 3.213720402205644e-06,
      "loss": 2.4497,
      "step": 1444000
    },
    {
      "epoch": 468.1037277147488,
      "grad_norm": 1.509551763534546,
      "learning_rate": 3.2104768083036e-06,
      "loss": 2.4551,
      "step": 1444100
    },
    {
      "epoch": 468.13614262560776,
      "grad_norm": 1.3692282438278198,
      "learning_rate": 3.2072332144015575e-06,
      "loss": 2.4473,
      "step": 1444200
    },
    {
      "epoch": 468.1685575364668,
      "grad_norm": 1.4101523160934448,
      "learning_rate": 3.2039896204995137e-06,
      "loss": 2.4382,
      "step": 1444300
    },
    {
      "epoch": 468.2009724473258,
      "grad_norm": 1.2382004261016846,
      "learning_rate": 3.2007460265974704e-06,
      "loss": 2.4661,
      "step": 1444400
    },
    {
      "epoch": 468.23338735818476,
      "grad_norm": 1.5566835403442383,
      "learning_rate": 3.197502432695427e-06,
      "loss": 2.469,
      "step": 1444500
    },
    {
      "epoch": 468.2658022690438,
      "grad_norm": 1.3456461429595947,
      "learning_rate": 3.194258838793383e-06,
      "loss": 2.4687,
      "step": 1444600
    },
    {
      "epoch": 468.29821717990274,
      "grad_norm": 1.497599720954895,
      "learning_rate": 3.19101524489134e-06,
      "loss": 2.4382,
      "step": 1444700
    },
    {
      "epoch": 468.33063209076175,
      "grad_norm": 1.6383930444717407,
      "learning_rate": 3.1877716509892964e-06,
      "loss": 2.4381,
      "step": 1444800
    },
    {
      "epoch": 468.36304700162077,
      "grad_norm": 1.379942774772644,
      "learning_rate": 3.1845280570872526e-06,
      "loss": 2.4556,
      "step": 1444900
    },
    {
      "epoch": 468.39546191247973,
      "grad_norm": 1.6356710195541382,
      "learning_rate": 3.1812844631852093e-06,
      "loss": 2.4552,
      "step": 1445000
    },
    {
      "epoch": 468.42787682333875,
      "grad_norm": 1.400618076324463,
      "learning_rate": 3.178040869283166e-06,
      "loss": 2.4562,
      "step": 1445100
    },
    {
      "epoch": 468.4602917341977,
      "grad_norm": 1.2960615158081055,
      "learning_rate": 3.174797275381122e-06,
      "loss": 2.4497,
      "step": 1445200
    },
    {
      "epoch": 468.4927066450567,
      "grad_norm": 1.6314351558685303,
      "learning_rate": 3.1715536814790787e-06,
      "loss": 2.4582,
      "step": 1445300
    },
    {
      "epoch": 468.52512155591575,
      "grad_norm": 1.3767446279525757,
      "learning_rate": 3.1683100875770353e-06,
      "loss": 2.4669,
      "step": 1445400
    },
    {
      "epoch": 468.5575364667747,
      "grad_norm": 1.4234111309051514,
      "learning_rate": 3.1650664936749915e-06,
      "loss": 2.4315,
      "step": 1445500
    },
    {
      "epoch": 468.5899513776337,
      "grad_norm": 1.4370208978652954,
      "learning_rate": 3.161822899772949e-06,
      "loss": 2.4323,
      "step": 1445600
    },
    {
      "epoch": 468.6223662884927,
      "grad_norm": 1.493667721748352,
      "learning_rate": 3.1585793058709052e-06,
      "loss": 2.4709,
      "step": 1445700
    },
    {
      "epoch": 468.6547811993517,
      "grad_norm": 1.3570752143859863,
      "learning_rate": 3.155335711968862e-06,
      "loss": 2.458,
      "step": 1445800
    },
    {
      "epoch": 468.6871961102107,
      "grad_norm": 1.3855476379394531,
      "learning_rate": 3.1520921180668185e-06,
      "loss": 2.4712,
      "step": 1445900
    },
    {
      "epoch": 468.7196110210697,
      "grad_norm": 1.3942219018936157,
      "learning_rate": 3.148880960103795e-06,
      "loss": 2.438,
      "step": 1446000
    },
    {
      "epoch": 468.7520259319287,
      "grad_norm": 1.5190932750701904,
      "learning_rate": 3.1456373662017516e-06,
      "loss": 2.4374,
      "step": 1446100
    },
    {
      "epoch": 468.78444084278766,
      "grad_norm": 1.4599696397781372,
      "learning_rate": 3.142393772299708e-06,
      "loss": 2.4426,
      "step": 1446200
    },
    {
      "epoch": 468.8168557536467,
      "grad_norm": 1.631340742111206,
      "learning_rate": 3.1391501783976645e-06,
      "loss": 2.4647,
      "step": 1446300
    },
    {
      "epoch": 468.8492706645057,
      "grad_norm": 1.3622193336486816,
      "learning_rate": 3.1359065844956215e-06,
      "loss": 2.4631,
      "step": 1446400
    },
    {
      "epoch": 468.88168557536466,
      "grad_norm": 1.536633849143982,
      "learning_rate": 3.132662990593578e-06,
      "loss": 2.4587,
      "step": 1446500
    },
    {
      "epoch": 468.9141004862237,
      "grad_norm": 1.3127409219741821,
      "learning_rate": 3.1294193966915348e-06,
      "loss": 2.4373,
      "step": 1446600
    },
    {
      "epoch": 468.94651539708263,
      "grad_norm": 1.5456035137176514,
      "learning_rate": 3.126175802789491e-06,
      "loss": 2.4376,
      "step": 1446700
    },
    {
      "epoch": 468.97893030794165,
      "grad_norm": 1.6142725944519043,
      "learning_rate": 3.1229322088874476e-06,
      "loss": 2.4515,
      "step": 1446800
    },
    {
      "epoch": 469.0,
      "eval_bleu": 1.044086854114942,
      "eval_loss": 4.28327751159668,
      "eval_runtime": 4.5187,
      "eval_samples_per_second": 108.882,
      "eval_steps_per_second": 1.77,
      "step": 1446865
    },
    {
      "epoch": 469.01134521880067,
      "grad_norm": 1.416102647781372,
      "learning_rate": 3.119688614985404e-06,
      "loss": 2.4559,
      "step": 1446900
    },
    {
      "epoch": 469.04376012965963,
      "grad_norm": 1.4989228248596191,
      "learning_rate": 3.1164450210833604e-06,
      "loss": 2.4274,
      "step": 1447000
    },
    {
      "epoch": 469.07617504051865,
      "grad_norm": 1.5653554201126099,
      "learning_rate": 3.113201427181317e-06,
      "loss": 2.4461,
      "step": 1447100
    },
    {
      "epoch": 469.1085899513776,
      "grad_norm": 1.4036474227905273,
      "learning_rate": 3.1099578332792737e-06,
      "loss": 2.434,
      "step": 1447200
    },
    {
      "epoch": 469.1410048622366,
      "grad_norm": 1.438398838043213,
      "learning_rate": 3.10671423937723e-06,
      "loss": 2.462,
      "step": 1447300
    },
    {
      "epoch": 469.17341977309565,
      "grad_norm": 1.4582798480987549,
      "learning_rate": 3.1034706454751865e-06,
      "loss": 2.4553,
      "step": 1447400
    },
    {
      "epoch": 469.2058346839546,
      "grad_norm": 1.6078060865402222,
      "learning_rate": 3.100227051573143e-06,
      "loss": 2.4634,
      "step": 1447500
    },
    {
      "epoch": 469.2382495948136,
      "grad_norm": 1.5294592380523682,
      "learning_rate": 3.0969834576710997e-06,
      "loss": 2.4482,
      "step": 1447600
    },
    {
      "epoch": 469.2706645056726,
      "grad_norm": 1.4871124029159546,
      "learning_rate": 3.0937398637690564e-06,
      "loss": 2.4424,
      "step": 1447700
    },
    {
      "epoch": 469.3030794165316,
      "grad_norm": 1.3750633001327515,
      "learning_rate": 3.090496269867013e-06,
      "loss": 2.482,
      "step": 1447800
    },
    {
      "epoch": 469.3354943273906,
      "grad_norm": 1.3479560613632202,
      "learning_rate": 3.087252675964969e-06,
      "loss": 2.4578,
      "step": 1447900
    },
    {
      "epoch": 469.3679092382496,
      "grad_norm": 1.4388196468353271,
      "learning_rate": 3.084009082062926e-06,
      "loss": 2.4662,
      "step": 1448000
    },
    {
      "epoch": 469.4003241491086,
      "grad_norm": 1.23672354221344,
      "learning_rate": 3.0807979240999028e-06,
      "loss": 2.4736,
      "step": 1448100
    },
    {
      "epoch": 469.43273905996756,
      "grad_norm": 1.878906488418579,
      "learning_rate": 3.0775543301978594e-06,
      "loss": 2.4859,
      "step": 1448200
    },
    {
      "epoch": 469.4651539708266,
      "grad_norm": 1.4069850444793701,
      "learning_rate": 3.074310736295816e-06,
      "loss": 2.4586,
      "step": 1448300
    },
    {
      "epoch": 469.4975688816856,
      "grad_norm": 1.4861233234405518,
      "learning_rate": 3.0710671423937726e-06,
      "loss": 2.4485,
      "step": 1448400
    },
    {
      "epoch": 469.52998379254456,
      "grad_norm": 1.5193252563476562,
      "learning_rate": 3.0678559844307496e-06,
      "loss": 2.4567,
      "step": 1448500
    },
    {
      "epoch": 469.5623987034036,
      "grad_norm": 1.403244137763977,
      "learning_rate": 3.064612390528706e-06,
      "loss": 2.4178,
      "step": 1448600
    },
    {
      "epoch": 469.59481361426253,
      "grad_norm": 1.4520572423934937,
      "learning_rate": 3.0613687966266624e-06,
      "loss": 2.4539,
      "step": 1448700
    },
    {
      "epoch": 469.62722852512155,
      "grad_norm": 1.3754116296768188,
      "learning_rate": 3.058125202724619e-06,
      "loss": 2.4692,
      "step": 1448800
    },
    {
      "epoch": 469.65964343598057,
      "grad_norm": 1.410886287689209,
      "learning_rate": 3.0548816088225752e-06,
      "loss": 2.4409,
      "step": 1448900
    },
    {
      "epoch": 469.69205834683953,
      "grad_norm": 1.3456809520721436,
      "learning_rate": 3.0516380149205323e-06,
      "loss": 2.4405,
      "step": 1449000
    },
    {
      "epoch": 469.72447325769855,
      "grad_norm": 1.6592462062835693,
      "learning_rate": 3.048394421018489e-06,
      "loss": 2.4413,
      "step": 1449100
    },
    {
      "epoch": 469.7568881685575,
      "grad_norm": 1.4005192518234253,
      "learning_rate": 3.045150827116445e-06,
      "loss": 2.4394,
      "step": 1449200
    },
    {
      "epoch": 469.7893030794165,
      "grad_norm": 1.5204097032546997,
      "learning_rate": 3.0419072332144017e-06,
      "loss": 2.4664,
      "step": 1449300
    },
    {
      "epoch": 469.82171799027554,
      "grad_norm": 1.5617252588272095,
      "learning_rate": 3.0386636393123584e-06,
      "loss": 2.446,
      "step": 1449400
    },
    {
      "epoch": 469.8541329011345,
      "grad_norm": 1.5329678058624268,
      "learning_rate": 3.0354200454103146e-06,
      "loss": 2.4349,
      "step": 1449500
    },
    {
      "epoch": 469.8865478119935,
      "grad_norm": 1.5770719051361084,
      "learning_rate": 3.032176451508271e-06,
      "loss": 2.4316,
      "step": 1449600
    },
    {
      "epoch": 469.9189627228525,
      "grad_norm": 1.531424641609192,
      "learning_rate": 3.028932857606228e-06,
      "loss": 2.4355,
      "step": 1449700
    },
    {
      "epoch": 469.9513776337115,
      "grad_norm": 1.3953889608383179,
      "learning_rate": 3.0256892637041845e-06,
      "loss": 2.4678,
      "step": 1449800
    },
    {
      "epoch": 469.9837925445705,
      "grad_norm": 1.7049366235733032,
      "learning_rate": 3.022445669802141e-06,
      "loss": 2.4574,
      "step": 1449900
    },
    {
      "epoch": 470.0,
      "eval_bleu": 1.1205844598624166,
      "eval_loss": 4.284604072570801,
      "eval_runtime": 4.5183,
      "eval_samples_per_second": 108.89,
      "eval_steps_per_second": 1.771,
      "step": 1449950
    },
    {
      "epoch": 470.0162074554295,
      "grad_norm": 1.4956938028335571,
      "learning_rate": 3.0192020759000973e-06,
      "loss": 2.4597,
      "step": 1450000
    },
    {
      "epoch": 470.0486223662885,
      "grad_norm": 1.4972233772277832,
      "learning_rate": 3.015958481998054e-06,
      "loss": 2.4444,
      "step": 1450100
    },
    {
      "epoch": 470.0810372771475,
      "grad_norm": 1.3973071575164795,
      "learning_rate": 3.0127148880960105e-06,
      "loss": 2.4706,
      "step": 1450200
    },
    {
      "epoch": 470.1134521880065,
      "grad_norm": 1.3690097332000732,
      "learning_rate": 3.0094712941939667e-06,
      "loss": 2.4506,
      "step": 1450300
    },
    {
      "epoch": 470.1458670988655,
      "grad_norm": 1.3716710805892944,
      "learning_rate": 3.0062277002919238e-06,
      "loss": 2.4458,
      "step": 1450400
    },
    {
      "epoch": 470.17828200972446,
      "grad_norm": 1.3248454332351685,
      "learning_rate": 3.0029841063898804e-06,
      "loss": 2.4435,
      "step": 1450500
    },
    {
      "epoch": 470.2106969205835,
      "grad_norm": 1.5388387441635132,
      "learning_rate": 2.9997405124878366e-06,
      "loss": 2.4616,
      "step": 1450600
    },
    {
      "epoch": 470.2431118314425,
      "grad_norm": 1.4676721096038818,
      "learning_rate": 2.9964969185857932e-06,
      "loss": 2.4366,
      "step": 1450700
    },
    {
      "epoch": 470.27552674230145,
      "grad_norm": 1.5218274593353271,
      "learning_rate": 2.99325332468375e-06,
      "loss": 2.4408,
      "step": 1450800
    },
    {
      "epoch": 470.30794165316047,
      "grad_norm": 1.788082242012024,
      "learning_rate": 2.990009730781706e-06,
      "loss": 2.4753,
      "step": 1450900
    },
    {
      "epoch": 470.34035656401943,
      "grad_norm": 1.449851632118225,
      "learning_rate": 2.9867661368796627e-06,
      "loss": 2.4511,
      "step": 1451000
    },
    {
      "epoch": 470.37277147487845,
      "grad_norm": 1.432139277458191,
      "learning_rate": 2.9835225429776193e-06,
      "loss": 2.4505,
      "step": 1451100
    },
    {
      "epoch": 470.40518638573747,
      "grad_norm": 1.6472023725509644,
      "learning_rate": 2.980278949075576e-06,
      "loss": 2.4495,
      "step": 1451200
    },
    {
      "epoch": 470.4376012965964,
      "grad_norm": 1.4395592212677002,
      "learning_rate": 2.977067791112553e-06,
      "loss": 2.4379,
      "step": 1451300
    },
    {
      "epoch": 470.47001620745544,
      "grad_norm": 1.387694239616394,
      "learning_rate": 2.9738241972105095e-06,
      "loss": 2.4617,
      "step": 1451400
    },
    {
      "epoch": 470.5024311183144,
      "grad_norm": 1.296791434288025,
      "learning_rate": 2.9705806033084657e-06,
      "loss": 2.4504,
      "step": 1451500
    },
    {
      "epoch": 470.5348460291734,
      "grad_norm": 1.4535775184631348,
      "learning_rate": 2.9673370094064223e-06,
      "loss": 2.4524,
      "step": 1451600
    },
    {
      "epoch": 470.56726094003244,
      "grad_norm": 1.545163631439209,
      "learning_rate": 2.964093415504379e-06,
      "loss": 2.4414,
      "step": 1451700
    },
    {
      "epoch": 470.5996758508914,
      "grad_norm": 1.294854998588562,
      "learning_rate": 2.9608498216023356e-06,
      "loss": 2.4537,
      "step": 1451800
    },
    {
      "epoch": 470.6320907617504,
      "grad_norm": 1.4761323928833008,
      "learning_rate": 2.9576062277002922e-06,
      "loss": 2.4539,
      "step": 1451900
    },
    {
      "epoch": 470.6645056726094,
      "grad_norm": 1.3382728099822998,
      "learning_rate": 2.954362633798249e-06,
      "loss": 2.4544,
      "step": 1452000
    },
    {
      "epoch": 470.6969205834684,
      "grad_norm": 1.3209419250488281,
      "learning_rate": 2.951119039896205e-06,
      "loss": 2.4534,
      "step": 1452100
    },
    {
      "epoch": 470.7293354943274,
      "grad_norm": 1.5097200870513916,
      "learning_rate": 2.9478754459941617e-06,
      "loss": 2.4661,
      "step": 1452200
    },
    {
      "epoch": 470.7617504051864,
      "grad_norm": 1.5319290161132812,
      "learning_rate": 2.9446318520921183e-06,
      "loss": 2.447,
      "step": 1452300
    },
    {
      "epoch": 470.7941653160454,
      "grad_norm": 1.7123748064041138,
      "learning_rate": 2.9413882581900745e-06,
      "loss": 2.4555,
      "step": 1452400
    },
    {
      "epoch": 470.82658022690435,
      "grad_norm": 1.3962128162384033,
      "learning_rate": 2.938144664288031e-06,
      "loss": 2.4482,
      "step": 1452500
    },
    {
      "epoch": 470.8589951377634,
      "grad_norm": 1.3549712896347046,
      "learning_rate": 2.934901070385988e-06,
      "loss": 2.4398,
      "step": 1452600
    },
    {
      "epoch": 470.8914100486224,
      "grad_norm": 1.331424355506897,
      "learning_rate": 2.9316574764839444e-06,
      "loss": 2.4495,
      "step": 1452700
    },
    {
      "epoch": 470.92382495948135,
      "grad_norm": 1.482092261314392,
      "learning_rate": 2.928413882581901e-06,
      "loss": 2.4541,
      "step": 1452800
    },
    {
      "epoch": 470.95623987034037,
      "grad_norm": 1.5189673900604248,
      "learning_rate": 2.9251702886798576e-06,
      "loss": 2.4468,
      "step": 1452900
    },
    {
      "epoch": 470.98865478119933,
      "grad_norm": 1.3518033027648926,
      "learning_rate": 2.921926694777814e-06,
      "loss": 2.4578,
      "step": 1453000
    },
    {
      "epoch": 471.0,
      "eval_bleu": 1.060878344436624,
      "eval_loss": 4.2838945388793945,
      "eval_runtime": 4.5061,
      "eval_samples_per_second": 109.186,
      "eval_steps_per_second": 1.775,
      "step": 1453035
    },
    {
      "epoch": 471.02106969205835,
      "grad_norm": 1.2779444456100464,
      "learning_rate": 2.9186831008757704e-06,
      "loss": 2.453,
      "step": 1453100
    },
    {
      "epoch": 471.05348460291737,
      "grad_norm": 1.6260583400726318,
      "learning_rate": 2.915439506973727e-06,
      "loss": 2.4577,
      "step": 1453200
    },
    {
      "epoch": 471.0858995137763,
      "grad_norm": 1.5159934759140015,
      "learning_rate": 2.9121959130716833e-06,
      "loss": 2.4611,
      "step": 1453300
    },
    {
      "epoch": 471.11831442463534,
      "grad_norm": 1.4941296577453613,
      "learning_rate": 2.9089523191696403e-06,
      "loss": 2.4539,
      "step": 1453400
    },
    {
      "epoch": 471.1507293354943,
      "grad_norm": 1.3669159412384033,
      "learning_rate": 2.9057087252675965e-06,
      "loss": 2.4397,
      "step": 1453500
    },
    {
      "epoch": 471.1831442463533,
      "grad_norm": 1.6486191749572754,
      "learning_rate": 2.902465131365553e-06,
      "loss": 2.4581,
      "step": 1453600
    },
    {
      "epoch": 471.21555915721234,
      "grad_norm": 1.4999579191207886,
      "learning_rate": 2.8992215374635098e-06,
      "loss": 2.441,
      "step": 1453700
    },
    {
      "epoch": 471.2479740680713,
      "grad_norm": 1.3745815753936768,
      "learning_rate": 2.895977943561466e-06,
      "loss": 2.4391,
      "step": 1453800
    },
    {
      "epoch": 471.2803889789303,
      "grad_norm": 1.3543925285339355,
      "learning_rate": 2.8927343496594226e-06,
      "loss": 2.4621,
      "step": 1453900
    },
    {
      "epoch": 471.3128038897893,
      "grad_norm": 1.3212294578552246,
      "learning_rate": 2.8894907557573792e-06,
      "loss": 2.4417,
      "step": 1454000
    },
    {
      "epoch": 471.3452188006483,
      "grad_norm": 1.4858107566833496,
      "learning_rate": 2.886247161855336e-06,
      "loss": 2.4541,
      "step": 1454100
    },
    {
      "epoch": 471.3776337115073,
      "grad_norm": 1.3700674772262573,
      "learning_rate": 2.8830035679532925e-06,
      "loss": 2.4554,
      "step": 1454200
    },
    {
      "epoch": 471.4100486223663,
      "grad_norm": 1.4948186874389648,
      "learning_rate": 2.879759974051249e-06,
      "loss": 2.4539,
      "step": 1454300
    },
    {
      "epoch": 471.4424635332253,
      "grad_norm": 1.4633095264434814,
      "learning_rate": 2.8765163801492053e-06,
      "loss": 2.4546,
      "step": 1454400
    },
    {
      "epoch": 471.47487844408425,
      "grad_norm": 1.4888750314712524,
      "learning_rate": 2.873272786247162e-06,
      "loss": 2.4479,
      "step": 1454500
    },
    {
      "epoch": 471.5072933549433,
      "grad_norm": 1.5919153690338135,
      "learning_rate": 2.8700291923451186e-06,
      "loss": 2.4483,
      "step": 1454600
    },
    {
      "epoch": 471.5397082658023,
      "grad_norm": 1.4165735244750977,
      "learning_rate": 2.8667855984430748e-06,
      "loss": 2.4464,
      "step": 1454700
    },
    {
      "epoch": 471.57212317666125,
      "grad_norm": 1.4237165451049805,
      "learning_rate": 2.863542004541032e-06,
      "loss": 2.4679,
      "step": 1454800
    },
    {
      "epoch": 471.60453808752027,
      "grad_norm": 1.6606358289718628,
      "learning_rate": 2.8602984106389884e-06,
      "loss": 2.4289,
      "step": 1454900
    },
    {
      "epoch": 471.63695299837923,
      "grad_norm": 1.492802381515503,
      "learning_rate": 2.8570548167369446e-06,
      "loss": 2.4551,
      "step": 1455000
    },
    {
      "epoch": 471.66936790923825,
      "grad_norm": 1.3479965925216675,
      "learning_rate": 2.8538112228349013e-06,
      "loss": 2.459,
      "step": 1455100
    },
    {
      "epoch": 471.70178282009726,
      "grad_norm": 1.5303339958190918,
      "learning_rate": 2.850567628932858e-06,
      "loss": 2.4788,
      "step": 1455200
    },
    {
      "epoch": 471.7341977309562,
      "grad_norm": 1.405687928199768,
      "learning_rate": 2.847356470969835e-06,
      "loss": 2.4579,
      "step": 1455300
    },
    {
      "epoch": 471.76661264181524,
      "grad_norm": 1.5572988986968994,
      "learning_rate": 2.844112877067791e-06,
      "loss": 2.4467,
      "step": 1455400
    },
    {
      "epoch": 471.7990275526742,
      "grad_norm": 1.3940187692642212,
      "learning_rate": 2.8408692831657477e-06,
      "loss": 2.4254,
      "step": 1455500
    },
    {
      "epoch": 471.8314424635332,
      "grad_norm": 1.4282482862472534,
      "learning_rate": 2.8376256892637043e-06,
      "loss": 2.4411,
      "step": 1455600
    },
    {
      "epoch": 471.86385737439224,
      "grad_norm": 1.2612498998641968,
      "learning_rate": 2.8344145313006812e-06,
      "loss": 2.4496,
      "step": 1455700
    },
    {
      "epoch": 471.8962722852512,
      "grad_norm": 1.506210207939148,
      "learning_rate": 2.831170937398638e-06,
      "loss": 2.4435,
      "step": 1455800
    },
    {
      "epoch": 471.9286871961102,
      "grad_norm": 1.388121247291565,
      "learning_rate": 2.8279273434965945e-06,
      "loss": 2.4568,
      "step": 1455900
    },
    {
      "epoch": 471.9611021069692,
      "grad_norm": 1.411889672279358,
      "learning_rate": 2.8246837495945507e-06,
      "loss": 2.4484,
      "step": 1456000
    },
    {
      "epoch": 471.9935170178282,
      "grad_norm": 1.4020190238952637,
      "learning_rate": 2.8214401556925073e-06,
      "loss": 2.4584,
      "step": 1456100
    },
    {
      "epoch": 472.0,
      "eval_bleu": 1.0975466327971266,
      "eval_loss": 4.282740592956543,
      "eval_runtime": 4.5417,
      "eval_samples_per_second": 108.329,
      "eval_steps_per_second": 1.761,
      "step": 1456120
    },
    {
      "epoch": 472.0259319286872,
      "grad_norm": 1.5013973712921143,
      "learning_rate": 2.818196561790464e-06,
      "loss": 2.4654,
      "step": 1456200
    },
    {
      "epoch": 472.0583468395462,
      "grad_norm": 1.357389211654663,
      "learning_rate": 2.8149529678884206e-06,
      "loss": 2.444,
      "step": 1456300
    },
    {
      "epoch": 472.0907617504052,
      "grad_norm": 1.4916160106658936,
      "learning_rate": 2.811709373986377e-06,
      "loss": 2.4377,
      "step": 1456400
    },
    {
      "epoch": 472.12317666126415,
      "grad_norm": 1.4206761121749878,
      "learning_rate": 2.808465780084334e-06,
      "loss": 2.452,
      "step": 1456500
    },
    {
      "epoch": 472.15559157212317,
      "grad_norm": 1.5258318185806274,
      "learning_rate": 2.80522218618229e-06,
      "loss": 2.4427,
      "step": 1456600
    },
    {
      "epoch": 472.1880064829822,
      "grad_norm": 1.376441478729248,
      "learning_rate": 2.8019785922802466e-06,
      "loss": 2.4359,
      "step": 1456700
    },
    {
      "epoch": 472.22042139384115,
      "grad_norm": 1.4827395677566528,
      "learning_rate": 2.7987674343172236e-06,
      "loss": 2.4708,
      "step": 1456800
    },
    {
      "epoch": 472.25283630470017,
      "grad_norm": 1.6799441576004028,
      "learning_rate": 2.7955238404151802e-06,
      "loss": 2.4694,
      "step": 1456900
    },
    {
      "epoch": 472.2852512155592,
      "grad_norm": 1.3578940629959106,
      "learning_rate": 2.7922802465131364e-06,
      "loss": 2.4502,
      "step": 1457000
    },
    {
      "epoch": 472.31766612641815,
      "grad_norm": 1.5435678958892822,
      "learning_rate": 2.7890366526110935e-06,
      "loss": 2.4573,
      "step": 1457100
    },
    {
      "epoch": 472.35008103727716,
      "grad_norm": 1.3100169897079468,
      "learning_rate": 2.7857930587090497e-06,
      "loss": 2.4304,
      "step": 1457200
    },
    {
      "epoch": 472.3824959481361,
      "grad_norm": 1.4512147903442383,
      "learning_rate": 2.7825494648070063e-06,
      "loss": 2.4496,
      "step": 1457300
    },
    {
      "epoch": 472.41491085899514,
      "grad_norm": 1.559665560722351,
      "learning_rate": 2.779305870904963e-06,
      "loss": 2.4578,
      "step": 1457400
    },
    {
      "epoch": 472.44732576985416,
      "grad_norm": 1.4274612665176392,
      "learning_rate": 2.776062277002919e-06,
      "loss": 2.4596,
      "step": 1457500
    },
    {
      "epoch": 472.4797406807131,
      "grad_norm": 1.4122276306152344,
      "learning_rate": 2.7728186831008758e-06,
      "loss": 2.4655,
      "step": 1457600
    },
    {
      "epoch": 472.51215559157214,
      "grad_norm": 1.2999615669250488,
      "learning_rate": 2.7695750891988324e-06,
      "loss": 2.477,
      "step": 1457700
    },
    {
      "epoch": 472.5445705024311,
      "grad_norm": 1.3034205436706543,
      "learning_rate": 2.766331495296789e-06,
      "loss": 2.4837,
      "step": 1457800
    },
    {
      "epoch": 472.5769854132901,
      "grad_norm": 1.5490566492080688,
      "learning_rate": 2.7630879013947456e-06,
      "loss": 2.4357,
      "step": 1457900
    },
    {
      "epoch": 472.60940032414914,
      "grad_norm": 1.4480189085006714,
      "learning_rate": 2.7598443074927023e-06,
      "loss": 2.4676,
      "step": 1458000
    },
    {
      "epoch": 472.6418152350081,
      "grad_norm": 1.5658131837844849,
      "learning_rate": 2.7566007135906585e-06,
      "loss": 2.4225,
      "step": 1458100
    },
    {
      "epoch": 472.6742301458671,
      "grad_norm": 1.713906168937683,
      "learning_rate": 2.753357119688615e-06,
      "loss": 2.4494,
      "step": 1458200
    },
    {
      "epoch": 472.7066450567261,
      "grad_norm": 1.4156848192214966,
      "learning_rate": 2.7501135257865717e-06,
      "loss": 2.4409,
      "step": 1458300
    },
    {
      "epoch": 472.7390599675851,
      "grad_norm": 1.3220415115356445,
      "learning_rate": 2.746869931884528e-06,
      "loss": 2.4326,
      "step": 1458400
    },
    {
      "epoch": 472.7714748784441,
      "grad_norm": 1.480993390083313,
      "learning_rate": 2.743626337982485e-06,
      "loss": 2.4329,
      "step": 1458500
    },
    {
      "epoch": 472.80388978930307,
      "grad_norm": 1.414660930633545,
      "learning_rate": 2.7403827440804416e-06,
      "loss": 2.4735,
      "step": 1458600
    },
    {
      "epoch": 472.8363047001621,
      "grad_norm": 1.441046953201294,
      "learning_rate": 2.737139150178398e-06,
      "loss": 2.4431,
      "step": 1458700
    },
    {
      "epoch": 472.86871961102105,
      "grad_norm": 1.4387288093566895,
      "learning_rate": 2.7338955562763544e-06,
      "loss": 2.4511,
      "step": 1458800
    },
    {
      "epoch": 472.90113452188007,
      "grad_norm": 1.3360786437988281,
      "learning_rate": 2.730651962374311e-06,
      "loss": 2.4263,
      "step": 1458900
    },
    {
      "epoch": 472.9335494327391,
      "grad_norm": 1.3241674900054932,
      "learning_rate": 2.7274083684722672e-06,
      "loss": 2.4594,
      "step": 1459000
    },
    {
      "epoch": 472.96596434359805,
      "grad_norm": 1.4240397214889526,
      "learning_rate": 2.724164774570224e-06,
      "loss": 2.4611,
      "step": 1459100
    },
    {
      "epoch": 472.99837925445706,
      "grad_norm": 1.7495973110198975,
      "learning_rate": 2.7209211806681805e-06,
      "loss": 2.4481,
      "step": 1459200
    },
    {
      "epoch": 473.0,
      "eval_bleu": 1.0716937649848157,
      "eval_loss": 4.282407283782959,
      "eval_runtime": 4.2502,
      "eval_samples_per_second": 115.76,
      "eval_steps_per_second": 1.882,
      "step": 1459205
    },
    {
      "epoch": 473.030794165316,
      "grad_norm": 1.4199771881103516,
      "learning_rate": 2.717677586766137e-06,
      "loss": 2.482,
      "step": 1459300
    },
    {
      "epoch": 473.06320907617504,
      "grad_norm": 1.5557595491409302,
      "learning_rate": 2.7144339928640937e-06,
      "loss": 2.4284,
      "step": 1459400
    },
    {
      "epoch": 473.09562398703406,
      "grad_norm": 1.500120997428894,
      "learning_rate": 2.71119039896205e-06,
      "loss": 2.4276,
      "step": 1459500
    },
    {
      "epoch": 473.128038897893,
      "grad_norm": 1.3116846084594727,
      "learning_rate": 2.7079468050600066e-06,
      "loss": 2.451,
      "step": 1459600
    },
    {
      "epoch": 473.16045380875204,
      "grad_norm": 1.519283413887024,
      "learning_rate": 2.704703211157963e-06,
      "loss": 2.4352,
      "step": 1459700
    },
    {
      "epoch": 473.192868719611,
      "grad_norm": 1.3895632028579712,
      "learning_rate": 2.7014596172559194e-06,
      "loss": 2.4561,
      "step": 1459800
    },
    {
      "epoch": 473.22528363047,
      "grad_norm": 1.3982676267623901,
      "learning_rate": 2.698216023353876e-06,
      "loss": 2.4518,
      "step": 1459900
    },
    {
      "epoch": 473.25769854132903,
      "grad_norm": 1.4576656818389893,
      "learning_rate": 2.694972429451833e-06,
      "loss": 2.4452,
      "step": 1460000
    },
    {
      "epoch": 473.290113452188,
      "grad_norm": 1.3742973804473877,
      "learning_rate": 2.6917288355497893e-06,
      "loss": 2.4424,
      "step": 1460100
    },
    {
      "epoch": 473.322528363047,
      "grad_norm": 1.470791220664978,
      "learning_rate": 2.688485241647746e-06,
      "loss": 2.4577,
      "step": 1460200
    },
    {
      "epoch": 473.354943273906,
      "grad_norm": 1.4616291522979736,
      "learning_rate": 2.6852416477457025e-06,
      "loss": 2.4465,
      "step": 1460300
    },
    {
      "epoch": 473.387358184765,
      "grad_norm": 1.360357642173767,
      "learning_rate": 2.6819980538436587e-06,
      "loss": 2.4391,
      "step": 1460400
    },
    {
      "epoch": 473.419773095624,
      "grad_norm": 1.4855525493621826,
      "learning_rate": 2.6787544599416153e-06,
      "loss": 2.4526,
      "step": 1460500
    },
    {
      "epoch": 473.45218800648297,
      "grad_norm": 1.5047725439071655,
      "learning_rate": 2.675510866039572e-06,
      "loss": 2.4624,
      "step": 1460600
    },
    {
      "epoch": 473.484602917342,
      "grad_norm": 1.4098445177078247,
      "learning_rate": 2.6722672721375286e-06,
      "loss": 2.442,
      "step": 1460700
    },
    {
      "epoch": 473.51701782820095,
      "grad_norm": 1.475683569908142,
      "learning_rate": 2.6690236782354852e-06,
      "loss": 2.4503,
      "step": 1460800
    },
    {
      "epoch": 473.54943273905997,
      "grad_norm": 1.437709093093872,
      "learning_rate": 2.6657800843334414e-06,
      "loss": 2.447,
      "step": 1460900
    },
    {
      "epoch": 473.581847649919,
      "grad_norm": 1.2911549806594849,
      "learning_rate": 2.662536490431398e-06,
      "loss": 2.4603,
      "step": 1461000
    },
    {
      "epoch": 473.61426256077795,
      "grad_norm": 1.2551912069320679,
      "learning_rate": 2.6592928965293547e-06,
      "loss": 2.4405,
      "step": 1461100
    },
    {
      "epoch": 473.64667747163696,
      "grad_norm": 1.3193728923797607,
      "learning_rate": 2.6560493026273113e-06,
      "loss": 2.4487,
      "step": 1461200
    },
    {
      "epoch": 473.6790923824959,
      "grad_norm": 1.3393136262893677,
      "learning_rate": 2.6528057087252675e-06,
      "loss": 2.4402,
      "step": 1461300
    },
    {
      "epoch": 473.71150729335494,
      "grad_norm": 1.1758356094360352,
      "learning_rate": 2.6495621148232246e-06,
      "loss": 2.4595,
      "step": 1461400
    },
    {
      "epoch": 473.74392220421396,
      "grad_norm": 1.334822416305542,
      "learning_rate": 2.6463185209211808e-06,
      "loss": 2.4462,
      "step": 1461500
    },
    {
      "epoch": 473.7763371150729,
      "grad_norm": 1.531571626663208,
      "learning_rate": 2.6430749270191374e-06,
      "loss": 2.4608,
      "step": 1461600
    },
    {
      "epoch": 473.80875202593194,
      "grad_norm": 1.3407084941864014,
      "learning_rate": 2.639831333117094e-06,
      "loss": 2.4448,
      "step": 1461700
    },
    {
      "epoch": 473.8411669367909,
      "grad_norm": 1.295817255973816,
      "learning_rate": 2.63658773921505e-06,
      "loss": 2.4465,
      "step": 1461800
    },
    {
      "epoch": 473.8735818476499,
      "grad_norm": 1.2516584396362305,
      "learning_rate": 2.633344145313007e-06,
      "loss": 2.4485,
      "step": 1461900
    },
    {
      "epoch": 473.90599675850893,
      "grad_norm": 1.4463777542114258,
      "learning_rate": 2.6301005514109635e-06,
      "loss": 2.4635,
      "step": 1462000
    },
    {
      "epoch": 473.9384116693679,
      "grad_norm": 1.3477340936660767,
      "learning_rate": 2.62685695750892e-06,
      "loss": 2.475,
      "step": 1462100
    },
    {
      "epoch": 473.9708265802269,
      "grad_norm": 1.4347426891326904,
      "learning_rate": 2.6236133636068767e-06,
      "loss": 2.4684,
      "step": 1462200
    },
    {
      "epoch": 474.0,
      "eval_bleu": 1.2201085535748186,
      "eval_loss": 4.282684803009033,
      "eval_runtime": 4.4117,
      "eval_samples_per_second": 111.521,
      "eval_steps_per_second": 1.813,
      "step": 1462290
    },
    {
      "epoch": 474.0032414910859,
      "grad_norm": 1.6523431539535522,
      "learning_rate": 2.6203697697048333e-06,
      "loss": 2.4558,
      "step": 1462300
    },
    {
      "epoch": 474.0356564019449,
      "grad_norm": 1.6079732179641724,
      "learning_rate": 2.6171261758027895e-06,
      "loss": 2.4456,
      "step": 1462400
    },
    {
      "epoch": 474.0680713128039,
      "grad_norm": 1.4731520414352417,
      "learning_rate": 2.613882581900746e-06,
      "loss": 2.4514,
      "step": 1462500
    },
    {
      "epoch": 474.10048622366287,
      "grad_norm": 1.433871865272522,
      "learning_rate": 2.6106389879987028e-06,
      "loss": 2.4351,
      "step": 1462600
    },
    {
      "epoch": 474.1329011345219,
      "grad_norm": 1.4300943613052368,
      "learning_rate": 2.607395394096659e-06,
      "loss": 2.4551,
      "step": 1462700
    },
    {
      "epoch": 474.16531604538085,
      "grad_norm": 1.6274077892303467,
      "learning_rate": 2.604184236133636e-06,
      "loss": 2.4477,
      "step": 1462800
    },
    {
      "epoch": 474.19773095623987,
      "grad_norm": 1.4021313190460205,
      "learning_rate": 2.600940642231593e-06,
      "loss": 2.4494,
      "step": 1462900
    },
    {
      "epoch": 474.2301458670989,
      "grad_norm": 1.4939930438995361,
      "learning_rate": 2.597697048329549e-06,
      "loss": 2.4635,
      "step": 1463000
    },
    {
      "epoch": 474.26256077795784,
      "grad_norm": 1.3609671592712402,
      "learning_rate": 2.594453454427506e-06,
      "loss": 2.4513,
      "step": 1463100
    },
    {
      "epoch": 474.29497568881686,
      "grad_norm": 1.4176558256149292,
      "learning_rate": 2.5912098605254624e-06,
      "loss": 2.468,
      "step": 1463200
    },
    {
      "epoch": 474.3273905996758,
      "grad_norm": 1.6209044456481934,
      "learning_rate": 2.5879987025624394e-06,
      "loss": 2.4348,
      "step": 1463300
    },
    {
      "epoch": 474.35980551053484,
      "grad_norm": 1.2541885375976562,
      "learning_rate": 2.5847551086603956e-06,
      "loss": 2.4415,
      "step": 1463400
    },
    {
      "epoch": 474.39222042139386,
      "grad_norm": 1.9818222522735596,
      "learning_rate": 2.5815115147583522e-06,
      "loss": 2.4472,
      "step": 1463500
    },
    {
      "epoch": 474.4246353322528,
      "grad_norm": 1.5714380741119385,
      "learning_rate": 2.578267920856309e-06,
      "loss": 2.4476,
      "step": 1463600
    },
    {
      "epoch": 474.45705024311184,
      "grad_norm": 1.4848530292510986,
      "learning_rate": 2.5750243269542655e-06,
      "loss": 2.4496,
      "step": 1463700
    },
    {
      "epoch": 474.48946515397085,
      "grad_norm": 1.6608692407608032,
      "learning_rate": 2.571780733052222e-06,
      "loss": 2.4429,
      "step": 1463800
    },
    {
      "epoch": 474.5218800648298,
      "grad_norm": 1.4617221355438232,
      "learning_rate": 2.5685371391501787e-06,
      "loss": 2.4344,
      "step": 1463900
    },
    {
      "epoch": 474.55429497568883,
      "grad_norm": 1.6104856729507446,
      "learning_rate": 2.565293545248135e-06,
      "loss": 2.4492,
      "step": 1464000
    },
    {
      "epoch": 474.5867098865478,
      "grad_norm": 1.4076699018478394,
      "learning_rate": 2.5620499513460916e-06,
      "loss": 2.4745,
      "step": 1464100
    },
    {
      "epoch": 474.6191247974068,
      "grad_norm": 1.277665138244629,
      "learning_rate": 2.558806357444048e-06,
      "loss": 2.4532,
      "step": 1464200
    },
    {
      "epoch": 474.65153970826583,
      "grad_norm": 1.7979192733764648,
      "learning_rate": 2.5555627635420044e-06,
      "loss": 2.4349,
      "step": 1464300
    },
    {
      "epoch": 474.6839546191248,
      "grad_norm": 1.5350885391235352,
      "learning_rate": 2.5523191696399614e-06,
      "loss": 2.4535,
      "step": 1464400
    },
    {
      "epoch": 474.7163695299838,
      "grad_norm": 1.4030805826187134,
      "learning_rate": 2.5490755757379176e-06,
      "loss": 2.4453,
      "step": 1464500
    },
    {
      "epoch": 474.74878444084277,
      "grad_norm": 1.4690606594085693,
      "learning_rate": 2.5458319818358743e-06,
      "loss": 2.4566,
      "step": 1464600
    },
    {
      "epoch": 474.7811993517018,
      "grad_norm": 1.4734218120574951,
      "learning_rate": 2.542588387933831e-06,
      "loss": 2.4508,
      "step": 1464700
    },
    {
      "epoch": 474.8136142625608,
      "grad_norm": 1.455573558807373,
      "learning_rate": 2.5393447940317875e-06,
      "loss": 2.4512,
      "step": 1464800
    },
    {
      "epoch": 474.84602917341977,
      "grad_norm": 1.4297093152999878,
      "learning_rate": 2.5361012001297437e-06,
      "loss": 2.4798,
      "step": 1464900
    },
    {
      "epoch": 474.8784440842788,
      "grad_norm": 1.3695968389511108,
      "learning_rate": 2.5328576062277003e-06,
      "loss": 2.4451,
      "step": 1465000
    },
    {
      "epoch": 474.91085899513774,
      "grad_norm": 1.5263965129852295,
      "learning_rate": 2.529614012325657e-06,
      "loss": 2.4515,
      "step": 1465100
    },
    {
      "epoch": 474.94327390599676,
      "grad_norm": 1.4840316772460938,
      "learning_rate": 2.5263704184236136e-06,
      "loss": 2.4496,
      "step": 1465200
    },
    {
      "epoch": 474.9756888168558,
      "grad_norm": 1.554362177848816,
      "learning_rate": 2.52312682452157e-06,
      "loss": 2.4595,
      "step": 1465300
    },
    {
      "epoch": 475.0,
      "eval_bleu": 1.1434788087505265,
      "eval_loss": 4.284297943115234,
      "eval_runtime": 4.6534,
      "eval_samples_per_second": 105.73,
      "eval_steps_per_second": 1.719,
      "step": 1465375
    },
    {
      "epoch": 475.00810372771474,
      "grad_norm": 1.5746428966522217,
      "learning_rate": 2.5198832306195264e-06,
      "loss": 2.4719,
      "step": 1465400
    },
    {
      "epoch": 475.04051863857376,
      "grad_norm": 1.5004360675811768,
      "learning_rate": 2.516639636717483e-06,
      "loss": 2.4402,
      "step": 1465500
    },
    {
      "epoch": 475.0729335494327,
      "grad_norm": 1.4585868120193481,
      "learning_rate": 2.5133960428154397e-06,
      "loss": 2.4336,
      "step": 1465600
    },
    {
      "epoch": 475.10534846029174,
      "grad_norm": 1.3568719625473022,
      "learning_rate": 2.510152448913396e-06,
      "loss": 2.4395,
      "step": 1465700
    },
    {
      "epoch": 475.13776337115075,
      "grad_norm": 1.2869253158569336,
      "learning_rate": 2.506908855011353e-06,
      "loss": 2.4637,
      "step": 1465800
    },
    {
      "epoch": 475.1701782820097,
      "grad_norm": 1.5588350296020508,
      "learning_rate": 2.5036652611093095e-06,
      "loss": 2.4315,
      "step": 1465900
    },
    {
      "epoch": 475.20259319286873,
      "grad_norm": 1.3071255683898926,
      "learning_rate": 2.5004216672072657e-06,
      "loss": 2.4665,
      "step": 1466000
    },
    {
      "epoch": 475.2350081037277,
      "grad_norm": 1.5151307582855225,
      "learning_rate": 2.4971780733052224e-06,
      "loss": 2.47,
      "step": 1466100
    },
    {
      "epoch": 475.2674230145867,
      "grad_norm": 1.4919114112854004,
      "learning_rate": 2.493934479403179e-06,
      "loss": 2.4464,
      "step": 1466200
    },
    {
      "epoch": 475.29983792544573,
      "grad_norm": 1.5750768184661865,
      "learning_rate": 2.490690885501135e-06,
      "loss": 2.4412,
      "step": 1466300
    },
    {
      "epoch": 475.3322528363047,
      "grad_norm": 1.6360512971878052,
      "learning_rate": 2.487447291599092e-06,
      "loss": 2.4229,
      "step": 1466400
    },
    {
      "epoch": 475.3646677471637,
      "grad_norm": 1.555460810661316,
      "learning_rate": 2.4842036976970484e-06,
      "loss": 2.4513,
      "step": 1466500
    },
    {
      "epoch": 475.39708265802267,
      "grad_norm": 1.4827810525894165,
      "learning_rate": 2.480960103795005e-06,
      "loss": 2.4704,
      "step": 1466600
    },
    {
      "epoch": 475.4294975688817,
      "grad_norm": 1.3894362449645996,
      "learning_rate": 2.4777165098929617e-06,
      "loss": 2.4356,
      "step": 1466700
    },
    {
      "epoch": 475.4619124797407,
      "grad_norm": 1.301947832107544,
      "learning_rate": 2.474472915990918e-06,
      "loss": 2.4397,
      "step": 1466800
    },
    {
      "epoch": 475.49432739059966,
      "grad_norm": 1.5632351636886597,
      "learning_rate": 2.4712293220888745e-06,
      "loss": 2.4406,
      "step": 1466900
    },
    {
      "epoch": 475.5267423014587,
      "grad_norm": 1.565687656402588,
      "learning_rate": 2.467985728186831e-06,
      "loss": 2.4561,
      "step": 1467000
    },
    {
      "epoch": 475.55915721231764,
      "grad_norm": 1.6359310150146484,
      "learning_rate": 2.4647421342847873e-06,
      "loss": 2.4641,
      "step": 1467100
    },
    {
      "epoch": 475.59157212317666,
      "grad_norm": 1.4400088787078857,
      "learning_rate": 2.461498540382744e-06,
      "loss": 2.449,
      "step": 1467200
    },
    {
      "epoch": 475.6239870340357,
      "grad_norm": 1.4781994819641113,
      "learning_rate": 2.4582873824197213e-06,
      "loss": 2.4611,
      "step": 1467300
    },
    {
      "epoch": 475.65640194489464,
      "grad_norm": 1.3006867170333862,
      "learning_rate": 2.455043788517678e-06,
      "loss": 2.4487,
      "step": 1467400
    },
    {
      "epoch": 475.68881685575366,
      "grad_norm": 1.4350498914718628,
      "learning_rate": 2.451800194615634e-06,
      "loss": 2.4392,
      "step": 1467500
    },
    {
      "epoch": 475.7212317666126,
      "grad_norm": 1.2906248569488525,
      "learning_rate": 2.448556600713591e-06,
      "loss": 2.4546,
      "step": 1467600
    },
    {
      "epoch": 475.75364667747164,
      "grad_norm": 1.4595911502838135,
      "learning_rate": 2.4453130068115474e-06,
      "loss": 2.4677,
      "step": 1467700
    },
    {
      "epoch": 475.78606158833065,
      "grad_norm": 1.307265043258667,
      "learning_rate": 2.4420694129095036e-06,
      "loss": 2.4517,
      "step": 1467800
    },
    {
      "epoch": 475.8184764991896,
      "grad_norm": 1.4077520370483398,
      "learning_rate": 2.4388258190074603e-06,
      "loss": 2.4381,
      "step": 1467900
    },
    {
      "epoch": 475.85089141004863,
      "grad_norm": 1.6277186870574951,
      "learning_rate": 2.435582225105417e-06,
      "loss": 2.4729,
      "step": 1468000
    },
    {
      "epoch": 475.8833063209076,
      "grad_norm": 1.453804612159729,
      "learning_rate": 2.4323386312033735e-06,
      "loss": 2.4453,
      "step": 1468100
    },
    {
      "epoch": 475.9157212317666,
      "grad_norm": 1.5049229860305786,
      "learning_rate": 2.42909503730133e-06,
      "loss": 2.4482,
      "step": 1468200
    },
    {
      "epoch": 475.94813614262563,
      "grad_norm": 1.4971823692321777,
      "learning_rate": 2.4258514433992868e-06,
      "loss": 2.4624,
      "step": 1468300
    },
    {
      "epoch": 475.9805510534846,
      "grad_norm": 1.309075951576233,
      "learning_rate": 2.422607849497243e-06,
      "loss": 2.4556,
      "step": 1468400
    },
    {
      "epoch": 476.0,
      "eval_bleu": 1.1387675932736288,
      "eval_loss": 4.281497478485107,
      "eval_runtime": 4.3953,
      "eval_samples_per_second": 111.939,
      "eval_steps_per_second": 1.82,
      "step": 1468460
    },
    {
      "epoch": 476.0129659643436,
      "grad_norm": 1.4504029750823975,
      "learning_rate": 2.4193642555951996e-06,
      "loss": 2.4386,
      "step": 1468500
    },
    {
      "epoch": 476.04538087520257,
      "grad_norm": 1.288887619972229,
      "learning_rate": 2.416120661693156e-06,
      "loss": 2.4398,
      "step": 1468600
    },
    {
      "epoch": 476.0777957860616,
      "grad_norm": 1.5750969648361206,
      "learning_rate": 2.4128770677911124e-06,
      "loss": 2.4617,
      "step": 1468700
    },
    {
      "epoch": 476.1102106969206,
      "grad_norm": 1.3350199460983276,
      "learning_rate": 2.4096334738890695e-06,
      "loss": 2.4595,
      "step": 1468800
    },
    {
      "epoch": 476.14262560777956,
      "grad_norm": 1.500062346458435,
      "learning_rate": 2.4063898799870257e-06,
      "loss": 2.4308,
      "step": 1468900
    },
    {
      "epoch": 476.1750405186386,
      "grad_norm": 1.5790280103683472,
      "learning_rate": 2.4031462860849823e-06,
      "loss": 2.4449,
      "step": 1469000
    },
    {
      "epoch": 476.20745542949754,
      "grad_norm": 1.58790123462677,
      "learning_rate": 2.399902692182939e-06,
      "loss": 2.4499,
      "step": 1469100
    },
    {
      "epoch": 476.23987034035656,
      "grad_norm": 1.3921259641647339,
      "learning_rate": 2.396659098280895e-06,
      "loss": 2.4549,
      "step": 1469200
    },
    {
      "epoch": 476.2722852512156,
      "grad_norm": 1.4745962619781494,
      "learning_rate": 2.393447940317872e-06,
      "loss": 2.4504,
      "step": 1469300
    },
    {
      "epoch": 476.30470016207454,
      "grad_norm": 1.3498388528823853,
      "learning_rate": 2.3902043464158287e-06,
      "loss": 2.4445,
      "step": 1469400
    },
    {
      "epoch": 476.33711507293356,
      "grad_norm": 1.4489048719406128,
      "learning_rate": 2.3869607525137857e-06,
      "loss": 2.4705,
      "step": 1469500
    },
    {
      "epoch": 476.3695299837925,
      "grad_norm": 1.3560144901275635,
      "learning_rate": 2.383717158611742e-06,
      "loss": 2.4527,
      "step": 1469600
    },
    {
      "epoch": 476.40194489465154,
      "grad_norm": 1.2478643655776978,
      "learning_rate": 2.3804735647096986e-06,
      "loss": 2.4534,
      "step": 1469700
    },
    {
      "epoch": 476.43435980551055,
      "grad_norm": 1.411489725112915,
      "learning_rate": 2.377229970807655e-06,
      "loss": 2.4782,
      "step": 1469800
    },
    {
      "epoch": 476.4667747163695,
      "grad_norm": 1.5052233934402466,
      "learning_rate": 2.3739863769056114e-06,
      "loss": 2.4351,
      "step": 1469900
    },
    {
      "epoch": 476.49918962722853,
      "grad_norm": 1.5058302879333496,
      "learning_rate": 2.370742783003568e-06,
      "loss": 2.4321,
      "step": 1470000
    },
    {
      "epoch": 476.5316045380875,
      "grad_norm": 1.4746973514556885,
      "learning_rate": 2.3674991891015246e-06,
      "loss": 2.4466,
      "step": 1470100
    },
    {
      "epoch": 476.5640194489465,
      "grad_norm": 1.3393577337265015,
      "learning_rate": 2.3642555951994813e-06,
      "loss": 2.4733,
      "step": 1470200
    },
    {
      "epoch": 476.5964343598055,
      "grad_norm": 1.506164789199829,
      "learning_rate": 2.361012001297438e-06,
      "loss": 2.4607,
      "step": 1470300
    },
    {
      "epoch": 476.6288492706645,
      "grad_norm": 1.449846625328064,
      "learning_rate": 2.357768407395394e-06,
      "loss": 2.4645,
      "step": 1470400
    },
    {
      "epoch": 476.6612641815235,
      "grad_norm": 1.6246682405471802,
      "learning_rate": 2.3545248134933507e-06,
      "loss": 2.4654,
      "step": 1470500
    },
    {
      "epoch": 476.6936790923825,
      "grad_norm": 1.4701354503631592,
      "learning_rate": 2.3512812195913073e-06,
      "loss": 2.4563,
      "step": 1470600
    },
    {
      "epoch": 476.7260940032415,
      "grad_norm": 1.3605942726135254,
      "learning_rate": 2.3480376256892635e-06,
      "loss": 2.4477,
      "step": 1470700
    },
    {
      "epoch": 476.7585089141005,
      "grad_norm": 1.517672061920166,
      "learning_rate": 2.34479403178722e-06,
      "loss": 2.4515,
      "step": 1470800
    },
    {
      "epoch": 476.79092382495946,
      "grad_norm": 1.4567819833755493,
      "learning_rate": 2.341550437885177e-06,
      "loss": 2.4573,
      "step": 1470900
    },
    {
      "epoch": 476.8233387358185,
      "grad_norm": 1.3917618989944458,
      "learning_rate": 2.3383068439831334e-06,
      "loss": 2.4261,
      "step": 1471000
    },
    {
      "epoch": 476.8557536466775,
      "grad_norm": 1.621071219444275,
      "learning_rate": 2.33506325008109e-06,
      "loss": 2.4663,
      "step": 1471100
    },
    {
      "epoch": 476.88816855753646,
      "grad_norm": 1.4139220714569092,
      "learning_rate": 2.3318196561790467e-06,
      "loss": 2.4469,
      "step": 1471200
    },
    {
      "epoch": 476.9205834683955,
      "grad_norm": 1.709389090538025,
      "learning_rate": 2.3286084982160236e-06,
      "loss": 2.4227,
      "step": 1471300
    },
    {
      "epoch": 476.95299837925444,
      "grad_norm": 1.4533147811889648,
      "learning_rate": 2.32536490431398e-06,
      "loss": 2.4329,
      "step": 1471400
    },
    {
      "epoch": 476.98541329011346,
      "grad_norm": 1.5556972026824951,
      "learning_rate": 2.3221213104119365e-06,
      "loss": 2.4515,
      "step": 1471500
    },
    {
      "epoch": 477.0,
      "eval_bleu": 1.1614386032313841,
      "eval_loss": 4.283620834350586,
      "eval_runtime": 4.5751,
      "eval_samples_per_second": 107.538,
      "eval_steps_per_second": 1.749,
      "step": 1471545
    },
    {
      "epoch": 477.0178282009725,
      "grad_norm": 1.3484852313995361,
      "learning_rate": 2.318877716509893e-06,
      "loss": 2.4421,
      "step": 1471600
    },
    {
      "epoch": 477.05024311183143,
      "grad_norm": 1.6494122743606567,
      "learning_rate": 2.3156341226078497e-06,
      "loss": 2.4425,
      "step": 1471700
    },
    {
      "epoch": 477.08265802269045,
      "grad_norm": 1.3785685300827026,
      "learning_rate": 2.3123905287058063e-06,
      "loss": 2.4471,
      "step": 1471800
    },
    {
      "epoch": 477.1150729335494,
      "grad_norm": 1.6028878688812256,
      "learning_rate": 2.309146934803763e-06,
      "loss": 2.456,
      "step": 1471900
    },
    {
      "epoch": 477.14748784440843,
      "grad_norm": 1.5090022087097168,
      "learning_rate": 2.305903340901719e-06,
      "loss": 2.4533,
      "step": 1472000
    },
    {
      "epoch": 477.17990275526745,
      "grad_norm": 1.6077325344085693,
      "learning_rate": 2.3026597469996758e-06,
      "loss": 2.4366,
      "step": 1472100
    },
    {
      "epoch": 477.2123176661264,
      "grad_norm": 1.4465068578720093,
      "learning_rate": 2.2994161530976324e-06,
      "loss": 2.4338,
      "step": 1472200
    },
    {
      "epoch": 477.2447325769854,
      "grad_norm": 1.3546944856643677,
      "learning_rate": 2.2961725591955886e-06,
      "loss": 2.4997,
      "step": 1472300
    },
    {
      "epoch": 477.2771474878444,
      "grad_norm": 1.4732329845428467,
      "learning_rate": 2.2929289652935452e-06,
      "loss": 2.4594,
      "step": 1472400
    },
    {
      "epoch": 477.3095623987034,
      "grad_norm": 1.3320834636688232,
      "learning_rate": 2.289685371391502e-06,
      "loss": 2.4311,
      "step": 1472500
    },
    {
      "epoch": 477.3419773095624,
      "grad_norm": 1.4437435865402222,
      "learning_rate": 2.2864417774894585e-06,
      "loss": 2.4399,
      "step": 1472600
    },
    {
      "epoch": 477.3743922204214,
      "grad_norm": 1.351324200630188,
      "learning_rate": 2.283198183587415e-06,
      "loss": 2.4632,
      "step": 1472700
    },
    {
      "epoch": 477.4068071312804,
      "grad_norm": 1.4533120393753052,
      "learning_rate": 2.2799545896853713e-06,
      "loss": 2.4312,
      "step": 1472800
    },
    {
      "epoch": 477.43922204213936,
      "grad_norm": 1.656408429145813,
      "learning_rate": 2.276710995783328e-06,
      "loss": 2.4651,
      "step": 1472900
    },
    {
      "epoch": 477.4716369529984,
      "grad_norm": 1.4216291904449463,
      "learning_rate": 2.2734674018812846e-06,
      "loss": 2.4293,
      "step": 1473000
    },
    {
      "epoch": 477.5040518638574,
      "grad_norm": 1.3366141319274902,
      "learning_rate": 2.2702238079792408e-06,
      "loss": 2.4302,
      "step": 1473100
    },
    {
      "epoch": 477.53646677471636,
      "grad_norm": 1.5013747215270996,
      "learning_rate": 2.266980214077198e-06,
      "loss": 2.4581,
      "step": 1473200
    },
    {
      "epoch": 477.5688816855754,
      "grad_norm": 1.4596232175827026,
      "learning_rate": 2.2637690561141748e-06,
      "loss": 2.457,
      "step": 1473300
    },
    {
      "epoch": 477.60129659643434,
      "grad_norm": 1.5500967502593994,
      "learning_rate": 2.2605254622121314e-06,
      "loss": 2.4606,
      "step": 1473400
    },
    {
      "epoch": 477.63371150729336,
      "grad_norm": 1.7660634517669678,
      "learning_rate": 2.2572818683100876e-06,
      "loss": 2.4523,
      "step": 1473500
    },
    {
      "epoch": 477.6661264181524,
      "grad_norm": 1.356963872909546,
      "learning_rate": 2.2540382744080442e-06,
      "loss": 2.4623,
      "step": 1473600
    },
    {
      "epoch": 477.69854132901133,
      "grad_norm": 1.3568720817565918,
      "learning_rate": 2.250794680506001e-06,
      "loss": 2.4461,
      "step": 1473700
    },
    {
      "epoch": 477.73095623987035,
      "grad_norm": 1.3790193796157837,
      "learning_rate": 2.247551086603957e-06,
      "loss": 2.4375,
      "step": 1473800
    },
    {
      "epoch": 477.7633711507293,
      "grad_norm": 1.460485577583313,
      "learning_rate": 2.244307492701914e-06,
      "loss": 2.4482,
      "step": 1473900
    },
    {
      "epoch": 477.79578606158833,
      "grad_norm": 1.4273797273635864,
      "learning_rate": 2.2410638987998703e-06,
      "loss": 2.4595,
      "step": 1474000
    },
    {
      "epoch": 477.82820097244735,
      "grad_norm": 1.8365637063980103,
      "learning_rate": 2.237820304897827e-06,
      "loss": 2.4393,
      "step": 1474100
    },
    {
      "epoch": 477.8606158833063,
      "grad_norm": 1.3493306636810303,
      "learning_rate": 2.2345767109957835e-06,
      "loss": 2.4614,
      "step": 1474200
    },
    {
      "epoch": 477.8930307941653,
      "grad_norm": 1.358997106552124,
      "learning_rate": 2.2313331170937397e-06,
      "loss": 2.4591,
      "step": 1474300
    },
    {
      "epoch": 477.9254457050243,
      "grad_norm": 1.3424431085586548,
      "learning_rate": 2.2280895231916964e-06,
      "loss": 2.4674,
      "step": 1474400
    },
    {
      "epoch": 477.9578606158833,
      "grad_norm": 1.6050481796264648,
      "learning_rate": 2.224845929289653e-06,
      "loss": 2.4405,
      "step": 1474500
    },
    {
      "epoch": 477.9902755267423,
      "grad_norm": 1.256545901298523,
      "learning_rate": 2.221602335387609e-06,
      "loss": 2.4513,
      "step": 1474600
    },
    {
      "epoch": 478.0,
      "eval_bleu": 1.156291358016565,
      "eval_loss": 4.283794403076172,
      "eval_runtime": 4.2906,
      "eval_samples_per_second": 114.67,
      "eval_steps_per_second": 1.865,
      "step": 1474630
    },
    {
      "epoch": 478.0226904376013,
      "grad_norm": 1.3981726169586182,
      "learning_rate": 2.2183587414855662e-06,
      "loss": 2.4555,
      "step": 1474700
    },
    {
      "epoch": 478.0551053484603,
      "grad_norm": 1.5471383333206177,
      "learning_rate": 2.215115147583523e-06,
      "loss": 2.4655,
      "step": 1474800
    },
    {
      "epoch": 478.08752025931926,
      "grad_norm": 1.856418490409851,
      "learning_rate": 2.211871553681479e-06,
      "loss": 2.4657,
      "step": 1474900
    },
    {
      "epoch": 478.1199351701783,
      "grad_norm": 1.454755187034607,
      "learning_rate": 2.2086279597794357e-06,
      "loss": 2.4187,
      "step": 1475000
    },
    {
      "epoch": 478.1523500810373,
      "grad_norm": 1.7124015092849731,
      "learning_rate": 2.2053843658773923e-06,
      "loss": 2.4407,
      "step": 1475100
    },
    {
      "epoch": 478.18476499189626,
      "grad_norm": 1.4007014036178589,
      "learning_rate": 2.2021407719753485e-06,
      "loss": 2.4642,
      "step": 1475200
    },
    {
      "epoch": 478.2171799027553,
      "grad_norm": 1.586297631263733,
      "learning_rate": 2.1989296140123255e-06,
      "loss": 2.469,
      "step": 1475300
    },
    {
      "epoch": 478.24959481361424,
      "grad_norm": 1.8394356966018677,
      "learning_rate": 2.1957184560493024e-06,
      "loss": 2.436,
      "step": 1475400
    },
    {
      "epoch": 478.28200972447326,
      "grad_norm": 1.5326316356658936,
      "learning_rate": 2.1924748621472595e-06,
      "loss": 2.436,
      "step": 1475500
    },
    {
      "epoch": 478.3144246353323,
      "grad_norm": 1.324573278427124,
      "learning_rate": 2.1892312682452157e-06,
      "loss": 2.434,
      "step": 1475600
    },
    {
      "epoch": 478.34683954619123,
      "grad_norm": 1.2989501953125,
      "learning_rate": 2.1859876743431723e-06,
      "loss": 2.4378,
      "step": 1475700
    },
    {
      "epoch": 478.37925445705025,
      "grad_norm": 1.6782469749450684,
      "learning_rate": 2.182744080441129e-06,
      "loss": 2.4434,
      "step": 1475800
    },
    {
      "epoch": 478.4116693679092,
      "grad_norm": 1.3460571765899658,
      "learning_rate": 2.1795004865390856e-06,
      "loss": 2.4535,
      "step": 1475900
    },
    {
      "epoch": 478.44408427876823,
      "grad_norm": 1.3189157247543335,
      "learning_rate": 2.1762568926370418e-06,
      "loss": 2.4653,
      "step": 1476000
    },
    {
      "epoch": 478.47649918962725,
      "grad_norm": 1.5538313388824463,
      "learning_rate": 2.1730132987349984e-06,
      "loss": 2.4431,
      "step": 1476100
    },
    {
      "epoch": 478.5089141004862,
      "grad_norm": 1.579728603363037,
      "learning_rate": 2.169769704832955e-06,
      "loss": 2.4541,
      "step": 1476200
    },
    {
      "epoch": 478.5413290113452,
      "grad_norm": 1.489071249961853,
      "learning_rate": 2.1665261109309116e-06,
      "loss": 2.4461,
      "step": 1476300
    },
    {
      "epoch": 478.5737439222042,
      "grad_norm": 1.4793756008148193,
      "learning_rate": 2.1633149529678886e-06,
      "loss": 2.4607,
      "step": 1476400
    },
    {
      "epoch": 478.6061588330632,
      "grad_norm": 1.5909769535064697,
      "learning_rate": 2.1600713590658452e-06,
      "loss": 2.4515,
      "step": 1476500
    },
    {
      "epoch": 478.6385737439222,
      "grad_norm": 1.5632054805755615,
      "learning_rate": 2.1568277651638014e-06,
      "loss": 2.4377,
      "step": 1476600
    },
    {
      "epoch": 478.6709886547812,
      "grad_norm": 1.3925197124481201,
      "learning_rate": 2.153584171261758e-06,
      "loss": 2.4129,
      "step": 1476700
    },
    {
      "epoch": 478.7034035656402,
      "grad_norm": 1.5877900123596191,
      "learning_rate": 2.1503405773597147e-06,
      "loss": 2.4582,
      "step": 1476800
    },
    {
      "epoch": 478.73581847649916,
      "grad_norm": 1.406538963317871,
      "learning_rate": 2.147096983457671e-06,
      "loss": 2.4572,
      "step": 1476900
    },
    {
      "epoch": 478.7682333873582,
      "grad_norm": 1.3184734582901,
      "learning_rate": 2.143853389555628e-06,
      "loss": 2.4628,
      "step": 1477000
    },
    {
      "epoch": 478.8006482982172,
      "grad_norm": 1.3223893642425537,
      "learning_rate": 2.1406097956535845e-06,
      "loss": 2.4533,
      "step": 1477100
    },
    {
      "epoch": 478.83306320907616,
      "grad_norm": 1.4256237745285034,
      "learning_rate": 2.1373662017515407e-06,
      "loss": 2.4614,
      "step": 1477200
    },
    {
      "epoch": 478.8654781199352,
      "grad_norm": 1.3944088220596313,
      "learning_rate": 2.1341226078494974e-06,
      "loss": 2.4581,
      "step": 1477300
    },
    {
      "epoch": 478.8978930307942,
      "grad_norm": 1.3942021131515503,
      "learning_rate": 2.130879013947454e-06,
      "loss": 2.4495,
      "step": 1477400
    },
    {
      "epoch": 478.93030794165315,
      "grad_norm": 1.5790501832962036,
      "learning_rate": 2.12763542004541e-06,
      "loss": 2.4565,
      "step": 1477500
    },
    {
      "epoch": 478.9627228525122,
      "grad_norm": 1.4194284677505493,
      "learning_rate": 2.124391826143367e-06,
      "loss": 2.4739,
      "step": 1477600
    },
    {
      "epoch": 478.99513776337113,
      "grad_norm": 1.4896212816238403,
      "learning_rate": 2.1211482322413234e-06,
      "loss": 2.4403,
      "step": 1477700
    },
    {
      "epoch": 479.0,
      "eval_bleu": 1.1248649926816041,
      "eval_loss": 4.284430027008057,
      "eval_runtime": 4.5148,
      "eval_samples_per_second": 108.976,
      "eval_steps_per_second": 1.772,
      "step": 1477715
    },
    {
      "epoch": 479.02755267423015,
      "grad_norm": 1.4861962795257568,
      "learning_rate": 2.11790463833928e-06,
      "loss": 2.4562,
      "step": 1477800
    },
    {
      "epoch": 479.05996758508917,
      "grad_norm": 1.36944580078125,
      "learning_rate": 2.1146610444372367e-06,
      "loss": 2.4431,
      "step": 1477900
    },
    {
      "epoch": 479.09238249594813,
      "grad_norm": 1.485560655593872,
      "learning_rate": 2.111417450535193e-06,
      "loss": 2.4579,
      "step": 1478000
    },
    {
      "epoch": 479.12479740680715,
      "grad_norm": 1.5324801206588745,
      "learning_rate": 2.1081738566331495e-06,
      "loss": 2.4402,
      "step": 1478100
    },
    {
      "epoch": 479.1572123176661,
      "grad_norm": 1.3310095071792603,
      "learning_rate": 2.104930262731106e-06,
      "loss": 2.4672,
      "step": 1478200
    },
    {
      "epoch": 479.1896272285251,
      "grad_norm": 1.3801692724227905,
      "learning_rate": 2.1016866688290624e-06,
      "loss": 2.4664,
      "step": 1478300
    },
    {
      "epoch": 479.22204213938414,
      "grad_norm": 1.3687491416931152,
      "learning_rate": 2.0984430749270194e-06,
      "loss": 2.4531,
      "step": 1478400
    },
    {
      "epoch": 479.2544570502431,
      "grad_norm": 1.5142368078231812,
      "learning_rate": 2.095199481024976e-06,
      "loss": 2.4415,
      "step": 1478500
    },
    {
      "epoch": 479.2868719611021,
      "grad_norm": 1.7424712181091309,
      "learning_rate": 2.0919558871229322e-06,
      "loss": 2.4476,
      "step": 1478600
    },
    {
      "epoch": 479.3192868719611,
      "grad_norm": 1.463572382926941,
      "learning_rate": 2.088712293220889e-06,
      "loss": 2.4543,
      "step": 1478700
    },
    {
      "epoch": 479.3517017828201,
      "grad_norm": 1.6627883911132812,
      "learning_rate": 2.0854686993188455e-06,
      "loss": 2.4642,
      "step": 1478800
    },
    {
      "epoch": 479.3841166936791,
      "grad_norm": 1.2827715873718262,
      "learning_rate": 2.0822251054168017e-06,
      "loss": 2.4435,
      "step": 1478900
    },
    {
      "epoch": 479.4165316045381,
      "grad_norm": 1.4413648843765259,
      "learning_rate": 2.0789815115147583e-06,
      "loss": 2.4267,
      "step": 1479000
    },
    {
      "epoch": 479.4489465153971,
      "grad_norm": 1.4086648225784302,
      "learning_rate": 2.0757379176127154e-06,
      "loss": 2.4653,
      "step": 1479100
    },
    {
      "epoch": 479.48136142625606,
      "grad_norm": 1.451735019683838,
      "learning_rate": 2.0724943237106716e-06,
      "loss": 2.4608,
      "step": 1479200
    },
    {
      "epoch": 479.5137763371151,
      "grad_norm": 1.4536257982254028,
      "learning_rate": 2.069250729808628e-06,
      "loss": 2.4425,
      "step": 1479300
    },
    {
      "epoch": 479.5461912479741,
      "grad_norm": 1.4917017221450806,
      "learning_rate": 2.066007135906585e-06,
      "loss": 2.448,
      "step": 1479400
    },
    {
      "epoch": 479.57860615883305,
      "grad_norm": 1.5934724807739258,
      "learning_rate": 2.062763542004541e-06,
      "loss": 2.4415,
      "step": 1479500
    },
    {
      "epoch": 479.6110210696921,
      "grad_norm": 1.415900707244873,
      "learning_rate": 2.0595199481024976e-06,
      "loss": 2.4606,
      "step": 1479600
    },
    {
      "epoch": 479.64343598055103,
      "grad_norm": 1.5603792667388916,
      "learning_rate": 2.0562763542004543e-06,
      "loss": 2.4486,
      "step": 1479700
    },
    {
      "epoch": 479.67585089141005,
      "grad_norm": 1.498226523399353,
      "learning_rate": 2.053032760298411e-06,
      "loss": 2.4541,
      "step": 1479800
    },
    {
      "epoch": 479.70826580226907,
      "grad_norm": 1.4219064712524414,
      "learning_rate": 2.0497891663963675e-06,
      "loss": 2.4204,
      "step": 1479900
    },
    {
      "epoch": 479.74068071312803,
      "grad_norm": 1.338982343673706,
      "learning_rate": 2.0465455724943237e-06,
      "loss": 2.4553,
      "step": 1480000
    },
    {
      "epoch": 479.77309562398705,
      "grad_norm": 1.4537627696990967,
      "learning_rate": 2.0433019785922803e-06,
      "loss": 2.452,
      "step": 1480100
    },
    {
      "epoch": 479.805510534846,
      "grad_norm": 1.2896000146865845,
      "learning_rate": 2.040058384690237e-06,
      "loss": 2.4736,
      "step": 1480200
    },
    {
      "epoch": 479.837925445705,
      "grad_norm": 1.4183919429779053,
      "learning_rate": 2.036814790788193e-06,
      "loss": 2.4601,
      "step": 1480300
    },
    {
      "epoch": 479.87034035656404,
      "grad_norm": 1.6095911264419556,
      "learning_rate": 2.03357119688615e-06,
      "loss": 2.4391,
      "step": 1480400
    },
    {
      "epoch": 479.902755267423,
      "grad_norm": 1.3443113565444946,
      "learning_rate": 2.0303276029841064e-06,
      "loss": 2.4642,
      "step": 1480500
    },
    {
      "epoch": 479.935170178282,
      "grad_norm": 1.214004635810852,
      "learning_rate": 2.027084009082063e-06,
      "loss": 2.4266,
      "step": 1480600
    },
    {
      "epoch": 479.967585089141,
      "grad_norm": 1.4544471502304077,
      "learning_rate": 2.0238404151800197e-06,
      "loss": 2.4414,
      "step": 1480700
    },
    {
      "epoch": 480.0,
      "grad_norm": 1.4496655464172363,
      "learning_rate": 2.0205968212779763e-06,
      "loss": 2.4469,
      "step": 1480800
    },
    {
      "epoch": 480.0,
      "eval_bleu": 1.2229254685552362,
      "eval_loss": 4.285191535949707,
      "eval_runtime": 4.2594,
      "eval_samples_per_second": 115.51,
      "eval_steps_per_second": 1.878,
      "step": 1480800
    },
    {
      "epoch": 480.032414910859,
      "grad_norm": 1.7161144018173218,
      "learning_rate": 2.0173532273759325e-06,
      "loss": 2.4432,
      "step": 1480900
    },
    {
      "epoch": 480.064829821718,
      "grad_norm": 1.308907151222229,
      "learning_rate": 2.014109633473889e-06,
      "loss": 2.4407,
      "step": 1481000
    },
    {
      "epoch": 480.097244732577,
      "grad_norm": 1.577595829963684,
      "learning_rate": 2.0108660395718457e-06,
      "loss": 2.455,
      "step": 1481100
    },
    {
      "epoch": 480.12965964343596,
      "grad_norm": 1.2806817293167114,
      "learning_rate": 2.0076548816088227e-06,
      "loss": 2.4636,
      "step": 1481200
    },
    {
      "epoch": 480.162074554295,
      "grad_norm": 1.5124441385269165,
      "learning_rate": 2.0044112877067793e-06,
      "loss": 2.4401,
      "step": 1481300
    },
    {
      "epoch": 480.194489465154,
      "grad_norm": 1.3326854705810547,
      "learning_rate": 2.001167693804736e-06,
      "loss": 2.4304,
      "step": 1481400
    },
    {
      "epoch": 480.22690437601295,
      "grad_norm": 1.3155606985092163,
      "learning_rate": 1.997924099902692e-06,
      "loss": 2.4508,
      "step": 1481500
    },
    {
      "epoch": 480.25931928687197,
      "grad_norm": 1.4201246500015259,
      "learning_rate": 1.9946805060006488e-06,
      "loss": 2.4616,
      "step": 1481600
    },
    {
      "epoch": 480.29173419773093,
      "grad_norm": 1.5162007808685303,
      "learning_rate": 1.9914369120986054e-06,
      "loss": 2.4676,
      "step": 1481700
    },
    {
      "epoch": 480.32414910858995,
      "grad_norm": 1.8056155443191528,
      "learning_rate": 1.9881933181965616e-06,
      "loss": 2.4375,
      "step": 1481800
    },
    {
      "epoch": 480.35656401944897,
      "grad_norm": 1.4619066715240479,
      "learning_rate": 1.9849497242945182e-06,
      "loss": 2.4562,
      "step": 1481900
    },
    {
      "epoch": 480.3889789303079,
      "grad_norm": 1.4073511362075806,
      "learning_rate": 1.9817061303924753e-06,
      "loss": 2.4305,
      "step": 1482000
    },
    {
      "epoch": 480.42139384116695,
      "grad_norm": 1.4579951763153076,
      "learning_rate": 1.9784625364904315e-06,
      "loss": 2.4666,
      "step": 1482100
    },
    {
      "epoch": 480.4538087520259,
      "grad_norm": 1.3996628522872925,
      "learning_rate": 1.975218942588388e-06,
      "loss": 2.4546,
      "step": 1482200
    },
    {
      "epoch": 480.4862236628849,
      "grad_norm": 1.3400371074676514,
      "learning_rate": 1.9719753486863447e-06,
      "loss": 2.4545,
      "step": 1482300
    },
    {
      "epoch": 480.51863857374394,
      "grad_norm": 1.3492783308029175,
      "learning_rate": 1.968731754784301e-06,
      "loss": 2.4274,
      "step": 1482400
    },
    {
      "epoch": 480.5510534846029,
      "grad_norm": 1.3342742919921875,
      "learning_rate": 1.9654881608822576e-06,
      "loss": 2.4523,
      "step": 1482500
    },
    {
      "epoch": 480.5834683954619,
      "grad_norm": 1.50938880443573,
      "learning_rate": 1.962244566980214e-06,
      "loss": 2.4761,
      "step": 1482600
    },
    {
      "epoch": 480.6158833063209,
      "grad_norm": 1.560255527496338,
      "learning_rate": 1.9590009730781704e-06,
      "loss": 2.4446,
      "step": 1482700
    },
    {
      "epoch": 480.6482982171799,
      "grad_norm": 1.408066987991333,
      "learning_rate": 1.9557573791761274e-06,
      "loss": 2.4555,
      "step": 1482800
    },
    {
      "epoch": 480.6807131280389,
      "grad_norm": 1.3443281650543213,
      "learning_rate": 1.952513785274084e-06,
      "loss": 2.4579,
      "step": 1482900
    },
    {
      "epoch": 480.7131280388979,
      "grad_norm": 1.7596282958984375,
      "learning_rate": 1.9492701913720403e-06,
      "loss": 2.4752,
      "step": 1483000
    },
    {
      "epoch": 480.7455429497569,
      "grad_norm": 1.355747103691101,
      "learning_rate": 1.946026597469997e-06,
      "loss": 2.4457,
      "step": 1483100
    },
    {
      "epoch": 480.77795786061586,
      "grad_norm": 1.4545525312423706,
      "learning_rate": 1.9427830035679535e-06,
      "loss": 2.4624,
      "step": 1483200
    },
    {
      "epoch": 480.8103727714749,
      "grad_norm": 1.3305503129959106,
      "learning_rate": 1.9395394096659097e-06,
      "loss": 2.4193,
      "step": 1483300
    },
    {
      "epoch": 480.8427876823339,
      "grad_norm": 1.4506266117095947,
      "learning_rate": 1.9362958157638663e-06,
      "loss": 2.4532,
      "step": 1483400
    },
    {
      "epoch": 480.87520259319285,
      "grad_norm": 1.3629406690597534,
      "learning_rate": 1.933052221861823e-06,
      "loss": 2.4411,
      "step": 1483500
    },
    {
      "epoch": 480.90761750405187,
      "grad_norm": 1.22833251953125,
      "learning_rate": 1.9298086279597796e-06,
      "loss": 2.4531,
      "step": 1483600
    },
    {
      "epoch": 480.94003241491083,
      "grad_norm": 1.388132095336914,
      "learning_rate": 1.9265650340577362e-06,
      "loss": 2.4601,
      "step": 1483700
    },
    {
      "epoch": 480.97244732576985,
      "grad_norm": 1.4092953205108643,
      "learning_rate": 1.9233214401556924e-06,
      "loss": 2.435,
      "step": 1483800
    },
    {
      "epoch": 481.0,
      "eval_bleu": 1.2371332822777856,
      "eval_loss": 4.284897327423096,
      "eval_runtime": 4.2529,
      "eval_samples_per_second": 115.686,
      "eval_steps_per_second": 1.881,
      "step": 1483885
    },
    {
      "epoch": 481.00486223662887,
      "grad_norm": 1.476477026939392,
      "learning_rate": 1.920077846253649e-06,
      "loss": 2.4631,
      "step": 1483900
    },
    {
      "epoch": 481.0372771474878,
      "grad_norm": 1.461338758468628,
      "learning_rate": 1.9168342523516057e-06,
      "loss": 2.4524,
      "step": 1484000
    },
    {
      "epoch": 481.06969205834685,
      "grad_norm": 1.6603469848632812,
      "learning_rate": 1.913590658449562e-06,
      "loss": 2.4448,
      "step": 1484100
    },
    {
      "epoch": 481.1021069692058,
      "grad_norm": 1.4961953163146973,
      "learning_rate": 1.910347064547519e-06,
      "loss": 2.4639,
      "step": 1484200
    },
    {
      "epoch": 481.1345218800648,
      "grad_norm": 1.7660441398620605,
      "learning_rate": 1.9071034706454753e-06,
      "loss": 2.406,
      "step": 1484300
    },
    {
      "epoch": 481.16693679092384,
      "grad_norm": 1.3339498043060303,
      "learning_rate": 1.903859876743432e-06,
      "loss": 2.4671,
      "step": 1484400
    },
    {
      "epoch": 481.1993517017828,
      "grad_norm": 1.4988640546798706,
      "learning_rate": 1.900648718780409e-06,
      "loss": 2.4548,
      "step": 1484500
    },
    {
      "epoch": 481.2317666126418,
      "grad_norm": 1.4475297927856445,
      "learning_rate": 1.8974051248783653e-06,
      "loss": 2.4458,
      "step": 1484600
    },
    {
      "epoch": 481.26418152350084,
      "grad_norm": 1.4059666395187378,
      "learning_rate": 1.8941615309763217e-06,
      "loss": 2.4594,
      "step": 1484700
    },
    {
      "epoch": 481.2965964343598,
      "grad_norm": 1.6600617170333862,
      "learning_rate": 1.8909179370742784e-06,
      "loss": 2.4496,
      "step": 1484800
    },
    {
      "epoch": 481.3290113452188,
      "grad_norm": 1.182424545288086,
      "learning_rate": 1.8876743431722348e-06,
      "loss": 2.4684,
      "step": 1484900
    },
    {
      "epoch": 481.3614262560778,
      "grad_norm": 1.3722578287124634,
      "learning_rate": 1.8844307492701916e-06,
      "loss": 2.4504,
      "step": 1485000
    },
    {
      "epoch": 481.3938411669368,
      "grad_norm": 1.3544936180114746,
      "learning_rate": 1.881187155368148e-06,
      "loss": 2.434,
      "step": 1485100
    },
    {
      "epoch": 481.4262560777958,
      "grad_norm": 1.4092437028884888,
      "learning_rate": 1.8779435614661046e-06,
      "loss": 2.4486,
      "step": 1485200
    },
    {
      "epoch": 481.4586709886548,
      "grad_norm": 1.293447494506836,
      "learning_rate": 1.874699967564061e-06,
      "loss": 2.4424,
      "step": 1485300
    },
    {
      "epoch": 481.4910858995138,
      "grad_norm": 1.3434237241744995,
      "learning_rate": 1.8714563736620175e-06,
      "loss": 2.4374,
      "step": 1485400
    },
    {
      "epoch": 481.52350081037275,
      "grad_norm": 1.4981937408447266,
      "learning_rate": 1.868212779759974e-06,
      "loss": 2.4449,
      "step": 1485500
    },
    {
      "epoch": 481.55591572123177,
      "grad_norm": 1.4379489421844482,
      "learning_rate": 1.8649691858579305e-06,
      "loss": 2.4452,
      "step": 1485600
    },
    {
      "epoch": 481.5883306320908,
      "grad_norm": 1.46268892288208,
      "learning_rate": 1.8617255919558874e-06,
      "loss": 2.4307,
      "step": 1485700
    },
    {
      "epoch": 481.62074554294975,
      "grad_norm": 1.4356058835983276,
      "learning_rate": 1.8584819980538438e-06,
      "loss": 2.4476,
      "step": 1485800
    },
    {
      "epoch": 481.65316045380877,
      "grad_norm": 1.318074107170105,
      "learning_rate": 1.8552384041518004e-06,
      "loss": 2.448,
      "step": 1485900
    },
    {
      "epoch": 481.6855753646677,
      "grad_norm": 1.228510856628418,
      "learning_rate": 1.8519948102497568e-06,
      "loss": 2.465,
      "step": 1486000
    },
    {
      "epoch": 481.71799027552674,
      "grad_norm": 1.5978436470031738,
      "learning_rate": 1.8487512163477132e-06,
      "loss": 2.4588,
      "step": 1486100
    },
    {
      "epoch": 481.75040518638576,
      "grad_norm": 1.3640578985214233,
      "learning_rate": 1.8455076224456698e-06,
      "loss": 2.4508,
      "step": 1486200
    },
    {
      "epoch": 481.7828200972447,
      "grad_norm": 1.6143287420272827,
      "learning_rate": 1.8422640285436263e-06,
      "loss": 2.46,
      "step": 1486300
    },
    {
      "epoch": 481.81523500810374,
      "grad_norm": 1.612278699874878,
      "learning_rate": 1.839020434641583e-06,
      "loss": 2.4752,
      "step": 1486400
    },
    {
      "epoch": 481.8476499189627,
      "grad_norm": 1.423252820968628,
      "learning_rate": 1.8357768407395395e-06,
      "loss": 2.4582,
      "step": 1486500
    },
    {
      "epoch": 481.8800648298217,
      "grad_norm": 1.422995686531067,
      "learning_rate": 1.8325332468374961e-06,
      "loss": 2.4344,
      "step": 1486600
    },
    {
      "epoch": 481.91247974068074,
      "grad_norm": 1.5242509841918945,
      "learning_rate": 1.8292896529354525e-06,
      "loss": 2.4564,
      "step": 1486700
    },
    {
      "epoch": 481.9448946515397,
      "grad_norm": 1.4369356632232666,
      "learning_rate": 1.8260460590334092e-06,
      "loss": 2.4415,
      "step": 1486800
    },
    {
      "epoch": 481.9773095623987,
      "grad_norm": 1.5144541263580322,
      "learning_rate": 1.8228024651313656e-06,
      "loss": 2.4632,
      "step": 1486900
    },
    {
      "epoch": 482.0,
      "eval_bleu": 1.160948230391436,
      "eval_loss": 4.286004543304443,
      "eval_runtime": 4.1014,
      "eval_samples_per_second": 119.958,
      "eval_steps_per_second": 1.951,
      "step": 1486970
    },
    {
      "epoch": 482.0097244732577,
      "grad_norm": 1.4017940759658813,
      "learning_rate": 1.819558871229322e-06,
      "loss": 2.4292,
      "step": 1487000
    },
    {
      "epoch": 482.0421393841167,
      "grad_norm": 1.6266523599624634,
      "learning_rate": 1.8163152773272788e-06,
      "loss": 2.4585,
      "step": 1487100
    },
    {
      "epoch": 482.0745542949757,
      "grad_norm": 1.5084657669067383,
      "learning_rate": 1.8130716834252355e-06,
      "loss": 2.4522,
      "step": 1487200
    },
    {
      "epoch": 482.1069692058347,
      "grad_norm": 1.4167726039886475,
      "learning_rate": 1.8098280895231919e-06,
      "loss": 2.4475,
      "step": 1487300
    },
    {
      "epoch": 482.1393841166937,
      "grad_norm": 1.3276686668395996,
      "learning_rate": 1.8065844956211483e-06,
      "loss": 2.4315,
      "step": 1487400
    },
    {
      "epoch": 482.17179902755265,
      "grad_norm": 1.621691346168518,
      "learning_rate": 1.803340901719105e-06,
      "loss": 2.4694,
      "step": 1487500
    },
    {
      "epoch": 482.20421393841167,
      "grad_norm": 1.5704753398895264,
      "learning_rate": 1.8000973078170613e-06,
      "loss": 2.4383,
      "step": 1487600
    },
    {
      "epoch": 482.2366288492707,
      "grad_norm": 1.3821758031845093,
      "learning_rate": 1.7968537139150177e-06,
      "loss": 2.4508,
      "step": 1487700
    },
    {
      "epoch": 482.26904376012965,
      "grad_norm": 1.7564970254898071,
      "learning_rate": 1.7936101200129744e-06,
      "loss": 2.4555,
      "step": 1487800
    },
    {
      "epoch": 482.30145867098867,
      "grad_norm": 1.4698472023010254,
      "learning_rate": 1.7903665261109312e-06,
      "loss": 2.4682,
      "step": 1487900
    },
    {
      "epoch": 482.3338735818476,
      "grad_norm": 1.436037302017212,
      "learning_rate": 1.7871229322088876e-06,
      "loss": 2.4609,
      "step": 1488000
    },
    {
      "epoch": 482.36628849270664,
      "grad_norm": 1.3756163120269775,
      "learning_rate": 1.783879338306844e-06,
      "loss": 2.4593,
      "step": 1488100
    },
    {
      "epoch": 482.39870340356566,
      "grad_norm": 1.530551791191101,
      "learning_rate": 1.7806357444048007e-06,
      "loss": 2.4614,
      "step": 1488200
    },
    {
      "epoch": 482.4311183144246,
      "grad_norm": 1.4706217050552368,
      "learning_rate": 1.777392150502757e-06,
      "loss": 2.443,
      "step": 1488300
    },
    {
      "epoch": 482.46353322528364,
      "grad_norm": 1.3659924268722534,
      "learning_rate": 1.7741485566007135e-06,
      "loss": 2.4485,
      "step": 1488400
    },
    {
      "epoch": 482.4959481361426,
      "grad_norm": 1.5930434465408325,
      "learning_rate": 1.7709373986376904e-06,
      "loss": 2.4569,
      "step": 1488500
    },
    {
      "epoch": 482.5283630470016,
      "grad_norm": 1.3319147825241089,
      "learning_rate": 1.7676938047356473e-06,
      "loss": 2.4316,
      "step": 1488600
    },
    {
      "epoch": 482.56077795786064,
      "grad_norm": 1.443170189857483,
      "learning_rate": 1.764450210833604e-06,
      "loss": 2.4583,
      "step": 1488700
    },
    {
      "epoch": 482.5931928687196,
      "grad_norm": 1.4941860437393188,
      "learning_rate": 1.7612066169315603e-06,
      "loss": 2.4376,
      "step": 1488800
    },
    {
      "epoch": 482.6256077795786,
      "grad_norm": 1.3605854511260986,
      "learning_rate": 1.7579630230295167e-06,
      "loss": 2.4327,
      "step": 1488900
    },
    {
      "epoch": 482.6580226904376,
      "grad_norm": 1.480870008468628,
      "learning_rate": 1.7547194291274733e-06,
      "loss": 2.4452,
      "step": 1489000
    },
    {
      "epoch": 482.6904376012966,
      "grad_norm": 1.5893927812576294,
      "learning_rate": 1.7514758352254298e-06,
      "loss": 2.4494,
      "step": 1489100
    },
    {
      "epoch": 482.7228525121556,
      "grad_norm": 1.4533002376556396,
      "learning_rate": 1.7482322413233862e-06,
      "loss": 2.4495,
      "step": 1489200
    },
    {
      "epoch": 482.7552674230146,
      "grad_norm": 1.5204007625579834,
      "learning_rate": 1.744988647421343e-06,
      "loss": 2.4501,
      "step": 1489300
    },
    {
      "epoch": 482.7876823338736,
      "grad_norm": 1.4974137544631958,
      "learning_rate": 1.7417450535192996e-06,
      "loss": 2.47,
      "step": 1489400
    },
    {
      "epoch": 482.82009724473255,
      "grad_norm": 1.6070795059204102,
      "learning_rate": 1.738501459617256e-06,
      "loss": 2.4381,
      "step": 1489500
    },
    {
      "epoch": 482.85251215559157,
      "grad_norm": 1.5584925413131714,
      "learning_rate": 1.7352578657152125e-06,
      "loss": 2.4618,
      "step": 1489600
    },
    {
      "epoch": 482.8849270664506,
      "grad_norm": 1.389082670211792,
      "learning_rate": 1.732014271813169e-06,
      "loss": 2.4329,
      "step": 1489700
    },
    {
      "epoch": 482.91734197730955,
      "grad_norm": 1.5896283388137817,
      "learning_rate": 1.7287706779111255e-06,
      "loss": 2.4473,
      "step": 1489800
    },
    {
      "epoch": 482.94975688816857,
      "grad_norm": 1.340622067451477,
      "learning_rate": 1.7255270840090821e-06,
      "loss": 2.4577,
      "step": 1489900
    },
    {
      "epoch": 482.9821717990275,
      "grad_norm": 1.269282579421997,
      "learning_rate": 1.7222834901070385e-06,
      "loss": 2.4469,
      "step": 1490000
    },
    {
      "epoch": 483.0,
      "eval_bleu": 1.2079286729745211,
      "eval_loss": 4.286766052246094,
      "eval_runtime": 4.1682,
      "eval_samples_per_second": 118.036,
      "eval_steps_per_second": 1.919,
      "step": 1490055
    },
    {
      "epoch": 483.01458670988654,
      "grad_norm": 1.3166143894195557,
      "learning_rate": 1.7190398962049954e-06,
      "loss": 2.4595,
      "step": 1490100
    },
    {
      "epoch": 483.04700162074556,
      "grad_norm": 1.207987904548645,
      "learning_rate": 1.7157963023029518e-06,
      "loss": 2.4584,
      "step": 1490200
    },
    {
      "epoch": 483.0794165316045,
      "grad_norm": 1.3045547008514404,
      "learning_rate": 1.7125527084009084e-06,
      "loss": 2.4582,
      "step": 1490300
    },
    {
      "epoch": 483.11183144246354,
      "grad_norm": 1.365206003189087,
      "learning_rate": 1.7093091144988648e-06,
      "loss": 2.4493,
      "step": 1490400
    },
    {
      "epoch": 483.1442463533225,
      "grad_norm": Infinity,
      "learning_rate": 1.7060655205968212e-06,
      "loss": 2.4433,
      "step": 1490500
    },
    {
      "epoch": 483.1766612641815,
      "grad_norm": 1.3459278345108032,
      "learning_rate": 1.7028543626337982e-06,
      "loss": 2.4431,
      "step": 1490600
    },
    {
      "epoch": 483.20907617504054,
      "grad_norm": 1.2873884439468384,
      "learning_rate": 1.6996107687317548e-06,
      "loss": 2.4694,
      "step": 1490700
    },
    {
      "epoch": 483.2414910858995,
      "grad_norm": 1.3671131134033203,
      "learning_rate": 1.6963671748297117e-06,
      "loss": 2.4476,
      "step": 1490800
    },
    {
      "epoch": 483.2739059967585,
      "grad_norm": 1.4198323488235474,
      "learning_rate": 1.693123580927668e-06,
      "loss": 2.4373,
      "step": 1490900
    },
    {
      "epoch": 483.3063209076175,
      "grad_norm": 1.4839627742767334,
      "learning_rate": 1.6898799870256245e-06,
      "loss": 2.4738,
      "step": 1491000
    },
    {
      "epoch": 483.3387358184765,
      "grad_norm": 1.7653698921203613,
      "learning_rate": 1.6866363931235811e-06,
      "loss": 2.4336,
      "step": 1491100
    },
    {
      "epoch": 483.3711507293355,
      "grad_norm": 1.567073106765747,
      "learning_rate": 1.6833927992215375e-06,
      "loss": 2.4242,
      "step": 1491200
    },
    {
      "epoch": 483.4035656401945,
      "grad_norm": 1.3424556255340576,
      "learning_rate": 1.680149205319494e-06,
      "loss": 2.4545,
      "step": 1491300
    },
    {
      "epoch": 483.4359805510535,
      "grad_norm": 1.5033044815063477,
      "learning_rate": 1.6769056114174506e-06,
      "loss": 2.4493,
      "step": 1491400
    },
    {
      "epoch": 483.4683954619125,
      "grad_norm": 1.4396973848342896,
      "learning_rate": 1.673662017515407e-06,
      "loss": 2.4412,
      "step": 1491500
    },
    {
      "epoch": 483.50081037277147,
      "grad_norm": 1.4033347368240356,
      "learning_rate": 1.6704184236133638e-06,
      "loss": 2.4585,
      "step": 1491600
    },
    {
      "epoch": 483.5332252836305,
      "grad_norm": 1.4684537649154663,
      "learning_rate": 1.6671748297113202e-06,
      "loss": 2.451,
      "step": 1491700
    },
    {
      "epoch": 483.56564019448945,
      "grad_norm": 1.322746992111206,
      "learning_rate": 1.6639636717482972e-06,
      "loss": 2.4445,
      "step": 1491800
    },
    {
      "epoch": 483.59805510534846,
      "grad_norm": 1.4253376722335815,
      "learning_rate": 1.6607200778462538e-06,
      "loss": 2.4692,
      "step": 1491900
    },
    {
      "epoch": 483.6304700162075,
      "grad_norm": 1.499619960784912,
      "learning_rate": 1.6574764839442102e-06,
      "loss": 2.4396,
      "step": 1492000
    },
    {
      "epoch": 483.66288492706644,
      "grad_norm": 1.4598015546798706,
      "learning_rate": 1.6542328900421666e-06,
      "loss": 2.4511,
      "step": 1492100
    },
    {
      "epoch": 483.69529983792546,
      "grad_norm": 1.5322723388671875,
      "learning_rate": 1.6509892961401233e-06,
      "loss": 2.4675,
      "step": 1492200
    },
    {
      "epoch": 483.7277147487844,
      "grad_norm": 1.3688544034957886,
      "learning_rate": 1.64774570223808e-06,
      "loss": 2.4447,
      "step": 1492300
    },
    {
      "epoch": 483.76012965964344,
      "grad_norm": 1.5245252847671509,
      "learning_rate": 1.6445021083360365e-06,
      "loss": 2.4411,
      "step": 1492400
    },
    {
      "epoch": 483.79254457050246,
      "grad_norm": 1.4535140991210938,
      "learning_rate": 1.641258514433993e-06,
      "loss": 2.4525,
      "step": 1492500
    },
    {
      "epoch": 483.8249594813614,
      "grad_norm": 1.1700947284698486,
      "learning_rate": 1.6380149205319495e-06,
      "loss": 2.4556,
      "step": 1492600
    },
    {
      "epoch": 483.85737439222044,
      "grad_norm": 1.6544232368469238,
      "learning_rate": 1.634771326629906e-06,
      "loss": 2.4438,
      "step": 1492700
    },
    {
      "epoch": 483.8897893030794,
      "grad_norm": 1.5010994672775269,
      "learning_rate": 1.6315277327278624e-06,
      "loss": 2.4698,
      "step": 1492800
    },
    {
      "epoch": 483.9222042139384,
      "grad_norm": 1.4408636093139648,
      "learning_rate": 1.628284138825819e-06,
      "loss": 2.4495,
      "step": 1492900
    },
    {
      "epoch": 483.95461912479743,
      "grad_norm": 1.3365235328674316,
      "learning_rate": 1.6250405449237758e-06,
      "loss": 2.4344,
      "step": 1493000
    },
    {
      "epoch": 483.9870340356564,
      "grad_norm": 1.6543668508529663,
      "learning_rate": 1.6217969510217323e-06,
      "loss": 2.4263,
      "step": 1493100
    },
    {
      "epoch": 484.0,
      "eval_bleu": 1.1458608994630615,
      "eval_loss": 4.286022186279297,
      "eval_runtime": 4.1588,
      "eval_samples_per_second": 118.303,
      "eval_steps_per_second": 1.924,
      "step": 1493140
    },
    {
      "epoch": 484.0194489465154,
      "grad_norm": 1.432267189025879,
      "learning_rate": 1.6185533571196887e-06,
      "loss": 2.4338,
      "step": 1493200
    },
    {
      "epoch": 484.05186385737437,
      "grad_norm": 1.867414116859436,
      "learning_rate": 1.6153097632176453e-06,
      "loss": 2.4498,
      "step": 1493300
    },
    {
      "epoch": 484.0842787682334,
      "grad_norm": 1.4938076734542847,
      "learning_rate": 1.6120661693156017e-06,
      "loss": 2.4387,
      "step": 1493400
    },
    {
      "epoch": 484.1166936790924,
      "grad_norm": 1.4654682874679565,
      "learning_rate": 1.6088225754135583e-06,
      "loss": 2.4633,
      "step": 1493500
    },
    {
      "epoch": 484.14910858995137,
      "grad_norm": 1.5623146295547485,
      "learning_rate": 1.6055789815115147e-06,
      "loss": 2.4381,
      "step": 1493600
    },
    {
      "epoch": 484.1815235008104,
      "grad_norm": 1.4228758811950684,
      "learning_rate": 1.6023353876094712e-06,
      "loss": 2.4688,
      "step": 1493700
    },
    {
      "epoch": 484.21393841166935,
      "grad_norm": 1.3198003768920898,
      "learning_rate": 1.599091793707428e-06,
      "loss": 2.4425,
      "step": 1493800
    },
    {
      "epoch": 484.24635332252836,
      "grad_norm": 1.4236832857131958,
      "learning_rate": 1.5958481998053846e-06,
      "loss": 2.4404,
      "step": 1493900
    },
    {
      "epoch": 484.2787682333874,
      "grad_norm": 1.323184847831726,
      "learning_rate": 1.592604605903341e-06,
      "loss": 2.4448,
      "step": 1494000
    },
    {
      "epoch": 484.31118314424634,
      "grad_norm": 1.4887140989303589,
      "learning_rate": 1.5893610120012974e-06,
      "loss": 2.4337,
      "step": 1494100
    },
    {
      "epoch": 484.34359805510536,
      "grad_norm": 1.4580817222595215,
      "learning_rate": 1.586117418099254e-06,
      "loss": 2.4518,
      "step": 1494200
    },
    {
      "epoch": 484.3760129659643,
      "grad_norm": 1.4394779205322266,
      "learning_rate": 1.5828738241972105e-06,
      "loss": 2.4503,
      "step": 1494300
    },
    {
      "epoch": 484.40842787682334,
      "grad_norm": 1.4863948822021484,
      "learning_rate": 1.579630230295167e-06,
      "loss": 2.4622,
      "step": 1494400
    },
    {
      "epoch": 484.44084278768236,
      "grad_norm": 1.5166597366333008,
      "learning_rate": 1.5763866363931237e-06,
      "loss": 2.4687,
      "step": 1494500
    },
    {
      "epoch": 484.4732576985413,
      "grad_norm": 1.380435585975647,
      "learning_rate": 1.5731430424910804e-06,
      "loss": 2.4644,
      "step": 1494600
    },
    {
      "epoch": 484.50567260940034,
      "grad_norm": 1.4408715963363647,
      "learning_rate": 1.5698994485890368e-06,
      "loss": 2.4413,
      "step": 1494700
    },
    {
      "epoch": 484.5380875202593,
      "grad_norm": 1.5017632246017456,
      "learning_rate": 1.5666558546869932e-06,
      "loss": 2.4529,
      "step": 1494800
    },
    {
      "epoch": 484.5705024311183,
      "grad_norm": 1.4697130918502808,
      "learning_rate": 1.5634122607849498e-06,
      "loss": 2.4699,
      "step": 1494900
    },
    {
      "epoch": 484.60291734197733,
      "grad_norm": 1.2408456802368164,
      "learning_rate": 1.5601686668829064e-06,
      "loss": 2.4458,
      "step": 1495000
    },
    {
      "epoch": 484.6353322528363,
      "grad_norm": 1.4321850538253784,
      "learning_rate": 1.5569250729808629e-06,
      "loss": 2.4391,
      "step": 1495100
    },
    {
      "epoch": 484.6677471636953,
      "grad_norm": 1.4960927963256836,
      "learning_rate": 1.5536814790788195e-06,
      "loss": 2.4258,
      "step": 1495200
    },
    {
      "epoch": 484.70016207455427,
      "grad_norm": 1.3868392705917358,
      "learning_rate": 1.5504378851767759e-06,
      "loss": 2.4238,
      "step": 1495300
    },
    {
      "epoch": 484.7325769854133,
      "grad_norm": 1.6154359579086304,
      "learning_rate": 1.5471942912747325e-06,
      "loss": 2.4307,
      "step": 1495400
    },
    {
      "epoch": 484.7649918962723,
      "grad_norm": 1.3661917448043823,
      "learning_rate": 1.543950697372689e-06,
      "loss": 2.4611,
      "step": 1495500
    },
    {
      "epoch": 484.79740680713127,
      "grad_norm": 1.5237476825714111,
      "learning_rate": 1.5407071034706456e-06,
      "loss": 2.4529,
      "step": 1495600
    },
    {
      "epoch": 484.8298217179903,
      "grad_norm": 1.4321556091308594,
      "learning_rate": 1.5374635095686022e-06,
      "loss": 2.4609,
      "step": 1495700
    },
    {
      "epoch": 484.86223662884925,
      "grad_norm": 1.5198408365249634,
      "learning_rate": 1.5342523516055791e-06,
      "loss": 2.4455,
      "step": 1495800
    },
    {
      "epoch": 484.89465153970826,
      "grad_norm": 1.5590577125549316,
      "learning_rate": 1.5310087577035355e-06,
      "loss": 2.4559,
      "step": 1495900
    },
    {
      "epoch": 484.9270664505673,
      "grad_norm": 1.391408085823059,
      "learning_rate": 1.5277651638014922e-06,
      "loss": 2.4532,
      "step": 1496000
    },
    {
      "epoch": 484.95948136142624,
      "grad_norm": 1.380841612815857,
      "learning_rate": 1.5245215698994488e-06,
      "loss": 2.4392,
      "step": 1496100
    },
    {
      "epoch": 484.99189627228526,
      "grad_norm": 1.4989827871322632,
      "learning_rate": 1.5212779759974052e-06,
      "loss": 2.4575,
      "step": 1496200
    },
    {
      "epoch": 485.0,
      "eval_bleu": 1.1902744835475902,
      "eval_loss": 4.286723613739014,
      "eval_runtime": 4.2752,
      "eval_samples_per_second": 115.083,
      "eval_steps_per_second": 1.871,
      "step": 1496225
    },
    {
      "epoch": 485.0243111831442,
      "grad_norm": 1.5415269136428833,
      "learning_rate": 1.5180343820953616e-06,
      "loss": 2.4485,
      "step": 1496300
    },
    {
      "epoch": 485.05672609400324,
      "grad_norm": 1.446391224861145,
      "learning_rate": 1.5147907881933183e-06,
      "loss": 2.4874,
      "step": 1496400
    },
    {
      "epoch": 485.08914100486226,
      "grad_norm": 1.3732444047927856,
      "learning_rate": 1.5115471942912749e-06,
      "loss": 2.4319,
      "step": 1496500
    },
    {
      "epoch": 485.1215559157212,
      "grad_norm": 1.4168150424957275,
      "learning_rate": 1.5083036003892313e-06,
      "loss": 2.4387,
      "step": 1496600
    },
    {
      "epoch": 485.15397082658023,
      "grad_norm": 1.3864059448242188,
      "learning_rate": 1.505060006487188e-06,
      "loss": 2.4399,
      "step": 1496700
    },
    {
      "epoch": 485.1863857374392,
      "grad_norm": 1.3209962844848633,
      "learning_rate": 1.5018164125851443e-06,
      "loss": 2.4399,
      "step": 1496800
    },
    {
      "epoch": 485.2188006482982,
      "grad_norm": 1.525498867034912,
      "learning_rate": 1.498572818683101e-06,
      "loss": 2.4851,
      "step": 1496900
    },
    {
      "epoch": 485.25121555915723,
      "grad_norm": 1.4817277193069458,
      "learning_rate": 1.4953292247810576e-06,
      "loss": 2.4506,
      "step": 1497000
    },
    {
      "epoch": 485.2836304700162,
      "grad_norm": 1.5193934440612793,
      "learning_rate": 1.492085630879014e-06,
      "loss": 2.4446,
      "step": 1497100
    },
    {
      "epoch": 485.3160453808752,
      "grad_norm": 1.5014936923980713,
      "learning_rate": 1.4888420369769706e-06,
      "loss": 2.4573,
      "step": 1497200
    },
    {
      "epoch": 485.34846029173417,
      "grad_norm": 1.1964954137802124,
      "learning_rate": 1.485598443074927e-06,
      "loss": 2.4505,
      "step": 1497300
    },
    {
      "epoch": 485.3808752025932,
      "grad_norm": 1.324562907218933,
      "learning_rate": 1.4823548491728837e-06,
      "loss": 2.432,
      "step": 1497400
    },
    {
      "epoch": 485.4132901134522,
      "grad_norm": 1.415592074394226,
      "learning_rate": 1.47911125527084e-06,
      "loss": 2.4374,
      "step": 1497500
    },
    {
      "epoch": 485.44570502431117,
      "grad_norm": 1.648967981338501,
      "learning_rate": 1.4758676613687967e-06,
      "loss": 2.4431,
      "step": 1497600
    },
    {
      "epoch": 485.4781199351702,
      "grad_norm": 1.3572626113891602,
      "learning_rate": 1.4726240674667533e-06,
      "loss": 2.4483,
      "step": 1497700
    },
    {
      "epoch": 485.51053484602915,
      "grad_norm": 1.4561971426010132,
      "learning_rate": 1.4694129095037303e-06,
      "loss": 2.4413,
      "step": 1497800
    },
    {
      "epoch": 485.54294975688816,
      "grad_norm": 1.47684645652771,
      "learning_rate": 1.4661693156016867e-06,
      "loss": 2.4541,
      "step": 1497900
    },
    {
      "epoch": 485.5753646677472,
      "grad_norm": 1.4345165491104126,
      "learning_rate": 1.4629257216996433e-06,
      "loss": 2.4534,
      "step": 1498000
    },
    {
      "epoch": 485.60777957860614,
      "grad_norm": 1.344788908958435,
      "learning_rate": 1.4596821277975997e-06,
      "loss": 2.4576,
      "step": 1498100
    },
    {
      "epoch": 485.64019448946516,
      "grad_norm": 1.3061368465423584,
      "learning_rate": 1.4564385338955564e-06,
      "loss": 2.4392,
      "step": 1498200
    },
    {
      "epoch": 485.6726094003242,
      "grad_norm": 1.5863046646118164,
      "learning_rate": 1.4531949399935128e-06,
      "loss": 2.4474,
      "step": 1498300
    },
    {
      "epoch": 485.70502431118314,
      "grad_norm": 1.5701619386672974,
      "learning_rate": 1.4499513460914694e-06,
      "loss": 2.4517,
      "step": 1498400
    },
    {
      "epoch": 485.73743922204216,
      "grad_norm": 1.1962950229644775,
      "learning_rate": 1.446707752189426e-06,
      "loss": 2.4519,
      "step": 1498500
    },
    {
      "epoch": 485.7698541329011,
      "grad_norm": 1.381271243095398,
      "learning_rate": 1.4434641582873824e-06,
      "loss": 2.4473,
      "step": 1498600
    },
    {
      "epoch": 485.80226904376013,
      "grad_norm": 1.4934356212615967,
      "learning_rate": 1.440220564385339e-06,
      "loss": 2.4178,
      "step": 1498700
    },
    {
      "epoch": 485.83468395461915,
      "grad_norm": 1.6286797523498535,
      "learning_rate": 1.4369769704832957e-06,
      "loss": 2.4529,
      "step": 1498800
    },
    {
      "epoch": 485.8670988654781,
      "grad_norm": 1.3039101362228394,
      "learning_rate": 1.433733376581252e-06,
      "loss": 2.434,
      "step": 1498900
    },
    {
      "epoch": 485.89951377633713,
      "grad_norm": 1.37546968460083,
      "learning_rate": 1.4304897826792085e-06,
      "loss": 2.4646,
      "step": 1499000
    },
    {
      "epoch": 485.9319286871961,
      "grad_norm": 1.5708411931991577,
      "learning_rate": 1.4272461887771651e-06,
      "loss": 2.4447,
      "step": 1499100
    },
    {
      "epoch": 485.9643435980551,
      "grad_norm": 1.2769168615341187,
      "learning_rate": 1.4240025948751218e-06,
      "loss": 2.4716,
      "step": 1499200
    },
    {
      "epoch": 485.9967585089141,
      "grad_norm": 1.6258301734924316,
      "learning_rate": 1.4207590009730782e-06,
      "loss": 2.4475,
      "step": 1499300
    },
    {
      "epoch": 486.0,
      "eval_bleu": 1.1437244319426296,
      "eval_loss": 4.286555767059326,
      "eval_runtime": 4.467,
      "eval_samples_per_second": 110.141,
      "eval_steps_per_second": 1.791,
      "step": 1499310
    },
    {
      "epoch": 486.0291734197731,
      "grad_norm": 1.2879198789596558,
      "learning_rate": 1.4175154070710348e-06,
      "loss": 2.4504,
      "step": 1499400
    },
    {
      "epoch": 486.0615883306321,
      "grad_norm": 1.6482762098312378,
      "learning_rate": 1.4142718131689914e-06,
      "loss": 2.4354,
      "step": 1499500
    },
    {
      "epoch": 486.09400324149107,
      "grad_norm": 1.523195505142212,
      "learning_rate": 1.4110282192669478e-06,
      "loss": 2.4259,
      "step": 1499600
    },
    {
      "epoch": 486.1264181523501,
      "grad_norm": 1.3437416553497314,
      "learning_rate": 1.4077846253649042e-06,
      "loss": 2.425,
      "step": 1499700
    },
    {
      "epoch": 486.1588330632091,
      "grad_norm": 1.3606228828430176,
      "learning_rate": 1.404541031462861e-06,
      "loss": 2.4663,
      "step": 1499800
    },
    {
      "epoch": 486.19124797406806,
      "grad_norm": 1.2823331356048584,
      "learning_rate": 1.4013298734998378e-06,
      "loss": 2.4589,
      "step": 1499900
    },
    {
      "epoch": 486.2236628849271,
      "grad_norm": 1.4707428216934204,
      "learning_rate": 1.3980862795977945e-06,
      "loss": 2.441,
      "step": 1500000
    },
    {
      "epoch": 486.25607779578604,
      "grad_norm": 1.336069941520691,
      "learning_rate": 1.3948426856957509e-06,
      "loss": 2.4439,
      "step": 1500100
    },
    {
      "epoch": 486.28849270664506,
      "grad_norm": 1.3775354623794556,
      "learning_rate": 1.3915990917937075e-06,
      "loss": 2.4375,
      "step": 1500200
    },
    {
      "epoch": 486.3209076175041,
      "grad_norm": 1.3397998809814453,
      "learning_rate": 1.3883554978916641e-06,
      "loss": 2.4408,
      "step": 1500300
    },
    {
      "epoch": 486.35332252836304,
      "grad_norm": 1.258758544921875,
      "learning_rate": 1.3851119039896205e-06,
      "loss": 2.4641,
      "step": 1500400
    },
    {
      "epoch": 486.38573743922205,
      "grad_norm": 1.421762466430664,
      "learning_rate": 1.381868310087577e-06,
      "loss": 2.4399,
      "step": 1500500
    },
    {
      "epoch": 486.418152350081,
      "grad_norm": 1.5981954336166382,
      "learning_rate": 1.3786247161855338e-06,
      "loss": 2.4473,
      "step": 1500600
    },
    {
      "epoch": 486.45056726094003,
      "grad_norm": 1.4397000074386597,
      "learning_rate": 1.3753811222834902e-06,
      "loss": 2.4225,
      "step": 1500700
    },
    {
      "epoch": 486.48298217179905,
      "grad_norm": 1.6240350008010864,
      "learning_rate": 1.3721699643204671e-06,
      "loss": 2.4534,
      "step": 1500800
    },
    {
      "epoch": 486.515397082658,
      "grad_norm": 1.3754216432571411,
      "learning_rate": 1.3689263704184236e-06,
      "loss": 2.4522,
      "step": 1500900
    },
    {
      "epoch": 486.54781199351703,
      "grad_norm": 1.3784306049346924,
      "learning_rate": 1.3656827765163802e-06,
      "loss": 2.4398,
      "step": 1501000
    },
    {
      "epoch": 486.580226904376,
      "grad_norm": 1.3570420742034912,
      "learning_rate": 1.3624391826143368e-06,
      "loss": 2.4481,
      "step": 1501100
    },
    {
      "epoch": 486.612641815235,
      "grad_norm": 1.554957389831543,
      "learning_rate": 1.3592280246513138e-06,
      "loss": 2.4673,
      "step": 1501200
    },
    {
      "epoch": 486.645056726094,
      "grad_norm": 1.593955397605896,
      "learning_rate": 1.3559844307492702e-06,
      "loss": 2.465,
      "step": 1501300
    },
    {
      "epoch": 486.677471636953,
      "grad_norm": 1.4979394674301147,
      "learning_rate": 1.3527408368472268e-06,
      "loss": 2.4499,
      "step": 1501400
    },
    {
      "epoch": 486.709886547812,
      "grad_norm": 1.3430018424987793,
      "learning_rate": 1.3495296788842038e-06,
      "loss": 2.4546,
      "step": 1501500
    },
    {
      "epoch": 486.74230145867097,
      "grad_norm": 1.517956256866455,
      "learning_rate": 1.3462860849821602e-06,
      "loss": 2.4458,
      "step": 1501600
    },
    {
      "epoch": 486.77471636953,
      "grad_norm": 1.4992448091506958,
      "learning_rate": 1.3430424910801168e-06,
      "loss": 2.4572,
      "step": 1501700
    },
    {
      "epoch": 486.807131280389,
      "grad_norm": 1.3526350259780884,
      "learning_rate": 1.3397988971780734e-06,
      "loss": 2.4744,
      "step": 1501800
    },
    {
      "epoch": 486.83954619124796,
      "grad_norm": 1.6019606590270996,
      "learning_rate": 1.3365553032760298e-06,
      "loss": 2.4898,
      "step": 1501900
    },
    {
      "epoch": 486.871961102107,
      "grad_norm": 1.4614592790603638,
      "learning_rate": 1.3333117093739865e-06,
      "loss": 2.4261,
      "step": 1502000
    },
    {
      "epoch": 486.90437601296594,
      "grad_norm": 1.551538348197937,
      "learning_rate": 1.330068115471943e-06,
      "loss": 2.4571,
      "step": 1502100
    },
    {
      "epoch": 486.93679092382496,
      "grad_norm": 1.5118190050125122,
      "learning_rate": 1.3268245215698995e-06,
      "loss": 2.442,
      "step": 1502200
    },
    {
      "epoch": 486.969205834684,
      "grad_norm": 1.3966139554977417,
      "learning_rate": 1.3235809276678561e-06,
      "loss": 2.4595,
      "step": 1502300
    },
    {
      "epoch": 487.0,
      "eval_bleu": 1.1109120278955957,
      "eval_loss": 4.286446571350098,
      "eval_runtime": 4.449,
      "eval_samples_per_second": 110.588,
      "eval_steps_per_second": 1.798,
      "step": 1502395
    },
    {
      "epoch": 487.00162074554294,
      "grad_norm": 1.3236383199691772,
      "learning_rate": 1.3203373337658125e-06,
      "loss": 2.4487,
      "step": 1502400
    },
    {
      "epoch": 487.03403565640195,
      "grad_norm": 1.4733195304870605,
      "learning_rate": 1.3170937398637692e-06,
      "loss": 2.4447,
      "step": 1502500
    },
    {
      "epoch": 487.0664505672609,
      "grad_norm": 1.3948578834533691,
      "learning_rate": 1.3138501459617256e-06,
      "loss": 2.4545,
      "step": 1502600
    },
    {
      "epoch": 487.09886547811993,
      "grad_norm": 1.3568851947784424,
      "learning_rate": 1.3106065520596822e-06,
      "loss": 2.4568,
      "step": 1502700
    },
    {
      "epoch": 487.13128038897895,
      "grad_norm": 1.3897191286087036,
      "learning_rate": 1.3073629581576386e-06,
      "loss": 2.4582,
      "step": 1502800
    },
    {
      "epoch": 487.1636952998379,
      "grad_norm": 1.550386905670166,
      "learning_rate": 1.3041193642555952e-06,
      "loss": 2.4515,
      "step": 1502900
    },
    {
      "epoch": 487.19611021069693,
      "grad_norm": 1.6859954595565796,
      "learning_rate": 1.3008757703535519e-06,
      "loss": 2.4576,
      "step": 1503000
    },
    {
      "epoch": 487.2285251215559,
      "grad_norm": 1.6107244491577148,
      "learning_rate": 1.2976321764515083e-06,
      "loss": 2.4666,
      "step": 1503100
    },
    {
      "epoch": 487.2609400324149,
      "grad_norm": 1.559218168258667,
      "learning_rate": 1.294388582549465e-06,
      "loss": 2.45,
      "step": 1503200
    },
    {
      "epoch": 487.2933549432739,
      "grad_norm": 1.4943991899490356,
      "learning_rate": 1.2911449886474215e-06,
      "loss": 2.4605,
      "step": 1503300
    },
    {
      "epoch": 487.3257698541329,
      "grad_norm": 1.3965400457382202,
      "learning_rate": 1.287901394745378e-06,
      "loss": 2.4572,
      "step": 1503400
    },
    {
      "epoch": 487.3581847649919,
      "grad_norm": 1.347722053527832,
      "learning_rate": 1.2846578008433344e-06,
      "loss": 2.4356,
      "step": 1503500
    },
    {
      "epoch": 487.39059967585086,
      "grad_norm": 1.4400345087051392,
      "learning_rate": 1.281414206941291e-06,
      "loss": 2.4466,
      "step": 1503600
    },
    {
      "epoch": 487.4230145867099,
      "grad_norm": 1.2664012908935547,
      "learning_rate": 1.2781706130392476e-06,
      "loss": 2.4576,
      "step": 1503700
    },
    {
      "epoch": 487.4554294975689,
      "grad_norm": 1.4302010536193848,
      "learning_rate": 1.274927019137204e-06,
      "loss": 2.4404,
      "step": 1503800
    },
    {
      "epoch": 487.48784440842786,
      "grad_norm": 1.4916298389434814,
      "learning_rate": 1.2716834252351606e-06,
      "loss": 2.4473,
      "step": 1503900
    },
    {
      "epoch": 487.5202593192869,
      "grad_norm": 1.5100033283233643,
      "learning_rate": 1.2684398313331173e-06,
      "loss": 2.441,
      "step": 1504000
    },
    {
      "epoch": 487.55267423014584,
      "grad_norm": 1.3824481964111328,
      "learning_rate": 1.2651962374310737e-06,
      "loss": 2.4382,
      "step": 1504100
    },
    {
      "epoch": 487.58508914100486,
      "grad_norm": 1.4282500743865967,
      "learning_rate": 1.26195264352903e-06,
      "loss": 2.4452,
      "step": 1504200
    },
    {
      "epoch": 487.6175040518639,
      "grad_norm": 1.370758056640625,
      "learning_rate": 1.2587090496269867e-06,
      "loss": 2.4527,
      "step": 1504300
    },
    {
      "epoch": 487.64991896272284,
      "grad_norm": 1.3738694190979004,
      "learning_rate": 1.2554654557249433e-06,
      "loss": 2.4505,
      "step": 1504400
    },
    {
      "epoch": 487.68233387358185,
      "grad_norm": 1.4714510440826416,
      "learning_rate": 1.2522218618228998e-06,
      "loss": 2.4341,
      "step": 1504500
    },
    {
      "epoch": 487.7147487844408,
      "grad_norm": 1.417901873588562,
      "learning_rate": 1.2489782679208564e-06,
      "loss": 2.4576,
      "step": 1504600
    },
    {
      "epoch": 487.74716369529983,
      "grad_norm": 1.4741147756576538,
      "learning_rate": 1.245734674018813e-06,
      "loss": 2.4505,
      "step": 1504700
    },
    {
      "epoch": 487.77957860615885,
      "grad_norm": 1.5093927383422852,
      "learning_rate": 1.2424910801167694e-06,
      "loss": 2.463,
      "step": 1504800
    },
    {
      "epoch": 487.8119935170178,
      "grad_norm": 1.474581241607666,
      "learning_rate": 1.2392474862147258e-06,
      "loss": 2.4476,
      "step": 1504900
    },
    {
      "epoch": 487.84440842787683,
      "grad_norm": 1.500645637512207,
      "learning_rate": 1.2360038923126827e-06,
      "loss": 2.4666,
      "step": 1505000
    },
    {
      "epoch": 487.87682333873585,
      "grad_norm": 1.5102876424789429,
      "learning_rate": 1.232760298410639e-06,
      "loss": 2.4343,
      "step": 1505100
    },
    {
      "epoch": 487.9092382495948,
      "grad_norm": 1.4946495294570923,
      "learning_rate": 1.2295167045085955e-06,
      "loss": 2.4493,
      "step": 1505200
    },
    {
      "epoch": 487.9416531604538,
      "grad_norm": 1.5606764554977417,
      "learning_rate": 1.2262731106065521e-06,
      "loss": 2.4486,
      "step": 1505300
    },
    {
      "epoch": 487.9740680713128,
      "grad_norm": 1.6785492897033691,
      "learning_rate": 1.2230295167045088e-06,
      "loss": 2.4427,
      "step": 1505400
    },
    {
      "epoch": 488.0,
      "eval_bleu": 1.1484457110423385,
      "eval_loss": 4.2858991622924805,
      "eval_runtime": 4.3769,
      "eval_samples_per_second": 112.407,
      "eval_steps_per_second": 1.828,
      "step": 1505480
    },
    {
      "epoch": 488.0064829821718,
      "grad_norm": 1.4335980415344238,
      "learning_rate": 1.2197859228024652e-06,
      "loss": 2.4423,
      "step": 1505500
    },
    {
      "epoch": 488.0388978930308,
      "grad_norm": 1.4023929834365845,
      "learning_rate": 1.2165423289004216e-06,
      "loss": 2.4418,
      "step": 1505600
    },
    {
      "epoch": 488.0713128038898,
      "grad_norm": 1.5172593593597412,
      "learning_rate": 1.2132987349983784e-06,
      "loss": 2.4411,
      "step": 1505700
    },
    {
      "epoch": 488.1037277147488,
      "grad_norm": 1.3920847177505493,
      "learning_rate": 1.2100551410963348e-06,
      "loss": 2.4526,
      "step": 1505800
    },
    {
      "epoch": 488.13614262560776,
      "grad_norm": 1.4536362886428833,
      "learning_rate": 1.2068115471942912e-06,
      "loss": 2.4446,
      "step": 1505900
    },
    {
      "epoch": 488.1685575364668,
      "grad_norm": 1.5622986555099487,
      "learning_rate": 1.2035679532922479e-06,
      "loss": 2.4594,
      "step": 1506000
    },
    {
      "epoch": 488.2009724473258,
      "grad_norm": 1.4612741470336914,
      "learning_rate": 1.2003243593902045e-06,
      "loss": 2.4586,
      "step": 1506100
    },
    {
      "epoch": 488.23338735818476,
      "grad_norm": 1.5124194622039795,
      "learning_rate": 1.197080765488161e-06,
      "loss": 2.4635,
      "step": 1506200
    },
    {
      "epoch": 488.2658022690438,
      "grad_norm": 1.639707088470459,
      "learning_rate": 1.1938371715861175e-06,
      "loss": 2.4559,
      "step": 1506300
    },
    {
      "epoch": 488.29821717990274,
      "grad_norm": 1.5601893663406372,
      "learning_rate": 1.190593577684074e-06,
      "loss": 2.4517,
      "step": 1506400
    },
    {
      "epoch": 488.33063209076175,
      "grad_norm": 1.4144089221954346,
      "learning_rate": 1.1873499837820306e-06,
      "loss": 2.4419,
      "step": 1506500
    },
    {
      "epoch": 488.36304700162077,
      "grad_norm": 1.5724971294403076,
      "learning_rate": 1.184106389879987e-06,
      "loss": 2.4474,
      "step": 1506600
    },
    {
      "epoch": 488.39546191247973,
      "grad_norm": 1.6881725788116455,
      "learning_rate": 1.1808627959779436e-06,
      "loss": 2.4367,
      "step": 1506700
    },
    {
      "epoch": 488.42787682333875,
      "grad_norm": 1.411505103111267,
      "learning_rate": 1.1776192020759002e-06,
      "loss": 2.4611,
      "step": 1506800
    },
    {
      "epoch": 488.4602917341977,
      "grad_norm": 1.3189746141433716,
      "learning_rate": 1.1743756081738567e-06,
      "loss": 2.4457,
      "step": 1506900
    },
    {
      "epoch": 488.4927066450567,
      "grad_norm": 1.5505768060684204,
      "learning_rate": 1.1711320142718133e-06,
      "loss": 2.4553,
      "step": 1507000
    },
    {
      "epoch": 488.52512155591575,
      "grad_norm": 1.6224192380905151,
      "learning_rate": 1.1678884203697697e-06,
      "loss": 2.4572,
      "step": 1507100
    },
    {
      "epoch": 488.5575364667747,
      "grad_norm": 1.4828379154205322,
      "learning_rate": 1.1646448264677263e-06,
      "loss": 2.4554,
      "step": 1507200
    },
    {
      "epoch": 488.5899513776337,
      "grad_norm": 1.311470866203308,
      "learning_rate": 1.161401232565683e-06,
      "loss": 2.455,
      "step": 1507300
    },
    {
      "epoch": 488.6223662884927,
      "grad_norm": 1.424688458442688,
      "learning_rate": 1.1581576386636394e-06,
      "loss": 2.4487,
      "step": 1507400
    },
    {
      "epoch": 488.6547811993517,
      "grad_norm": 1.4478495121002197,
      "learning_rate": 1.154914044761596e-06,
      "loss": 2.4446,
      "step": 1507500
    },
    {
      "epoch": 488.6871961102107,
      "grad_norm": 1.4906197786331177,
      "learning_rate": 1.1516704508595524e-06,
      "loss": 2.4495,
      "step": 1507600
    },
    {
      "epoch": 488.7196110210697,
      "grad_norm": 1.4509553909301758,
      "learning_rate": 1.148426856957509e-06,
      "loss": 2.4519,
      "step": 1507700
    },
    {
      "epoch": 488.7520259319287,
      "grad_norm": 1.420404314994812,
      "learning_rate": 1.1451832630554654e-06,
      "loss": 2.4461,
      "step": 1507800
    },
    {
      "epoch": 488.78444084278766,
      "grad_norm": 1.4913814067840576,
      "learning_rate": 1.141939669153422e-06,
      "loss": 2.4542,
      "step": 1507900
    },
    {
      "epoch": 488.8168557536467,
      "grad_norm": 1.504140853881836,
      "learning_rate": 1.1386960752513787e-06,
      "loss": 2.431,
      "step": 1508000
    },
    {
      "epoch": 488.8492706645057,
      "grad_norm": 1.4242134094238281,
      "learning_rate": 1.135452481349335e-06,
      "loss": 2.4472,
      "step": 1508100
    },
    {
      "epoch": 488.88168557536466,
      "grad_norm": 1.5689507722854614,
      "learning_rate": 1.1322088874472915e-06,
      "loss": 2.4437,
      "step": 1508200
    },
    {
      "epoch": 488.9141004862237,
      "grad_norm": 1.5063717365264893,
      "learning_rate": 1.1289977294842687e-06,
      "loss": 2.456,
      "step": 1508300
    },
    {
      "epoch": 488.94651539708263,
      "grad_norm": 1.7413506507873535,
      "learning_rate": 1.125754135582225e-06,
      "loss": 2.4508,
      "step": 1508400
    },
    {
      "epoch": 488.97893030794165,
      "grad_norm": 1.4163013696670532,
      "learning_rate": 1.1225105416801817e-06,
      "loss": 2.4405,
      "step": 1508500
    },
    {
      "epoch": 489.0,
      "eval_bleu": 1.1630862014346288,
      "eval_loss": 4.286910533905029,
      "eval_runtime": 4.936,
      "eval_samples_per_second": 99.675,
      "eval_steps_per_second": 1.621,
      "step": 1508565
    },
    {
      "epoch": 489.01134521880067,
      "grad_norm": 1.4666894674301147,
      "learning_rate": 1.1192669477781381e-06,
      "loss": 2.4496,
      "step": 1508600
    },
    {
      "epoch": 489.04376012965963,
      "grad_norm": 1.3983523845672607,
      "learning_rate": 1.1160233538760948e-06,
      "loss": 2.4413,
      "step": 1508700
    },
    {
      "epoch": 489.07617504051865,
      "grad_norm": 1.3696200847625732,
      "learning_rate": 1.1127797599740514e-06,
      "loss": 2.4444,
      "step": 1508800
    },
    {
      "epoch": 489.1085899513776,
      "grad_norm": 1.521433711051941,
      "learning_rate": 1.1095361660720078e-06,
      "loss": 2.4558,
      "step": 1508900
    },
    {
      "epoch": 489.1410048622366,
      "grad_norm": 1.4039043188095093,
      "learning_rate": 1.1062925721699644e-06,
      "loss": 2.4411,
      "step": 1509000
    },
    {
      "epoch": 489.17341977309565,
      "grad_norm": 1.4030799865722656,
      "learning_rate": 1.103048978267921e-06,
      "loss": 2.4423,
      "step": 1509100
    },
    {
      "epoch": 489.2058346839546,
      "grad_norm": 1.443549633026123,
      "learning_rate": 1.0998053843658775e-06,
      "loss": 2.4395,
      "step": 1509200
    },
    {
      "epoch": 489.2382495948136,
      "grad_norm": 1.388920545578003,
      "learning_rate": 1.0965617904638339e-06,
      "loss": 2.4566,
      "step": 1509300
    },
    {
      "epoch": 489.2706645056726,
      "grad_norm": 1.4303441047668457,
      "learning_rate": 1.0933181965617905e-06,
      "loss": 2.4508,
      "step": 1509400
    },
    {
      "epoch": 489.3030794165316,
      "grad_norm": 1.1919803619384766,
      "learning_rate": 1.0900746026597471e-06,
      "loss": 2.4281,
      "step": 1509500
    },
    {
      "epoch": 489.3354943273906,
      "grad_norm": 1.32016122341156,
      "learning_rate": 1.0868310087577035e-06,
      "loss": 2.4517,
      "step": 1509600
    },
    {
      "epoch": 489.3679092382496,
      "grad_norm": 1.625098466873169,
      "learning_rate": 1.08358741485566e-06,
      "loss": 2.4474,
      "step": 1509700
    },
    {
      "epoch": 489.4003241491086,
      "grad_norm": 1.4416759014129639,
      "learning_rate": 1.0803438209536168e-06,
      "loss": 2.4445,
      "step": 1509800
    },
    {
      "epoch": 489.43273905996756,
      "grad_norm": 1.342756986618042,
      "learning_rate": 1.0771002270515732e-06,
      "loss": 2.4522,
      "step": 1509900
    },
    {
      "epoch": 489.4651539708266,
      "grad_norm": 1.744744896888733,
      "learning_rate": 1.0738566331495296e-06,
      "loss": 2.4688,
      "step": 1510000
    },
    {
      "epoch": 489.4975688816856,
      "grad_norm": 1.4433091878890991,
      "learning_rate": 1.0706130392474862e-06,
      "loss": 2.4528,
      "step": 1510100
    },
    {
      "epoch": 489.52998379254456,
      "grad_norm": 1.4785490036010742,
      "learning_rate": 1.0673694453454429e-06,
      "loss": 2.4523,
      "step": 1510200
    },
    {
      "epoch": 489.5623987034036,
      "grad_norm": 1.5832775831222534,
      "learning_rate": 1.0641258514433993e-06,
      "loss": 2.4342,
      "step": 1510300
    },
    {
      "epoch": 489.59481361426253,
      "grad_norm": 1.8649753332138062,
      "learning_rate": 1.060882257541356e-06,
      "loss": 2.4491,
      "step": 1510400
    },
    {
      "epoch": 489.62722852512155,
      "grad_norm": 1.4714897871017456,
      "learning_rate": 1.0576386636393125e-06,
      "loss": 2.4427,
      "step": 1510500
    },
    {
      "epoch": 489.65964343598057,
      "grad_norm": 1.3894634246826172,
      "learning_rate": 1.054395069737269e-06,
      "loss": 2.4442,
      "step": 1510600
    },
    {
      "epoch": 489.69205834683953,
      "grad_norm": 1.4720497131347656,
      "learning_rate": 1.0511514758352254e-06,
      "loss": 2.4492,
      "step": 1510700
    },
    {
      "epoch": 489.72447325769855,
      "grad_norm": 1.4862297773361206,
      "learning_rate": 1.0479078819331822e-06,
      "loss": 2.4596,
      "step": 1510800
    },
    {
      "epoch": 489.7568881685575,
      "grad_norm": 1.6820316314697266,
      "learning_rate": 1.0446642880311386e-06,
      "loss": 2.4392,
      "step": 1510900
    },
    {
      "epoch": 489.7893030794165,
      "grad_norm": 1.4424535036087036,
      "learning_rate": 1.041420694129095e-06,
      "loss": 2.4383,
      "step": 1511000
    },
    {
      "epoch": 489.82171799027554,
      "grad_norm": 1.5035977363586426,
      "learning_rate": 1.0381771002270516e-06,
      "loss": 2.4785,
      "step": 1511100
    },
    {
      "epoch": 489.8541329011345,
      "grad_norm": 1.4136055707931519,
      "learning_rate": 1.0349659422640286e-06,
      "loss": 2.4587,
      "step": 1511200
    },
    {
      "epoch": 489.8865478119935,
      "grad_norm": 1.5340838432312012,
      "learning_rate": 1.0317223483619852e-06,
      "loss": 2.4516,
      "step": 1511300
    },
    {
      "epoch": 489.9189627228525,
      "grad_norm": 1.3925172090530396,
      "learning_rate": 1.0284787544599416e-06,
      "loss": 2.4482,
      "step": 1511400
    },
    {
      "epoch": 489.9513776337115,
      "grad_norm": 1.4795997142791748,
      "learning_rate": 1.025235160557898e-06,
      "loss": 2.4487,
      "step": 1511500
    },
    {
      "epoch": 489.9837925445705,
      "grad_norm": 1.5323370695114136,
      "learning_rate": 1.0219915666558549e-06,
      "loss": 2.45,
      "step": 1511600
    },
    {
      "epoch": 490.0,
      "eval_bleu": 1.1532562771725767,
      "eval_loss": 4.286435127258301,
      "eval_runtime": 4.7879,
      "eval_samples_per_second": 102.759,
      "eval_steps_per_second": 1.671,
      "step": 1511650
    },
    {
      "epoch": 490.0162074554295,
      "grad_norm": 1.635863184928894,
      "learning_rate": 1.0187479727538113e-06,
      "loss": 2.4415,
      "step": 1511700
    },
    {
      "epoch": 490.0486223662885,
      "grad_norm": 1.5407404899597168,
      "learning_rate": 1.0155043788517677e-06,
      "loss": 2.4688,
      "step": 1511800
    },
    {
      "epoch": 490.0810372771475,
      "grad_norm": 1.3889758586883545,
      "learning_rate": 1.0122607849497243e-06,
      "loss": 2.4142,
      "step": 1511900
    },
    {
      "epoch": 490.1134521880065,
      "grad_norm": 1.4797312021255493,
      "learning_rate": 1.009017191047681e-06,
      "loss": 2.4675,
      "step": 1512000
    },
    {
      "epoch": 490.1458670988655,
      "grad_norm": 1.3746082782745361,
      "learning_rate": 1.0057735971456374e-06,
      "loss": 2.4525,
      "step": 1512100
    },
    {
      "epoch": 490.17828200972446,
      "grad_norm": 1.4047622680664062,
      "learning_rate": 1.002530003243594e-06,
      "loss": 2.4392,
      "step": 1512200
    },
    {
      "epoch": 490.2106969205835,
      "grad_norm": 1.2835625410079956,
      "learning_rate": 9.992864093415506e-07,
      "loss": 2.4604,
      "step": 1512300
    },
    {
      "epoch": 490.2431118314425,
      "grad_norm": 1.5203591585159302,
      "learning_rate": 9.96042815439507e-07,
      "loss": 2.4432,
      "step": 1512400
    },
    {
      "epoch": 490.27552674230145,
      "grad_norm": 1.2309852838516235,
      "learning_rate": 9.927992215374635e-07,
      "loss": 2.4563,
      "step": 1512500
    },
    {
      "epoch": 490.30794165316047,
      "grad_norm": 1.5489928722381592,
      "learning_rate": 9.8955562763542e-07,
      "loss": 2.4352,
      "step": 1512600
    },
    {
      "epoch": 490.34035656401943,
      "grad_norm": 1.3841177225112915,
      "learning_rate": 9.863120337333767e-07,
      "loss": 2.4563,
      "step": 1512700
    },
    {
      "epoch": 490.37277147487845,
      "grad_norm": 1.3402765989303589,
      "learning_rate": 9.830684398313331e-07,
      "loss": 2.4421,
      "step": 1512800
    },
    {
      "epoch": 490.40518638573747,
      "grad_norm": 1.762528896331787,
      "learning_rate": 9.798248459292897e-07,
      "loss": 2.4591,
      "step": 1512900
    },
    {
      "epoch": 490.4376012965964,
      "grad_norm": 1.484789252281189,
      "learning_rate": 9.765812520272464e-07,
      "loss": 2.4807,
      "step": 1513000
    },
    {
      "epoch": 490.47001620745544,
      "grad_norm": 1.591277837753296,
      "learning_rate": 9.733376581252028e-07,
      "loss": 2.4355,
      "step": 1513100
    },
    {
      "epoch": 490.5024311183144,
      "grad_norm": 1.4291859865188599,
      "learning_rate": 9.701265001621797e-07,
      "loss": 2.4473,
      "step": 1513200
    },
    {
      "epoch": 490.5348460291734,
      "grad_norm": 1.4063376188278198,
      "learning_rate": 9.668829062601361e-07,
      "loss": 2.4752,
      "step": 1513300
    },
    {
      "epoch": 490.56726094003244,
      "grad_norm": 1.2979158163070679,
      "learning_rate": 9.63639312358093e-07,
      "loss": 2.4597,
      "step": 1513400
    },
    {
      "epoch": 490.5996758508914,
      "grad_norm": 1.462860345840454,
      "learning_rate": 9.603957184560494e-07,
      "loss": 2.4235,
      "step": 1513500
    },
    {
      "epoch": 490.6320907617504,
      "grad_norm": 1.4198342561721802,
      "learning_rate": 9.571521245540058e-07,
      "loss": 2.445,
      "step": 1513600
    },
    {
      "epoch": 490.6645056726094,
      "grad_norm": 1.5412044525146484,
      "learning_rate": 9.539085306519624e-07,
      "loss": 2.4345,
      "step": 1513700
    },
    {
      "epoch": 490.6969205834684,
      "grad_norm": 1.423719048500061,
      "learning_rate": 9.506649367499191e-07,
      "loss": 2.4453,
      "step": 1513800
    },
    {
      "epoch": 490.7293354943274,
      "grad_norm": 1.4500505924224854,
      "learning_rate": 9.474213428478755e-07,
      "loss": 2.4445,
      "step": 1513900
    },
    {
      "epoch": 490.7617504051864,
      "grad_norm": 1.4421031475067139,
      "learning_rate": 9.44177748945832e-07,
      "loss": 2.4345,
      "step": 1514000
    },
    {
      "epoch": 490.7941653160454,
      "grad_norm": 1.4176689386367798,
      "learning_rate": 9.409341550437885e-07,
      "loss": 2.446,
      "step": 1514100
    },
    {
      "epoch": 490.82658022690435,
      "grad_norm": 1.584322214126587,
      "learning_rate": 9.376905611417451e-07,
      "loss": 2.4367,
      "step": 1514200
    },
    {
      "epoch": 490.8589951377634,
      "grad_norm": 1.6202141046524048,
      "learning_rate": 9.344469672397017e-07,
      "loss": 2.4478,
      "step": 1514300
    },
    {
      "epoch": 490.8914100486224,
      "grad_norm": 1.3556311130523682,
      "learning_rate": 9.312033733376581e-07,
      "loss": 2.4505,
      "step": 1514400
    },
    {
      "epoch": 490.92382495948135,
      "grad_norm": 1.401987910270691,
      "learning_rate": 9.279597794356148e-07,
      "loss": 2.4664,
      "step": 1514500
    },
    {
      "epoch": 490.95623987034037,
      "grad_norm": 1.4377385377883911,
      "learning_rate": 9.247161855335713e-07,
      "loss": 2.4352,
      "step": 1514600
    },
    {
      "epoch": 490.98865478119933,
      "grad_norm": 1.3250089883804321,
      "learning_rate": 9.214725916315277e-07,
      "loss": 2.4649,
      "step": 1514700
    },
    {
      "epoch": 491.0,
      "eval_bleu": 1.1536303312391094,
      "eval_loss": 4.286590576171875,
      "eval_runtime": 4.7668,
      "eval_samples_per_second": 103.214,
      "eval_steps_per_second": 1.678,
      "step": 1514735
    },
    {
      "epoch": 491.02106969205835,
      "grad_norm": 1.453273057937622,
      "learning_rate": 9.182289977294843e-07,
      "loss": 2.4702,
      "step": 1514800
    },
    {
      "epoch": 491.05348460291737,
      "grad_norm": 1.4343918561935425,
      "learning_rate": 9.149854038274409e-07,
      "loss": 2.4486,
      "step": 1514900
    },
    {
      "epoch": 491.0858995137763,
      "grad_norm": 1.391693353652954,
      "learning_rate": 9.117418099253974e-07,
      "loss": 2.456,
      "step": 1515000
    },
    {
      "epoch": 491.11831442463534,
      "grad_norm": 1.3970844745635986,
      "learning_rate": 9.084982160233539e-07,
      "loss": 2.4419,
      "step": 1515100
    },
    {
      "epoch": 491.1507293354943,
      "grad_norm": 1.5985287427902222,
      "learning_rate": 9.052546221213105e-07,
      "loss": 2.4333,
      "step": 1515200
    },
    {
      "epoch": 491.1831442463533,
      "grad_norm": 1.6772377490997314,
      "learning_rate": 9.020110282192671e-07,
      "loss": 2.465,
      "step": 1515300
    },
    {
      "epoch": 491.21555915721234,
      "grad_norm": 1.5270942449569702,
      "learning_rate": 8.987674343172235e-07,
      "loss": 2.4543,
      "step": 1515400
    },
    {
      "epoch": 491.2479740680713,
      "grad_norm": 1.344170093536377,
      "learning_rate": 8.9552384041518e-07,
      "loss": 2.4341,
      "step": 1515500
    },
    {
      "epoch": 491.2803889789303,
      "grad_norm": 1.442557692527771,
      "learning_rate": 8.922802465131366e-07,
      "loss": 2.441,
      "step": 1515600
    },
    {
      "epoch": 491.3128038897893,
      "grad_norm": 1.5940355062484741,
      "learning_rate": 8.890366526110931e-07,
      "loss": 2.4559,
      "step": 1515700
    },
    {
      "epoch": 491.3452188006483,
      "grad_norm": 1.4678317308425903,
      "learning_rate": 8.857930587090497e-07,
      "loss": 2.46,
      "step": 1515800
    },
    {
      "epoch": 491.3776337115073,
      "grad_norm": 1.6224137544631958,
      "learning_rate": 8.825494648070062e-07,
      "loss": 2.4464,
      "step": 1515900
    },
    {
      "epoch": 491.4100486223663,
      "grad_norm": 1.6289548873901367,
      "learning_rate": 8.793383068439832e-07,
      "loss": 2.4654,
      "step": 1516000
    },
    {
      "epoch": 491.4424635332253,
      "grad_norm": 1.2692307233810425,
      "learning_rate": 8.760947129419398e-07,
      "loss": 2.4403,
      "step": 1516100
    },
    {
      "epoch": 491.47487844408425,
      "grad_norm": 1.4752975702285767,
      "learning_rate": 8.728511190398962e-07,
      "loss": 2.4286,
      "step": 1516200
    },
    {
      "epoch": 491.5072933549433,
      "grad_norm": 1.4256024360656738,
      "learning_rate": 8.696075251378527e-07,
      "loss": 2.4472,
      "step": 1516300
    },
    {
      "epoch": 491.5397082658023,
      "grad_norm": 1.5239590406417847,
      "learning_rate": 8.663639312358093e-07,
      "loss": 2.4524,
      "step": 1516400
    },
    {
      "epoch": 491.57212317666125,
      "grad_norm": 1.3697667121887207,
      "learning_rate": 8.631203373337658e-07,
      "loss": 2.469,
      "step": 1516500
    },
    {
      "epoch": 491.60453808752027,
      "grad_norm": 1.5567944049835205,
      "learning_rate": 8.598767434317224e-07,
      "loss": 2.4186,
      "step": 1516600
    },
    {
      "epoch": 491.63695299837923,
      "grad_norm": 1.4945214986801147,
      "learning_rate": 8.56633149529679e-07,
      "loss": 2.4335,
      "step": 1516700
    },
    {
      "epoch": 491.66936790923825,
      "grad_norm": 1.2957797050476074,
      "learning_rate": 8.533895556276355e-07,
      "loss": 2.4503,
      "step": 1516800
    },
    {
      "epoch": 491.70178282009726,
      "grad_norm": 1.3699318170547485,
      "learning_rate": 8.50145961725592e-07,
      "loss": 2.4515,
      "step": 1516900
    },
    {
      "epoch": 491.7341977309562,
      "grad_norm": 1.391642689704895,
      "learning_rate": 8.469023678235484e-07,
      "loss": 2.4599,
      "step": 1517000
    },
    {
      "epoch": 491.76661264181524,
      "grad_norm": 1.669768214225769,
      "learning_rate": 8.436587739215052e-07,
      "loss": 2.4454,
      "step": 1517100
    },
    {
      "epoch": 491.7990275526742,
      "grad_norm": 1.4508665800094604,
      "learning_rate": 8.404151800194616e-07,
      "loss": 2.4439,
      "step": 1517200
    },
    {
      "epoch": 491.8314424635332,
      "grad_norm": 1.5337777137756348,
      "learning_rate": 8.371715861174181e-07,
      "loss": 2.4453,
      "step": 1517300
    },
    {
      "epoch": 491.86385737439224,
      "grad_norm": 1.455649495124817,
      "learning_rate": 8.339279922153746e-07,
      "loss": 2.4581,
      "step": 1517400
    },
    {
      "epoch": 491.8962722852512,
      "grad_norm": 1.464177131652832,
      "learning_rate": 8.306843983133312e-07,
      "loss": 2.4673,
      "step": 1517500
    },
    {
      "epoch": 491.9286871961102,
      "grad_norm": 1.4377663135528564,
      "learning_rate": 8.274408044112878e-07,
      "loss": 2.4491,
      "step": 1517600
    },
    {
      "epoch": 491.9611021069692,
      "grad_norm": 1.5737247467041016,
      "learning_rate": 8.241972105092443e-07,
      "loss": 2.4336,
      "step": 1517700
    },
    {
      "epoch": 491.9935170178282,
      "grad_norm": 1.3168505430221558,
      "learning_rate": 8.209536166072009e-07,
      "loss": 2.4385,
      "step": 1517800
    },
    {
      "epoch": 492.0,
      "eval_bleu": 1.1827362362820941,
      "eval_loss": 4.28706169128418,
      "eval_runtime": 4.5881,
      "eval_samples_per_second": 107.233,
      "eval_steps_per_second": 1.744,
      "step": 1517820
    },
    {
      "epoch": 492.0259319286872,
      "grad_norm": 1.5305652618408203,
      "learning_rate": 8.177100227051574e-07,
      "loss": 2.4261,
      "step": 1517900
    },
    {
      "epoch": 492.0583468395462,
      "grad_norm": 1.376144289970398,
      "learning_rate": 8.144664288031138e-07,
      "loss": 2.4404,
      "step": 1518000
    },
    {
      "epoch": 492.0907617504052,
      "grad_norm": 1.533568263053894,
      "learning_rate": 8.112228349010704e-07,
      "loss": 2.4431,
      "step": 1518100
    },
    {
      "epoch": 492.12317666126415,
      "grad_norm": 1.325772762298584,
      "learning_rate": 8.07979240999027e-07,
      "loss": 2.4489,
      "step": 1518200
    },
    {
      "epoch": 492.15559157212317,
      "grad_norm": 1.384736180305481,
      "learning_rate": 8.047356470969835e-07,
      "loss": 2.4405,
      "step": 1518300
    },
    {
      "epoch": 492.1880064829822,
      "grad_norm": 1.3580166101455688,
      "learning_rate": 8.0149205319494e-07,
      "loss": 2.4646,
      "step": 1518400
    },
    {
      "epoch": 492.22042139384115,
      "grad_norm": 1.450156807899475,
      "learning_rate": 7.982484592928966e-07,
      "loss": 2.4351,
      "step": 1518500
    },
    {
      "epoch": 492.25283630470017,
      "grad_norm": 1.3420586585998535,
      "learning_rate": 7.950048653908532e-07,
      "loss": 2.4486,
      "step": 1518600
    },
    {
      "epoch": 492.2852512155592,
      "grad_norm": 1.387912392616272,
      "learning_rate": 7.917612714888096e-07,
      "loss": 2.4454,
      "step": 1518700
    },
    {
      "epoch": 492.31766612641815,
      "grad_norm": 1.3625282049179077,
      "learning_rate": 7.885176775867661e-07,
      "loss": 2.4336,
      "step": 1518800
    },
    {
      "epoch": 492.35008103727716,
      "grad_norm": 1.5244920253753662,
      "learning_rate": 7.852740836847227e-07,
      "loss": 2.466,
      "step": 1518900
    },
    {
      "epoch": 492.3824959481361,
      "grad_norm": 1.534045934677124,
      "learning_rate": 7.820304897826792e-07,
      "loss": 2.4457,
      "step": 1519000
    },
    {
      "epoch": 492.41491085899514,
      "grad_norm": 1.4597675800323486,
      "learning_rate": 7.787868958806358e-07,
      "loss": 2.4503,
      "step": 1519100
    },
    {
      "epoch": 492.44732576985416,
      "grad_norm": 1.4379417896270752,
      "learning_rate": 7.755433019785923e-07,
      "loss": 2.4398,
      "step": 1519200
    },
    {
      "epoch": 492.4797406807131,
      "grad_norm": 1.4620505571365356,
      "learning_rate": 7.722997080765489e-07,
      "loss": 2.4505,
      "step": 1519300
    },
    {
      "epoch": 492.51215559157214,
      "grad_norm": 1.5481394529342651,
      "learning_rate": 7.690561141745054e-07,
      "loss": 2.4436,
      "step": 1519400
    },
    {
      "epoch": 492.5445705024311,
      "grad_norm": 1.3378167152404785,
      "learning_rate": 7.65812520272462e-07,
      "loss": 2.4546,
      "step": 1519500
    },
    {
      "epoch": 492.5769854132901,
      "grad_norm": 1.617820382118225,
      "learning_rate": 7.625689263704185e-07,
      "loss": 2.4439,
      "step": 1519600
    },
    {
      "epoch": 492.60940032414914,
      "grad_norm": 1.593737006187439,
      "learning_rate": 7.59325332468375e-07,
      "loss": 2.4519,
      "step": 1519700
    },
    {
      "epoch": 492.6418152350081,
      "grad_norm": 1.3399304151535034,
      "learning_rate": 7.560817385663315e-07,
      "loss": 2.477,
      "step": 1519800
    },
    {
      "epoch": 492.6742301458671,
      "grad_norm": 1.4558911323547363,
      "learning_rate": 7.528381446642881e-07,
      "loss": 2.4453,
      "step": 1519900
    },
    {
      "epoch": 492.7066450567261,
      "grad_norm": 1.5804717540740967,
      "learning_rate": 7.49626986701265e-07,
      "loss": 2.4407,
      "step": 1520000
    },
    {
      "epoch": 492.7390599675851,
      "grad_norm": 1.5329484939575195,
      "learning_rate": 7.464482646772625e-07,
      "loss": 2.4525,
      "step": 1520100
    },
    {
      "epoch": 492.7714748784441,
      "grad_norm": 1.4130632877349854,
      "learning_rate": 7.43204670775219e-07,
      "loss": 2.4563,
      "step": 1520200
    },
    {
      "epoch": 492.80388978930307,
      "grad_norm": 1.4851791858673096,
      "learning_rate": 7.399610768731755e-07,
      "loss": 2.4506,
      "step": 1520300
    },
    {
      "epoch": 492.8363047001621,
      "grad_norm": 1.3671897649765015,
      "learning_rate": 7.36717482971132e-07,
      "loss": 2.4492,
      "step": 1520400
    },
    {
      "epoch": 492.86871961102105,
      "grad_norm": 1.338750958442688,
      "learning_rate": 7.334738890690885e-07,
      "loss": 2.4568,
      "step": 1520500
    },
    {
      "epoch": 492.90113452188007,
      "grad_norm": 1.293012022972107,
      "learning_rate": 7.302302951670452e-07,
      "loss": 2.4611,
      "step": 1520600
    },
    {
      "epoch": 492.9335494327391,
      "grad_norm": 1.5204904079437256,
      "learning_rate": 7.269867012650017e-07,
      "loss": 2.4364,
      "step": 1520700
    },
    {
      "epoch": 492.96596434359805,
      "grad_norm": 1.437333106994629,
      "learning_rate": 7.237431073629582e-07,
      "loss": 2.4506,
      "step": 1520800
    },
    {
      "epoch": 492.99837925445706,
      "grad_norm": 1.3837480545043945,
      "learning_rate": 7.204995134609147e-07,
      "loss": 2.4584,
      "step": 1520900
    },
    {
      "epoch": 493.0,
      "eval_bleu": 1.1945915379118635,
      "eval_loss": 4.286932945251465,
      "eval_runtime": 4.5821,
      "eval_samples_per_second": 107.375,
      "eval_steps_per_second": 1.746,
      "step": 1520905
    },
    {
      "epoch": 493.030794165316,
      "grad_norm": 1.540300965309143,
      "learning_rate": 7.172559195588713e-07,
      "loss": 2.4714,
      "step": 1521000
    },
    {
      "epoch": 493.06320907617504,
      "grad_norm": 1.5230666399002075,
      "learning_rate": 7.140123256568278e-07,
      "loss": 2.4418,
      "step": 1521100
    },
    {
      "epoch": 493.09562398703406,
      "grad_norm": 1.39883553981781,
      "learning_rate": 7.107687317547843e-07,
      "loss": 2.4674,
      "step": 1521200
    },
    {
      "epoch": 493.128038897893,
      "grad_norm": 1.3467836380004883,
      "learning_rate": 7.075251378527409e-07,
      "loss": 2.4394,
      "step": 1521300
    },
    {
      "epoch": 493.16045380875204,
      "grad_norm": 1.3522709608078003,
      "learning_rate": 7.042815439506973e-07,
      "loss": 2.473,
      "step": 1521400
    },
    {
      "epoch": 493.192868719611,
      "grad_norm": 1.4483226537704468,
      "learning_rate": 7.01037950048654e-07,
      "loss": 2.427,
      "step": 1521500
    },
    {
      "epoch": 493.22528363047,
      "grad_norm": 1.358325719833374,
      "learning_rate": 6.977943561466105e-07,
      "loss": 2.4412,
      "step": 1521600
    },
    {
      "epoch": 493.25769854132903,
      "grad_norm": 1.4345366954803467,
      "learning_rate": 6.94550762244567e-07,
      "loss": 2.4669,
      "step": 1521700
    },
    {
      "epoch": 493.290113452188,
      "grad_norm": 1.551898717880249,
      "learning_rate": 6.913071683425236e-07,
      "loss": 2.4566,
      "step": 1521800
    },
    {
      "epoch": 493.322528363047,
      "grad_norm": 1.3601856231689453,
      "learning_rate": 6.8806357444048e-07,
      "loss": 2.4365,
      "step": 1521900
    },
    {
      "epoch": 493.354943273906,
      "grad_norm": 1.4332671165466309,
      "learning_rate": 6.848199805384367e-07,
      "loss": 2.44,
      "step": 1522000
    },
    {
      "epoch": 493.387358184765,
      "grad_norm": 1.4422719478607178,
      "learning_rate": 6.815763866363932e-07,
      "loss": 2.4555,
      "step": 1522100
    },
    {
      "epoch": 493.419773095624,
      "grad_norm": 1.5346248149871826,
      "learning_rate": 6.783327927343497e-07,
      "loss": 2.4218,
      "step": 1522200
    },
    {
      "epoch": 493.45218800648297,
      "grad_norm": 1.6864597797393799,
      "learning_rate": 6.750891988323062e-07,
      "loss": 2.4563,
      "step": 1522300
    },
    {
      "epoch": 493.484602917342,
      "grad_norm": 1.5220080614089966,
      "learning_rate": 6.718456049302627e-07,
      "loss": 2.4528,
      "step": 1522400
    },
    {
      "epoch": 493.51701782820095,
      "grad_norm": 1.360263705253601,
      "learning_rate": 6.686020110282194e-07,
      "loss": 2.4443,
      "step": 1522500
    },
    {
      "epoch": 493.54943273905997,
      "grad_norm": 1.4722599983215332,
      "learning_rate": 6.653584171261759e-07,
      "loss": 2.4387,
      "step": 1522600
    },
    {
      "epoch": 493.581847649919,
      "grad_norm": 1.3721637725830078,
      "learning_rate": 6.621148232241324e-07,
      "loss": 2.4838,
      "step": 1522700
    },
    {
      "epoch": 493.61426256077795,
      "grad_norm": 1.435193657875061,
      "learning_rate": 6.588712293220889e-07,
      "loss": 2.4445,
      "step": 1522800
    },
    {
      "epoch": 493.64667747163696,
      "grad_norm": 1.568830966949463,
      "learning_rate": 6.556276354200454e-07,
      "loss": 2.4501,
      "step": 1522900
    },
    {
      "epoch": 493.6790923824959,
      "grad_norm": 1.3495848178863525,
      "learning_rate": 6.52384041518002e-07,
      "loss": 2.4274,
      "step": 1523000
    },
    {
      "epoch": 493.71150729335494,
      "grad_norm": 1.6059471368789673,
      "learning_rate": 6.491404476159585e-07,
      "loss": 2.4341,
      "step": 1523100
    },
    {
      "epoch": 493.74392220421396,
      "grad_norm": 1.5165314674377441,
      "learning_rate": 6.45896853713915e-07,
      "loss": 2.4416,
      "step": 1523200
    },
    {
      "epoch": 493.7763371150729,
      "grad_norm": 1.989180564880371,
      "learning_rate": 6.426532598118716e-07,
      "loss": 2.4564,
      "step": 1523300
    },
    {
      "epoch": 493.80875202593194,
      "grad_norm": 1.4899506568908691,
      "learning_rate": 6.394096659098281e-07,
      "loss": 2.4318,
      "step": 1523400
    },
    {
      "epoch": 493.8411669367909,
      "grad_norm": 1.4653879404067993,
      "learning_rate": 6.361985079468051e-07,
      "loss": 2.4472,
      "step": 1523500
    },
    {
      "epoch": 493.8735818476499,
      "grad_norm": 1.5683954954147339,
      "learning_rate": 6.329549140447616e-07,
      "loss": 2.4451,
      "step": 1523600
    },
    {
      "epoch": 493.90599675850893,
      "grad_norm": 1.5107030868530273,
      "learning_rate": 6.297113201427181e-07,
      "loss": 2.4653,
      "step": 1523700
    },
    {
      "epoch": 493.9384116693679,
      "grad_norm": 1.5962331295013428,
      "learning_rate": 6.264677262406747e-07,
      "loss": 2.4536,
      "step": 1523800
    },
    {
      "epoch": 493.9708265802269,
      "grad_norm": 1.2367318868637085,
      "learning_rate": 6.232241323386313e-07,
      "loss": 2.4321,
      "step": 1523900
    },
    {
      "epoch": 494.0,
      "eval_bleu": 1.1822098930128668,
      "eval_loss": 4.287677764892578,
      "eval_runtime": 4.5527,
      "eval_samples_per_second": 108.067,
      "eval_steps_per_second": 1.757,
      "step": 1523990
    },
    {
      "epoch": 494.0032414910859,
      "grad_norm": 1.3812426328659058,
      "learning_rate": 6.199805384365878e-07,
      "loss": 2.4384,
      "step": 1524000
    },
    {
      "epoch": 494.0356564019449,
      "grad_norm": 1.474693775177002,
      "learning_rate": 6.167369445345443e-07,
      "loss": 2.4507,
      "step": 1524100
    },
    {
      "epoch": 494.0680713128039,
      "grad_norm": 1.497280478477478,
      "learning_rate": 6.134933506325008e-07,
      "loss": 2.431,
      "step": 1524200
    },
    {
      "epoch": 494.10048622366287,
      "grad_norm": 1.3418511152267456,
      "learning_rate": 6.102497567304574e-07,
      "loss": 2.4567,
      "step": 1524300
    },
    {
      "epoch": 494.1329011345219,
      "grad_norm": 1.3061615228652954,
      "learning_rate": 6.07006162828414e-07,
      "loss": 2.4774,
      "step": 1524400
    },
    {
      "epoch": 494.16531604538085,
      "grad_norm": 1.5080338716506958,
      "learning_rate": 6.037625689263704e-07,
      "loss": 2.4454,
      "step": 1524500
    },
    {
      "epoch": 494.19773095623987,
      "grad_norm": 1.3331966400146484,
      "learning_rate": 6.00518975024327e-07,
      "loss": 2.4311,
      "step": 1524600
    },
    {
      "epoch": 494.2301458670989,
      "grad_norm": 1.3950616121292114,
      "learning_rate": 5.972753811222834e-07,
      "loss": 2.432,
      "step": 1524700
    },
    {
      "epoch": 494.26256077795784,
      "grad_norm": 1.5644242763519287,
      "learning_rate": 5.940317872202401e-07,
      "loss": 2.4327,
      "step": 1524800
    },
    {
      "epoch": 494.29497568881686,
      "grad_norm": 1.5976248979568481,
      "learning_rate": 5.907881933181966e-07,
      "loss": 2.4725,
      "step": 1524900
    },
    {
      "epoch": 494.3273905996758,
      "grad_norm": 1.531783938407898,
      "learning_rate": 5.875445994161531e-07,
      "loss": 2.4526,
      "step": 1525000
    },
    {
      "epoch": 494.35980551053484,
      "grad_norm": 1.3891500234603882,
      "learning_rate": 5.843010055141097e-07,
      "loss": 2.4403,
      "step": 1525100
    },
    {
      "epoch": 494.39222042139386,
      "grad_norm": 1.4913278818130493,
      "learning_rate": 5.810574116120661e-07,
      "loss": 2.4377,
      "step": 1525200
    },
    {
      "epoch": 494.4246353322528,
      "grad_norm": 1.2698156833648682,
      "learning_rate": 5.778138177100228e-07,
      "loss": 2.4568,
      "step": 1525300
    },
    {
      "epoch": 494.45705024311184,
      "grad_norm": 1.4779208898544312,
      "learning_rate": 5.745702238079793e-07,
      "loss": 2.4513,
      "step": 1525400
    },
    {
      "epoch": 494.48946515397085,
      "grad_norm": 1.3962548971176147,
      "learning_rate": 5.713266299059358e-07,
      "loss": 2.4578,
      "step": 1525500
    },
    {
      "epoch": 494.5218800648298,
      "grad_norm": 1.5196775197982788,
      "learning_rate": 5.680830360038923e-07,
      "loss": 2.4335,
      "step": 1525600
    },
    {
      "epoch": 494.55429497568883,
      "grad_norm": 1.4781368970870972,
      "learning_rate": 5.648394421018488e-07,
      "loss": 2.4724,
      "step": 1525700
    },
    {
      "epoch": 494.5867098865478,
      "grad_norm": 1.5667929649353027,
      "learning_rate": 5.615958481998055e-07,
      "loss": 2.4557,
      "step": 1525800
    },
    {
      "epoch": 494.6191247974068,
      "grad_norm": 1.6413427591323853,
      "learning_rate": 5.58352254297762e-07,
      "loss": 2.4609,
      "step": 1525900
    },
    {
      "epoch": 494.65153970826583,
      "grad_norm": 1.289330005645752,
      "learning_rate": 5.551086603957185e-07,
      "loss": 2.4343,
      "step": 1526000
    },
    {
      "epoch": 494.6839546191248,
      "grad_norm": 1.4957752227783203,
      "learning_rate": 5.51865066493675e-07,
      "loss": 2.4625,
      "step": 1526100
    },
    {
      "epoch": 494.7163695299838,
      "grad_norm": 1.659665584564209,
      "learning_rate": 5.486214725916315e-07,
      "loss": 2.4392,
      "step": 1526200
    },
    {
      "epoch": 494.74878444084277,
      "grad_norm": 1.378824234008789,
      "learning_rate": 5.453778786895881e-07,
      "loss": 2.4571,
      "step": 1526300
    },
    {
      "epoch": 494.7811993517018,
      "grad_norm": 1.6127475500106812,
      "learning_rate": 5.421342847875447e-07,
      "loss": 2.4562,
      "step": 1526400
    },
    {
      "epoch": 494.8136142625608,
      "grad_norm": 1.4529908895492554,
      "learning_rate": 5.388906908855011e-07,
      "loss": 2.4368,
      "step": 1526500
    },
    {
      "epoch": 494.84602917341977,
      "grad_norm": 1.4873889684677124,
      "learning_rate": 5.356470969834577e-07,
      "loss": 2.43,
      "step": 1526600
    },
    {
      "epoch": 494.8784440842788,
      "grad_norm": 1.5970031023025513,
      "learning_rate": 5.324035030814142e-07,
      "loss": 2.4489,
      "step": 1526700
    },
    {
      "epoch": 494.91085899513774,
      "grad_norm": 1.2503434419631958,
      "learning_rate": 5.291599091793708e-07,
      "loss": 2.4336,
      "step": 1526800
    },
    {
      "epoch": 494.94327390599676,
      "grad_norm": 1.488980770111084,
      "learning_rate": 5.259163152773273e-07,
      "loss": 2.4375,
      "step": 1526900
    },
    {
      "epoch": 494.9756888168558,
      "grad_norm": 1.3602190017700195,
      "learning_rate": 5.226727213752838e-07,
      "loss": 2.4474,
      "step": 1527000
    },
    {
      "epoch": 495.0,
      "eval_bleu": 1.1562590744008792,
      "eval_loss": 4.287039756774902,
      "eval_runtime": 4.5842,
      "eval_samples_per_second": 107.324,
      "eval_steps_per_second": 1.745,
      "step": 1527075
    },
    {
      "epoch": 495.00810372771474,
      "grad_norm": 1.4450045824050903,
      "learning_rate": 5.194291274732404e-07,
      "loss": 2.4559,
      "step": 1527100
    },
    {
      "epoch": 495.04051863857376,
      "grad_norm": 1.3989150524139404,
      "learning_rate": 5.161855335711968e-07,
      "loss": 2.4356,
      "step": 1527200
    },
    {
      "epoch": 495.0729335494327,
      "grad_norm": 1.485352635383606,
      "learning_rate": 5.129419396691535e-07,
      "loss": 2.4234,
      "step": 1527300
    },
    {
      "epoch": 495.10534846029174,
      "grad_norm": 1.4281986951828003,
      "learning_rate": 5.0969834576711e-07,
      "loss": 2.4371,
      "step": 1527400
    },
    {
      "epoch": 495.13776337115075,
      "grad_norm": 1.5867162942886353,
      "learning_rate": 5.064547518650665e-07,
      "loss": 2.4599,
      "step": 1527500
    },
    {
      "epoch": 495.1701782820097,
      "grad_norm": 1.3672566413879395,
      "learning_rate": 5.032111579630231e-07,
      "loss": 2.4466,
      "step": 1527600
    },
    {
      "epoch": 495.20259319286873,
      "grad_norm": 1.5897343158721924,
      "learning_rate": 4.999675640609795e-07,
      "loss": 2.4163,
      "step": 1527700
    },
    {
      "epoch": 495.2350081037277,
      "grad_norm": 1.4825189113616943,
      "learning_rate": 4.967239701589362e-07,
      "loss": 2.4329,
      "step": 1527800
    },
    {
      "epoch": 495.2674230145867,
      "grad_norm": 1.469922423362732,
      "learning_rate": 4.934803762568927e-07,
      "loss": 2.443,
      "step": 1527900
    },
    {
      "epoch": 495.29983792544573,
      "grad_norm": 1.4470511674880981,
      "learning_rate": 4.902367823548492e-07,
      "loss": 2.4539,
      "step": 1528000
    },
    {
      "epoch": 495.3322528363047,
      "grad_norm": 1.4060029983520508,
      "learning_rate": 4.869931884528057e-07,
      "loss": 2.4437,
      "step": 1528100
    },
    {
      "epoch": 495.3646677471637,
      "grad_norm": 1.534729242324829,
      "learning_rate": 4.837495945507622e-07,
      "loss": 2.4345,
      "step": 1528200
    },
    {
      "epoch": 495.39708265802267,
      "grad_norm": 1.4537512063980103,
      "learning_rate": 4.805060006487188e-07,
      "loss": 2.4493,
      "step": 1528300
    },
    {
      "epoch": 495.4294975688817,
      "grad_norm": 1.7131621837615967,
      "learning_rate": 4.772624067466754e-07,
      "loss": 2.4561,
      "step": 1528400
    },
    {
      "epoch": 495.4619124797407,
      "grad_norm": 1.4150348901748657,
      "learning_rate": 4.740188128446319e-07,
      "loss": 2.4575,
      "step": 1528500
    },
    {
      "epoch": 495.49432739059966,
      "grad_norm": 1.5869510173797607,
      "learning_rate": 4.707752189425884e-07,
      "loss": 2.4684,
      "step": 1528600
    },
    {
      "epoch": 495.5267423014587,
      "grad_norm": 1.2879462242126465,
      "learning_rate": 4.6753162504054495e-07,
      "loss": 2.4391,
      "step": 1528700
    },
    {
      "epoch": 495.55915721231764,
      "grad_norm": 1.639865756034851,
      "learning_rate": 4.6428803113850147e-07,
      "loss": 2.4437,
      "step": 1528800
    },
    {
      "epoch": 495.59157212317666,
      "grad_norm": 1.3944602012634277,
      "learning_rate": 4.6104443723645804e-07,
      "loss": 2.4402,
      "step": 1528900
    },
    {
      "epoch": 495.6239870340357,
      "grad_norm": 1.3784420490264893,
      "learning_rate": 4.578008433344145e-07,
      "loss": 2.4287,
      "step": 1529000
    },
    {
      "epoch": 495.65640194489464,
      "grad_norm": 1.4605973958969116,
      "learning_rate": 4.545572494323711e-07,
      "loss": 2.4355,
      "step": 1529100
    },
    {
      "epoch": 495.68881685575366,
      "grad_norm": 1.358365535736084,
      "learning_rate": 4.513136555303276e-07,
      "loss": 2.4665,
      "step": 1529200
    },
    {
      "epoch": 495.7212317666126,
      "grad_norm": 1.3745713233947754,
      "learning_rate": 4.4807006162828417e-07,
      "loss": 2.4549,
      "step": 1529300
    },
    {
      "epoch": 495.75364667747164,
      "grad_norm": 1.3233140707015991,
      "learning_rate": 4.4482646772624074e-07,
      "loss": 2.4576,
      "step": 1529400
    },
    {
      "epoch": 495.78606158833065,
      "grad_norm": 1.4196857213974,
      "learning_rate": 4.416153097632177e-07,
      "loss": 2.4704,
      "step": 1529500
    },
    {
      "epoch": 495.8184764991896,
      "grad_norm": 1.5800679922103882,
      "learning_rate": 4.3837171586117416e-07,
      "loss": 2.4649,
      "step": 1529600
    },
    {
      "epoch": 495.85089141004863,
      "grad_norm": 1.4118820428848267,
      "learning_rate": 4.3512812195913074e-07,
      "loss": 2.4464,
      "step": 1529700
    },
    {
      "epoch": 495.8833063209076,
      "grad_norm": 1.3704288005828857,
      "learning_rate": 4.318845280570873e-07,
      "loss": 2.4555,
      "step": 1529800
    },
    {
      "epoch": 495.9157212317666,
      "grad_norm": 1.518232822418213,
      "learning_rate": 4.2864093415504383e-07,
      "loss": 2.451,
      "step": 1529900
    },
    {
      "epoch": 495.94813614262563,
      "grad_norm": 1.3300535678863525,
      "learning_rate": 4.253973402530004e-07,
      "loss": 2.4517,
      "step": 1530000
    },
    {
      "epoch": 495.9805510534846,
      "grad_norm": 1.5792336463928223,
      "learning_rate": 4.2215374635095687e-07,
      "loss": 2.4663,
      "step": 1530100
    },
    {
      "epoch": 496.0,
      "eval_bleu": 1.18602916747843,
      "eval_loss": 4.287602424621582,
      "eval_runtime": 4.4478,
      "eval_samples_per_second": 110.617,
      "eval_steps_per_second": 1.799,
      "step": 1530160
    },
    {
      "epoch": 496.0129659643436,
      "grad_norm": 1.366186261177063,
      "learning_rate": 4.1891015244891344e-07,
      "loss": 2.4324,
      "step": 1530200
    },
    {
      "epoch": 496.04538087520257,
      "grad_norm": 1.470534086227417,
      "learning_rate": 4.156665585468699e-07,
      "loss": 2.4651,
      "step": 1530300
    },
    {
      "epoch": 496.0777957860616,
      "grad_norm": 1.6328932046890259,
      "learning_rate": 4.124229646448265e-07,
      "loss": 2.4526,
      "step": 1530400
    },
    {
      "epoch": 496.1102106969206,
      "grad_norm": 1.5220258235931396,
      "learning_rate": 4.09179370742783e-07,
      "loss": 2.4532,
      "step": 1530500
    },
    {
      "epoch": 496.14262560777956,
      "grad_norm": 1.4933264255523682,
      "learning_rate": 4.0593577684073957e-07,
      "loss": 2.4367,
      "step": 1530600
    },
    {
      "epoch": 496.1750405186386,
      "grad_norm": 1.3032692670822144,
      "learning_rate": 4.0269218293869614e-07,
      "loss": 2.4399,
      "step": 1530700
    },
    {
      "epoch": 496.20745542949754,
      "grad_norm": 1.5066300630569458,
      "learning_rate": 3.994485890366526e-07,
      "loss": 2.4577,
      "step": 1530800
    },
    {
      "epoch": 496.23987034035656,
      "grad_norm": 1.3427854776382446,
      "learning_rate": 3.962049951346092e-07,
      "loss": 2.4283,
      "step": 1530900
    },
    {
      "epoch": 496.2722852512156,
      "grad_norm": 1.4977473020553589,
      "learning_rate": 3.929614012325657e-07,
      "loss": 2.4261,
      "step": 1531000
    },
    {
      "epoch": 496.30470016207454,
      "grad_norm": 1.3742719888687134,
      "learning_rate": 3.897178073305222e-07,
      "loss": 2.4657,
      "step": 1531100
    },
    {
      "epoch": 496.33711507293356,
      "grad_norm": 1.5123826265335083,
      "learning_rate": 3.864742134284788e-07,
      "loss": 2.456,
      "step": 1531200
    },
    {
      "epoch": 496.3695299837925,
      "grad_norm": 1.5464699268341064,
      "learning_rate": 3.832306195264353e-07,
      "loss": 2.445,
      "step": 1531300
    },
    {
      "epoch": 496.40194489465154,
      "grad_norm": 1.1887942552566528,
      "learning_rate": 3.7998702562439183e-07,
      "loss": 2.4622,
      "step": 1531400
    },
    {
      "epoch": 496.43435980551055,
      "grad_norm": 1.38742995262146,
      "learning_rate": 3.767758676613688e-07,
      "loss": 2.458,
      "step": 1531500
    },
    {
      "epoch": 496.4667747163695,
      "grad_norm": 1.40122652053833,
      "learning_rate": 3.7353227375932536e-07,
      "loss": 2.4612,
      "step": 1531600
    },
    {
      "epoch": 496.49918962722853,
      "grad_norm": 1.282629132270813,
      "learning_rate": 3.702886798572819e-07,
      "loss": 2.4366,
      "step": 1531700
    },
    {
      "epoch": 496.5316045380875,
      "grad_norm": 1.4401514530181885,
      "learning_rate": 3.6704508595523845e-07,
      "loss": 2.4363,
      "step": 1531800
    },
    {
      "epoch": 496.5640194489465,
      "grad_norm": 1.3876280784606934,
      "learning_rate": 3.6380149205319497e-07,
      "loss": 2.4542,
      "step": 1531900
    },
    {
      "epoch": 496.5964343598055,
      "grad_norm": 1.364829659461975,
      "learning_rate": 3.605578981511515e-07,
      "loss": 2.4297,
      "step": 1532000
    },
    {
      "epoch": 496.6288492706645,
      "grad_norm": 1.5803673267364502,
      "learning_rate": 3.57314304249108e-07,
      "loss": 2.4536,
      "step": 1532100
    },
    {
      "epoch": 496.6612641815235,
      "grad_norm": 1.3974370956420898,
      "learning_rate": 3.540707103470646e-07,
      "loss": 2.4445,
      "step": 1532200
    },
    {
      "epoch": 496.6936790923825,
      "grad_norm": 1.526903748512268,
      "learning_rate": 3.508271164450211e-07,
      "loss": 2.4454,
      "step": 1532300
    },
    {
      "epoch": 496.7260940032415,
      "grad_norm": 1.4873133897781372,
      "learning_rate": 3.475835225429776e-07,
      "loss": 2.4513,
      "step": 1532400
    },
    {
      "epoch": 496.7585089141005,
      "grad_norm": 1.542394995689392,
      "learning_rate": 3.443399286409342e-07,
      "loss": 2.4504,
      "step": 1532500
    },
    {
      "epoch": 496.79092382495946,
      "grad_norm": 1.432591438293457,
      "learning_rate": 3.410963347388907e-07,
      "loss": 2.4434,
      "step": 1532600
    },
    {
      "epoch": 496.8233387358185,
      "grad_norm": 1.6437666416168213,
      "learning_rate": 3.3785274083684723e-07,
      "loss": 2.4518,
      "step": 1532700
    },
    {
      "epoch": 496.8557536466775,
      "grad_norm": 1.4821455478668213,
      "learning_rate": 3.346091469348038e-07,
      "loss": 2.4638,
      "step": 1532800
    },
    {
      "epoch": 496.88816855753646,
      "grad_norm": 1.57512629032135,
      "learning_rate": 3.313655530327603e-07,
      "loss": 2.4326,
      "step": 1532900
    },
    {
      "epoch": 496.9205834683955,
      "grad_norm": 1.536126971244812,
      "learning_rate": 3.2812195913071684e-07,
      "loss": 2.46,
      "step": 1533000
    },
    {
      "epoch": 496.95299837925444,
      "grad_norm": 1.4155819416046143,
      "learning_rate": 3.2487836522867336e-07,
      "loss": 2.469,
      "step": 1533100
    },
    {
      "epoch": 496.98541329011346,
      "grad_norm": 1.522581696510315,
      "learning_rate": 3.2166720726565036e-07,
      "loss": 2.4275,
      "step": 1533200
    },
    {
      "epoch": 497.0,
      "eval_bleu": 1.1731784527416813,
      "eval_loss": 4.287563323974609,
      "eval_runtime": 4.5889,
      "eval_samples_per_second": 107.216,
      "eval_steps_per_second": 1.743,
      "step": 1533245
    },
    {
      "epoch": 497.0178282009725,
      "grad_norm": 1.5114834308624268,
      "learning_rate": 3.184236133636069e-07,
      "loss": 2.4654,
      "step": 1533300
    },
    {
      "epoch": 497.05024311183143,
      "grad_norm": 1.4649815559387207,
      "learning_rate": 3.151800194615634e-07,
      "loss": 2.4459,
      "step": 1533400
    },
    {
      "epoch": 497.08265802269045,
      "grad_norm": 1.5188543796539307,
      "learning_rate": 3.1193642555952e-07,
      "loss": 2.4453,
      "step": 1533500
    },
    {
      "epoch": 497.1150729335494,
      "grad_norm": 1.5630321502685547,
      "learning_rate": 3.086928316574765e-07,
      "loss": 2.4357,
      "step": 1533600
    },
    {
      "epoch": 497.14748784440843,
      "grad_norm": 1.3839327096939087,
      "learning_rate": 3.05449237755433e-07,
      "loss": 2.4511,
      "step": 1533700
    },
    {
      "epoch": 497.17990275526745,
      "grad_norm": 1.4887242317199707,
      "learning_rate": 3.0220564385338953e-07,
      "loss": 2.4476,
      "step": 1533800
    },
    {
      "epoch": 497.2123176661264,
      "grad_norm": 1.4823628664016724,
      "learning_rate": 2.989620499513461e-07,
      "loss": 2.4341,
      "step": 1533900
    },
    {
      "epoch": 497.2447325769854,
      "grad_norm": 1.3256241083145142,
      "learning_rate": 2.957184560493027e-07,
      "loss": 2.4811,
      "step": 1534000
    },
    {
      "epoch": 497.2771474878444,
      "grad_norm": 1.330309510231018,
      "learning_rate": 2.924748621472592e-07,
      "loss": 2.436,
      "step": 1534100
    },
    {
      "epoch": 497.3095623987034,
      "grad_norm": 1.4070607423782349,
      "learning_rate": 2.892312682452157e-07,
      "loss": 2.4372,
      "step": 1534200
    },
    {
      "epoch": 497.3419773095624,
      "grad_norm": 1.4011930227279663,
      "learning_rate": 2.8598767434317224e-07,
      "loss": 2.4329,
      "step": 1534300
    },
    {
      "epoch": 497.3743922204214,
      "grad_norm": 1.4019997119903564,
      "learning_rate": 2.8274408044112876e-07,
      "loss": 2.4508,
      "step": 1534400
    },
    {
      "epoch": 497.4068071312804,
      "grad_norm": 1.4997788667678833,
      "learning_rate": 2.7950048653908533e-07,
      "loss": 2.4587,
      "step": 1534500
    },
    {
      "epoch": 497.43922204213936,
      "grad_norm": 1.3545490503311157,
      "learning_rate": 2.7625689263704185e-07,
      "loss": 2.445,
      "step": 1534600
    },
    {
      "epoch": 497.4716369529984,
      "grad_norm": 1.4088901281356812,
      "learning_rate": 2.7301329873499837e-07,
      "loss": 2.45,
      "step": 1534700
    },
    {
      "epoch": 497.5040518638574,
      "grad_norm": 1.3852808475494385,
      "learning_rate": 2.6976970483295494e-07,
      "loss": 2.4369,
      "step": 1534800
    },
    {
      "epoch": 497.53646677471636,
      "grad_norm": 1.5498874187469482,
      "learning_rate": 2.6652611093091146e-07,
      "loss": 2.4345,
      "step": 1534900
    },
    {
      "epoch": 497.5688816855754,
      "grad_norm": 1.4841458797454834,
      "learning_rate": 2.6328251702886803e-07,
      "loss": 2.4532,
      "step": 1535000
    },
    {
      "epoch": 497.60129659643434,
      "grad_norm": 1.3668291568756104,
      "learning_rate": 2.6003892312682455e-07,
      "loss": 2.4652,
      "step": 1535100
    },
    {
      "epoch": 497.63371150729336,
      "grad_norm": 1.6118491888046265,
      "learning_rate": 2.5679532922478107e-07,
      "loss": 2.4447,
      "step": 1535200
    },
    {
      "epoch": 497.6661264181524,
      "grad_norm": 1.699951410293579,
      "learning_rate": 2.535517353227376e-07,
      "loss": 2.4439,
      "step": 1535300
    },
    {
      "epoch": 497.69854132901133,
      "grad_norm": 1.446325421333313,
      "learning_rate": 2.503081414206941e-07,
      "loss": 2.454,
      "step": 1535400
    },
    {
      "epoch": 497.73095623987035,
      "grad_norm": 1.3532060384750366,
      "learning_rate": 2.470645475186507e-07,
      "loss": 2.4384,
      "step": 1535500
    },
    {
      "epoch": 497.7633711507293,
      "grad_norm": 1.4994956254959106,
      "learning_rate": 2.438209536166072e-07,
      "loss": 2.4546,
      "step": 1535600
    },
    {
      "epoch": 497.79578606158833,
      "grad_norm": 1.3312835693359375,
      "learning_rate": 2.4057735971456377e-07,
      "loss": 2.4422,
      "step": 1535700
    },
    {
      "epoch": 497.82820097244735,
      "grad_norm": 1.540209174156189,
      "learning_rate": 2.373337658125203e-07,
      "loss": 2.4302,
      "step": 1535800
    },
    {
      "epoch": 497.8606158833063,
      "grad_norm": 1.4306256771087646,
      "learning_rate": 2.3409017191047684e-07,
      "loss": 2.4419,
      "step": 1535900
    },
    {
      "epoch": 497.8930307941653,
      "grad_norm": 1.3888057470321655,
      "learning_rate": 2.3084657800843336e-07,
      "loss": 2.4441,
      "step": 1536000
    },
    {
      "epoch": 497.9254457050243,
      "grad_norm": 1.577316164970398,
      "learning_rate": 2.276029841063899e-07,
      "loss": 2.463,
      "step": 1536100
    },
    {
      "epoch": 497.9578606158833,
      "grad_norm": 1.4843602180480957,
      "learning_rate": 2.2435939020434642e-07,
      "loss": 2.4688,
      "step": 1536200
    },
    {
      "epoch": 497.9902755267423,
      "grad_norm": 1.4643906354904175,
      "learning_rate": 2.2111579630230294e-07,
      "loss": 2.4453,
      "step": 1536300
    },
    {
      "epoch": 498.0,
      "eval_bleu": 1.171784160184727,
      "eval_loss": 4.288303375244141,
      "eval_runtime": 4.352,
      "eval_samples_per_second": 113.05,
      "eval_steps_per_second": 1.838,
      "step": 1536330
    },
    {
      "epoch": 498.0226904376013,
      "grad_norm": 1.5093863010406494,
      "learning_rate": 2.178722024002595e-07,
      "loss": 2.4816,
      "step": 1536400
    },
    {
      "epoch": 498.0551053484603,
      "grad_norm": 1.8753721714019775,
      "learning_rate": 2.14628608498216e-07,
      "loss": 2.4331,
      "step": 1536500
    },
    {
      "epoch": 498.08752025931926,
      "grad_norm": 1.4223369359970093,
      "learning_rate": 2.1138501459617258e-07,
      "loss": 2.4353,
      "step": 1536600
    },
    {
      "epoch": 498.1199351701783,
      "grad_norm": 1.342275619506836,
      "learning_rate": 2.0814142069412913e-07,
      "loss": 2.459,
      "step": 1536700
    },
    {
      "epoch": 498.1523500810373,
      "grad_norm": 1.3808270692825317,
      "learning_rate": 2.0489782679208565e-07,
      "loss": 2.4561,
      "step": 1536800
    },
    {
      "epoch": 498.18476499189626,
      "grad_norm": 1.6753066778182983,
      "learning_rate": 2.016542328900422e-07,
      "loss": 2.4587,
      "step": 1536900
    },
    {
      "epoch": 498.2171799027553,
      "grad_norm": 1.349586009979248,
      "learning_rate": 1.984106389879987e-07,
      "loss": 2.4717,
      "step": 1537000
    },
    {
      "epoch": 498.24959481361424,
      "grad_norm": 1.644284963607788,
      "learning_rate": 1.9516704508595526e-07,
      "loss": 2.4541,
      "step": 1537100
    },
    {
      "epoch": 498.28200972447326,
      "grad_norm": 1.68172025680542,
      "learning_rate": 1.919558871229322e-07,
      "loss": 2.4322,
      "step": 1537200
    },
    {
      "epoch": 498.3144246353323,
      "grad_norm": 1.425111174583435,
      "learning_rate": 1.8871229322088876e-07,
      "loss": 2.4395,
      "step": 1537300
    },
    {
      "epoch": 498.34683954619123,
      "grad_norm": 1.311265468597412,
      "learning_rate": 1.854686993188453e-07,
      "loss": 2.4527,
      "step": 1537400
    },
    {
      "epoch": 498.37925445705025,
      "grad_norm": 1.3838928937911987,
      "learning_rate": 1.8222510541680182e-07,
      "loss": 2.463,
      "step": 1537500
    },
    {
      "epoch": 498.4116693679092,
      "grad_norm": 1.2881238460540771,
      "learning_rate": 1.7898151151475837e-07,
      "loss": 2.4329,
      "step": 1537600
    },
    {
      "epoch": 498.44408427876823,
      "grad_norm": 1.6142317056655884,
      "learning_rate": 1.7573791761271489e-07,
      "loss": 2.445,
      "step": 1537700
    },
    {
      "epoch": 498.47649918962725,
      "grad_norm": 1.5324498414993286,
      "learning_rate": 1.7249432371067143e-07,
      "loss": 2.4392,
      "step": 1537800
    },
    {
      "epoch": 498.5089141004862,
      "grad_norm": 1.5146815776824951,
      "learning_rate": 1.6925072980862798e-07,
      "loss": 2.4413,
      "step": 1537900
    },
    {
      "epoch": 498.5413290113452,
      "grad_norm": 1.5731006860733032,
      "learning_rate": 1.660071359065845e-07,
      "loss": 2.4374,
      "step": 1538000
    },
    {
      "epoch": 498.5737439222042,
      "grad_norm": 1.4749914407730103,
      "learning_rate": 1.6276354200454104e-07,
      "loss": 2.4427,
      "step": 1538100
    },
    {
      "epoch": 498.6061588330632,
      "grad_norm": 1.3033647537231445,
      "learning_rate": 1.5955238404151802e-07,
      "loss": 2.4403,
      "step": 1538200
    },
    {
      "epoch": 498.6385737439222,
      "grad_norm": 1.4695546627044678,
      "learning_rate": 1.5630879013947454e-07,
      "loss": 2.4569,
      "step": 1538300
    },
    {
      "epoch": 498.6709886547812,
      "grad_norm": 1.4403502941131592,
      "learning_rate": 1.530651962374311e-07,
      "loss": 2.4478,
      "step": 1538400
    },
    {
      "epoch": 498.7034035656402,
      "grad_norm": 1.6208382844924927,
      "learning_rate": 1.498216023353876e-07,
      "loss": 2.4506,
      "step": 1538500
    },
    {
      "epoch": 498.73581847649916,
      "grad_norm": 1.363591194152832,
      "learning_rate": 1.4657800843334415e-07,
      "loss": 2.4365,
      "step": 1538600
    },
    {
      "epoch": 498.7682333873582,
      "grad_norm": 1.4566388130187988,
      "learning_rate": 1.433344145313007e-07,
      "loss": 2.4606,
      "step": 1538700
    },
    {
      "epoch": 498.8006482982172,
      "grad_norm": 1.5738685131072998,
      "learning_rate": 1.4009082062925722e-07,
      "loss": 2.4421,
      "step": 1538800
    },
    {
      "epoch": 498.83306320907616,
      "grad_norm": 1.49549400806427,
      "learning_rate": 1.3684722672721376e-07,
      "loss": 2.459,
      "step": 1538900
    },
    {
      "epoch": 498.8654781199352,
      "grad_norm": 1.5799579620361328,
      "learning_rate": 1.3360363282517028e-07,
      "loss": 2.4267,
      "step": 1539000
    },
    {
      "epoch": 498.8978930307942,
      "grad_norm": 1.3981361389160156,
      "learning_rate": 1.3036003892312683e-07,
      "loss": 2.4433,
      "step": 1539100
    },
    {
      "epoch": 498.93030794165315,
      "grad_norm": 1.50419020652771,
      "learning_rate": 1.2711644502108338e-07,
      "loss": 2.4498,
      "step": 1539200
    },
    {
      "epoch": 498.9627228525122,
      "grad_norm": 1.537103295326233,
      "learning_rate": 1.238728511190399e-07,
      "loss": 2.4332,
      "step": 1539300
    },
    {
      "epoch": 498.99513776337113,
      "grad_norm": 1.5034493207931519,
      "learning_rate": 1.2062925721699644e-07,
      "loss": 2.4721,
      "step": 1539400
    },
    {
      "epoch": 499.0,
      "eval_bleu": 1.1704327908471237,
      "eval_loss": 4.288196563720703,
      "eval_runtime": 4.4049,
      "eval_samples_per_second": 111.694,
      "eval_steps_per_second": 1.816,
      "step": 1539415
    },
    {
      "epoch": 499.02755267423015,
      "grad_norm": 1.4843047857284546,
      "learning_rate": 1.1738566331495296e-07,
      "loss": 2.4637,
      "step": 1539500
    },
    {
      "epoch": 499.05996758508917,
      "grad_norm": 1.5145426988601685,
      "learning_rate": 1.1414206941290952e-07,
      "loss": 2.4348,
      "step": 1539600
    },
    {
      "epoch": 499.09238249594813,
      "grad_norm": 1.4769166707992554,
      "learning_rate": 1.1089847551086605e-07,
      "loss": 2.4578,
      "step": 1539700
    },
    {
      "epoch": 499.12479740680715,
      "grad_norm": 1.513363003730774,
      "learning_rate": 1.0765488160882258e-07,
      "loss": 2.44,
      "step": 1539800
    },
    {
      "epoch": 499.1572123176661,
      "grad_norm": 1.6039855480194092,
      "learning_rate": 1.044112877067791e-07,
      "loss": 2.4604,
      "step": 1539900
    },
    {
      "epoch": 499.1896272285251,
      "grad_norm": 1.741748332977295,
      "learning_rate": 1.0116769380473566e-07,
      "loss": 2.4245,
      "step": 1540000
    },
    {
      "epoch": 499.22204213938414,
      "grad_norm": 1.640771746635437,
      "learning_rate": 9.79240999026922e-08,
      "loss": 2.4164,
      "step": 1540100
    },
    {
      "epoch": 499.2544570502431,
      "grad_norm": 1.4824665784835815,
      "learning_rate": 9.468050600064873e-08,
      "loss": 2.4787,
      "step": 1540200
    },
    {
      "epoch": 499.2868719611021,
      "grad_norm": 1.6601845026016235,
      "learning_rate": 9.143691209860526e-08,
      "loss": 2.4556,
      "step": 1540300
    },
    {
      "epoch": 499.3192868719611,
      "grad_norm": 1.3648719787597656,
      "learning_rate": 8.81933181965618e-08,
      "loss": 2.461,
      "step": 1540400
    },
    {
      "epoch": 499.3517017828201,
      "grad_norm": 1.4650787115097046,
      "learning_rate": 8.494972429451833e-08,
      "loss": 2.4341,
      "step": 1540500
    },
    {
      "epoch": 499.3841166936791,
      "grad_norm": 1.4248570203781128,
      "learning_rate": 8.170613039247487e-08,
      "loss": 2.438,
      "step": 1540600
    },
    {
      "epoch": 499.4165316045381,
      "grad_norm": 1.2604389190673828,
      "learning_rate": 7.849497242945184e-08,
      "loss": 2.4587,
      "step": 1540700
    },
    {
      "epoch": 499.4489465153971,
      "grad_norm": 1.4102168083190918,
      "learning_rate": 7.525137852740837e-08,
      "loss": 2.431,
      "step": 1540800
    },
    {
      "epoch": 499.48136142625606,
      "grad_norm": 1.3883030414581299,
      "learning_rate": 7.20077846253649e-08,
      "loss": 2.4469,
      "step": 1540900
    },
    {
      "epoch": 499.5137763371151,
      "grad_norm": 1.3888022899627686,
      "learning_rate": 6.876419072332145e-08,
      "loss": 2.4393,
      "step": 1541000
    },
    {
      "epoch": 499.5461912479741,
      "grad_norm": 1.544062614440918,
      "learning_rate": 6.552059682127798e-08,
      "loss": 2.4444,
      "step": 1541100
    },
    {
      "epoch": 499.57860615883305,
      "grad_norm": 1.498220682144165,
      "learning_rate": 6.227700291923452e-08,
      "loss": 2.4491,
      "step": 1541200
    },
    {
      "epoch": 499.6110210696921,
      "grad_norm": 1.5678222179412842,
      "learning_rate": 5.9033409017191054e-08,
      "loss": 2.4591,
      "step": 1541300
    },
    {
      "epoch": 499.64343598055103,
      "grad_norm": 1.2708051204681396,
      "learning_rate": 5.578981511514758e-08,
      "loss": 2.4577,
      "step": 1541400
    },
    {
      "epoch": 499.67585089141005,
      "grad_norm": 1.3777756690979004,
      "learning_rate": 5.2546221213104126e-08,
      "loss": 2.4602,
      "step": 1541500
    },
    {
      "epoch": 499.70826580226907,
      "grad_norm": 1.609967589378357,
      "learning_rate": 4.930262731106066e-08,
      "loss": 2.436,
      "step": 1541600
    },
    {
      "epoch": 499.74068071312803,
      "grad_norm": 1.476889967918396,
      "learning_rate": 4.605903340901719e-08,
      "loss": 2.4443,
      "step": 1541700
    },
    {
      "epoch": 499.77309562398705,
      "grad_norm": 1.5835492610931396,
      "learning_rate": 4.281543950697373e-08,
      "loss": 2.4567,
      "step": 1541800
    },
    {
      "epoch": 499.805510534846,
      "grad_norm": 1.267732858657837,
      "learning_rate": 3.9571845604930263e-08,
      "loss": 2.4515,
      "step": 1541900
    },
    {
      "epoch": 499.837925445705,
      "grad_norm": 1.4822500944137573,
      "learning_rate": 3.63282517028868e-08,
      "loss": 2.4459,
      "step": 1542000
    },
    {
      "epoch": 499.87034035656404,
      "grad_norm": 1.3227568864822388,
      "learning_rate": 3.3084657800843335e-08,
      "loss": 2.441,
      "step": 1542100
    },
    {
      "epoch": 499.902755267423,
      "grad_norm": 1.4433232545852661,
      "learning_rate": 2.9841063898799875e-08,
      "loss": 2.455,
      "step": 1542200
    },
    {
      "epoch": 499.935170178282,
      "grad_norm": 1.5798453092575073,
      "learning_rate": 2.6597469996756404e-08,
      "loss": 2.4419,
      "step": 1542300
    },
    {
      "epoch": 499.967585089141,
      "grad_norm": 1.355961561203003,
      "learning_rate": 2.3353876094712943e-08,
      "loss": 2.4536,
      "step": 1542400
    },
    {
      "epoch": 500.0,
      "grad_norm": 1.309005618095398,
      "learning_rate": 2.011028219266948e-08,
      "loss": 2.4519,
      "step": 1542500
    },
    {
      "epoch": 500.0,
      "eval_bleu": 1.1831033798582093,
      "eval_loss": 4.288188457489014,
      "eval_runtime": 4.5668,
      "eval_samples_per_second": 107.733,
      "eval_steps_per_second": 1.752,
      "step": 1542500
    }
  ],
  "logging_steps": 100,
  "max_steps": 1542500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 500,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.9670009206983885e+17,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
