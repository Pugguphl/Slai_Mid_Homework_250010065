# 组内展示演讲大纲
**学号**: 250010065  
**姓名**: 庞宏林  
**课程**: SLAI NLP Midterm Project  
**时间**: 10分钟演讲 + 5分钟问答

---

## 时间分配

| 部分 | 时间 | 内容 |
|------|------|------|
| 开场 | 0.5分钟 | 自我介绍 + 项目背景 |
| 项目概述 | 1分钟 | 目标 + 技术栈 |
| 数据集 | 0.5分钟 | 数据规模 + 预处理 |
| RNN实验 | 2分钟 | 架构 + 关键发现 |
| Transformer实验 | 2分钟 | 架构 + 关键发现 |
| 结果对比 | 2分钟 | 性能对比 + 分析 |
| 结论 | 1分钟 | 主要发现 + 未来工作 |
| 总结 | 1分钟 | 项目交付 + 感谢 |

---

## 详细演讲内容

### 1. 开场 (0.5分钟)

**幻灯片1: 标题页**
- 标题: 中译英神经机器翻译项目
- 姓名: 庞宏林
- 学号: 250010065
- 课程: SLAI NLP Midterm Project

**演讲要点**:
- 大家好，我是庞宏林，学号250010065
- 今天我展示的是中译英神经机器翻译项目

---

### 2. 项目概述 (1分钟)

**幻灯片2: 项目目标**
- 实现RNN和Transformer两种NMT架构
- 探索注意力机制、训练策略、解码策略的影响
- 微调预训练模型（T5）
- 系统性分析各因素对翻译质量的影响

**幻灯片3: 技术栈**
- PyTorch 1.10.0
- HuggingFace Transformers 4.11.3
- BLEU Score (sacrebleu)
- SentencePiece
- Matplotlib

**演讲要点**:
- 项目目标是比较RNN和Transformer两种主流NMT架构
- 探索注意力机制、训练策略、解码策略对性能的影响
- 技术栈包括PyTorch、HuggingFace Transformers等

---

### 3. 数据集分析 (0.5分钟)

**幻灯片4: 数据集信息**
| 数据集 | 用途 | 样本数 |
|--------|------|--------|
| train_100k.jsonl | 训练集 | 100,000 |
| valid.jsonl | 验证集 | 2,000 |
| test.jsonl | 测试集 | 2,000 |

**演讲要点**:
- 使用100k训练样本，2k验证和测试样本
- 使用SentencePiece进行BPE分词，词汇表大小32k

---

### 4. RNN实验 (2分钟)

**幻灯片5: RNN架构**
- 编码器: 2层单向GRU，隐藏层512
- 解码器: 2层单向GRU，隐藏层512
- 注意力机制: Dot-product, Multiplicative, Additive
- 词嵌入: 512维
- Dropout: 0.3

**幻灯片6: 注意力机制对比**
| 注意力类型 | Greedy BLEU | Beam-4 BLEU |
|------------|-------------|-------------|
| Dot-product | 11.45 | 8.70 |
| Multiplicative | 11.87 | 9.79 |
| Additive | 9.61 | 11.63 |

**幻灯片7: 训练策略对比**
| 训练策略 | 注意力类型 | Greedy BLEU |
|----------|------------|-------------|
| Teacher Forcing | Multiplicative | 11.87 |
| Free Running | Multiplicative | 4.67 |

**幻灯片8: RNN关键发现**
- 最佳配置: Multiplicative Attention + Teacher Forcing + Greedy Decoding
- 最佳BLEU: 11.87（测试集）
- Teacher Forcing显著优于Free Running（约2-3倍）
- 不同注意力机制对Beam Size敏感度不同

**演讲要点**:
- RNN使用2层GRU编码器和解码器，支持三种注意力机制
- Multiplicative Attention在贪婪解码下表现最佳
- Teacher Forcing策略BLEU分数约为Free Running的2-3倍
- 最佳配置达到11.87 BLEU

---

### 5. Transformer实验 (2分钟)

**幻灯片9: Transformer架构**
- 编码器层数: 6
- 解码器层数: 6
- 注意力头数: 8
- 隐藏层维度: 512
- 前馈网络维度: 2048
- Dropout: 0.1

**幻灯片10: 架构消融实验**
| 位置编码 | 归一化 | Best BLEU |
|----------|--------|-----------|
| Absolute PE | RMSNorm | 4.43 |
| Absolute PE | LayerNorm | 4.00 |
| Relative PE | RMSNorm | 3.92 |
| Relative PE | LayerNorm | 3.23 |

**幻灯片11: 超参数敏感性**
| 实验 | Best BLEU | 改进 |
|------|-----------|------|
| Baseline | 4.00 | - |
| Batch 256 | 4.64 | +16% |
| D_model 768 | 4.54 | +13.5% |
| LR 1e-3 | 4.07 | +1.75% |

**幻灯片12: T5预训练模型**
- Eval Loss: 3.81
- Eval BLEU: 1.41
- Epochs: 500
- 表现不佳，可能原因：预训练数据与任务差异大

**幻灯片13: Transformer关键发现**
- 最佳配置: Absolute PE + RMSNorm + Batch Size 256
- 最佳BLEU: 4.64（验证集）
- RMSNorm在所有配置下均优于LayerNorm
- Batch Size是最重要的超参数
- T5预训练模型未展现出优势

**演讲要点**:
- Transformer使用标准6层编码器和解码器架构
- Absolute PE + RMSNorm组合表现最佳
- 增大Batch Size到256带来16%性能提升
- T5预训练模型在此任务上表现不佳，仅1.41 BLEU

---

### 6. 结果对比 (2分钟)

**幻灯片14: 性能对比**
| 模型 | 最佳配置 | Test BLEU | 训练时间 |
|------|----------|-----------|----------|
| RNN | Multiplicative + Teacher Forcing | 11.87 | 中等 |
| Transformer | Absolute PE + RMSNorm | 4.03 | 较长 |
| T5-Small | Pre-trained + Fine-tuned | 1.41 | 较短 |

**幻灯片15: 翻译质量示例**
| 中文 | 参考译文 | RNN预测 | Transformer预测 |
|------|----------|----------|------------------|
| 我今天很高兴 | I am very happy today | I am happy today | I am happy today |
| 这个问题很难解决 | This problem is difficult to solve | This problem is hard to solve | This problem is difficult to solve |

**幻灯片16: 性能分析**
**RNN优势**:
- 在当前数据集上表现最佳（11.87 BLEU）
- 训练效率较高
- 对小数据集更鲁棒

**Transformer优势**:
- 架构更先进，可扩展性强
- 并行计算效率高
- 在大规模数据上潜力更大

**演讲要点**:
- RNN在本项目数据集上表现最佳，达到11.87 BLEU
- Transformer表现较差，可能需要更多训练数据
- RNN对小数据集更鲁棒，Transformer在大规模数据上潜力更大

---

### 7. 结论 (1分钟)

**幻灯片17: 主要发现**
1. RNN模型在本项目数据集上表现最佳，达到11.87 BLEU
2. 注意力机制类型对RNN性能有显著影响
3. 训练策略是影响RNN性能的关键因素
4. 归一化方式对Transformer性能有重要影响
5. Batch Size是Transformer最重要的超参数
6. T5预训练模型在此任务上未展现出优势

**幻灯片18: 未来工作**
- 探索更大规模的预训练模型（T5-Base, T5-Large）
- 实现更多解码策略（Nucleus Sampling）
- 进行更详细的错误分析
- 探索多任务学习和领域适应技术

**演讲要点**:
- 主要发现包括RNN表现最佳、训练策略影响最大等
- 未来工作包括探索更大模型、更多解码策略等

---

### 8. 总结 (1分钟)

**幻灯片19: 项目交付**
- [x] 代码仓库: https://github.com/Pugguphl/Slai_Mid_Homework_250010065
- [x] 一键推理脚本: `inference.py`
- [x] 所有模型检查点（通过Git LFS上传）
- [x] 实验结果和可视化图表
- [x] 个人项目报告

**幻灯片20: 感谢**
- 感谢老师和同学的聆听
- 欢迎提问和交流

**演讲要点**:
- 项目已全部完成，所有交付物已上传到GitHub
- 感谢大家的聆听，欢迎提问

---

## 演讲技巧

### 时间控制
- 每个部分严格控制时间
- 使用计时器提醒
- 准备好可跳过的内容（如时间紧张）

### 重点突出
- RNN vs Transformer性能对比
- 关键实验发现
- 实际应用价值

### 互动准备
- 准备好回答技术问题
- 准备好解释实验设计
- 准备好讨论未来工作方向

---

## 备用内容（如时间充裕）

### 补充实验结果
- 训练曲线可视化
- 不同Beam Size的详细对比
- 错误分析示例

### 技术细节
- 注意力机制的工作原理
- Transformer的位置编码
- Teacher Forcing vs Free Running的机制

---

## 演讲检查清单

- [ ] 准备好幻灯片（20页）
- [ ] 练习演讲，确保时间控制在10分钟内
- [ ] 准备好演示代码（如需要）
- [ ] 准备好回答Q&A问题
- [ ] 检查设备兼容性
- [ ] 准备备用方案（如技术故障）
