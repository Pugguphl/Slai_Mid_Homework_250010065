# Q&A 准备文档
**学号**: 250010065  
**姓名**: 庞宏林  
**课程**: SLAI NLP Midterm Project

---

## 可能的问题和答案

### 1. 为什么RNN模型的表现比Transformer好？

**回答**：
RNN在本项目数据集上表现更好的主要原因有：

1. **数据集规模**：本项目使用100k训练样本，相对较小。RNN架构参数较少，在小数据集上更不容易过拟合，泛化能力更好。

2. **训练策略**：RNN使用了Teacher Forcing策略，这在训练初期对模型学习对齐关系非常有帮助。而Transformer可能需要更多的训练epoch才能收敛。

3. **模型复杂度**：Transformer模型更复杂，需要更多的训练数据和更长的训练时间才能发挥优势。在本项目的训练设置下，Transformer可能还没有充分训练。

4. **超参数调优**：RNN的超参数（如hidden size=512, dropout=0.3）更适合当前任务，而Transformer的超参数可能需要进一步优化。

**补充说明**：在大规模数据集（如WMT）上，Transformer通常表现优于RNN。本项目的结果反映了在小数据集场景下，更简单的模型可能更有优势。

---

### 2. Teacher Forcing和Free Running有什么区别？为什么Teacher Forcing效果更好？

**回答**：

**Teacher Forcing**：
- 在训练时，解码器使用真实的上一个词作为输入
- 例如：输入"我" → 真实标签"am" → 真实标签"happy"
- 优点：训练稳定，收敛快
- 缺点：训练和推理时输入分布不一致（exposure bias）

**Free Running**：
- 在训练时，解码器使用自己预测的词作为输入
- 例如：输入"我" → 预测"am" → 预测"happy"
- 优点：训练和推理一致
- 缺点：早期预测错误会累积，导致训练不稳定

**为什么Teacher Forcing效果更好**：
1. **误差累积**：Free Running在训练早期，模型预测质量差，错误会不断累积，导致训练困难
2. **学习对齐**：Teacher Forcing帮助模型更快学习到源语言和目标语言的对齐关系
3. **训练稳定性**：Teacher Forcing提供了稳定的训练信号，模型更容易收敛

**实验结果**：Teacher Forcing的BLEU分数约为Free Running的2-3倍，验证了其有效性。

---

### 3. 为什么T5预训练模型表现这么差？

**回答**：

T5-Small在本项目上表现不佳（仅1.41 BLEU）的主要原因：

1. **预训练数据不匹配**：
   - T5是在大规模英文数据集（C4）上预训练的
   - 预训练任务主要是文本生成和完形填空，不是翻译任务
   - 中英翻译任务与预训练任务差异较大

2. **分词器不匹配**：
   - T5使用SentencePiece分词器，但词汇表是在英文数据上训练的
   - 中文token可能被切分成多个subword，影响模型理解

3. **训练数据不足**：
   - 100k训练样本对于微调预训练模型可能不够
   - 预训练模型通常需要更多数据才能发挥优势

4. **训练时间不足**：
   - 虽然训练了500个epoch，但可能还需要更多训练时间
   - 学习率调度、warmup等超参数可能需要进一步优化

5. **模型规模限制**：
   - T5-Small参数量较少（约60M），可能不足以处理中英翻译任务
   - 更大的模型（如T5-Base, T5-Large）可能表现更好

**改进建议**：
- 使用专门的中英翻译预训练模型（如mBART, HuggingFace的opus-mt）
- 增加训练数据量
- 调整学习率和训练策略
- 尝试更大的模型

---

### 4. RMSNorm和LayerNorm有什么区别？为什么RMSNorm表现更好？

**回答**：

**LayerNorm**：
- 标准化公式：`y = (x - μ) / σ * γ + β`
- 计算均值μ和标准差σ
- 需要计算两次统计量（均值和方差）

**RMSNorm**：
- 标准化公式：`y = x / RMS(x) * γ`
- 只计算均方根RMS(x)，不计算均值
- 计算更简单，计算量更少

**为什么RMSNorm表现更好**：

1. **计算效率**：RMSNorm计算量更少，训练速度更快
2. **稳定性**：在某些情况下，RMSNorm比LayerNorm更稳定
3. **适合Transformer**：RMSNorm在Transformer架构中表现良好，特别是在NMT任务上
4. **减少过拟合**：RMSNorm的简化公式可能有助于减少过拟合

**实验结果**：在所有Transformer配置下，RMSNorm的BLEU分数都比LayerNorm高约0.4-0.7。

---

### 5. 为什么增加Batch Size能显著提升Transformer性能？

**回答**：

**实验结果**：Batch Size从默认值增加到256后，BLEU分数从4.00提升到4.64，提升了16%。

**原因分析**：

1. **梯度估计更准确**：
   - 更大的Batch Size意味着每次更新使用更多样本
   - 梯度估计更准确，训练更稳定

2. **学习率调整**：
   - 更大的Batch Size通常可以使用更大的学习率
   - 本实验中使用了线性warmup和余弦衰减调度，适合大Batch训练

3. **并行计算效率**：
   - GPU并行计算在大Batch下效率更高
   - 减少了通信开销

4. **正则化效果**：
   - 小Batch Size有更强的正则化效果（类似dropout）
   - 大Batch Size减少了随机性，可能有助于模型收敛

5. **优化器行为**：
   - Adam优化器在大Batch下表现更好
   - 动量估计更准确

**注意事项**：
- 过大的Batch Size可能导致泛化能力下降
- 需要相应调整学习率和其他超参数

---

### 6. Absolute PE和Relative PE有什么区别？

**回答**：

**Absolute Positional Encoding (Absolute PE)**：
- 为每个位置分配一个固定的位置向量
- 位置向量直接加到词嵌入上
- 例如：位置1 → [0.1, 0.2, ...]，位置2 → [0.3, 0.4, ...]
- 优点：简单，计算高效
- 缺点：无法直接建模相对位置关系

**Relative Positional Encoding (Relative PE)**：
- 建模位置之间的相对距离
- 在注意力计算中考虑相对位置信息
- 例如：位置i和j的距离 = j - i
- 优点：能更好地建模位置关系
- 缺点：计算更复杂，参数更多

**实验结果**：Absolute PE在本项目上表现更好（约0.4-0.7 BLEU提升）。

**可能原因**：
1. **数据集特点**：中英翻译任务中，绝对位置信息可能更重要
2. **训练稳定性**：Absolute PE训练更稳定，更容易收敛
3. **模型规模**：在小模型上，Absolute PE可能更合适
4. **序列长度**：本项目的序列长度限制在128，相对位置信息的作用可能有限

---

### 7. Beam Search和Greedy Decoding有什么区别？为什么有时Greedy表现更好？

**回答**：

**Greedy Decoding**：
- 每一步选择概率最高的词
- 例如：P("am"|I) > P("am"|I) → 选择"am"
- 优点：计算快速，简单
- 缺点：可能陷入局部最优

**Beam Search**：
- 每一步保留top-k个候选序列（beam size = k）
- 例如：beam=4时，保留4个最可能的序列
- 优点：探索更多可能性，通常质量更高
- 缺点：计算量大，可能选择次优序列

**为什么有时Greedy表现更好**：

1. **模型不确定性**：
   - 如果模型预测的概率分布很集中，Greedy和Beam Search结果相似
   - 如果模型预测不确定，Beam Search可能选择错误的序列

2. **Beam Size选择**：
   - 过小的Beam Size（如4）可能不够
   - 过大的Beam Size（如8）可能引入噪声
   - 需要根据任务调整Beam Size

3. **长度惩罚**：
   - Beam Search倾向于生成更短的序列
   - 可能需要长度归一化

**实验结果**：
- RNN模型：Greedy表现最好（11.87 BLEU）
- Additive Attention：Beam-4表现最好（11.63 BLEU）
- 不同模型和配置对解码策略的敏感度不同

---

### 8. 你的实验有什么局限性？

**回答**：

**数据集局限**：
1. **数据规模**：100k训练样本相对较小，限制了模型性能
2. **数据质量**：没有对数据集进行详细的质量分析
3. **领域限制**：数据集可能来自特定领域（如新闻），泛化能力有限

**模型局限**：
1. **训练时间**：Transformer可能需要更多训练epoch才能收敛
2. **超参数调优**：没有进行系统性的超参数搜索
3. **模型规模**：只使用了较小的模型（T5-Small, d_model=512）

**评估局限**：
1. **评估指标**：只使用了BLEU分数，没有使用其他指标（如METEOR, TER）
2. **人工评估**：没有进行人工翻译质量评估
3. **错误分析**：没有进行详细的错误分析

**实验设计局限**：
1. **消融实验**：只进行了部分消融实验
2. **对比基线**：没有与SOTA模型进行对比
3. **可重复性**：没有报告多次运行的标准差

**未来改进**：
- 增加训练数据量
- 进行更系统的超参数调优
- 使用更大的模型
- 进行人工评估和错误分析
- 与SOTA模型对比

---

### 9. 如果给你更多时间，你会改进什么？

**回答**：

**模型改进**：
1. **探索更大的预训练模型**：
   - 尝试T5-Base, T5-Large
   - 尝试mBART, mT5等专门的多语言模型
   - 尝试HuggingFace的opus-mt系列

2. **优化训练策略**：
   - 使用curriculum learning（从简单到难）
   - 尝试不同的学习率调度策略
   - 使用label smoothing

3. **模型架构改进**：
   - 尝试更深的Transformer（12层）
   - 尝试不同的注意力机制（如relative attention）
   - 尝试encoder-only或decoder-only架构

**数据改进**：
1. **数据增强**：
   - 使用回译（back-translation）
   - 使用同义词替换
   - 使用噪声注入

2. **数据清洗**：
   - 去除低质量样本
   - 去除重复样本
   - 过滤过短或过长的序列

**评估改进**：
1. **多指标评估**：
   - 使用METEOR, TER, chrF等指标
   - 进行人工评估
   - 进行语义相似度评估

2. **错误分析**：
   - 分析翻译错误类型（如词序、词汇选择）
   - 分析长句和短句的翻译质量差异
   - 分析不同领域的翻译质量

**实验改进**：
1. **系统化实验**：
   - 使用超参数搜索（如grid search, Bayesian optimization）
   - 进行多次运行，报告标准差
   - 与SOTA模型对比

2. **可解释性分析**：
   - 可视化注意力权重
   - 分析模型学到的对齐关系
   - 分析模型对不同语言现象的处理能力

---

### 10. 你的项目有什么实际应用价值？

**回答**：

**学术价值**：
1. **对比研究**：系统比较了RNN和Transformer在NMT任务上的表现
2. **消融实验**：分析了不同组件对性能的影响
3. **小数据集研究**：探索了在小数据集场景下的模型选择和训练策略

**工程价值**：
1. **可复现性**：提供了完整的代码和训练脚本
2. **一键推理**：提供了简单的推理接口
3. **模型选择指导**：为小数据集场景下的模型选择提供了参考

**实际应用**：
1. **低资源语言翻译**：对于数据稀缺的语言对，RNN可能是更好的选择
2. **快速原型开发**：RNN训练更快，适合快速迭代和原型开发
3. **边缘设备部署**：RNN模型更小，适合在资源受限的设备上部署

**教育价值**：
1. **教学案例**：可以作为NMT课程的教学案例
2. **实践指南**：提供了从数据预处理到模型训练的完整流程
3. **实验设计**：展示了如何进行系统性的实验和结果分析

**未来应用**：
- 可以扩展到其他语言对
- 可以集成到实际翻译系统中
- 可以作为研究不同NMT架构的基础

---

## 演讲技巧

### 回答问题的原则
1. **诚实回答**：不知道就说不知道，不要编造
2. **结构清晰**：先给结论，再解释原因
3. **引用数据**：用实验结果支持你的观点
4. **承认局限**：诚实地讨论实验的局限性
5. **展示思考**：即使回答不完美，展示你的思考过程

### 应对挑战性问题
1. **保持冷静**：不要慌张，深呼吸
2. **理解问题**：如果不确定，请对方澄清
3. **分步回答**：复杂问题可以分解成几个部分
4. **承认不足**：如果确实不知道，可以说"这是一个很好的问题，我之前没有考虑过"
5. **联系实验**：尽可能联系你的实验结果

### 时间管理
1. **简洁回答**：每个问题控制在1-2分钟内
2. **重点突出**：先说最重要的点
3. **准备简短答案**：准备一些问题的简短版本
4. **礼貌结束**：如果时间不够，可以说"这个问题很有趣，我们可以会后继续讨论"

---

## 备用材料

### 关键数据速查表

**RNN最佳配置**：
- 模型：Multiplicative Attention
- 训练策略：Teacher Forcing
- 解码策略：Greedy
- BLEU：11.87

**Transformer最佳配置**：
- 位置编码：Absolute PE
- 归一化：RMSNorm
- Batch Size：256
- BLEU：4.64

**T5-Small**：
- BLEU：1.41
- Epochs：500

### 重要文件路径
- 代码仓库：https://github.com/Pugguphl/Slai_Mid_Homework_250010065
- 项目报告：reports/250010065_庞宏林_项目报告.md
- 演讲大纲：reports/演讲大纲.md
- 推理脚本：inference.py
- 实验结果：experiments/results/
