{
  "best_global_step": 246800,
  "best_metric": 1.4050730653141843,
  "best_model_checkpoint": "/mnt/afs/250010065/SLAI_NLP_Mid_project/zh-en-nmt-midterm/experiments/logs/t5_small/checkpoint-246800",
  "epoch": 80.0,
  "eval_steps": 500,
  "global_step": 246800,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.03241491085899514,
      "grad_norm": 1.6797802448272705,
      "learning_rate": 4.800000000000001e-06,
      "loss": 4.5726,
      "step": 100
    },
    {
      "epoch": 0.06482982171799027,
      "grad_norm": 1.0331605672836304,
      "learning_rate": 9.800000000000001e-06,
      "loss": 4.3618,
      "step": 200
    },
    {
      "epoch": 0.09724473257698542,
      "grad_norm": 0.9222862124443054,
      "learning_rate": 1.48e-05,
      "loss": 4.1515,
      "step": 300
    },
    {
      "epoch": 0.12965964343598055,
      "grad_norm": 1.4852453470230103,
      "learning_rate": 1.9800000000000004e-05,
      "loss": 4.0236,
      "step": 400
    },
    {
      "epoch": 0.1620745542949757,
      "grad_norm": 1.1541982889175415,
      "learning_rate": 2.48e-05,
      "loss": 3.9762,
      "step": 500
    },
    {
      "epoch": 0.19448946515397084,
      "grad_norm": 0.9210267066955566,
      "learning_rate": 2.98e-05,
      "loss": 3.9351,
      "step": 600
    },
    {
      "epoch": 0.22690437601296595,
      "grad_norm": 0.9970620274543762,
      "learning_rate": 3.48e-05,
      "loss": 3.9247,
      "step": 700
    },
    {
      "epoch": 0.2593192868719611,
      "grad_norm": 1.0250582695007324,
      "learning_rate": 3.9800000000000005e-05,
      "loss": 3.8842,
      "step": 800
    },
    {
      "epoch": 0.2917341977309562,
      "grad_norm": 0.9728512763977051,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 3.8498,
      "step": 900
    },
    {
      "epoch": 0.3241491085899514,
      "grad_norm": 0.9381589889526367,
      "learning_rate": 4.9800000000000004e-05,
      "loss": 3.8169,
      "step": 1000
    },
    {
      "epoch": 0.3565640194489465,
      "grad_norm": 0.9093391299247742,
      "learning_rate": 4.999688614985404e-05,
      "loss": 3.8367,
      "step": 1100
    },
    {
      "epoch": 0.3889789303079417,
      "grad_norm": 0.8823839426040649,
      "learning_rate": 4.9993642555951996e-05,
      "loss": 3.7899,
      "step": 1200
    },
    {
      "epoch": 0.4213938411669368,
      "grad_norm": 0.9424126148223877,
      "learning_rate": 4.9990398962049955e-05,
      "loss": 3.7958,
      "step": 1300
    },
    {
      "epoch": 0.4538087520259319,
      "grad_norm": 0.8339336514472961,
      "learning_rate": 4.998715536814791e-05,
      "loss": 3.77,
      "step": 1400
    },
    {
      "epoch": 0.4862236628849271,
      "grad_norm": 1.1111314296722412,
      "learning_rate": 4.9983911774245866e-05,
      "loss": 3.7718,
      "step": 1500
    },
    {
      "epoch": 0.5186385737439222,
      "grad_norm": 0.8459322452545166,
      "learning_rate": 4.9980668180343824e-05,
      "loss": 3.7747,
      "step": 1600
    },
    {
      "epoch": 0.5510534846029174,
      "grad_norm": 0.8715502023696899,
      "learning_rate": 4.9977424586441776e-05,
      "loss": 3.7546,
      "step": 1700
    },
    {
      "epoch": 0.5834683954619124,
      "grad_norm": 0.8342731595039368,
      "learning_rate": 4.9974180992539735e-05,
      "loss": 3.7504,
      "step": 1800
    },
    {
      "epoch": 0.6158833063209076,
      "grad_norm": 0.9409109950065613,
      "learning_rate": 4.9970937398637694e-05,
      "loss": 3.7476,
      "step": 1900
    },
    {
      "epoch": 0.6482982171799028,
      "grad_norm": 1.8225523233413696,
      "learning_rate": 4.9967693804735646e-05,
      "loss": 3.7431,
      "step": 2000
    },
    {
      "epoch": 0.6807131280388979,
      "grad_norm": 0.9544966220855713,
      "learning_rate": 4.9964450210833604e-05,
      "loss": 3.7417,
      "step": 2100
    },
    {
      "epoch": 0.713128038897893,
      "grad_norm": 0.9057488441467285,
      "learning_rate": 4.996120661693156e-05,
      "loss": 3.7253,
      "step": 2200
    },
    {
      "epoch": 0.7455429497568882,
      "grad_norm": 0.9065762758255005,
      "learning_rate": 4.995796302302952e-05,
      "loss": 3.7116,
      "step": 2300
    },
    {
      "epoch": 0.7779578606158833,
      "grad_norm": 0.9931285977363586,
      "learning_rate": 4.995471942912748e-05,
      "loss": 3.7322,
      "step": 2400
    },
    {
      "epoch": 0.8103727714748784,
      "grad_norm": 0.9617375135421753,
      "learning_rate": 4.995147583522543e-05,
      "loss": 3.7178,
      "step": 2500
    },
    {
      "epoch": 0.8427876823338736,
      "grad_norm": 0.8823535442352295,
      "learning_rate": 4.994823224132339e-05,
      "loss": 3.7404,
      "step": 2600
    },
    {
      "epoch": 0.8752025931928687,
      "grad_norm": 0.984575092792511,
      "learning_rate": 4.994498864742135e-05,
      "loss": 3.7064,
      "step": 2700
    },
    {
      "epoch": 0.9076175040518638,
      "grad_norm": 0.9684839844703674,
      "learning_rate": 4.99417450535193e-05,
      "loss": 3.6906,
      "step": 2800
    },
    {
      "epoch": 0.940032414910859,
      "grad_norm": 0.8473362326622009,
      "learning_rate": 4.993850145961726e-05,
      "loss": 3.6977,
      "step": 2900
    },
    {
      "epoch": 0.9724473257698542,
      "grad_norm": 0.7248867154121399,
      "learning_rate": 4.993525786571521e-05,
      "loss": 3.7144,
      "step": 3000
    },
    {
      "epoch": 1.0,
      "eval_bleu": 0.6681105934635609,
      "eval_loss": 3.7649145126342773,
      "eval_runtime": 5.288,
      "eval_samples_per_second": 93.041,
      "eval_steps_per_second": 1.513,
      "step": 3085
    },
    {
      "epoch": 1.0048622366288493,
      "grad_norm": 0.8666132688522339,
      "learning_rate": 4.993201427181317e-05,
      "loss": 3.6911,
      "step": 3100
    },
    {
      "epoch": 1.0372771474878444,
      "grad_norm": 0.857800304889679,
      "learning_rate": 4.992877067791113e-05,
      "loss": 3.6687,
      "step": 3200
    },
    {
      "epoch": 1.0696920583468394,
      "grad_norm": 0.8115066289901733,
      "learning_rate": 4.992552708400908e-05,
      "loss": 3.6751,
      "step": 3300
    },
    {
      "epoch": 1.1021069692058347,
      "grad_norm": 0.9506049752235413,
      "learning_rate": 4.992228349010704e-05,
      "loss": 3.6704,
      "step": 3400
    },
    {
      "epoch": 1.1345218800648298,
      "grad_norm": 0.791263222694397,
      "learning_rate": 4.9919039896205e-05,
      "loss": 3.6736,
      "step": 3500
    },
    {
      "epoch": 1.1669367909238249,
      "grad_norm": 0.7485505938529968,
      "learning_rate": 4.991579630230295e-05,
      "loss": 3.6679,
      "step": 3600
    },
    {
      "epoch": 1.1993517017828201,
      "grad_norm": 0.9193425178527832,
      "learning_rate": 4.991255270840091e-05,
      "loss": 3.6793,
      "step": 3700
    },
    {
      "epoch": 1.2317666126418152,
      "grad_norm": 0.9649174809455872,
      "learning_rate": 4.990930911449886e-05,
      "loss": 3.6664,
      "step": 3800
    },
    {
      "epoch": 1.2641815235008105,
      "grad_norm": 0.7906646132469177,
      "learning_rate": 4.990606552059682e-05,
      "loss": 3.6765,
      "step": 3900
    },
    {
      "epoch": 1.2965964343598055,
      "grad_norm": 0.8507904410362244,
      "learning_rate": 4.990282192669478e-05,
      "loss": 3.6478,
      "step": 4000
    },
    {
      "epoch": 1.3290113452188006,
      "grad_norm": 0.900198221206665,
      "learning_rate": 4.989957833279273e-05,
      "loss": 3.6549,
      "step": 4100
    },
    {
      "epoch": 1.3614262560777957,
      "grad_norm": 0.9227755665779114,
      "learning_rate": 4.989633473889069e-05,
      "loss": 3.656,
      "step": 4200
    },
    {
      "epoch": 1.393841166936791,
      "grad_norm": 0.8107711672782898,
      "learning_rate": 4.989309114498865e-05,
      "loss": 3.6749,
      "step": 4300
    },
    {
      "epoch": 1.426256077795786,
      "grad_norm": 1.0376216173171997,
      "learning_rate": 4.988984755108661e-05,
      "loss": 3.6481,
      "step": 4400
    },
    {
      "epoch": 1.4586709886547813,
      "grad_norm": 0.7833175659179688,
      "learning_rate": 4.988660395718456e-05,
      "loss": 3.6524,
      "step": 4500
    },
    {
      "epoch": 1.4910858995137763,
      "grad_norm": 0.9450885057449341,
      "learning_rate": 4.988336036328252e-05,
      "loss": 3.64,
      "step": 4600
    },
    {
      "epoch": 1.5235008103727714,
      "grad_norm": 0.933602511882782,
      "learning_rate": 4.988011676938048e-05,
      "loss": 3.6573,
      "step": 4700
    },
    {
      "epoch": 1.5559157212317665,
      "grad_norm": 0.7349653840065002,
      "learning_rate": 4.9876873175478436e-05,
      "loss": 3.65,
      "step": 4800
    },
    {
      "epoch": 1.5883306320907618,
      "grad_norm": 0.8325422406196594,
      "learning_rate": 4.987362958157639e-05,
      "loss": 3.6482,
      "step": 4900
    },
    {
      "epoch": 1.620745542949757,
      "grad_norm": 0.7393724322319031,
      "learning_rate": 4.987038598767435e-05,
      "loss": 3.6471,
      "step": 5000
    },
    {
      "epoch": 1.653160453808752,
      "grad_norm": 0.7905532121658325,
      "learning_rate": 4.9867142393772306e-05,
      "loss": 3.6355,
      "step": 5100
    },
    {
      "epoch": 1.6855753646677472,
      "grad_norm": 0.7634092569351196,
      "learning_rate": 4.986389879987026e-05,
      "loss": 3.6369,
      "step": 5200
    },
    {
      "epoch": 1.7179902755267422,
      "grad_norm": 0.7685794234275818,
      "learning_rate": 4.9860655205968216e-05,
      "loss": 3.6298,
      "step": 5300
    },
    {
      "epoch": 1.7504051863857373,
      "grad_norm": 0.915002703666687,
      "learning_rate": 4.9857411612066175e-05,
      "loss": 3.6304,
      "step": 5400
    },
    {
      "epoch": 1.7828200972447326,
      "grad_norm": 0.8249419927597046,
      "learning_rate": 4.985416801816413e-05,
      "loss": 3.6202,
      "step": 5500
    },
    {
      "epoch": 1.8152350081037278,
      "grad_norm": 0.8105984926223755,
      "learning_rate": 4.9850924424262086e-05,
      "loss": 3.6394,
      "step": 5600
    },
    {
      "epoch": 1.847649918962723,
      "grad_norm": 0.9352609515190125,
      "learning_rate": 4.9847680830360045e-05,
      "loss": 3.6446,
      "step": 5700
    },
    {
      "epoch": 1.880064829821718,
      "grad_norm": 0.9292877912521362,
      "learning_rate": 4.9844437236458e-05,
      "loss": 3.6332,
      "step": 5800
    },
    {
      "epoch": 1.912479740680713,
      "grad_norm": 0.9372442960739136,
      "learning_rate": 4.9841193642555955e-05,
      "loss": 3.6229,
      "step": 5900
    },
    {
      "epoch": 1.9448946515397083,
      "grad_norm": 0.9329570531845093,
      "learning_rate": 4.983795004865391e-05,
      "loss": 3.6037,
      "step": 6000
    },
    {
      "epoch": 1.9773095623987034,
      "grad_norm": 0.8103736639022827,
      "learning_rate": 4.9834706454751866e-05,
      "loss": 3.6365,
      "step": 6100
    },
    {
      "epoch": 2.0,
      "eval_bleu": 0.9132883486169052,
      "eval_loss": 3.7527167797088623,
      "eval_runtime": 6.1765,
      "eval_samples_per_second": 79.657,
      "eval_steps_per_second": 1.295,
      "step": 6170
    },
    {
      "epoch": 2.0097244732576987,
      "grad_norm": 0.8591398596763611,
      "learning_rate": 4.9831462860849825e-05,
      "loss": 3.6147,
      "step": 6200
    },
    {
      "epoch": 2.0421393841166937,
      "grad_norm": 0.8726011514663696,
      "learning_rate": 4.982821926694778e-05,
      "loss": 3.6171,
      "step": 6300
    },
    {
      "epoch": 2.0745542949756888,
      "grad_norm": 0.7608801126480103,
      "learning_rate": 4.9824975673045736e-05,
      "loss": 3.6012,
      "step": 6400
    },
    {
      "epoch": 2.106969205834684,
      "grad_norm": 0.8192885518074036,
      "learning_rate": 4.9821732079143694e-05,
      "loss": 3.584,
      "step": 6500
    },
    {
      "epoch": 2.139384116693679,
      "grad_norm": 0.9664657711982727,
      "learning_rate": 4.9818488485241646e-05,
      "loss": 3.6164,
      "step": 6600
    },
    {
      "epoch": 2.1717990275526744,
      "grad_norm": 0.8984004259109497,
      "learning_rate": 4.9815244891339605e-05,
      "loss": 3.6105,
      "step": 6700
    },
    {
      "epoch": 2.2042139384116695,
      "grad_norm": 0.7975320219993591,
      "learning_rate": 4.9812001297437564e-05,
      "loss": 3.6167,
      "step": 6800
    },
    {
      "epoch": 2.2366288492706645,
      "grad_norm": 0.8556150794029236,
      "learning_rate": 4.9808757703535516e-05,
      "loss": 3.5952,
      "step": 6900
    },
    {
      "epoch": 2.2690437601296596,
      "grad_norm": 0.9917799234390259,
      "learning_rate": 4.9805514109633474e-05,
      "loss": 3.5874,
      "step": 7000
    },
    {
      "epoch": 2.3014586709886546,
      "grad_norm": 0.8774116039276123,
      "learning_rate": 4.980227051573143e-05,
      "loss": 3.581,
      "step": 7100
    },
    {
      "epoch": 2.3338735818476497,
      "grad_norm": 0.8475454449653625,
      "learning_rate": 4.979902692182939e-05,
      "loss": 3.5832,
      "step": 7200
    },
    {
      "epoch": 2.366288492706645,
      "grad_norm": 0.8292209506034851,
      "learning_rate": 4.979578332792735e-05,
      "loss": 3.5974,
      "step": 7300
    },
    {
      "epoch": 2.3987034035656403,
      "grad_norm": 0.9741121530532837,
      "learning_rate": 4.97925397340253e-05,
      "loss": 3.5995,
      "step": 7400
    },
    {
      "epoch": 2.4311183144246353,
      "grad_norm": 0.9680750370025635,
      "learning_rate": 4.978929614012326e-05,
      "loss": 3.5854,
      "step": 7500
    },
    {
      "epoch": 2.4635332252836304,
      "grad_norm": 0.8713087439537048,
      "learning_rate": 4.978605254622122e-05,
      "loss": 3.6188,
      "step": 7600
    },
    {
      "epoch": 2.4959481361426255,
      "grad_norm": 0.8612383008003235,
      "learning_rate": 4.978280895231917e-05,
      "loss": 3.5888,
      "step": 7700
    },
    {
      "epoch": 2.528363047001621,
      "grad_norm": 0.9502345323562622,
      "learning_rate": 4.977956535841713e-05,
      "loss": 3.5876,
      "step": 7800
    },
    {
      "epoch": 2.560777957860616,
      "grad_norm": 0.9232063293457031,
      "learning_rate": 4.977632176451508e-05,
      "loss": 3.596,
      "step": 7900
    },
    {
      "epoch": 2.593192868719611,
      "grad_norm": 0.7830067276954651,
      "learning_rate": 4.977307817061304e-05,
      "loss": 3.5806,
      "step": 8000
    },
    {
      "epoch": 2.625607779578606,
      "grad_norm": 0.8996331095695496,
      "learning_rate": 4.9769834576711e-05,
      "loss": 3.5794,
      "step": 8100
    },
    {
      "epoch": 2.658022690437601,
      "grad_norm": 0.8579016923904419,
      "learning_rate": 4.976662341874798e-05,
      "loss": 3.5972,
      "step": 8200
    },
    {
      "epoch": 2.6904376012965967,
      "grad_norm": 0.8599563837051392,
      "learning_rate": 4.976337982484593e-05,
      "loss": 3.5792,
      "step": 8300
    },
    {
      "epoch": 2.7228525121555913,
      "grad_norm": 0.7435423135757446,
      "learning_rate": 4.976013623094389e-05,
      "loss": 3.5904,
      "step": 8400
    },
    {
      "epoch": 2.755267423014587,
      "grad_norm": 0.8878489136695862,
      "learning_rate": 4.975689263704185e-05,
      "loss": 3.5916,
      "step": 8500
    },
    {
      "epoch": 2.787682333873582,
      "grad_norm": 0.6734551787376404,
      "learning_rate": 4.97536490431398e-05,
      "loss": 3.586,
      "step": 8600
    },
    {
      "epoch": 2.820097244732577,
      "grad_norm": 0.843337893486023,
      "learning_rate": 4.975040544923776e-05,
      "loss": 3.5657,
      "step": 8700
    },
    {
      "epoch": 2.852512155591572,
      "grad_norm": 0.8057633638381958,
      "learning_rate": 4.974716185533572e-05,
      "loss": 3.5782,
      "step": 8800
    },
    {
      "epoch": 2.884927066450567,
      "grad_norm": 0.9760859608650208,
      "learning_rate": 4.974391826143367e-05,
      "loss": 3.5723,
      "step": 8900
    },
    {
      "epoch": 2.9173419773095626,
      "grad_norm": 0.8412949442863464,
      "learning_rate": 4.974067466753163e-05,
      "loss": 3.5803,
      "step": 9000
    },
    {
      "epoch": 2.9497568881685576,
      "grad_norm": 0.7343051433563232,
      "learning_rate": 4.973743107362958e-05,
      "loss": 3.5941,
      "step": 9100
    },
    {
      "epoch": 2.9821717990275527,
      "grad_norm": 0.7558436393737793,
      "learning_rate": 4.973418747972754e-05,
      "loss": 3.5902,
      "step": 9200
    },
    {
      "epoch": 3.0,
      "eval_bleu": 0.585025333444598,
      "eval_loss": 3.7381343841552734,
      "eval_runtime": 6.1926,
      "eval_samples_per_second": 79.45,
      "eval_steps_per_second": 1.292,
      "step": 9255
    },
    {
      "epoch": 3.0145867098865478,
      "grad_norm": 0.7880876660346985,
      "learning_rate": 4.97309438858255e-05,
      "loss": 3.5631,
      "step": 9300
    },
    {
      "epoch": 3.047001620745543,
      "grad_norm": 0.9972786903381348,
      "learning_rate": 4.972770029192345e-05,
      "loss": 3.5578,
      "step": 9400
    },
    {
      "epoch": 3.079416531604538,
      "grad_norm": 0.742519736289978,
      "learning_rate": 4.972445669802141e-05,
      "loss": 3.5679,
      "step": 9500
    },
    {
      "epoch": 3.1118314424635334,
      "grad_norm": 0.8826646208763123,
      "learning_rate": 4.9721213104119367e-05,
      "loss": 3.5504,
      "step": 9600
    },
    {
      "epoch": 3.1442463533225284,
      "grad_norm": 0.7517140507698059,
      "learning_rate": 4.971796951021732e-05,
      "loss": 3.5588,
      "step": 9700
    },
    {
      "epoch": 3.1766612641815235,
      "grad_norm": 0.9241285920143127,
      "learning_rate": 4.971472591631528e-05,
      "loss": 3.5546,
      "step": 9800
    },
    {
      "epoch": 3.2090761750405186,
      "grad_norm": 0.7730284333229065,
      "learning_rate": 4.9711482322413236e-05,
      "loss": 3.5471,
      "step": 9900
    },
    {
      "epoch": 3.2414910858995136,
      "grad_norm": 0.7868578433990479,
      "learning_rate": 4.9708238728511195e-05,
      "loss": 3.5575,
      "step": 10000
    },
    {
      "epoch": 3.2739059967585087,
      "grad_norm": 0.7329961061477661,
      "learning_rate": 4.9704995134609154e-05,
      "loss": 3.5642,
      "step": 10100
    },
    {
      "epoch": 3.306320907617504,
      "grad_norm": 0.7275894284248352,
      "learning_rate": 4.9701783976646125e-05,
      "loss": 3.5543,
      "step": 10200
    },
    {
      "epoch": 3.3387358184764993,
      "grad_norm": 0.9888803958892822,
      "learning_rate": 4.969854038274408e-05,
      "loss": 3.5582,
      "step": 10300
    },
    {
      "epoch": 3.3711507293354943,
      "grad_norm": 0.7633535265922546,
      "learning_rate": 4.9695296788842035e-05,
      "loss": 3.5413,
      "step": 10400
    },
    {
      "epoch": 3.4035656401944894,
      "grad_norm": 0.8675153255462646,
      "learning_rate": 4.9692053194939994e-05,
      "loss": 3.5401,
      "step": 10500
    },
    {
      "epoch": 3.4359805510534844,
      "grad_norm": 0.7103675603866577,
      "learning_rate": 4.968880960103795e-05,
      "loss": 3.5554,
      "step": 10600
    },
    {
      "epoch": 3.46839546191248,
      "grad_norm": 0.8221423029899597,
      "learning_rate": 4.968559844307493e-05,
      "loss": 3.5357,
      "step": 10700
    },
    {
      "epoch": 3.500810372771475,
      "grad_norm": 0.7904021143913269,
      "learning_rate": 4.968235484917288e-05,
      "loss": 3.5593,
      "step": 10800
    },
    {
      "epoch": 3.53322528363047,
      "grad_norm": 0.9460060000419617,
      "learning_rate": 4.967911125527084e-05,
      "loss": 3.5398,
      "step": 10900
    },
    {
      "epoch": 3.565640194489465,
      "grad_norm": 0.8974375128746033,
      "learning_rate": 4.96758676613688e-05,
      "loss": 3.5414,
      "step": 11000
    },
    {
      "epoch": 3.59805510534846,
      "grad_norm": 0.9071260690689087,
      "learning_rate": 4.967262406746675e-05,
      "loss": 3.5415,
      "step": 11100
    },
    {
      "epoch": 3.6304700162074557,
      "grad_norm": 0.8502393364906311,
      "learning_rate": 4.966938047356471e-05,
      "loss": 3.5611,
      "step": 11200
    },
    {
      "epoch": 3.6628849270664503,
      "grad_norm": 0.7328165173530579,
      "learning_rate": 4.966613687966267e-05,
      "loss": 3.5417,
      "step": 11300
    },
    {
      "epoch": 3.695299837925446,
      "grad_norm": 0.7291750311851501,
      "learning_rate": 4.966289328576063e-05,
      "loss": 3.5695,
      "step": 11400
    },
    {
      "epoch": 3.727714748784441,
      "grad_norm": 0.8253569006919861,
      "learning_rate": 4.965964969185859e-05,
      "loss": 3.5445,
      "step": 11500
    },
    {
      "epoch": 3.760129659643436,
      "grad_norm": 0.7859416604042053,
      "learning_rate": 4.965640609795654e-05,
      "loss": 3.5315,
      "step": 11600
    },
    {
      "epoch": 3.792544570502431,
      "grad_norm": 0.8389645218849182,
      "learning_rate": 4.96531625040545e-05,
      "loss": 3.5709,
      "step": 11700
    },
    {
      "epoch": 3.824959481361426,
      "grad_norm": 0.7343776226043701,
      "learning_rate": 4.964991891015245e-05,
      "loss": 3.5557,
      "step": 11800
    },
    {
      "epoch": 3.8573743922204216,
      "grad_norm": 0.8770685791969299,
      "learning_rate": 4.964667531625041e-05,
      "loss": 3.5513,
      "step": 11900
    },
    {
      "epoch": 3.8897893030794166,
      "grad_norm": 0.9362691640853882,
      "learning_rate": 4.964343172234837e-05,
      "loss": 3.5615,
      "step": 12000
    },
    {
      "epoch": 3.9222042139384117,
      "grad_norm": 0.707268238067627,
      "learning_rate": 4.964018812844632e-05,
      "loss": 3.5344,
      "step": 12100
    },
    {
      "epoch": 3.9546191247974067,
      "grad_norm": 0.8025115132331848,
      "learning_rate": 4.963694453454428e-05,
      "loss": 3.5376,
      "step": 12200
    },
    {
      "epoch": 3.987034035656402,
      "grad_norm": 0.8135232329368591,
      "learning_rate": 4.9633700940642237e-05,
      "loss": 3.5404,
      "step": 12300
    },
    {
      "epoch": 4.0,
      "eval_bleu": 0.6307012100847111,
      "eval_loss": 3.7309165000915527,
      "eval_runtime": 6.1573,
      "eval_samples_per_second": 79.905,
      "eval_steps_per_second": 1.299,
      "step": 12340
    },
    {
      "epoch": 4.019448946515397,
      "grad_norm": 0.8447877168655396,
      "learning_rate": 4.963045734674019e-05,
      "loss": 3.536,
      "step": 12400
    },
    {
      "epoch": 4.051863857374392,
      "grad_norm": 0.7224787473678589,
      "learning_rate": 4.962721375283815e-05,
      "loss": 3.5391,
      "step": 12500
    },
    {
      "epoch": 4.084278768233387,
      "grad_norm": 0.7559031248092651,
      "learning_rate": 4.9623970158936106e-05,
      "loss": 3.5234,
      "step": 12600
    },
    {
      "epoch": 4.116693679092383,
      "grad_norm": 0.9309917688369751,
      "learning_rate": 4.962072656503406e-05,
      "loss": 3.5223,
      "step": 12700
    },
    {
      "epoch": 4.1491085899513775,
      "grad_norm": 0.8326523900032043,
      "learning_rate": 4.961748297113202e-05,
      "loss": 3.5204,
      "step": 12800
    },
    {
      "epoch": 4.181523500810373,
      "grad_norm": 0.8649126887321472,
      "learning_rate": 4.961423937722997e-05,
      "loss": 3.5287,
      "step": 12900
    },
    {
      "epoch": 4.213938411669368,
      "grad_norm": 0.8267214894294739,
      "learning_rate": 4.961099578332793e-05,
      "loss": 3.5079,
      "step": 13000
    },
    {
      "epoch": 4.246353322528363,
      "grad_norm": 1.0432873964309692,
      "learning_rate": 4.9607752189425886e-05,
      "loss": 3.5347,
      "step": 13100
    },
    {
      "epoch": 4.278768233387358,
      "grad_norm": 0.7139114141464233,
      "learning_rate": 4.960450859552384e-05,
      "loss": 3.5224,
      "step": 13200
    },
    {
      "epoch": 4.311183144246353,
      "grad_norm": 0.886198103427887,
      "learning_rate": 4.96012650016218e-05,
      "loss": 3.5309,
      "step": 13300
    },
    {
      "epoch": 4.343598055105349,
      "grad_norm": 0.9162964224815369,
      "learning_rate": 4.9598021407719756e-05,
      "loss": 3.5082,
      "step": 13400
    },
    {
      "epoch": 4.376012965964343,
      "grad_norm": 0.8677259683609009,
      "learning_rate": 4.9594777813817714e-05,
      "loss": 3.5319,
      "step": 13500
    },
    {
      "epoch": 4.408427876823339,
      "grad_norm": 0.9781454801559448,
      "learning_rate": 4.9591534219915666e-05,
      "loss": 3.5306,
      "step": 13600
    },
    {
      "epoch": 4.4408427876823335,
      "grad_norm": 0.8927208781242371,
      "learning_rate": 4.9588290626013625e-05,
      "loss": 3.5119,
      "step": 13700
    },
    {
      "epoch": 4.473257698541329,
      "grad_norm": 1.0158946514129639,
      "learning_rate": 4.9585047032111584e-05,
      "loss": 3.5242,
      "step": 13800
    },
    {
      "epoch": 4.5056726094003245,
      "grad_norm": 0.8837922811508179,
      "learning_rate": 4.958180343820954e-05,
      "loss": 3.5304,
      "step": 13900
    },
    {
      "epoch": 4.538087520259319,
      "grad_norm": 0.8325304388999939,
      "learning_rate": 4.9578559844307495e-05,
      "loss": 3.5056,
      "step": 14000
    },
    {
      "epoch": 4.570502431118315,
      "grad_norm": 0.9770678281784058,
      "learning_rate": 4.957531625040545e-05,
      "loss": 3.5208,
      "step": 14100
    },
    {
      "epoch": 4.602917341977309,
      "grad_norm": 0.7594199776649475,
      "learning_rate": 4.957207265650341e-05,
      "loss": 3.5273,
      "step": 14200
    },
    {
      "epoch": 4.635332252836305,
      "grad_norm": 0.7928706407546997,
      "learning_rate": 4.9568829062601364e-05,
      "loss": 3.5146,
      "step": 14300
    },
    {
      "epoch": 4.667747163695299,
      "grad_norm": 0.7732164263725281,
      "learning_rate": 4.956558546869932e-05,
      "loss": 3.5062,
      "step": 14400
    },
    {
      "epoch": 4.700162074554295,
      "grad_norm": 0.7874186635017395,
      "learning_rate": 4.956234187479728e-05,
      "loss": 3.5378,
      "step": 14500
    },
    {
      "epoch": 4.73257698541329,
      "grad_norm": 0.8824012875556946,
      "learning_rate": 4.9559098280895233e-05,
      "loss": 3.5158,
      "step": 14600
    },
    {
      "epoch": 4.764991896272285,
      "grad_norm": 0.8137038350105286,
      "learning_rate": 4.955588712293221e-05,
      "loss": 3.5088,
      "step": 14700
    },
    {
      "epoch": 4.7974068071312805,
      "grad_norm": 0.8203847408294678,
      "learning_rate": 4.955264352903017e-05,
      "loss": 3.5154,
      "step": 14800
    },
    {
      "epoch": 4.829821717990275,
      "grad_norm": 0.8928176164627075,
      "learning_rate": 4.954939993512813e-05,
      "loss": 3.5043,
      "step": 14900
    },
    {
      "epoch": 4.862236628849271,
      "grad_norm": 0.9409657120704651,
      "learning_rate": 4.954615634122608e-05,
      "loss": 3.5109,
      "step": 15000
    },
    {
      "epoch": 4.894651539708266,
      "grad_norm": 0.7923205494880676,
      "learning_rate": 4.954291274732404e-05,
      "loss": 3.5054,
      "step": 15100
    },
    {
      "epoch": 4.927066450567261,
      "grad_norm": 0.7994697093963623,
      "learning_rate": 4.953966915342199e-05,
      "loss": 3.5066,
      "step": 15200
    },
    {
      "epoch": 4.959481361426256,
      "grad_norm": 0.926158607006073,
      "learning_rate": 4.953642555951995e-05,
      "loss": 3.501,
      "step": 15300
    },
    {
      "epoch": 4.991896272285251,
      "grad_norm": 0.7580047249794006,
      "learning_rate": 4.953318196561791e-05,
      "loss": 3.5229,
      "step": 15400
    },
    {
      "epoch": 5.0,
      "eval_bleu": 0.6592546159668585,
      "eval_loss": 3.729104518890381,
      "eval_runtime": 6.2421,
      "eval_samples_per_second": 78.819,
      "eval_steps_per_second": 1.282,
      "step": 15425
    },
    {
      "epoch": 5.024311183144246,
      "grad_norm": 0.8578425645828247,
      "learning_rate": 4.952993837171586e-05,
      "loss": 3.487,
      "step": 15500
    },
    {
      "epoch": 5.056726094003242,
      "grad_norm": 0.8427363634109497,
      "learning_rate": 4.952669477781382e-05,
      "loss": 3.5067,
      "step": 15600
    },
    {
      "epoch": 5.0891410048622365,
      "grad_norm": 0.7459796071052551,
      "learning_rate": 4.952345118391178e-05,
      "loss": 3.4981,
      "step": 15700
    },
    {
      "epoch": 5.121555915721232,
      "grad_norm": 0.8611906170845032,
      "learning_rate": 4.952020759000973e-05,
      "loss": 3.4891,
      "step": 15800
    },
    {
      "epoch": 5.153970826580227,
      "grad_norm": 0.7575498819351196,
      "learning_rate": 4.951696399610769e-05,
      "loss": 3.4769,
      "step": 15900
    },
    {
      "epoch": 5.186385737439222,
      "grad_norm": 0.7891557216644287,
      "learning_rate": 4.951372040220565e-05,
      "loss": 3.4988,
      "step": 16000
    },
    {
      "epoch": 5.218800648298217,
      "grad_norm": 0.8713921904563904,
      "learning_rate": 4.95104768083036e-05,
      "loss": 3.4873,
      "step": 16100
    },
    {
      "epoch": 5.251215559157212,
      "grad_norm": 0.8601099252700806,
      "learning_rate": 4.950723321440156e-05,
      "loss": 3.4976,
      "step": 16200
    },
    {
      "epoch": 5.283630470016208,
      "grad_norm": 0.8631005883216858,
      "learning_rate": 4.950398962049951e-05,
      "loss": 3.5095,
      "step": 16300
    },
    {
      "epoch": 5.316045380875202,
      "grad_norm": 0.7837833762168884,
      "learning_rate": 4.950074602659747e-05,
      "loss": 3.5081,
      "step": 16400
    },
    {
      "epoch": 5.348460291734198,
      "grad_norm": 0.8504587411880493,
      "learning_rate": 4.949750243269543e-05,
      "loss": 3.5043,
      "step": 16500
    },
    {
      "epoch": 5.3808752025931925,
      "grad_norm": 0.9819002151489258,
      "learning_rate": 4.949425883879339e-05,
      "loss": 3.499,
      "step": 16600
    },
    {
      "epoch": 5.413290113452188,
      "grad_norm": 0.7411984205245972,
      "learning_rate": 4.9491015244891345e-05,
      "loss": 3.4931,
      "step": 16700
    },
    {
      "epoch": 5.4457050243111835,
      "grad_norm": 0.900629460811615,
      "learning_rate": 4.9487804086928316e-05,
      "loss": 3.493,
      "step": 16800
    },
    {
      "epoch": 5.478119935170178,
      "grad_norm": 0.8661559820175171,
      "learning_rate": 4.9484560493026275e-05,
      "loss": 3.514,
      "step": 16900
    },
    {
      "epoch": 5.510534846029174,
      "grad_norm": 1.045635461807251,
      "learning_rate": 4.948131689912423e-05,
      "loss": 3.4997,
      "step": 17000
    },
    {
      "epoch": 5.542949756888168,
      "grad_norm": 0.9000488519668579,
      "learning_rate": 4.9478073305222186e-05,
      "loss": 3.5048,
      "step": 17100
    },
    {
      "epoch": 5.575364667747164,
      "grad_norm": 0.8551437854766846,
      "learning_rate": 4.9474829711320145e-05,
      "loss": 3.4743,
      "step": 17200
    },
    {
      "epoch": 5.607779578606159,
      "grad_norm": 0.8399674892425537,
      "learning_rate": 4.9471586117418103e-05,
      "loss": 3.493,
      "step": 17300
    },
    {
      "epoch": 5.640194489465154,
      "grad_norm": 0.8089284896850586,
      "learning_rate": 4.946834252351606e-05,
      "loss": 3.4899,
      "step": 17400
    },
    {
      "epoch": 5.672609400324149,
      "grad_norm": 0.9978110194206238,
      "learning_rate": 4.9465098929614014e-05,
      "loss": 3.4806,
      "step": 17500
    },
    {
      "epoch": 5.705024311183144,
      "grad_norm": 0.7624926567077637,
      "learning_rate": 4.946185533571197e-05,
      "loss": 3.4773,
      "step": 17600
    },
    {
      "epoch": 5.7374392220421395,
      "grad_norm": 0.7706282138824463,
      "learning_rate": 4.945861174180993e-05,
      "loss": 3.5073,
      "step": 17700
    },
    {
      "epoch": 5.769854132901134,
      "grad_norm": 0.8986166715621948,
      "learning_rate": 4.9455368147907884e-05,
      "loss": 3.4777,
      "step": 17800
    },
    {
      "epoch": 5.80226904376013,
      "grad_norm": 0.849778950214386,
      "learning_rate": 4.945212455400584e-05,
      "loss": 3.498,
      "step": 17900
    },
    {
      "epoch": 5.834683954619125,
      "grad_norm": 0.8570396304130554,
      "learning_rate": 4.94488809601038e-05,
      "loss": 3.492,
      "step": 18000
    },
    {
      "epoch": 5.86709886547812,
      "grad_norm": 0.807104229927063,
      "learning_rate": 4.944563736620175e-05,
      "loss": 3.4758,
      "step": 18100
    },
    {
      "epoch": 5.899513776337115,
      "grad_norm": 0.9720584750175476,
      "learning_rate": 4.944239377229971e-05,
      "loss": 3.4737,
      "step": 18200
    },
    {
      "epoch": 5.93192868719611,
      "grad_norm": 1.0952348709106445,
      "learning_rate": 4.943915017839767e-05,
      "loss": 3.482,
      "step": 18300
    },
    {
      "epoch": 5.964343598055105,
      "grad_norm": 0.7507976293563843,
      "learning_rate": 4.943590658449562e-05,
      "loss": 3.4961,
      "step": 18400
    },
    {
      "epoch": 5.9967585089141,
      "grad_norm": 0.9111955165863037,
      "learning_rate": 4.943266299059358e-05,
      "loss": 3.4761,
      "step": 18500
    },
    {
      "epoch": 6.0,
      "eval_bleu": 0.7970503778240688,
      "eval_loss": 3.715768575668335,
      "eval_runtime": 6.2508,
      "eval_samples_per_second": 78.71,
      "eval_steps_per_second": 1.28,
      "step": 18510
    },
    {
      "epoch": 6.0291734197730955,
      "grad_norm": 0.8710297346115112,
      "learning_rate": 4.942941939669153e-05,
      "loss": 3.4704,
      "step": 18600
    },
    {
      "epoch": 6.061588330632091,
      "grad_norm": 0.9526450634002686,
      "learning_rate": 4.942617580278949e-05,
      "loss": 3.4765,
      "step": 18700
    },
    {
      "epoch": 6.094003241491086,
      "grad_norm": 0.8353201746940613,
      "learning_rate": 4.942296464482647e-05,
      "loss": 3.4644,
      "step": 18800
    },
    {
      "epoch": 6.126418152350081,
      "grad_norm": 0.7566527724266052,
      "learning_rate": 4.941972105092443e-05,
      "loss": 3.4615,
      "step": 18900
    },
    {
      "epoch": 6.158833063209076,
      "grad_norm": 0.875457763671875,
      "learning_rate": 4.941647745702238e-05,
      "loss": 3.4777,
      "step": 19000
    },
    {
      "epoch": 6.191247974068071,
      "grad_norm": 0.9333813786506653,
      "learning_rate": 4.941323386312034e-05,
      "loss": 3.4946,
      "step": 19100
    },
    {
      "epoch": 6.223662884927067,
      "grad_norm": 1.037817120552063,
      "learning_rate": 4.94099902692183e-05,
      "loss": 3.475,
      "step": 19200
    },
    {
      "epoch": 6.256077795786061,
      "grad_norm": 0.877434492111206,
      "learning_rate": 4.940674667531625e-05,
      "loss": 3.4872,
      "step": 19300
    },
    {
      "epoch": 6.288492706645057,
      "grad_norm": 1.012824296951294,
      "learning_rate": 4.940350308141421e-05,
      "loss": 3.4737,
      "step": 19400
    },
    {
      "epoch": 6.3209076175040515,
      "grad_norm": 0.8615337014198303,
      "learning_rate": 4.940025948751217e-05,
      "loss": 3.4645,
      "step": 19500
    },
    {
      "epoch": 6.353322528363047,
      "grad_norm": 0.9462717175483704,
      "learning_rate": 4.939701589361012e-05,
      "loss": 3.4556,
      "step": 19600
    },
    {
      "epoch": 6.3857374392220425,
      "grad_norm": 0.8230668306350708,
      "learning_rate": 4.939377229970808e-05,
      "loss": 3.468,
      "step": 19700
    },
    {
      "epoch": 6.418152350081037,
      "grad_norm": 0.8385706543922424,
      "learning_rate": 4.939052870580603e-05,
      "loss": 3.4688,
      "step": 19800
    },
    {
      "epoch": 6.450567260940033,
      "grad_norm": 0.9133042097091675,
      "learning_rate": 4.938728511190399e-05,
      "loss": 3.4669,
      "step": 19900
    },
    {
      "epoch": 6.482982171799027,
      "grad_norm": 0.8159068822860718,
      "learning_rate": 4.938404151800195e-05,
      "loss": 3.4771,
      "step": 20000
    },
    {
      "epoch": 6.515397082658023,
      "grad_norm": 0.900025486946106,
      "learning_rate": 4.9380797924099906e-05,
      "loss": 3.4575,
      "step": 20100
    },
    {
      "epoch": 6.547811993517017,
      "grad_norm": 0.8121553063392639,
      "learning_rate": 4.937755433019786e-05,
      "loss": 3.4718,
      "step": 20200
    },
    {
      "epoch": 6.580226904376013,
      "grad_norm": 0.765598475933075,
      "learning_rate": 4.937431073629582e-05,
      "loss": 3.4812,
      "step": 20300
    },
    {
      "epoch": 6.612641815235008,
      "grad_norm": 1.0192369222640991,
      "learning_rate": 4.9371067142393776e-05,
      "loss": 3.4746,
      "step": 20400
    },
    {
      "epoch": 6.645056726094003,
      "grad_norm": 0.8071571588516235,
      "learning_rate": 4.9367823548491734e-05,
      "loss": 3.4544,
      "step": 20500
    },
    {
      "epoch": 6.6774716369529985,
      "grad_norm": 0.9795412421226501,
      "learning_rate": 4.936457995458969e-05,
      "loss": 3.4779,
      "step": 20600
    },
    {
      "epoch": 6.709886547811994,
      "grad_norm": 0.8193641901016235,
      "learning_rate": 4.9361336360687645e-05,
      "loss": 3.4751,
      "step": 20700
    },
    {
      "epoch": 6.742301458670989,
      "grad_norm": 0.7876419425010681,
      "learning_rate": 4.935812520272462e-05,
      "loss": 3.462,
      "step": 20800
    },
    {
      "epoch": 6.774716369529984,
      "grad_norm": 0.8023172616958618,
      "learning_rate": 4.935488160882258e-05,
      "loss": 3.4654,
      "step": 20900
    },
    {
      "epoch": 6.807131280388979,
      "grad_norm": 0.7273173332214355,
      "learning_rate": 4.9351638014920534e-05,
      "loss": 3.4568,
      "step": 21000
    },
    {
      "epoch": 6.839546191247974,
      "grad_norm": 0.9810540676116943,
      "learning_rate": 4.934839442101849e-05,
      "loss": 3.4545,
      "step": 21100
    },
    {
      "epoch": 6.871961102106969,
      "grad_norm": 0.7891786694526672,
      "learning_rate": 4.934515082711645e-05,
      "loss": 3.4496,
      "step": 21200
    },
    {
      "epoch": 6.904376012965964,
      "grad_norm": 0.7615857720375061,
      "learning_rate": 4.93419072332144e-05,
      "loss": 3.4717,
      "step": 21300
    },
    {
      "epoch": 6.93679092382496,
      "grad_norm": 0.7989175319671631,
      "learning_rate": 4.933866363931236e-05,
      "loss": 3.4872,
      "step": 21400
    },
    {
      "epoch": 6.9692058346839545,
      "grad_norm": 0.8268770575523376,
      "learning_rate": 4.933542004541032e-05,
      "loss": 3.4723,
      "step": 21500
    },
    {
      "epoch": 7.0,
      "eval_bleu": 0.8829464028862931,
      "eval_loss": 3.7154741287231445,
      "eval_runtime": 6.2564,
      "eval_samples_per_second": 78.64,
      "eval_steps_per_second": 1.279,
      "step": 21595
    },
    {
      "epoch": 7.00162074554295,
      "grad_norm": 0.7187463641166687,
      "learning_rate": 4.933217645150827e-05,
      "loss": 3.4584,
      "step": 21600
    },
    {
      "epoch": 7.034035656401945,
      "grad_norm": 0.8198065757751465,
      "learning_rate": 4.932893285760623e-05,
      "loss": 3.4526,
      "step": 21700
    },
    {
      "epoch": 7.06645056726094,
      "grad_norm": 0.8343683481216431,
      "learning_rate": 4.932568926370419e-05,
      "loss": 3.4495,
      "step": 21800
    },
    {
      "epoch": 7.098865478119935,
      "grad_norm": 0.8503733277320862,
      "learning_rate": 4.932244566980214e-05,
      "loss": 3.444,
      "step": 21900
    },
    {
      "epoch": 7.13128038897893,
      "grad_norm": 1.1223938465118408,
      "learning_rate": 4.93192020759001e-05,
      "loss": 3.4382,
      "step": 22000
    },
    {
      "epoch": 7.163695299837926,
      "grad_norm": 0.7490034699440002,
      "learning_rate": 4.931595848199805e-05,
      "loss": 3.446,
      "step": 22100
    },
    {
      "epoch": 7.19611021069692,
      "grad_norm": 0.7460349202156067,
      "learning_rate": 4.931271488809601e-05,
      "loss": 3.464,
      "step": 22200
    },
    {
      "epoch": 7.228525121555916,
      "grad_norm": 0.926662027835846,
      "learning_rate": 4.930947129419397e-05,
      "loss": 3.4402,
      "step": 22300
    },
    {
      "epoch": 7.2609400324149105,
      "grad_norm": 0.790433943271637,
      "learning_rate": 4.930622770029192e-05,
      "loss": 3.4433,
      "step": 22400
    },
    {
      "epoch": 7.293354943273906,
      "grad_norm": 0.8982833027839661,
      "learning_rate": 4.930298410638988e-05,
      "loss": 3.4302,
      "step": 22500
    },
    {
      "epoch": 7.3257698541329015,
      "grad_norm": 0.8117097020149231,
      "learning_rate": 4.929974051248784e-05,
      "loss": 3.4557,
      "step": 22600
    },
    {
      "epoch": 7.358184764991896,
      "grad_norm": 0.8021724820137024,
      "learning_rate": 4.929649691858579e-05,
      "loss": 3.4362,
      "step": 22700
    },
    {
      "epoch": 7.390599675850892,
      "grad_norm": 0.7559502124786377,
      "learning_rate": 4.929325332468375e-05,
      "loss": 3.4414,
      "step": 22800
    },
    {
      "epoch": 7.423014586709886,
      "grad_norm": 0.791221559047699,
      "learning_rate": 4.929004216672073e-05,
      "loss": 3.4565,
      "step": 22900
    },
    {
      "epoch": 7.455429497568882,
      "grad_norm": 0.786803662776947,
      "learning_rate": 4.928679857281869e-05,
      "loss": 3.4702,
      "step": 23000
    },
    {
      "epoch": 7.487844408427877,
      "grad_norm": 0.816240131855011,
      "learning_rate": 4.928355497891664e-05,
      "loss": 3.4549,
      "step": 23100
    },
    {
      "epoch": 7.520259319286872,
      "grad_norm": 0.7387259006500244,
      "learning_rate": 4.92803113850146e-05,
      "loss": 3.4418,
      "step": 23200
    },
    {
      "epoch": 7.552674230145867,
      "grad_norm": 0.9476787447929382,
      "learning_rate": 4.9277067791112556e-05,
      "loss": 3.4491,
      "step": 23300
    },
    {
      "epoch": 7.585089141004862,
      "grad_norm": 0.8801339864730835,
      "learning_rate": 4.927382419721051e-05,
      "loss": 3.4376,
      "step": 23400
    },
    {
      "epoch": 7.6175040518638575,
      "grad_norm": 0.9195380806922913,
      "learning_rate": 4.927058060330847e-05,
      "loss": 3.4532,
      "step": 23500
    },
    {
      "epoch": 7.649918962722852,
      "grad_norm": 0.8113082647323608,
      "learning_rate": 4.926733700940642e-05,
      "loss": 3.466,
      "step": 23600
    },
    {
      "epoch": 7.682333873581848,
      "grad_norm": 0.7607602477073669,
      "learning_rate": 4.926409341550438e-05,
      "loss": 3.4379,
      "step": 23700
    },
    {
      "epoch": 7.714748784440843,
      "grad_norm": 0.8079110980033875,
      "learning_rate": 4.9260849821602337e-05,
      "loss": 3.4649,
      "step": 23800
    },
    {
      "epoch": 7.747163695299838,
      "grad_norm": 0.8193179965019226,
      "learning_rate": 4.9257606227700295e-05,
      "loss": 3.4421,
      "step": 23900
    },
    {
      "epoch": 7.779578606158833,
      "grad_norm": 0.9294320344924927,
      "learning_rate": 4.9254362633798254e-05,
      "loss": 3.4465,
      "step": 24000
    },
    {
      "epoch": 7.811993517017828,
      "grad_norm": 0.7508214116096497,
      "learning_rate": 4.925111903989621e-05,
      "loss": 3.4473,
      "step": 24100
    },
    {
      "epoch": 7.844408427876823,
      "grad_norm": 0.9428737163543701,
      "learning_rate": 4.9247875445994165e-05,
      "loss": 3.4572,
      "step": 24200
    },
    {
      "epoch": 7.876823338735819,
      "grad_norm": 1.0330564975738525,
      "learning_rate": 4.9244631852092124e-05,
      "loss": 3.4457,
      "step": 24300
    },
    {
      "epoch": 7.9092382495948135,
      "grad_norm": 0.7217562794685364,
      "learning_rate": 4.9241388258190075e-05,
      "loss": 3.4545,
      "step": 24400
    },
    {
      "epoch": 7.941653160453809,
      "grad_norm": 0.8484567999839783,
      "learning_rate": 4.9238144664288034e-05,
      "loss": 3.4536,
      "step": 24500
    },
    {
      "epoch": 7.974068071312804,
      "grad_norm": 0.7798573970794678,
      "learning_rate": 4.923490107038599e-05,
      "loss": 3.4417,
      "step": 24600
    },
    {
      "epoch": 8.0,
      "eval_bleu": 1.0004787921048046,
      "eval_loss": 3.715090274810791,
      "eval_runtime": 5.2249,
      "eval_samples_per_second": 94.165,
      "eval_steps_per_second": 1.531,
      "step": 24680
    },
    {
      "epoch": 8.006482982171798,
      "grad_norm": 0.7933902740478516,
      "learning_rate": 4.9231657476483945e-05,
      "loss": 3.4374,
      "step": 24700
    },
    {
      "epoch": 8.038897893030795,
      "grad_norm": 0.8368844985961914,
      "learning_rate": 4.9228413882581904e-05,
      "loss": 3.4302,
      "step": 24800
    },
    {
      "epoch": 8.07131280388979,
      "grad_norm": 0.9147778153419495,
      "learning_rate": 4.922520272461888e-05,
      "loss": 3.4412,
      "step": 24900
    },
    {
      "epoch": 8.103727714748784,
      "grad_norm": 0.7827622294425964,
      "learning_rate": 4.922195913071684e-05,
      "loss": 3.4346,
      "step": 25000
    },
    {
      "epoch": 8.13614262560778,
      "grad_norm": 0.8498703837394714,
      "learning_rate": 4.921871553681479e-05,
      "loss": 3.4432,
      "step": 25100
    },
    {
      "epoch": 8.168557536466775,
      "grad_norm": 0.7766798138618469,
      "learning_rate": 4.921547194291275e-05,
      "loss": 3.4229,
      "step": 25200
    },
    {
      "epoch": 8.20097244732577,
      "grad_norm": 0.752259373664856,
      "learning_rate": 4.921222834901071e-05,
      "loss": 3.4388,
      "step": 25300
    },
    {
      "epoch": 8.233387358184766,
      "grad_norm": 0.8153160810470581,
      "learning_rate": 4.920898475510866e-05,
      "loss": 3.4353,
      "step": 25400
    },
    {
      "epoch": 8.26580226904376,
      "grad_norm": 0.8984590172767639,
      "learning_rate": 4.920574116120662e-05,
      "loss": 3.4161,
      "step": 25500
    },
    {
      "epoch": 8.298217179902755,
      "grad_norm": 0.814103364944458,
      "learning_rate": 4.920249756730458e-05,
      "loss": 3.4428,
      "step": 25600
    },
    {
      "epoch": 8.33063209076175,
      "grad_norm": 0.8530371785163879,
      "learning_rate": 4.919925397340253e-05,
      "loss": 3.4367,
      "step": 25700
    },
    {
      "epoch": 8.363047001620746,
      "grad_norm": 0.7768608331680298,
      "learning_rate": 4.919601037950049e-05,
      "loss": 3.4536,
      "step": 25800
    },
    {
      "epoch": 8.39546191247974,
      "grad_norm": 1.1406590938568115,
      "learning_rate": 4.919276678559844e-05,
      "loss": 3.4259,
      "step": 25900
    },
    {
      "epoch": 8.427876823338735,
      "grad_norm": 0.7988888621330261,
      "learning_rate": 4.91895231916964e-05,
      "loss": 3.4382,
      "step": 26000
    },
    {
      "epoch": 8.460291734197732,
      "grad_norm": 0.8671777248382568,
      "learning_rate": 4.918627959779436e-05,
      "loss": 3.4198,
      "step": 26100
    },
    {
      "epoch": 8.492706645056726,
      "grad_norm": 0.8334693908691406,
      "learning_rate": 4.918303600389231e-05,
      "loss": 3.4444,
      "step": 26200
    },
    {
      "epoch": 8.525121555915721,
      "grad_norm": 0.8665046691894531,
      "learning_rate": 4.917979240999027e-05,
      "loss": 3.4436,
      "step": 26300
    },
    {
      "epoch": 8.557536466774716,
      "grad_norm": 0.766768753528595,
      "learning_rate": 4.917654881608823e-05,
      "loss": 3.4249,
      "step": 26400
    },
    {
      "epoch": 8.589951377633712,
      "grad_norm": 0.7333664894104004,
      "learning_rate": 4.917330522218618e-05,
      "loss": 3.4264,
      "step": 26500
    },
    {
      "epoch": 8.622366288492707,
      "grad_norm": 0.8098047375679016,
      "learning_rate": 4.917006162828414e-05,
      "loss": 3.4393,
      "step": 26600
    },
    {
      "epoch": 8.654781199351701,
      "grad_norm": 0.9209089875221252,
      "learning_rate": 4.91668180343821e-05,
      "loss": 3.4346,
      "step": 26700
    },
    {
      "epoch": 8.687196110210698,
      "grad_norm": 0.7632781863212585,
      "learning_rate": 4.916357444048006e-05,
      "loss": 3.4183,
      "step": 26800
    },
    {
      "epoch": 8.719611021069692,
      "grad_norm": 1.0314304828643799,
      "learning_rate": 4.916036328251703e-05,
      "loss": 3.4186,
      "step": 26900
    },
    {
      "epoch": 8.752025931928687,
      "grad_norm": 0.8825028538703918,
      "learning_rate": 4.915711968861499e-05,
      "loss": 3.4412,
      "step": 27000
    },
    {
      "epoch": 8.784440842787681,
      "grad_norm": 0.7875024080276489,
      "learning_rate": 4.915387609471294e-05,
      "loss": 3.4339,
      "step": 27100
    },
    {
      "epoch": 8.816855753646678,
      "grad_norm": 0.7780051231384277,
      "learning_rate": 4.91506325008109e-05,
      "loss": 3.4171,
      "step": 27200
    },
    {
      "epoch": 8.849270664505672,
      "grad_norm": 0.7799684405326843,
      "learning_rate": 4.9147388906908856e-05,
      "loss": 3.4268,
      "step": 27300
    },
    {
      "epoch": 8.881685575364667,
      "grad_norm": 0.8735037446022034,
      "learning_rate": 4.9144145313006815e-05,
      "loss": 3.4239,
      "step": 27400
    },
    {
      "epoch": 8.914100486223663,
      "grad_norm": 0.8465932607650757,
      "learning_rate": 4.9140901719104774e-05,
      "loss": 3.4346,
      "step": 27500
    },
    {
      "epoch": 8.946515397082658,
      "grad_norm": 0.8385270237922668,
      "learning_rate": 4.913765812520273e-05,
      "loss": 3.4337,
      "step": 27600
    },
    {
      "epoch": 8.978930307941653,
      "grad_norm": 0.7858222723007202,
      "learning_rate": 4.9134414531300684e-05,
      "loss": 3.4272,
      "step": 27700
    },
    {
      "epoch": 9.0,
      "eval_bleu": 0.8281434421816143,
      "eval_loss": 3.7080612182617188,
      "eval_runtime": 6.2515,
      "eval_samples_per_second": 78.701,
      "eval_steps_per_second": 1.28,
      "step": 27765
    },
    {
      "epoch": 9.011345218800649,
      "grad_norm": 0.9232629537582397,
      "learning_rate": 4.913117093739864e-05,
      "loss": 3.4231,
      "step": 27800
    },
    {
      "epoch": 9.043760129659644,
      "grad_norm": 0.7361759543418884,
      "learning_rate": 4.91279273434966e-05,
      "loss": 3.4169,
      "step": 27900
    },
    {
      "epoch": 9.076175040518638,
      "grad_norm": 0.9403635263442993,
      "learning_rate": 4.9124683749594554e-05,
      "loss": 3.4145,
      "step": 28000
    },
    {
      "epoch": 9.108589951377633,
      "grad_norm": 0.7525384426116943,
      "learning_rate": 4.912144015569251e-05,
      "loss": 3.4141,
      "step": 28100
    },
    {
      "epoch": 9.14100486223663,
      "grad_norm": 0.7766296863555908,
      "learning_rate": 4.9118196561790464e-05,
      "loss": 3.4354,
      "step": 28200
    },
    {
      "epoch": 9.173419773095624,
      "grad_norm": 0.8282290697097778,
      "learning_rate": 4.911495296788842e-05,
      "loss": 3.4192,
      "step": 28300
    },
    {
      "epoch": 9.205834683954619,
      "grad_norm": 0.907522439956665,
      "learning_rate": 4.911170937398638e-05,
      "loss": 3.4056,
      "step": 28400
    },
    {
      "epoch": 9.238249594813615,
      "grad_norm": 0.8636339902877808,
      "learning_rate": 4.9108465780084334e-05,
      "loss": 3.4223,
      "step": 28500
    },
    {
      "epoch": 9.27066450567261,
      "grad_norm": 0.8065399527549744,
      "learning_rate": 4.910522218618229e-05,
      "loss": 3.4101,
      "step": 28600
    },
    {
      "epoch": 9.303079416531604,
      "grad_norm": 1.034312129020691,
      "learning_rate": 4.910197859228025e-05,
      "loss": 3.4059,
      "step": 28700
    },
    {
      "epoch": 9.335494327390599,
      "grad_norm": 0.903397262096405,
      "learning_rate": 4.9098734998378203e-05,
      "loss": 3.4079,
      "step": 28800
    },
    {
      "epoch": 9.367909238249595,
      "grad_norm": 0.8351680040359497,
      "learning_rate": 4.909552384041518e-05,
      "loss": 3.4078,
      "step": 28900
    },
    {
      "epoch": 9.40032414910859,
      "grad_norm": 0.9417070746421814,
      "learning_rate": 4.909228024651314e-05,
      "loss": 3.4189,
      "step": 29000
    },
    {
      "epoch": 9.432739059967584,
      "grad_norm": 0.939625084400177,
      "learning_rate": 4.90890366526111e-05,
      "loss": 3.425,
      "step": 29100
    },
    {
      "epoch": 9.46515397082658,
      "grad_norm": 0.8781614899635315,
      "learning_rate": 4.908579305870905e-05,
      "loss": 3.4161,
      "step": 29200
    },
    {
      "epoch": 9.497568881685575,
      "grad_norm": 0.9106858968734741,
      "learning_rate": 4.908254946480701e-05,
      "loss": 3.4279,
      "step": 29300
    },
    {
      "epoch": 9.52998379254457,
      "grad_norm": 0.8736319541931152,
      "learning_rate": 4.907930587090496e-05,
      "loss": 3.4155,
      "step": 29400
    },
    {
      "epoch": 9.562398703403566,
      "grad_norm": 0.8725676536560059,
      "learning_rate": 4.907606227700292e-05,
      "loss": 3.4105,
      "step": 29500
    },
    {
      "epoch": 9.594813614262561,
      "grad_norm": 0.7922500371932983,
      "learning_rate": 4.907281868310088e-05,
      "loss": 3.4112,
      "step": 29600
    },
    {
      "epoch": 9.627228525121556,
      "grad_norm": 0.8992781043052673,
      "learning_rate": 4.906957508919883e-05,
      "loss": 3.3912,
      "step": 29700
    },
    {
      "epoch": 9.65964343598055,
      "grad_norm": 0.9855263233184814,
      "learning_rate": 4.906633149529679e-05,
      "loss": 3.4094,
      "step": 29800
    },
    {
      "epoch": 9.692058346839547,
      "grad_norm": 0.8549126386642456,
      "learning_rate": 4.906308790139475e-05,
      "loss": 3.416,
      "step": 29900
    },
    {
      "epoch": 9.724473257698541,
      "grad_norm": 1.1066476106643677,
      "learning_rate": 4.90598443074927e-05,
      "loss": 3.4096,
      "step": 30000
    },
    {
      "epoch": 9.756888168557536,
      "grad_norm": 0.8771908283233643,
      "learning_rate": 4.905660071359066e-05,
      "loss": 3.4134,
      "step": 30100
    },
    {
      "epoch": 9.789303079416532,
      "grad_norm": 0.9264482855796814,
      "learning_rate": 4.905335711968862e-05,
      "loss": 3.4298,
      "step": 30200
    },
    {
      "epoch": 9.821717990275527,
      "grad_norm": 0.8281336426734924,
      "learning_rate": 4.905011352578657e-05,
      "loss": 3.4376,
      "step": 30300
    },
    {
      "epoch": 9.854132901134522,
      "grad_norm": 0.8348588347434998,
      "learning_rate": 4.904686993188453e-05,
      "loss": 3.3983,
      "step": 30400
    },
    {
      "epoch": 9.886547811993516,
      "grad_norm": 0.8369958400726318,
      "learning_rate": 4.904362633798249e-05,
      "loss": 3.4077,
      "step": 30500
    },
    {
      "epoch": 9.918962722852513,
      "grad_norm": 0.7194177508354187,
      "learning_rate": 4.9040382744080446e-05,
      "loss": 3.4204,
      "step": 30600
    },
    {
      "epoch": 9.951377633711507,
      "grad_norm": 0.8229153752326965,
      "learning_rate": 4.9037139150178405e-05,
      "loss": 3.4241,
      "step": 30700
    },
    {
      "epoch": 9.983792544570502,
      "grad_norm": 0.8907066583633423,
      "learning_rate": 4.903389555627636e-05,
      "loss": 3.437,
      "step": 30800
    },
    {
      "epoch": 10.0,
      "eval_bleu": 0.9655614480615706,
      "eval_loss": 3.7094779014587402,
      "eval_runtime": 4.6576,
      "eval_samples_per_second": 105.634,
      "eval_steps_per_second": 1.718,
      "step": 30850
    },
    {
      "epoch": 10.016207455429498,
      "grad_norm": 0.7805720567703247,
      "learning_rate": 4.9030684398313334e-05,
      "loss": 3.3959,
      "step": 30900
    },
    {
      "epoch": 10.048622366288493,
      "grad_norm": 0.886361300945282,
      "learning_rate": 4.902744080441129e-05,
      "loss": 3.3859,
      "step": 31000
    },
    {
      "epoch": 10.081037277147487,
      "grad_norm": 0.8394315242767334,
      "learning_rate": 4.9024197210509245e-05,
      "loss": 3.3799,
      "step": 31100
    },
    {
      "epoch": 10.113452188006484,
      "grad_norm": 0.940592885017395,
      "learning_rate": 4.9020953616607204e-05,
      "loss": 3.3863,
      "step": 31200
    },
    {
      "epoch": 10.145867098865478,
      "grad_norm": 0.7883304953575134,
      "learning_rate": 4.901771002270516e-05,
      "loss": 3.3925,
      "step": 31300
    },
    {
      "epoch": 10.178282009724473,
      "grad_norm": 0.9217875599861145,
      "learning_rate": 4.901446642880312e-05,
      "loss": 3.3942,
      "step": 31400
    },
    {
      "epoch": 10.210696920583468,
      "grad_norm": 0.8611441850662231,
      "learning_rate": 4.901122283490107e-05,
      "loss": 3.3959,
      "step": 31500
    },
    {
      "epoch": 10.243111831442464,
      "grad_norm": 0.8232434988021851,
      "learning_rate": 4.900797924099903e-05,
      "loss": 3.4082,
      "step": 31600
    },
    {
      "epoch": 10.275526742301459,
      "grad_norm": 0.9620345830917358,
      "learning_rate": 4.9004735647096984e-05,
      "loss": 3.4111,
      "step": 31700
    },
    {
      "epoch": 10.307941653160453,
      "grad_norm": 0.7967596650123596,
      "learning_rate": 4.900149205319494e-05,
      "loss": 3.3961,
      "step": 31800
    },
    {
      "epoch": 10.34035656401945,
      "grad_norm": 0.830910325050354,
      "learning_rate": 4.89982484592929e-05,
      "loss": 3.3953,
      "step": 31900
    },
    {
      "epoch": 10.372771474878444,
      "grad_norm": 0.8295581340789795,
      "learning_rate": 4.8995004865390854e-05,
      "loss": 3.4144,
      "step": 32000
    },
    {
      "epoch": 10.405186385737439,
      "grad_norm": 0.737991452217102,
      "learning_rate": 4.899176127148881e-05,
      "loss": 3.3846,
      "step": 32100
    },
    {
      "epoch": 10.437601296596434,
      "grad_norm": 0.7291622161865234,
      "learning_rate": 4.898851767758677e-05,
      "loss": 3.3945,
      "step": 32200
    },
    {
      "epoch": 10.47001620745543,
      "grad_norm": 0.8039703965187073,
      "learning_rate": 4.898527408368472e-05,
      "loss": 3.4027,
      "step": 32300
    },
    {
      "epoch": 10.502431118314425,
      "grad_norm": 0.8011025786399841,
      "learning_rate": 4.898203048978268e-05,
      "loss": 3.3893,
      "step": 32400
    },
    {
      "epoch": 10.53484602917342,
      "grad_norm": 0.7956675291061401,
      "learning_rate": 4.897878689588064e-05,
      "loss": 3.3985,
      "step": 32500
    },
    {
      "epoch": 10.567260940032416,
      "grad_norm": 0.8990044593811035,
      "learning_rate": 4.897554330197859e-05,
      "loss": 3.4053,
      "step": 32600
    },
    {
      "epoch": 10.59967585089141,
      "grad_norm": 0.7837672829627991,
      "learning_rate": 4.897229970807655e-05,
      "loss": 3.4173,
      "step": 32700
    },
    {
      "epoch": 10.632090761750405,
      "grad_norm": 0.8988843560218811,
      "learning_rate": 4.89690561141745e-05,
      "loss": 3.4172,
      "step": 32800
    },
    {
      "epoch": 10.664505672609401,
      "grad_norm": 0.8741223812103271,
      "learning_rate": 4.896581252027246e-05,
      "loss": 3.419,
      "step": 32900
    },
    {
      "epoch": 10.696920583468396,
      "grad_norm": 0.8134416937828064,
      "learning_rate": 4.896260136230944e-05,
      "loss": 3.3937,
      "step": 33000
    },
    {
      "epoch": 10.72933549432739,
      "grad_norm": 0.8315702676773071,
      "learning_rate": 4.89593577684074e-05,
      "loss": 3.4054,
      "step": 33100
    },
    {
      "epoch": 10.761750405186385,
      "grad_norm": 0.8554663062095642,
      "learning_rate": 4.895611417450535e-05,
      "loss": 3.3954,
      "step": 33200
    },
    {
      "epoch": 10.794165316045381,
      "grad_norm": 0.9145713448524475,
      "learning_rate": 4.895287058060331e-05,
      "loss": 3.3993,
      "step": 33300
    },
    {
      "epoch": 10.826580226904376,
      "grad_norm": 0.7831789255142212,
      "learning_rate": 4.894962698670127e-05,
      "loss": 3.3896,
      "step": 33400
    },
    {
      "epoch": 10.85899513776337,
      "grad_norm": 0.8647592067718506,
      "learning_rate": 4.894638339279922e-05,
      "loss": 3.3969,
      "step": 33500
    },
    {
      "epoch": 10.891410048622367,
      "grad_norm": 0.9198357462882996,
      "learning_rate": 4.894313979889718e-05,
      "loss": 3.4223,
      "step": 33600
    },
    {
      "epoch": 10.923824959481362,
      "grad_norm": 0.8358126282691956,
      "learning_rate": 4.893989620499514e-05,
      "loss": 3.4089,
      "step": 33700
    },
    {
      "epoch": 10.956239870340356,
      "grad_norm": 0.8537364602088928,
      "learning_rate": 4.893665261109309e-05,
      "loss": 3.4152,
      "step": 33800
    },
    {
      "epoch": 10.988654781199351,
      "grad_norm": 0.7143152952194214,
      "learning_rate": 4.893340901719105e-05,
      "loss": 3.416,
      "step": 33900
    },
    {
      "epoch": 11.0,
      "eval_bleu": 0.8220910486297761,
      "eval_loss": 3.7041382789611816,
      "eval_runtime": 5.6209,
      "eval_samples_per_second": 87.531,
      "eval_steps_per_second": 1.423,
      "step": 33935
    },
    {
      "epoch": 11.021069692058347,
      "grad_norm": 0.9280109405517578,
      "learning_rate": 4.893016542328901e-05,
      "loss": 3.3761,
      "step": 34000
    },
    {
      "epoch": 11.053484602917342,
      "grad_norm": 0.8342655897140503,
      "learning_rate": 4.8926921829386966e-05,
      "loss": 3.3783,
      "step": 34100
    },
    {
      "epoch": 11.085899513776337,
      "grad_norm": 0.8240007162094116,
      "learning_rate": 4.8923678235484924e-05,
      "loss": 3.3821,
      "step": 34200
    },
    {
      "epoch": 11.118314424635333,
      "grad_norm": 0.9899024963378906,
      "learning_rate": 4.8920434641582876e-05,
      "loss": 3.397,
      "step": 34300
    },
    {
      "epoch": 11.150729335494328,
      "grad_norm": 0.7792001366615295,
      "learning_rate": 4.8917191047680835e-05,
      "loss": 3.3986,
      "step": 34400
    },
    {
      "epoch": 11.183144246353322,
      "grad_norm": 0.7633922696113586,
      "learning_rate": 4.8913947453778794e-05,
      "loss": 3.3747,
      "step": 34500
    },
    {
      "epoch": 11.215559157212319,
      "grad_norm": 0.9633058309555054,
      "learning_rate": 4.8910703859876746e-05,
      "loss": 3.3843,
      "step": 34600
    },
    {
      "epoch": 11.247974068071313,
      "grad_norm": 0.8085567951202393,
      "learning_rate": 4.8907460265974704e-05,
      "loss": 3.3749,
      "step": 34700
    },
    {
      "epoch": 11.280388978930308,
      "grad_norm": 0.8480896949768066,
      "learning_rate": 4.890421667207266e-05,
      "loss": 3.4032,
      "step": 34800
    },
    {
      "epoch": 11.312803889789302,
      "grad_norm": 0.8480569124221802,
      "learning_rate": 4.8900973078170615e-05,
      "loss": 3.4031,
      "step": 34900
    },
    {
      "epoch": 11.345218800648299,
      "grad_norm": 0.9324108958244324,
      "learning_rate": 4.8897729484268574e-05,
      "loss": 3.3864,
      "step": 35000
    },
    {
      "epoch": 11.377633711507293,
      "grad_norm": 0.8135185837745667,
      "learning_rate": 4.889451832630555e-05,
      "loss": 3.3888,
      "step": 35100
    },
    {
      "epoch": 11.410048622366288,
      "grad_norm": 0.8032956123352051,
      "learning_rate": 4.889127473240351e-05,
      "loss": 3.3824,
      "step": 35200
    },
    {
      "epoch": 11.442463533225284,
      "grad_norm": 0.916986882686615,
      "learning_rate": 4.888803113850146e-05,
      "loss": 3.3777,
      "step": 35300
    },
    {
      "epoch": 11.474878444084279,
      "grad_norm": 0.8818184733390808,
      "learning_rate": 4.888478754459942e-05,
      "loss": 3.3833,
      "step": 35400
    },
    {
      "epoch": 11.507293354943274,
      "grad_norm": 0.7291337847709656,
      "learning_rate": 4.888154395069737e-05,
      "loss": 3.3991,
      "step": 35500
    },
    {
      "epoch": 11.539708265802268,
      "grad_norm": 0.8448677062988281,
      "learning_rate": 4.887830035679533e-05,
      "loss": 3.378,
      "step": 35600
    },
    {
      "epoch": 11.572123176661265,
      "grad_norm": 0.8209165930747986,
      "learning_rate": 4.887505676289329e-05,
      "loss": 3.3733,
      "step": 35700
    },
    {
      "epoch": 11.60453808752026,
      "grad_norm": 0.8654229044914246,
      "learning_rate": 4.887181316899124e-05,
      "loss": 3.3982,
      "step": 35800
    },
    {
      "epoch": 11.636952998379254,
      "grad_norm": 0.8399866819381714,
      "learning_rate": 4.88685695750892e-05,
      "loss": 3.3733,
      "step": 35900
    },
    {
      "epoch": 11.66936790923825,
      "grad_norm": 0.7576026916503906,
      "learning_rate": 4.886532598118716e-05,
      "loss": 3.376,
      "step": 36000
    },
    {
      "epoch": 11.701782820097245,
      "grad_norm": 0.7589974999427795,
      "learning_rate": 4.886208238728511e-05,
      "loss": 3.3802,
      "step": 36100
    },
    {
      "epoch": 11.73419773095624,
      "grad_norm": 0.8555134534835815,
      "learning_rate": 4.885883879338307e-05,
      "loss": 3.3968,
      "step": 36200
    },
    {
      "epoch": 11.766612641815236,
      "grad_norm": 0.8349774479866028,
      "learning_rate": 4.885559519948102e-05,
      "loss": 3.3892,
      "step": 36300
    },
    {
      "epoch": 11.79902755267423,
      "grad_norm": 0.8189638257026672,
      "learning_rate": 4.885235160557898e-05,
      "loss": 3.3923,
      "step": 36400
    },
    {
      "epoch": 11.831442463533225,
      "grad_norm": 0.6906423568725586,
      "learning_rate": 4.884910801167694e-05,
      "loss": 3.3937,
      "step": 36500
    },
    {
      "epoch": 11.86385737439222,
      "grad_norm": 0.8934265375137329,
      "learning_rate": 4.884586441777489e-05,
      "loss": 3.3939,
      "step": 36600
    },
    {
      "epoch": 11.896272285251216,
      "grad_norm": 0.7904755473136902,
      "learning_rate": 4.884262082387285e-05,
      "loss": 3.3738,
      "step": 36700
    },
    {
      "epoch": 11.92868719611021,
      "grad_norm": 0.9543753862380981,
      "learning_rate": 4.883937722997081e-05,
      "loss": 3.3948,
      "step": 36800
    },
    {
      "epoch": 11.961102106969205,
      "grad_norm": 0.896672248840332,
      "learning_rate": 4.883613363606876e-05,
      "loss": 3.3878,
      "step": 36900
    },
    {
      "epoch": 11.993517017828202,
      "grad_norm": 0.8005030751228333,
      "learning_rate": 4.883289004216672e-05,
      "loss": 3.3822,
      "step": 37000
    },
    {
      "epoch": 12.0,
      "eval_bleu": 1.0355870735427646,
      "eval_loss": 3.7030842304229736,
      "eval_runtime": 4.696,
      "eval_samples_per_second": 104.771,
      "eval_steps_per_second": 1.704,
      "step": 37020
    },
    {
      "epoch": 12.025931928687196,
      "grad_norm": 0.8065046072006226,
      "learning_rate": 4.88296788842037e-05,
      "loss": 3.3761,
      "step": 37100
    },
    {
      "epoch": 12.058346839546191,
      "grad_norm": 0.7649751901626587,
      "learning_rate": 4.882643529030166e-05,
      "loss": 3.3568,
      "step": 37200
    },
    {
      "epoch": 12.090761750405186,
      "grad_norm": 0.834297776222229,
      "learning_rate": 4.882319169639961e-05,
      "loss": 3.3731,
      "step": 37300
    },
    {
      "epoch": 12.123176661264182,
      "grad_norm": 0.8063680529594421,
      "learning_rate": 4.881994810249757e-05,
      "loss": 3.3624,
      "step": 37400
    },
    {
      "epoch": 12.155591572123177,
      "grad_norm": 0.8803783655166626,
      "learning_rate": 4.8816704508595526e-05,
      "loss": 3.3798,
      "step": 37500
    },
    {
      "epoch": 12.188006482982171,
      "grad_norm": 1.0279874801635742,
      "learning_rate": 4.8813460914693485e-05,
      "loss": 3.3594,
      "step": 37600
    },
    {
      "epoch": 12.220421393841168,
      "grad_norm": 0.8617162108421326,
      "learning_rate": 4.881021732079144e-05,
      "loss": 3.3673,
      "step": 37700
    },
    {
      "epoch": 12.252836304700162,
      "grad_norm": 0.7166876196861267,
      "learning_rate": 4.8806973726889396e-05,
      "loss": 3.3537,
      "step": 37800
    },
    {
      "epoch": 12.285251215559157,
      "grad_norm": 0.8462795615196228,
      "learning_rate": 4.8803730132987355e-05,
      "loss": 3.3847,
      "step": 37900
    },
    {
      "epoch": 12.317666126418152,
      "grad_norm": 0.9206039905548096,
      "learning_rate": 4.880048653908531e-05,
      "loss": 3.373,
      "step": 38000
    },
    {
      "epoch": 12.350081037277148,
      "grad_norm": 0.9789337515830994,
      "learning_rate": 4.8797242945183265e-05,
      "loss": 3.377,
      "step": 38100
    },
    {
      "epoch": 12.382495948136143,
      "grad_norm": 0.8599682450294495,
      "learning_rate": 4.8793999351281224e-05,
      "loss": 3.3582,
      "step": 38200
    },
    {
      "epoch": 12.414910858995137,
      "grad_norm": 0.8834472298622131,
      "learning_rate": 4.879075575737918e-05,
      "loss": 3.3686,
      "step": 38300
    },
    {
      "epoch": 12.447325769854134,
      "grad_norm": 0.7704265713691711,
      "learning_rate": 4.8787512163477135e-05,
      "loss": 3.4032,
      "step": 38400
    },
    {
      "epoch": 12.479740680713128,
      "grad_norm": 0.7884098291397095,
      "learning_rate": 4.8784268569575093e-05,
      "loss": 3.3907,
      "step": 38500
    },
    {
      "epoch": 12.512155591572123,
      "grad_norm": 0.8370823860168457,
      "learning_rate": 4.8781024975673045e-05,
      "loss": 3.4021,
      "step": 38600
    },
    {
      "epoch": 12.544570502431117,
      "grad_norm": 0.8876157402992249,
      "learning_rate": 4.8777781381771004e-05,
      "loss": 3.3774,
      "step": 38700
    },
    {
      "epoch": 12.576985413290114,
      "grad_norm": 0.7321596145629883,
      "learning_rate": 4.877453778786896e-05,
      "loss": 3.3743,
      "step": 38800
    },
    {
      "epoch": 12.609400324149108,
      "grad_norm": 0.796008288860321,
      "learning_rate": 4.8771294193966915e-05,
      "loss": 3.3736,
      "step": 38900
    },
    {
      "epoch": 12.641815235008103,
      "grad_norm": 0.8847756385803223,
      "learning_rate": 4.8768050600064874e-05,
      "loss": 3.376,
      "step": 39000
    },
    {
      "epoch": 12.6742301458671,
      "grad_norm": 0.9310047030448914,
      "learning_rate": 4.876480700616283e-05,
      "loss": 3.3628,
      "step": 39100
    },
    {
      "epoch": 12.706645056726094,
      "grad_norm": 0.896452009677887,
      "learning_rate": 4.876162828413883e-05,
      "loss": 3.3843,
      "step": 39200
    },
    {
      "epoch": 12.739059967585089,
      "grad_norm": 0.9355397820472717,
      "learning_rate": 4.875838469023679e-05,
      "loss": 3.3903,
      "step": 39300
    },
    {
      "epoch": 12.771474878444085,
      "grad_norm": 0.8211333751678467,
      "learning_rate": 4.875514109633474e-05,
      "loss": 3.3779,
      "step": 39400
    },
    {
      "epoch": 12.80388978930308,
      "grad_norm": 0.8220682740211487,
      "learning_rate": 4.87518975024327e-05,
      "loss": 3.3622,
      "step": 39500
    },
    {
      "epoch": 12.836304700162074,
      "grad_norm": 0.8838047981262207,
      "learning_rate": 4.874865390853066e-05,
      "loss": 3.3647,
      "step": 39600
    },
    {
      "epoch": 12.868719611021069,
      "grad_norm": 0.9197787642478943,
      "learning_rate": 4.874541031462861e-05,
      "loss": 3.3738,
      "step": 39700
    },
    {
      "epoch": 12.901134521880065,
      "grad_norm": 0.9371458292007446,
      "learning_rate": 4.874216672072657e-05,
      "loss": 3.352,
      "step": 39800
    },
    {
      "epoch": 12.93354943273906,
      "grad_norm": 1.0395110845565796,
      "learning_rate": 4.873892312682453e-05,
      "loss": 3.3835,
      "step": 39900
    },
    {
      "epoch": 12.965964343598054,
      "grad_norm": 0.7214142084121704,
      "learning_rate": 4.873567953292248e-05,
      "loss": 3.3663,
      "step": 40000
    },
    {
      "epoch": 12.998379254457051,
      "grad_norm": 0.8672387599945068,
      "learning_rate": 4.873243593902044e-05,
      "loss": 3.3846,
      "step": 40100
    },
    {
      "epoch": 13.0,
      "eval_bleu": 1.1511267285676772,
      "eval_loss": 3.7078146934509277,
      "eval_runtime": 4.9055,
      "eval_samples_per_second": 100.296,
      "eval_steps_per_second": 1.631,
      "step": 40105
    },
    {
      "epoch": 13.030794165316046,
      "grad_norm": 0.82475346326828,
      "learning_rate": 4.8729192345118396e-05,
      "loss": 3.3671,
      "step": 40200
    },
    {
      "epoch": 13.06320907617504,
      "grad_norm": 0.7938998341560364,
      "learning_rate": 4.872594875121635e-05,
      "loss": 3.3538,
      "step": 40300
    },
    {
      "epoch": 13.095623987034037,
      "grad_norm": 0.8610314130783081,
      "learning_rate": 4.872270515731431e-05,
      "loss": 3.3625,
      "step": 40400
    },
    {
      "epoch": 13.128038897893031,
      "grad_norm": 0.7855746746063232,
      "learning_rate": 4.871946156341226e-05,
      "loss": 3.3612,
      "step": 40500
    },
    {
      "epoch": 13.160453808752026,
      "grad_norm": 0.7854007482528687,
      "learning_rate": 4.871621796951022e-05,
      "loss": 3.381,
      "step": 40600
    },
    {
      "epoch": 13.19286871961102,
      "grad_norm": 0.8754494786262512,
      "learning_rate": 4.8712974375608176e-05,
      "loss": 3.365,
      "step": 40700
    },
    {
      "epoch": 13.225283630470017,
      "grad_norm": 0.7802200317382812,
      "learning_rate": 4.870973078170613e-05,
      "loss": 3.359,
      "step": 40800
    },
    {
      "epoch": 13.257698541329011,
      "grad_norm": 0.9991410970687866,
      "learning_rate": 4.870648718780409e-05,
      "loss": 3.3673,
      "step": 40900
    },
    {
      "epoch": 13.290113452188006,
      "grad_norm": 0.8323585987091064,
      "learning_rate": 4.8703243593902046e-05,
      "loss": 3.3508,
      "step": 41000
    },
    {
      "epoch": 13.322528363047002,
      "grad_norm": 0.8840786814689636,
      "learning_rate": 4.87e-05,
      "loss": 3.3744,
      "step": 41100
    },
    {
      "epoch": 13.354943273905997,
      "grad_norm": 0.8019024729728699,
      "learning_rate": 4.869675640609796e-05,
      "loss": 3.3801,
      "step": 41200
    },
    {
      "epoch": 13.387358184764992,
      "grad_norm": 0.8235087394714355,
      "learning_rate": 4.8693512812195915e-05,
      "loss": 3.3556,
      "step": 41300
    },
    {
      "epoch": 13.419773095623986,
      "grad_norm": 0.9544911980628967,
      "learning_rate": 4.8690269218293874e-05,
      "loss": 3.3472,
      "step": 41400
    },
    {
      "epoch": 13.452188006482983,
      "grad_norm": 0.8453919291496277,
      "learning_rate": 4.868702562439183e-05,
      "loss": 3.3555,
      "step": 41500
    },
    {
      "epoch": 13.484602917341977,
      "grad_norm": 0.7895106077194214,
      "learning_rate": 4.8683782030489785e-05,
      "loss": 3.3539,
      "step": 41600
    },
    {
      "epoch": 13.517017828200972,
      "grad_norm": 0.8703041672706604,
      "learning_rate": 4.8680538436587744e-05,
      "loss": 3.3631,
      "step": 41700
    },
    {
      "epoch": 13.549432739059968,
      "grad_norm": 0.8142976760864258,
      "learning_rate": 4.86772948426857e-05,
      "loss": 3.3624,
      "step": 41800
    },
    {
      "epoch": 13.581847649918963,
      "grad_norm": 0.8009085059165955,
      "learning_rate": 4.8674051248783654e-05,
      "loss": 3.3558,
      "step": 41900
    },
    {
      "epoch": 13.614262560777957,
      "grad_norm": 0.8004591464996338,
      "learning_rate": 4.867080765488161e-05,
      "loss": 3.3486,
      "step": 42000
    },
    {
      "epoch": 13.646677471636952,
      "grad_norm": 0.978524386882782,
      "learning_rate": 4.866756406097957e-05,
      "loss": 3.3715,
      "step": 42100
    },
    {
      "epoch": 13.679092382495948,
      "grad_norm": 0.8850526213645935,
      "learning_rate": 4.8664320467077524e-05,
      "loss": 3.3646,
      "step": 42200
    },
    {
      "epoch": 13.711507293354943,
      "grad_norm": 0.8361409306526184,
      "learning_rate": 4.866107687317548e-05,
      "loss": 3.3578,
      "step": 42300
    },
    {
      "epoch": 13.743922204213938,
      "grad_norm": 0.8129996061325073,
      "learning_rate": 4.8657833279273434e-05,
      "loss": 3.3673,
      "step": 42400
    },
    {
      "epoch": 13.776337115072934,
      "grad_norm": 0.8515762090682983,
      "learning_rate": 4.865458968537139e-05,
      "loss": 3.3832,
      "step": 42500
    },
    {
      "epoch": 13.808752025931929,
      "grad_norm": 0.8100226521492004,
      "learning_rate": 4.865134609146935e-05,
      "loss": 3.3578,
      "step": 42600
    },
    {
      "epoch": 13.841166936790923,
      "grad_norm": 0.8504425883293152,
      "learning_rate": 4.8648102497567304e-05,
      "loss": 3.3675,
      "step": 42700
    },
    {
      "epoch": 13.87358184764992,
      "grad_norm": 0.8565456867218018,
      "learning_rate": 4.864485890366526e-05,
      "loss": 3.3336,
      "step": 42800
    },
    {
      "epoch": 13.905996758508914,
      "grad_norm": 0.7441787719726562,
      "learning_rate": 4.864161530976322e-05,
      "loss": 3.3771,
      "step": 42900
    },
    {
      "epoch": 13.938411669367909,
      "grad_norm": 0.8569518327713013,
      "learning_rate": 4.863837171586117e-05,
      "loss": 3.3445,
      "step": 43000
    },
    {
      "epoch": 13.970826580226904,
      "grad_norm": 0.9077377319335938,
      "learning_rate": 4.863512812195913e-05,
      "loss": 3.3531,
      "step": 43100
    },
    {
      "epoch": 14.0,
      "eval_bleu": 1.0132972277992491,
      "eval_loss": 3.7006540298461914,
      "eval_runtime": 4.8889,
      "eval_samples_per_second": 100.637,
      "eval_steps_per_second": 1.636,
      "step": 43190
    },
    {
      "epoch": 14.0032414910859,
      "grad_norm": 0.8988618850708008,
      "learning_rate": 4.863191696399611e-05,
      "loss": 3.3517,
      "step": 43200
    },
    {
      "epoch": 14.035656401944895,
      "grad_norm": 0.9054036736488342,
      "learning_rate": 4.862867337009407e-05,
      "loss": 3.3631,
      "step": 43300
    },
    {
      "epoch": 14.06807131280389,
      "grad_norm": 0.7324233651161194,
      "learning_rate": 4.862542977619202e-05,
      "loss": 3.3433,
      "step": 43400
    },
    {
      "epoch": 14.100486223662886,
      "grad_norm": 0.7733725905418396,
      "learning_rate": 4.862218618228998e-05,
      "loss": 3.3461,
      "step": 43500
    },
    {
      "epoch": 14.13290113452188,
      "grad_norm": 0.8425273299217224,
      "learning_rate": 4.861894258838793e-05,
      "loss": 3.3542,
      "step": 43600
    },
    {
      "epoch": 14.165316045380875,
      "grad_norm": 0.8036438226699829,
      "learning_rate": 4.861569899448589e-05,
      "loss": 3.3511,
      "step": 43700
    },
    {
      "epoch": 14.19773095623987,
      "grad_norm": 0.8570528626441956,
      "learning_rate": 4.861245540058385e-05,
      "loss": 3.3503,
      "step": 43800
    },
    {
      "epoch": 14.230145867098866,
      "grad_norm": 0.8350628614425659,
      "learning_rate": 4.86092118066818e-05,
      "loss": 3.3469,
      "step": 43900
    },
    {
      "epoch": 14.26256077795786,
      "grad_norm": 0.8465161919593811,
      "learning_rate": 4.860596821277976e-05,
      "loss": 3.3369,
      "step": 44000
    },
    {
      "epoch": 14.294975688816855,
      "grad_norm": 1.080013394355774,
      "learning_rate": 4.860272461887772e-05,
      "loss": 3.3493,
      "step": 44100
    },
    {
      "epoch": 14.327390599675851,
      "grad_norm": 0.9563872814178467,
      "learning_rate": 4.859948102497568e-05,
      "loss": 3.3461,
      "step": 44200
    },
    {
      "epoch": 14.359805510534846,
      "grad_norm": 0.9281702041625977,
      "learning_rate": 4.8596237431073636e-05,
      "loss": 3.3342,
      "step": 44300
    },
    {
      "epoch": 14.39222042139384,
      "grad_norm": 0.9704748392105103,
      "learning_rate": 4.859299383717159e-05,
      "loss": 3.3419,
      "step": 44400
    },
    {
      "epoch": 14.424635332252837,
      "grad_norm": 0.8847483396530151,
      "learning_rate": 4.8589750243269546e-05,
      "loss": 3.367,
      "step": 44500
    },
    {
      "epoch": 14.457050243111832,
      "grad_norm": 0.869661808013916,
      "learning_rate": 4.8586506649367505e-05,
      "loss": 3.3467,
      "step": 44600
    },
    {
      "epoch": 14.489465153970826,
      "grad_norm": 0.7724795937538147,
      "learning_rate": 4.858326305546546e-05,
      "loss": 3.3668,
      "step": 44700
    },
    {
      "epoch": 14.521880064829821,
      "grad_norm": 0.7809782028198242,
      "learning_rate": 4.8580019461563416e-05,
      "loss": 3.341,
      "step": 44800
    },
    {
      "epoch": 14.554294975688817,
      "grad_norm": 0.7822554707527161,
      "learning_rate": 4.8576775867661375e-05,
      "loss": 3.374,
      "step": 44900
    },
    {
      "epoch": 14.586709886547812,
      "grad_norm": 0.7946828603744507,
      "learning_rate": 4.857353227375933e-05,
      "loss": 3.3275,
      "step": 45000
    },
    {
      "epoch": 14.619124797406807,
      "grad_norm": 0.8408553600311279,
      "learning_rate": 4.8570288679857285e-05,
      "loss": 3.338,
      "step": 45100
    },
    {
      "epoch": 14.651539708265803,
      "grad_norm": 0.859521746635437,
      "learning_rate": 4.856707752189426e-05,
      "loss": 3.3738,
      "step": 45200
    },
    {
      "epoch": 14.683954619124798,
      "grad_norm": 0.8851708769798279,
      "learning_rate": 4.856383392799222e-05,
      "loss": 3.3401,
      "step": 45300
    },
    {
      "epoch": 14.716369529983792,
      "grad_norm": 0.9310758113861084,
      "learning_rate": 4.856062277002919e-05,
      "loss": 3.3401,
      "step": 45400
    },
    {
      "epoch": 14.748784440842787,
      "grad_norm": 0.9620280265808105,
      "learning_rate": 4.855737917612715e-05,
      "loss": 3.3586,
      "step": 45500
    },
    {
      "epoch": 14.781199351701783,
      "grad_norm": 0.9921422600746155,
      "learning_rate": 4.855413558222511e-05,
      "loss": 3.3392,
      "step": 45600
    },
    {
      "epoch": 14.813614262560778,
      "grad_norm": 0.8462003469467163,
      "learning_rate": 4.855089198832307e-05,
      "loss": 3.335,
      "step": 45700
    },
    {
      "epoch": 14.846029173419772,
      "grad_norm": 0.7915138602256775,
      "learning_rate": 4.854764839442102e-05,
      "loss": 3.3378,
      "step": 45800
    },
    {
      "epoch": 14.878444084278769,
      "grad_norm": 0.7702897191047668,
      "learning_rate": 4.854440480051898e-05,
      "loss": 3.3663,
      "step": 45900
    },
    {
      "epoch": 14.910858995137763,
      "grad_norm": 0.8512548804283142,
      "learning_rate": 4.854116120661694e-05,
      "loss": 3.3492,
      "step": 46000
    },
    {
      "epoch": 14.943273905996758,
      "grad_norm": 0.9207866191864014,
      "learning_rate": 4.853791761271489e-05,
      "loss": 3.3616,
      "step": 46100
    },
    {
      "epoch": 14.975688816855754,
      "grad_norm": 0.9523090124130249,
      "learning_rate": 4.853467401881285e-05,
      "loss": 3.3529,
      "step": 46200
    },
    {
      "epoch": 15.0,
      "eval_bleu": 1.1537377872178993,
      "eval_loss": 3.7087063789367676,
      "eval_runtime": 5.1524,
      "eval_samples_per_second": 95.489,
      "eval_steps_per_second": 1.553,
      "step": 46275
    },
    {
      "epoch": 15.008103727714749,
      "grad_norm": 0.9417809247970581,
      "learning_rate": 4.85314304249108e-05,
      "loss": 3.3442,
      "step": 46300
    },
    {
      "epoch": 15.040518638573744,
      "grad_norm": 0.8943811058998108,
      "learning_rate": 4.852818683100876e-05,
      "loss": 3.3184,
      "step": 46400
    },
    {
      "epoch": 15.072933549432738,
      "grad_norm": 0.7843816876411438,
      "learning_rate": 4.852494323710672e-05,
      "loss": 3.3397,
      "step": 46500
    },
    {
      "epoch": 15.105348460291735,
      "grad_norm": 0.9370750188827515,
      "learning_rate": 4.852169964320467e-05,
      "loss": 3.34,
      "step": 46600
    },
    {
      "epoch": 15.13776337115073,
      "grad_norm": 0.9445221424102783,
      "learning_rate": 4.851845604930263e-05,
      "loss": 3.3319,
      "step": 46700
    },
    {
      "epoch": 15.170178282009724,
      "grad_norm": 0.8792291879653931,
      "learning_rate": 4.851521245540059e-05,
      "loss": 3.3427,
      "step": 46800
    },
    {
      "epoch": 15.20259319286872,
      "grad_norm": 0.8570343255996704,
      "learning_rate": 4.851196886149854e-05,
      "loss": 3.339,
      "step": 46900
    },
    {
      "epoch": 15.235008103727715,
      "grad_norm": 0.850071370601654,
      "learning_rate": 4.85087252675965e-05,
      "loss": 3.3269,
      "step": 47000
    },
    {
      "epoch": 15.26742301458671,
      "grad_norm": 0.7588948607444763,
      "learning_rate": 4.850548167369446e-05,
      "loss": 3.3426,
      "step": 47100
    },
    {
      "epoch": 15.299837925445704,
      "grad_norm": 0.9772857427597046,
      "learning_rate": 4.850223807979241e-05,
      "loss": 3.333,
      "step": 47200
    },
    {
      "epoch": 15.3322528363047,
      "grad_norm": 0.8456433415412903,
      "learning_rate": 4.849899448589037e-05,
      "loss": 3.3507,
      "step": 47300
    },
    {
      "epoch": 15.364667747163695,
      "grad_norm": 0.723223090171814,
      "learning_rate": 4.849575089198832e-05,
      "loss": 3.356,
      "step": 47400
    },
    {
      "epoch": 15.39708265802269,
      "grad_norm": 0.8496412634849548,
      "learning_rate": 4.849250729808628e-05,
      "loss": 3.3239,
      "step": 47500
    },
    {
      "epoch": 15.429497568881686,
      "grad_norm": 0.8923414945602417,
      "learning_rate": 4.848926370418424e-05,
      "loss": 3.3335,
      "step": 47600
    },
    {
      "epoch": 15.46191247974068,
      "grad_norm": 0.8233221173286438,
      "learning_rate": 4.848602011028219e-05,
      "loss": 3.3336,
      "step": 47700
    },
    {
      "epoch": 15.494327390599675,
      "grad_norm": 0.7854989171028137,
      "learning_rate": 4.848277651638015e-05,
      "loss": 3.3426,
      "step": 47800
    },
    {
      "epoch": 15.526742301458672,
      "grad_norm": 0.7916027903556824,
      "learning_rate": 4.847953292247811e-05,
      "loss": 3.3319,
      "step": 47900
    },
    {
      "epoch": 15.559157212317666,
      "grad_norm": 0.969714343547821,
      "learning_rate": 4.8476289328576066e-05,
      "loss": 3.3404,
      "step": 48000
    },
    {
      "epoch": 15.591572123176661,
      "grad_norm": 0.8394405245780945,
      "learning_rate": 4.8473045734674025e-05,
      "loss": 3.3418,
      "step": 48100
    },
    {
      "epoch": 15.623987034035656,
      "grad_norm": 0.8447447419166565,
      "learning_rate": 4.846980214077198e-05,
      "loss": 3.3265,
      "step": 48200
    },
    {
      "epoch": 15.656401944894652,
      "grad_norm": 0.6869207620620728,
      "learning_rate": 4.8466558546869935e-05,
      "loss": 3.3485,
      "step": 48300
    },
    {
      "epoch": 15.688816855753647,
      "grad_norm": 0.8515943884849548,
      "learning_rate": 4.8463314952967894e-05,
      "loss": 3.3234,
      "step": 48400
    },
    {
      "epoch": 15.721231766612641,
      "grad_norm": 0.8852052688598633,
      "learning_rate": 4.8460071359065846e-05,
      "loss": 3.3463,
      "step": 48500
    },
    {
      "epoch": 15.753646677471638,
      "grad_norm": 0.9988545179367065,
      "learning_rate": 4.8456827765163805e-05,
      "loss": 3.3462,
      "step": 48600
    },
    {
      "epoch": 15.786061588330632,
      "grad_norm": 0.7237036824226379,
      "learning_rate": 4.8453584171261764e-05,
      "loss": 3.3419,
      "step": 48700
    },
    {
      "epoch": 15.818476499189627,
      "grad_norm": 0.8551185131072998,
      "learning_rate": 4.8450340577359716e-05,
      "loss": 3.3508,
      "step": 48800
    },
    {
      "epoch": 15.850891410048622,
      "grad_norm": 0.817893385887146,
      "learning_rate": 4.8447096983457674e-05,
      "loss": 3.3353,
      "step": 48900
    },
    {
      "epoch": 15.883306320907618,
      "grad_norm": 1.0606361627578735,
      "learning_rate": 4.844385338955563e-05,
      "loss": 3.3246,
      "step": 49000
    },
    {
      "epoch": 15.915721231766613,
      "grad_norm": 0.9245190024375916,
      "learning_rate": 4.844064223159261e-05,
      "loss": 3.3463,
      "step": 49100
    },
    {
      "epoch": 15.948136142625607,
      "grad_norm": 0.8503705263137817,
      "learning_rate": 4.843739863769056e-05,
      "loss": 3.3541,
      "step": 49200
    },
    {
      "epoch": 15.980551053484604,
      "grad_norm": 0.8341427445411682,
      "learning_rate": 4.843415504378852e-05,
      "loss": 3.3379,
      "step": 49300
    },
    {
      "epoch": 16.0,
      "eval_bleu": 1.203098299317556,
      "eval_loss": 3.7052061557769775,
      "eval_runtime": 4.7458,
      "eval_samples_per_second": 103.671,
      "eval_steps_per_second": 1.686,
      "step": 49360
    },
    {
      "epoch": 16.012965964343596,
      "grad_norm": 1.1828787326812744,
      "learning_rate": 4.843091144988648e-05,
      "loss": 3.3108,
      "step": 49400
    },
    {
      "epoch": 16.045380875202593,
      "grad_norm": 0.8910539746284485,
      "learning_rate": 4.842766785598443e-05,
      "loss": 3.3057,
      "step": 49500
    },
    {
      "epoch": 16.07779578606159,
      "grad_norm": 0.7665195465087891,
      "learning_rate": 4.842442426208239e-05,
      "loss": 3.336,
      "step": 49600
    },
    {
      "epoch": 16.110210696920582,
      "grad_norm": 0.7954614758491516,
      "learning_rate": 4.842118066818034e-05,
      "loss": 3.3348,
      "step": 49700
    },
    {
      "epoch": 16.14262560777958,
      "grad_norm": 0.7679864168167114,
      "learning_rate": 4.84179370742783e-05,
      "loss": 3.328,
      "step": 49800
    },
    {
      "epoch": 16.175040518638575,
      "grad_norm": 0.855457067489624,
      "learning_rate": 4.841469348037626e-05,
      "loss": 3.3238,
      "step": 49900
    },
    {
      "epoch": 16.207455429497568,
      "grad_norm": 0.7976498007774353,
      "learning_rate": 4.841144988647421e-05,
      "loss": 3.3274,
      "step": 50000
    },
    {
      "epoch": 16.239870340356564,
      "grad_norm": 0.8051173686981201,
      "learning_rate": 4.840820629257217e-05,
      "loss": 3.3208,
      "step": 50100
    },
    {
      "epoch": 16.27228525121556,
      "grad_norm": 0.8869597911834717,
      "learning_rate": 4.840496269867013e-05,
      "loss": 3.3304,
      "step": 50200
    },
    {
      "epoch": 16.304700162074553,
      "grad_norm": 0.8209633827209473,
      "learning_rate": 4.840171910476808e-05,
      "loss": 3.3184,
      "step": 50300
    },
    {
      "epoch": 16.33711507293355,
      "grad_norm": 0.8491031527519226,
      "learning_rate": 4.839847551086604e-05,
      "loss": 3.3238,
      "step": 50400
    },
    {
      "epoch": 16.369529983792546,
      "grad_norm": 0.965071976184845,
      "learning_rate": 4.8395231916964e-05,
      "loss": 3.3382,
      "step": 50500
    },
    {
      "epoch": 16.40194489465154,
      "grad_norm": 0.790172278881073,
      "learning_rate": 4.839198832306195e-05,
      "loss": 3.3187,
      "step": 50600
    },
    {
      "epoch": 16.434359805510535,
      "grad_norm": 0.7999597787857056,
      "learning_rate": 4.838874472915991e-05,
      "loss": 3.3305,
      "step": 50700
    },
    {
      "epoch": 16.46677471636953,
      "grad_norm": 0.6984388828277588,
      "learning_rate": 4.838550113525787e-05,
      "loss": 3.3406,
      "step": 50800
    },
    {
      "epoch": 16.499189627228525,
      "grad_norm": 0.9485738277435303,
      "learning_rate": 4.838225754135583e-05,
      "loss": 3.3277,
      "step": 50900
    },
    {
      "epoch": 16.53160453808752,
      "grad_norm": 1.1639106273651123,
      "learning_rate": 4.837901394745378e-05,
      "loss": 3.3373,
      "step": 51000
    },
    {
      "epoch": 16.564019448946514,
      "grad_norm": 0.8269292116165161,
      "learning_rate": 4.837577035355174e-05,
      "loss": 3.3266,
      "step": 51100
    },
    {
      "epoch": 16.59643435980551,
      "grad_norm": 0.9172484874725342,
      "learning_rate": 4.83725267596497e-05,
      "loss": 3.3415,
      "step": 51200
    },
    {
      "epoch": 16.628849270664507,
      "grad_norm": 0.8772260546684265,
      "learning_rate": 4.8369283165747656e-05,
      "loss": 3.335,
      "step": 51300
    },
    {
      "epoch": 16.6612641815235,
      "grad_norm": 0.7764860391616821,
      "learning_rate": 4.836603957184561e-05,
      "loss": 3.3266,
      "step": 51400
    },
    {
      "epoch": 16.693679092382496,
      "grad_norm": 0.9977094531059265,
      "learning_rate": 4.8362795977943567e-05,
      "loss": 3.3129,
      "step": 51500
    },
    {
      "epoch": 16.726094003241492,
      "grad_norm": 0.9557291269302368,
      "learning_rate": 4.835955238404152e-05,
      "loss": 3.3239,
      "step": 51600
    },
    {
      "epoch": 16.758508914100485,
      "grad_norm": 0.9720548391342163,
      "learning_rate": 4.835630879013948e-05,
      "loss": 3.3113,
      "step": 51700
    },
    {
      "epoch": 16.79092382495948,
      "grad_norm": 0.8471057415008545,
      "learning_rate": 4.8353065196237436e-05,
      "loss": 3.3469,
      "step": 51800
    },
    {
      "epoch": 16.823338735818478,
      "grad_norm": 0.9792506098747253,
      "learning_rate": 4.834982160233539e-05,
      "loss": 3.3326,
      "step": 51900
    },
    {
      "epoch": 16.85575364667747,
      "grad_norm": 0.8296935558319092,
      "learning_rate": 4.834657800843335e-05,
      "loss": 3.3315,
      "step": 52000
    },
    {
      "epoch": 16.888168557536467,
      "grad_norm": 0.8569448590278625,
      "learning_rate": 4.8343334414531305e-05,
      "loss": 3.3397,
      "step": 52100
    },
    {
      "epoch": 16.920583468395463,
      "grad_norm": 0.880716860294342,
      "learning_rate": 4.834009082062926e-05,
      "loss": 3.3256,
      "step": 52200
    },
    {
      "epoch": 16.952998379254456,
      "grad_norm": 0.8841325044631958,
      "learning_rate": 4.8336847226727216e-05,
      "loss": 3.3444,
      "step": 52300
    },
    {
      "epoch": 16.985413290113453,
      "grad_norm": 0.8976461291313171,
      "learning_rate": 4.8333603632825175e-05,
      "loss": 3.3193,
      "step": 52400
    },
    {
      "epoch": 17.0,
      "eval_bleu": 1.195822658771148,
      "eval_loss": 3.703604221343994,
      "eval_runtime": 4.5689,
      "eval_samples_per_second": 107.685,
      "eval_steps_per_second": 1.751,
      "step": 52445
    },
    {
      "epoch": 17.01782820097245,
      "grad_norm": 0.7072165012359619,
      "learning_rate": 4.833036003892313e-05,
      "loss": 3.3122,
      "step": 52500
    },
    {
      "epoch": 17.050243111831442,
      "grad_norm": 0.8071543574333191,
      "learning_rate": 4.8327116445021086e-05,
      "loss": 3.3092,
      "step": 52600
    },
    {
      "epoch": 17.08265802269044,
      "grad_norm": 0.7598993182182312,
      "learning_rate": 4.832387285111904e-05,
      "loss": 3.3185,
      "step": 52700
    },
    {
      "epoch": 17.11507293354943,
      "grad_norm": 0.8395270705223083,
      "learning_rate": 4.8320629257216996e-05,
      "loss": 3.3079,
      "step": 52800
    },
    {
      "epoch": 17.147487844408428,
      "grad_norm": 1.0392311811447144,
      "learning_rate": 4.8317385663314955e-05,
      "loss": 3.3151,
      "step": 52900
    },
    {
      "epoch": 17.179902755267424,
      "grad_norm": 0.9456632137298584,
      "learning_rate": 4.831414206941291e-05,
      "loss": 3.3176,
      "step": 53000
    },
    {
      "epoch": 17.212317666126417,
      "grad_norm": 0.8059220910072327,
      "learning_rate": 4.8310930911449885e-05,
      "loss": 3.3103,
      "step": 53100
    },
    {
      "epoch": 17.244732576985413,
      "grad_norm": 1.0290120840072632,
      "learning_rate": 4.8307687317547844e-05,
      "loss": 3.3065,
      "step": 53200
    },
    {
      "epoch": 17.27714748784441,
      "grad_norm": 0.8471291661262512,
      "learning_rate": 4.83044437236458e-05,
      "loss": 3.3138,
      "step": 53300
    },
    {
      "epoch": 17.309562398703402,
      "grad_norm": 0.7629430890083313,
      "learning_rate": 4.8301200129743754e-05,
      "loss": 3.322,
      "step": 53400
    },
    {
      "epoch": 17.3419773095624,
      "grad_norm": 0.867941677570343,
      "learning_rate": 4.829798897178073e-05,
      "loss": 3.3355,
      "step": 53500
    },
    {
      "epoch": 17.374392220421395,
      "grad_norm": 0.902538537979126,
      "learning_rate": 4.829474537787869e-05,
      "loss": 3.2908,
      "step": 53600
    },
    {
      "epoch": 17.406807131280388,
      "grad_norm": 0.8464263677597046,
      "learning_rate": 4.829150178397665e-05,
      "loss": 3.3251,
      "step": 53700
    },
    {
      "epoch": 17.439222042139384,
      "grad_norm": 1.0067048072814941,
      "learning_rate": 4.82882581900746e-05,
      "loss": 3.331,
      "step": 53800
    },
    {
      "epoch": 17.47163695299838,
      "grad_norm": 0.9391472935676575,
      "learning_rate": 4.828501459617256e-05,
      "loss": 3.323,
      "step": 53900
    },
    {
      "epoch": 17.504051863857374,
      "grad_norm": 1.079687476158142,
      "learning_rate": 4.828177100227052e-05,
      "loss": 3.3001,
      "step": 54000
    },
    {
      "epoch": 17.53646677471637,
      "grad_norm": 0.8194747567176819,
      "learning_rate": 4.827852740836847e-05,
      "loss": 3.3266,
      "step": 54100
    },
    {
      "epoch": 17.568881685575363,
      "grad_norm": 0.8438014388084412,
      "learning_rate": 4.827528381446643e-05,
      "loss": 3.3145,
      "step": 54200
    },
    {
      "epoch": 17.60129659643436,
      "grad_norm": 0.8633841276168823,
      "learning_rate": 4.827204022056439e-05,
      "loss": 3.3108,
      "step": 54300
    },
    {
      "epoch": 17.633711507293356,
      "grad_norm": 0.8475048542022705,
      "learning_rate": 4.826879662666234e-05,
      "loss": 3.3335,
      "step": 54400
    },
    {
      "epoch": 17.66612641815235,
      "grad_norm": 0.9571916460990906,
      "learning_rate": 4.82655530327603e-05,
      "loss": 3.3182,
      "step": 54500
    },
    {
      "epoch": 17.698541329011345,
      "grad_norm": 0.8556811213493347,
      "learning_rate": 4.826230943885826e-05,
      "loss": 3.3139,
      "step": 54600
    },
    {
      "epoch": 17.73095623987034,
      "grad_norm": 0.8787357211112976,
      "learning_rate": 4.825906584495622e-05,
      "loss": 3.3287,
      "step": 54700
    },
    {
      "epoch": 17.763371150729334,
      "grad_norm": 0.9204088449478149,
      "learning_rate": 4.8255822251054175e-05,
      "loss": 3.3125,
      "step": 54800
    },
    {
      "epoch": 17.79578606158833,
      "grad_norm": 0.7290030121803284,
      "learning_rate": 4.825257865715213e-05,
      "loss": 3.3438,
      "step": 54900
    },
    {
      "epoch": 17.828200972447327,
      "grad_norm": 0.7656348347663879,
      "learning_rate": 4.8249335063250086e-05,
      "loss": 3.3073,
      "step": 55000
    },
    {
      "epoch": 17.86061588330632,
      "grad_norm": 0.8350306153297424,
      "learning_rate": 4.8246091469348045e-05,
      "loss": 3.3236,
      "step": 55100
    },
    {
      "epoch": 17.893030794165316,
      "grad_norm": 0.7929808497428894,
      "learning_rate": 4.8242847875446e-05,
      "loss": 3.3128,
      "step": 55200
    },
    {
      "epoch": 17.925445705024313,
      "grad_norm": 0.8643037676811218,
      "learning_rate": 4.8239604281543956e-05,
      "loss": 3.3097,
      "step": 55300
    },
    {
      "epoch": 17.957860615883305,
      "grad_norm": 0.8506174683570862,
      "learning_rate": 4.823636068764191e-05,
      "loss": 3.3399,
      "step": 55400
    },
    {
      "epoch": 17.990275526742302,
      "grad_norm": 0.8854289650917053,
      "learning_rate": 4.8233117093739866e-05,
      "loss": 3.3094,
      "step": 55500
    },
    {
      "epoch": 18.0,
      "eval_bleu": 0.8687162579092379,
      "eval_loss": 3.7058145999908447,
      "eval_runtime": 4.4928,
      "eval_samples_per_second": 109.509,
      "eval_steps_per_second": 1.781,
      "step": 55530
    },
    {
      "epoch": 18.022690437601298,
      "grad_norm": 0.8606845736503601,
      "learning_rate": 4.8229873499837825e-05,
      "loss": 3.3222,
      "step": 55600
    },
    {
      "epoch": 18.05510534846029,
      "grad_norm": 0.8115867376327515,
      "learning_rate": 4.822662990593578e-05,
      "loss": 3.3019,
      "step": 55700
    },
    {
      "epoch": 18.087520259319287,
      "grad_norm": 0.7867512106895447,
      "learning_rate": 4.8223386312033736e-05,
      "loss": 3.2931,
      "step": 55800
    },
    {
      "epoch": 18.11993517017828,
      "grad_norm": 0.8762028813362122,
      "learning_rate": 4.8220142718131694e-05,
      "loss": 3.2898,
      "step": 55900
    },
    {
      "epoch": 18.152350081037277,
      "grad_norm": 0.9606066346168518,
      "learning_rate": 4.8216899124229646e-05,
      "loss": 3.3274,
      "step": 56000
    },
    {
      "epoch": 18.184764991896273,
      "grad_norm": 1.045259714126587,
      "learning_rate": 4.8213655530327605e-05,
      "loss": 3.3259,
      "step": 56100
    },
    {
      "epoch": 18.217179902755266,
      "grad_norm": 0.8279439806938171,
      "learning_rate": 4.821041193642556e-05,
      "loss": 3.3054,
      "step": 56200
    },
    {
      "epoch": 18.249594813614262,
      "grad_norm": 0.9112544655799866,
      "learning_rate": 4.8207168342523516e-05,
      "loss": 3.299,
      "step": 56300
    },
    {
      "epoch": 18.28200972447326,
      "grad_norm": 0.8794892430305481,
      "learning_rate": 4.8203924748621475e-05,
      "loss": 3.292,
      "step": 56400
    },
    {
      "epoch": 18.31442463533225,
      "grad_norm": 1.036142110824585,
      "learning_rate": 4.8200681154719427e-05,
      "loss": 3.3175,
      "step": 56500
    },
    {
      "epoch": 18.346839546191248,
      "grad_norm": 0.8354015350341797,
      "learning_rate": 4.8197437560817385e-05,
      "loss": 3.2939,
      "step": 56600
    },
    {
      "epoch": 18.379254457050244,
      "grad_norm": 0.764054536819458,
      "learning_rate": 4.8194193966915344e-05,
      "loss": 3.3252,
      "step": 56700
    },
    {
      "epoch": 18.411669367909237,
      "grad_norm": 0.8956862688064575,
      "learning_rate": 4.8190950373013296e-05,
      "loss": 3.2999,
      "step": 56800
    },
    {
      "epoch": 18.444084278768234,
      "grad_norm": 0.8947429656982422,
      "learning_rate": 4.8187706779111255e-05,
      "loss": 3.3158,
      "step": 56900
    },
    {
      "epoch": 18.47649918962723,
      "grad_norm": 1.0378081798553467,
      "learning_rate": 4.8184463185209214e-05,
      "loss": 3.3317,
      "step": 57000
    },
    {
      "epoch": 18.508914100486223,
      "grad_norm": 0.838066577911377,
      "learning_rate": 4.818121959130717e-05,
      "loss": 3.2985,
      "step": 57100
    },
    {
      "epoch": 18.54132901134522,
      "grad_norm": 0.8149452805519104,
      "learning_rate": 4.817797599740513e-05,
      "loss": 3.3282,
      "step": 57200
    },
    {
      "epoch": 18.573743922204216,
      "grad_norm": 0.9831416606903076,
      "learning_rate": 4.817473240350308e-05,
      "loss": 3.3104,
      "step": 57300
    },
    {
      "epoch": 18.60615883306321,
      "grad_norm": 0.7938660383224487,
      "learning_rate": 4.817148880960104e-05,
      "loss": 3.2968,
      "step": 57400
    },
    {
      "epoch": 18.638573743922205,
      "grad_norm": 1.1699098348617554,
      "learning_rate": 4.816827765163802e-05,
      "loss": 3.2999,
      "step": 57500
    },
    {
      "epoch": 18.670988654781198,
      "grad_norm": 0.8718113899230957,
      "learning_rate": 4.816503405773597e-05,
      "loss": 3.3119,
      "step": 57600
    },
    {
      "epoch": 18.703403565640194,
      "grad_norm": 0.868701696395874,
      "learning_rate": 4.816179046383393e-05,
      "loss": 3.3008,
      "step": 57700
    },
    {
      "epoch": 18.73581847649919,
      "grad_norm": 0.8717154860496521,
      "learning_rate": 4.815854686993189e-05,
      "loss": 3.3121,
      "step": 57800
    },
    {
      "epoch": 18.768233387358183,
      "grad_norm": 0.9083405137062073,
      "learning_rate": 4.815530327602985e-05,
      "loss": 3.297,
      "step": 57900
    },
    {
      "epoch": 18.80064829821718,
      "grad_norm": 0.8566670417785645,
      "learning_rate": 4.81520596821278e-05,
      "loss": 3.2977,
      "step": 58000
    },
    {
      "epoch": 18.833063209076176,
      "grad_norm": 0.9121987223625183,
      "learning_rate": 4.814881608822576e-05,
      "loss": 3.3108,
      "step": 58100
    },
    {
      "epoch": 18.86547811993517,
      "grad_norm": 0.7821193337440491,
      "learning_rate": 4.814557249432372e-05,
      "loss": 3.3126,
      "step": 58200
    },
    {
      "epoch": 18.897893030794165,
      "grad_norm": 0.8799347877502441,
      "learning_rate": 4.814232890042167e-05,
      "loss": 3.3102,
      "step": 58300
    },
    {
      "epoch": 18.93030794165316,
      "grad_norm": 0.9495940804481506,
      "learning_rate": 4.813908530651963e-05,
      "loss": 3.3159,
      "step": 58400
    },
    {
      "epoch": 18.962722852512155,
      "grad_norm": 0.925613284111023,
      "learning_rate": 4.813584171261759e-05,
      "loss": 3.3048,
      "step": 58500
    },
    {
      "epoch": 18.99513776337115,
      "grad_norm": 0.8383057713508606,
      "learning_rate": 4.813259811871554e-05,
      "loss": 3.2963,
      "step": 58600
    },
    {
      "epoch": 19.0,
      "eval_bleu": 1.053537543419616,
      "eval_loss": 3.7026805877685547,
      "eval_runtime": 4.3468,
      "eval_samples_per_second": 113.186,
      "eval_steps_per_second": 1.84,
      "step": 58615
    },
    {
      "epoch": 19.027552674230147,
      "grad_norm": 0.8733224272727966,
      "learning_rate": 4.81293545248135e-05,
      "loss": 3.3124,
      "step": 58700
    },
    {
      "epoch": 19.05996758508914,
      "grad_norm": 0.8435048460960388,
      "learning_rate": 4.812611093091145e-05,
      "loss": 3.2836,
      "step": 58800
    },
    {
      "epoch": 19.092382495948137,
      "grad_norm": 0.8296048045158386,
      "learning_rate": 4.812286733700941e-05,
      "loss": 3.3003,
      "step": 58900
    },
    {
      "epoch": 19.124797406807133,
      "grad_norm": 0.8272748589515686,
      "learning_rate": 4.811962374310737e-05,
      "loss": 3.2925,
      "step": 59000
    },
    {
      "epoch": 19.157212317666126,
      "grad_norm": 0.9040014743804932,
      "learning_rate": 4.811638014920532e-05,
      "loss": 3.298,
      "step": 59100
    },
    {
      "epoch": 19.189627228525122,
      "grad_norm": 0.8117141127586365,
      "learning_rate": 4.811313655530328e-05,
      "loss": 3.2969,
      "step": 59200
    },
    {
      "epoch": 19.222042139384115,
      "grad_norm": 1.0276426076889038,
      "learning_rate": 4.8109892961401236e-05,
      "loss": 3.292,
      "step": 59300
    },
    {
      "epoch": 19.25445705024311,
      "grad_norm": 0.9828009009361267,
      "learning_rate": 4.810664936749919e-05,
      "loss": 3.2969,
      "step": 59400
    },
    {
      "epoch": 19.286871961102108,
      "grad_norm": 0.8220027685165405,
      "learning_rate": 4.810340577359715e-05,
      "loss": 3.2806,
      "step": 59500
    },
    {
      "epoch": 19.3192868719611,
      "grad_norm": 0.8581296801567078,
      "learning_rate": 4.8100194615634125e-05,
      "loss": 3.2983,
      "step": 59600
    },
    {
      "epoch": 19.351701782820097,
      "grad_norm": 0.8208685517311096,
      "learning_rate": 4.8096951021732084e-05,
      "loss": 3.295,
      "step": 59700
    },
    {
      "epoch": 19.384116693679093,
      "grad_norm": 0.934032678604126,
      "learning_rate": 4.8093707427830035e-05,
      "loss": 3.3043,
      "step": 59800
    },
    {
      "epoch": 19.416531604538086,
      "grad_norm": 0.7759830951690674,
      "learning_rate": 4.8090463833927994e-05,
      "loss": 3.2706,
      "step": 59900
    },
    {
      "epoch": 19.448946515397083,
      "grad_norm": 0.931555986404419,
      "learning_rate": 4.8087220240025946e-05,
      "loss": 3.3087,
      "step": 60000
    },
    {
      "epoch": 19.48136142625608,
      "grad_norm": 0.8685643076896667,
      "learning_rate": 4.8083976646123905e-05,
      "loss": 3.3153,
      "step": 60100
    },
    {
      "epoch": 19.513776337115072,
      "grad_norm": 0.8379449248313904,
      "learning_rate": 4.8080733052221864e-05,
      "loss": 3.2893,
      "step": 60200
    },
    {
      "epoch": 19.54619124797407,
      "grad_norm": 0.7996879816055298,
      "learning_rate": 4.8077489458319816e-05,
      "loss": 3.3094,
      "step": 60300
    },
    {
      "epoch": 19.578606158833065,
      "grad_norm": 0.8677024245262146,
      "learning_rate": 4.8074245864417774e-05,
      "loss": 3.2891,
      "step": 60400
    },
    {
      "epoch": 19.611021069692057,
      "grad_norm": 0.8134167194366455,
      "learning_rate": 4.807100227051573e-05,
      "loss": 3.2992,
      "step": 60500
    },
    {
      "epoch": 19.643435980551054,
      "grad_norm": 0.8002317547798157,
      "learning_rate": 4.806775867661369e-05,
      "loss": 3.2983,
      "step": 60600
    },
    {
      "epoch": 19.67585089141005,
      "grad_norm": 0.8397817015647888,
      "learning_rate": 4.806451508271165e-05,
      "loss": 3.3276,
      "step": 60700
    },
    {
      "epoch": 19.708265802269043,
      "grad_norm": 0.7835744023323059,
      "learning_rate": 4.806127148880961e-05,
      "loss": 3.2919,
      "step": 60800
    },
    {
      "epoch": 19.74068071312804,
      "grad_norm": 0.9619253277778625,
      "learning_rate": 4.805802789490756e-05,
      "loss": 3.3119,
      "step": 60900
    },
    {
      "epoch": 19.773095623987032,
      "grad_norm": 0.7659015655517578,
      "learning_rate": 4.805478430100552e-05,
      "loss": 3.3119,
      "step": 61000
    },
    {
      "epoch": 19.80551053484603,
      "grad_norm": 1.057233214378357,
      "learning_rate": 4.805154070710347e-05,
      "loss": 3.3071,
      "step": 61100
    },
    {
      "epoch": 19.837925445705025,
      "grad_norm": 0.8695149421691895,
      "learning_rate": 4.804829711320143e-05,
      "loss": 3.291,
      "step": 61200
    },
    {
      "epoch": 19.870340356564018,
      "grad_norm": 0.9096798896789551,
      "learning_rate": 4.804505351929939e-05,
      "loss": 3.3153,
      "step": 61300
    },
    {
      "epoch": 19.902755267423014,
      "grad_norm": 0.7868876457214355,
      "learning_rate": 4.804180992539734e-05,
      "loss": 3.2865,
      "step": 61400
    },
    {
      "epoch": 19.93517017828201,
      "grad_norm": 0.8772745728492737,
      "learning_rate": 4.80385663314953e-05,
      "loss": 3.2781,
      "step": 61500
    },
    {
      "epoch": 19.967585089141004,
      "grad_norm": 0.8301225304603577,
      "learning_rate": 4.803532273759326e-05,
      "loss": 3.294,
      "step": 61600
    },
    {
      "epoch": 20.0,
      "grad_norm": 1.1368858814239502,
      "learning_rate": 4.803211157963024e-05,
      "loss": 3.2997,
      "step": 61700
    },
    {
      "epoch": 20.0,
      "eval_bleu": 0.9826102736216523,
      "eval_loss": 3.7071375846862793,
      "eval_runtime": 4.9268,
      "eval_samples_per_second": 99.861,
      "eval_steps_per_second": 1.624,
      "step": 61700
    },
    {
      "epoch": 20.032414910858996,
      "grad_norm": 1.1134631633758545,
      "learning_rate": 4.802886798572819e-05,
      "loss": 3.2765,
      "step": 61800
    },
    {
      "epoch": 20.06482982171799,
      "grad_norm": 1.1370668411254883,
      "learning_rate": 4.802562439182615e-05,
      "loss": 3.2903,
      "step": 61900
    },
    {
      "epoch": 20.097244732576986,
      "grad_norm": 0.9958800077438354,
      "learning_rate": 4.8022380797924106e-05,
      "loss": 3.2906,
      "step": 62000
    },
    {
      "epoch": 20.129659643435982,
      "grad_norm": 0.8909556269645691,
      "learning_rate": 4.801913720402206e-05,
      "loss": 3.2941,
      "step": 62100
    },
    {
      "epoch": 20.162074554294975,
      "grad_norm": 0.9672849774360657,
      "learning_rate": 4.801589361012002e-05,
      "loss": 3.2968,
      "step": 62200
    },
    {
      "epoch": 20.19448946515397,
      "grad_norm": 0.9129165410995483,
      "learning_rate": 4.801265001621797e-05,
      "loss": 3.2741,
      "step": 62300
    },
    {
      "epoch": 20.226904376012968,
      "grad_norm": 0.9510704874992371,
      "learning_rate": 4.800940642231593e-05,
      "loss": 3.2917,
      "step": 62400
    },
    {
      "epoch": 20.25931928687196,
      "grad_norm": 0.8545668721199036,
      "learning_rate": 4.8006162828413886e-05,
      "loss": 3.3004,
      "step": 62500
    },
    {
      "epoch": 20.291734197730957,
      "grad_norm": 0.9030771255493164,
      "learning_rate": 4.800291923451184e-05,
      "loss": 3.284,
      "step": 62600
    },
    {
      "epoch": 20.32414910858995,
      "grad_norm": 0.806459903717041,
      "learning_rate": 4.79996756406098e-05,
      "loss": 3.2815,
      "step": 62700
    },
    {
      "epoch": 20.356564019448946,
      "grad_norm": 0.7578245997428894,
      "learning_rate": 4.7996432046707756e-05,
      "loss": 3.3051,
      "step": 62800
    },
    {
      "epoch": 20.388978930307943,
      "grad_norm": 0.8351677656173706,
      "learning_rate": 4.799318845280571e-05,
      "loss": 3.295,
      "step": 62900
    },
    {
      "epoch": 20.421393841166935,
      "grad_norm": 0.8181738257408142,
      "learning_rate": 4.7989944858903667e-05,
      "loss": 3.3037,
      "step": 63000
    },
    {
      "epoch": 20.45380875202593,
      "grad_norm": 0.837717592716217,
      "learning_rate": 4.7986701265001625e-05,
      "loss": 3.2854,
      "step": 63100
    },
    {
      "epoch": 20.486223662884928,
      "grad_norm": 0.7934706807136536,
      "learning_rate": 4.798345767109958e-05,
      "loss": 3.2991,
      "step": 63200
    },
    {
      "epoch": 20.51863857374392,
      "grad_norm": 0.9163109064102173,
      "learning_rate": 4.7980214077197536e-05,
      "loss": 3.2921,
      "step": 63300
    },
    {
      "epoch": 20.551053484602917,
      "grad_norm": 0.8181686997413635,
      "learning_rate": 4.7976970483295495e-05,
      "loss": 3.275,
      "step": 63400
    },
    {
      "epoch": 20.583468395461914,
      "grad_norm": 0.7634698748588562,
      "learning_rate": 4.797372688939345e-05,
      "loss": 3.2977,
      "step": 63500
    },
    {
      "epoch": 20.615883306320907,
      "grad_norm": 0.9156355857849121,
      "learning_rate": 4.7970483295491405e-05,
      "loss": 3.3015,
      "step": 63600
    },
    {
      "epoch": 20.648298217179903,
      "grad_norm": 0.8498892784118652,
      "learning_rate": 4.796727213752838e-05,
      "loss": 3.3057,
      "step": 63700
    },
    {
      "epoch": 20.6807131280389,
      "grad_norm": 0.8119916319847107,
      "learning_rate": 4.7964028543626335e-05,
      "loss": 3.286,
      "step": 63800
    },
    {
      "epoch": 20.713128038897892,
      "grad_norm": 0.9372416734695435,
      "learning_rate": 4.7960784949724294e-05,
      "loss": 3.2988,
      "step": 63900
    },
    {
      "epoch": 20.74554294975689,
      "grad_norm": 0.8600747585296631,
      "learning_rate": 4.795754135582225e-05,
      "loss": 3.2839,
      "step": 64000
    },
    {
      "epoch": 20.777957860615885,
      "grad_norm": 0.8918856978416443,
      "learning_rate": 4.795429776192021e-05,
      "loss": 3.2818,
      "step": 64100
    },
    {
      "epoch": 20.810372771474878,
      "grad_norm": 0.9223859310150146,
      "learning_rate": 4.795105416801817e-05,
      "loss": 3.297,
      "step": 64200
    },
    {
      "epoch": 20.842787682333874,
      "grad_norm": 0.8737244009971619,
      "learning_rate": 4.794781057411612e-05,
      "loss": 3.2848,
      "step": 64300
    },
    {
      "epoch": 20.875202593192867,
      "grad_norm": 0.8471442461013794,
      "learning_rate": 4.794456698021408e-05,
      "loss": 3.2863,
      "step": 64400
    },
    {
      "epoch": 20.907617504051863,
      "grad_norm": 0.9783878326416016,
      "learning_rate": 4.794132338631204e-05,
      "loss": 3.3035,
      "step": 64500
    },
    {
      "epoch": 20.94003241491086,
      "grad_norm": 0.8622522950172424,
      "learning_rate": 4.793811222834901e-05,
      "loss": 3.2853,
      "step": 64600
    },
    {
      "epoch": 20.972447325769853,
      "grad_norm": 0.840093731880188,
      "learning_rate": 4.793486863444697e-05,
      "loss": 3.2858,
      "step": 64700
    },
    {
      "epoch": 21.0,
      "eval_bleu": 1.1014286887450142,
      "eval_loss": 3.7099175453186035,
      "eval_runtime": 4.6987,
      "eval_samples_per_second": 104.711,
      "eval_steps_per_second": 1.703,
      "step": 64785
    },
    {
      "epoch": 21.00486223662885,
      "grad_norm": 0.8245484828948975,
      "learning_rate": 4.793162504054493e-05,
      "loss": 3.2706,
      "step": 64800
    },
    {
      "epoch": 21.037277147487845,
      "grad_norm": 0.9257804751396179,
      "learning_rate": 4.792838144664289e-05,
      "loss": 3.2747,
      "step": 64900
    },
    {
      "epoch": 21.06969205834684,
      "grad_norm": 0.7854569554328918,
      "learning_rate": 4.792513785274084e-05,
      "loss": 3.28,
      "step": 65000
    },
    {
      "epoch": 21.102106969205835,
      "grad_norm": 0.8012800216674805,
      "learning_rate": 4.79218942588388e-05,
      "loss": 3.2717,
      "step": 65100
    },
    {
      "epoch": 21.13452188006483,
      "grad_norm": 0.8799124956130981,
      "learning_rate": 4.7918650664936756e-05,
      "loss": 3.2735,
      "step": 65200
    },
    {
      "epoch": 21.166936790923824,
      "grad_norm": 0.849551796913147,
      "learning_rate": 4.791540707103471e-05,
      "loss": 3.2864,
      "step": 65300
    },
    {
      "epoch": 21.19935170178282,
      "grad_norm": 0.8223614692687988,
      "learning_rate": 4.791216347713267e-05,
      "loss": 3.2752,
      "step": 65400
    },
    {
      "epoch": 21.231766612641817,
      "grad_norm": 0.8502179384231567,
      "learning_rate": 4.7908919883230626e-05,
      "loss": 3.2787,
      "step": 65500
    },
    {
      "epoch": 21.26418152350081,
      "grad_norm": 0.8437319397926331,
      "learning_rate": 4.790567628932858e-05,
      "loss": 3.275,
      "step": 65600
    },
    {
      "epoch": 21.296596434359806,
      "grad_norm": 0.7735568284988403,
      "learning_rate": 4.7902432695426537e-05,
      "loss": 3.2817,
      "step": 65700
    },
    {
      "epoch": 21.329011345218802,
      "grad_norm": 1.1938954591751099,
      "learning_rate": 4.789918910152449e-05,
      "loss": 3.2727,
      "step": 65800
    },
    {
      "epoch": 21.361426256077795,
      "grad_norm": 0.7884756326675415,
      "learning_rate": 4.789594550762245e-05,
      "loss": 3.2724,
      "step": 65900
    },
    {
      "epoch": 21.39384116693679,
      "grad_norm": 0.8534108996391296,
      "learning_rate": 4.7892701913720406e-05,
      "loss": 3.2727,
      "step": 66000
    },
    {
      "epoch": 21.426256077795784,
      "grad_norm": 0.9867222905158997,
      "learning_rate": 4.788945831981836e-05,
      "loss": 3.2826,
      "step": 66100
    },
    {
      "epoch": 21.45867098865478,
      "grad_norm": 0.9031102657318115,
      "learning_rate": 4.788621472591632e-05,
      "loss": 3.2772,
      "step": 66200
    },
    {
      "epoch": 21.491085899513777,
      "grad_norm": 0.779308021068573,
      "learning_rate": 4.7882971132014275e-05,
      "loss": 3.3002,
      "step": 66300
    },
    {
      "epoch": 21.52350081037277,
      "grad_norm": 0.9621580839157104,
      "learning_rate": 4.787972753811223e-05,
      "loss": 3.274,
      "step": 66400
    },
    {
      "epoch": 21.555915721231766,
      "grad_norm": 0.7667760252952576,
      "learning_rate": 4.7876483944210186e-05,
      "loss": 3.2978,
      "step": 66500
    },
    {
      "epoch": 21.588330632090763,
      "grad_norm": 0.7862148284912109,
      "learning_rate": 4.7873240350308145e-05,
      "loss": 3.2959,
      "step": 66600
    },
    {
      "epoch": 21.620745542949756,
      "grad_norm": 0.8871970772743225,
      "learning_rate": 4.78699967564061e-05,
      "loss": 3.2739,
      "step": 66700
    },
    {
      "epoch": 21.653160453808752,
      "grad_norm": 0.7562496066093445,
      "learning_rate": 4.7866753162504056e-05,
      "loss": 3.2841,
      "step": 66800
    },
    {
      "epoch": 21.68557536466775,
      "grad_norm": 0.85268634557724,
      "learning_rate": 4.786350956860201e-05,
      "loss": 3.2959,
      "step": 66900
    },
    {
      "epoch": 21.71799027552674,
      "grad_norm": 0.8858005404472351,
      "learning_rate": 4.7860265974699966e-05,
      "loss": 3.2994,
      "step": 67000
    },
    {
      "epoch": 21.750405186385738,
      "grad_norm": 0.8111391663551331,
      "learning_rate": 4.7857022380797925e-05,
      "loss": 3.2861,
      "step": 67100
    },
    {
      "epoch": 21.782820097244734,
      "grad_norm": 0.7381735444068909,
      "learning_rate": 4.7853778786895884e-05,
      "loss": 3.2817,
      "step": 67200
    },
    {
      "epoch": 21.815235008103727,
      "grad_norm": 0.8784536719322205,
      "learning_rate": 4.785053519299384e-05,
      "loss": 3.2789,
      "step": 67300
    },
    {
      "epoch": 21.847649918962723,
      "grad_norm": 0.8383498787879944,
      "learning_rate": 4.78472915990918e-05,
      "loss": 3.2889,
      "step": 67400
    },
    {
      "epoch": 21.88006482982172,
      "grad_norm": 0.9337917566299438,
      "learning_rate": 4.784404800518975e-05,
      "loss": 3.27,
      "step": 67500
    },
    {
      "epoch": 21.912479740680713,
      "grad_norm": 0.9968885779380798,
      "learning_rate": 4.784080441128771e-05,
      "loss": 3.2668,
      "step": 67600
    },
    {
      "epoch": 21.94489465153971,
      "grad_norm": 0.8969876170158386,
      "learning_rate": 4.783756081738567e-05,
      "loss": 3.2851,
      "step": 67700
    },
    {
      "epoch": 21.977309562398702,
      "grad_norm": 0.8953835964202881,
      "learning_rate": 4.783431722348362e-05,
      "loss": 3.2716,
      "step": 67800
    },
    {
      "epoch": 22.0,
      "eval_bleu": 0.9119924154633104,
      "eval_loss": 3.700565814971924,
      "eval_runtime": 5.0067,
      "eval_samples_per_second": 98.267,
      "eval_steps_per_second": 1.598,
      "step": 67870
    },
    {
      "epoch": 22.009724473257698,
      "grad_norm": 0.8443865776062012,
      "learning_rate": 4.783107362958158e-05,
      "loss": 3.2791,
      "step": 67900
    },
    {
      "epoch": 22.042139384116695,
      "grad_norm": 0.9276329874992371,
      "learning_rate": 4.782783003567953e-05,
      "loss": 3.2821,
      "step": 68000
    },
    {
      "epoch": 22.074554294975687,
      "grad_norm": 0.9285898804664612,
      "learning_rate": 4.782458644177749e-05,
      "loss": 3.2617,
      "step": 68100
    },
    {
      "epoch": 22.106969205834684,
      "grad_norm": 0.9778681397438049,
      "learning_rate": 4.782134284787545e-05,
      "loss": 3.2641,
      "step": 68200
    },
    {
      "epoch": 22.13938411669368,
      "grad_norm": 0.8844485282897949,
      "learning_rate": 4.78180992539734e-05,
      "loss": 3.2813,
      "step": 68300
    },
    {
      "epoch": 22.171799027552673,
      "grad_norm": 0.8028387427330017,
      "learning_rate": 4.781485566007136e-05,
      "loss": 3.2677,
      "step": 68400
    },
    {
      "epoch": 22.20421393841167,
      "grad_norm": 0.8116012811660767,
      "learning_rate": 4.781161206616932e-05,
      "loss": 3.2886,
      "step": 68500
    },
    {
      "epoch": 22.236628849270666,
      "grad_norm": 0.9412678480148315,
      "learning_rate": 4.78084009082063e-05,
      "loss": 3.261,
      "step": 68600
    },
    {
      "epoch": 22.26904376012966,
      "grad_norm": 0.9318462014198303,
      "learning_rate": 4.780515731430425e-05,
      "loss": 3.2685,
      "step": 68700
    },
    {
      "epoch": 22.301458670988655,
      "grad_norm": 0.9659602046012878,
      "learning_rate": 4.780191372040221e-05,
      "loss": 3.2638,
      "step": 68800
    },
    {
      "epoch": 22.33387358184765,
      "grad_norm": 0.8653566241264343,
      "learning_rate": 4.779867012650017e-05,
      "loss": 3.282,
      "step": 68900
    },
    {
      "epoch": 22.366288492706644,
      "grad_norm": 0.9683631062507629,
      "learning_rate": 4.779542653259812e-05,
      "loss": 3.2834,
      "step": 69000
    },
    {
      "epoch": 22.39870340356564,
      "grad_norm": 1.147221326828003,
      "learning_rate": 4.779218293869608e-05,
      "loss": 3.2735,
      "step": 69100
    },
    {
      "epoch": 22.431118314424637,
      "grad_norm": 0.8243588209152222,
      "learning_rate": 4.778893934479403e-05,
      "loss": 3.2776,
      "step": 69200
    },
    {
      "epoch": 22.46353322528363,
      "grad_norm": 0.9338653087615967,
      "learning_rate": 4.778569575089199e-05,
      "loss": 3.2752,
      "step": 69300
    },
    {
      "epoch": 22.495948136142626,
      "grad_norm": 0.8621575236320496,
      "learning_rate": 4.778248459292897e-05,
      "loss": 3.2549,
      "step": 69400
    },
    {
      "epoch": 22.52836304700162,
      "grad_norm": 0.874284029006958,
      "learning_rate": 4.7779240999026926e-05,
      "loss": 3.2896,
      "step": 69500
    },
    {
      "epoch": 22.560777957860616,
      "grad_norm": 0.8924210667610168,
      "learning_rate": 4.777599740512488e-05,
      "loss": 3.269,
      "step": 69600
    },
    {
      "epoch": 22.593192868719612,
      "grad_norm": 0.877267599105835,
      "learning_rate": 4.7772753811222836e-05,
      "loss": 3.274,
      "step": 69700
    },
    {
      "epoch": 22.625607779578605,
      "grad_norm": 0.8649918437004089,
      "learning_rate": 4.7769510217320795e-05,
      "loss": 3.2644,
      "step": 69800
    },
    {
      "epoch": 22.6580226904376,
      "grad_norm": 0.7795793414115906,
      "learning_rate": 4.776626662341875e-05,
      "loss": 3.2712,
      "step": 69900
    },
    {
      "epoch": 22.690437601296598,
      "grad_norm": 0.7522197961807251,
      "learning_rate": 4.7763023029516706e-05,
      "loss": 3.2518,
      "step": 70000
    },
    {
      "epoch": 22.72285251215559,
      "grad_norm": 0.8157826066017151,
      "learning_rate": 4.7759779435614664e-05,
      "loss": 3.2726,
      "step": 70100
    },
    {
      "epoch": 22.755267423014587,
      "grad_norm": 0.972424328327179,
      "learning_rate": 4.7756535841712616e-05,
      "loss": 3.2604,
      "step": 70200
    },
    {
      "epoch": 22.787682333873583,
      "grad_norm": 0.9948761463165283,
      "learning_rate": 4.7753292247810575e-05,
      "loss": 3.2781,
      "step": 70300
    },
    {
      "epoch": 22.820097244732576,
      "grad_norm": 0.9745439887046814,
      "learning_rate": 4.7750048653908534e-05,
      "loss": 3.282,
      "step": 70400
    },
    {
      "epoch": 22.852512155591572,
      "grad_norm": 0.8443552851676941,
      "learning_rate": 4.7746805060006486e-05,
      "loss": 3.2611,
      "step": 70500
    },
    {
      "epoch": 22.88492706645057,
      "grad_norm": 0.8128862380981445,
      "learning_rate": 4.7743561466104445e-05,
      "loss": 3.2838,
      "step": 70600
    },
    {
      "epoch": 22.91734197730956,
      "grad_norm": 0.9741435647010803,
      "learning_rate": 4.77403178722024e-05,
      "loss": 3.2748,
      "step": 70700
    },
    {
      "epoch": 22.949756888168558,
      "grad_norm": 0.9337469935417175,
      "learning_rate": 4.773707427830036e-05,
      "loss": 3.2896,
      "step": 70800
    },
    {
      "epoch": 22.982171799027554,
      "grad_norm": 0.897948682308197,
      "learning_rate": 4.7733830684398314e-05,
      "loss": 3.2766,
      "step": 70900
    },
    {
      "epoch": 23.0,
      "eval_bleu": 1.0757074920816854,
      "eval_loss": 3.710995674133301,
      "eval_runtime": 4.6227,
      "eval_samples_per_second": 106.431,
      "eval_steps_per_second": 1.731,
      "step": 70955
    },
    {
      "epoch": 23.014586709886547,
      "grad_norm": 0.8243562579154968,
      "learning_rate": 4.773058709049627e-05,
      "loss": 3.2634,
      "step": 71000
    },
    {
      "epoch": 23.047001620745544,
      "grad_norm": 0.848716676235199,
      "learning_rate": 4.772734349659423e-05,
      "loss": 3.2553,
      "step": 71100
    },
    {
      "epoch": 23.079416531604537,
      "grad_norm": 1.1212663650512695,
      "learning_rate": 4.772409990269219e-05,
      "loss": 3.2634,
      "step": 71200
    },
    {
      "epoch": 23.111831442463533,
      "grad_norm": 0.9614636898040771,
      "learning_rate": 4.772085630879014e-05,
      "loss": 3.2429,
      "step": 71300
    },
    {
      "epoch": 23.14424635332253,
      "grad_norm": 0.8283793330192566,
      "learning_rate": 4.77176127148881e-05,
      "loss": 3.2661,
      "step": 71400
    },
    {
      "epoch": 23.176661264181522,
      "grad_norm": 0.7509739398956299,
      "learning_rate": 4.771436912098605e-05,
      "loss": 3.2677,
      "step": 71500
    },
    {
      "epoch": 23.20907617504052,
      "grad_norm": 0.8966099619865417,
      "learning_rate": 4.771112552708401e-05,
      "loss": 3.2474,
      "step": 71600
    },
    {
      "epoch": 23.241491085899515,
      "grad_norm": 0.818715512752533,
      "learning_rate": 4.770788193318197e-05,
      "loss": 3.2694,
      "step": 71700
    },
    {
      "epoch": 23.273905996758508,
      "grad_norm": 0.9014068841934204,
      "learning_rate": 4.770463833927992e-05,
      "loss": 3.2798,
      "step": 71800
    },
    {
      "epoch": 23.306320907617504,
      "grad_norm": 0.8279420733451843,
      "learning_rate": 4.770139474537788e-05,
      "loss": 3.2661,
      "step": 71900
    },
    {
      "epoch": 23.3387358184765,
      "grad_norm": 0.9249817728996277,
      "learning_rate": 4.769815115147584e-05,
      "loss": 3.2655,
      "step": 72000
    },
    {
      "epoch": 23.371150729335493,
      "grad_norm": 0.9194801449775696,
      "learning_rate": 4.769490755757379e-05,
      "loss": 3.2654,
      "step": 72100
    },
    {
      "epoch": 23.40356564019449,
      "grad_norm": 0.9574617743492126,
      "learning_rate": 4.769166396367175e-05,
      "loss": 3.2685,
      "step": 72200
    },
    {
      "epoch": 23.435980551053486,
      "grad_norm": 0.797116756439209,
      "learning_rate": 4.768842036976971e-05,
      "loss": 3.2618,
      "step": 72300
    },
    {
      "epoch": 23.46839546191248,
      "grad_norm": 0.8915625214576721,
      "learning_rate": 4.768517677586766e-05,
      "loss": 3.2899,
      "step": 72400
    },
    {
      "epoch": 23.500810372771475,
      "grad_norm": 0.921606719493866,
      "learning_rate": 4.768193318196562e-05,
      "loss": 3.2602,
      "step": 72500
    },
    {
      "epoch": 23.533225283630472,
      "grad_norm": 0.8431615233421326,
      "learning_rate": 4.767868958806357e-05,
      "loss": 3.2759,
      "step": 72600
    },
    {
      "epoch": 23.565640194489465,
      "grad_norm": 0.855699360370636,
      "learning_rate": 4.767544599416153e-05,
      "loss": 3.2658,
      "step": 72700
    },
    {
      "epoch": 23.59805510534846,
      "grad_norm": 1.0212626457214355,
      "learning_rate": 4.767220240025949e-05,
      "loss": 3.2523,
      "step": 72800
    },
    {
      "epoch": 23.630470016207454,
      "grad_norm": 0.8608694672584534,
      "learning_rate": 4.766895880635744e-05,
      "loss": 3.2513,
      "step": 72900
    },
    {
      "epoch": 23.66288492706645,
      "grad_norm": 0.9163949489593506,
      "learning_rate": 4.76657152124554e-05,
      "loss": 3.2875,
      "step": 73000
    },
    {
      "epoch": 23.695299837925447,
      "grad_norm": 1.1037729978561401,
      "learning_rate": 4.766247161855336e-05,
      "loss": 3.2567,
      "step": 73100
    },
    {
      "epoch": 23.72771474878444,
      "grad_norm": 0.786390483379364,
      "learning_rate": 4.765922802465132e-05,
      "loss": 3.2938,
      "step": 73200
    },
    {
      "epoch": 23.760129659643436,
      "grad_norm": 0.9198330044746399,
      "learning_rate": 4.7655984430749276e-05,
      "loss": 3.2374,
      "step": 73300
    },
    {
      "epoch": 23.792544570502432,
      "grad_norm": 0.7918649315834045,
      "learning_rate": 4.765274083684723e-05,
      "loss": 3.2679,
      "step": 73400
    },
    {
      "epoch": 23.824959481361425,
      "grad_norm": 0.9288239479064941,
      "learning_rate": 4.7649529678884206e-05,
      "loss": 3.2618,
      "step": 73500
    },
    {
      "epoch": 23.85737439222042,
      "grad_norm": 0.9144817590713501,
      "learning_rate": 4.764628608498216e-05,
      "loss": 3.2662,
      "step": 73600
    },
    {
      "epoch": 23.889789303079418,
      "grad_norm": 1.0328450202941895,
      "learning_rate": 4.764304249108012e-05,
      "loss": 3.2645,
      "step": 73700
    },
    {
      "epoch": 23.92220421393841,
      "grad_norm": 0.927993893623352,
      "learning_rate": 4.7639798897178076e-05,
      "loss": 3.2488,
      "step": 73800
    },
    {
      "epoch": 23.954619124797407,
      "grad_norm": 0.9437822103500366,
      "learning_rate": 4.7636555303276034e-05,
      "loss": 3.2539,
      "step": 73900
    },
    {
      "epoch": 23.987034035656404,
      "grad_norm": 0.8007331490516663,
      "learning_rate": 4.763331170937399e-05,
      "loss": 3.2619,
      "step": 74000
    },
    {
      "epoch": 24.0,
      "eval_bleu": 1.0763185368358918,
      "eval_loss": 3.7178757190704346,
      "eval_runtime": 4.5369,
      "eval_samples_per_second": 108.443,
      "eval_steps_per_second": 1.763,
      "step": 74040
    },
    {
      "epoch": 24.019448946515396,
      "grad_norm": 0.8768735527992249,
      "learning_rate": 4.7630068115471945e-05,
      "loss": 3.255,
      "step": 74100
    },
    {
      "epoch": 24.051863857374393,
      "grad_norm": 0.803268313407898,
      "learning_rate": 4.7626824521569904e-05,
      "loss": 3.2513,
      "step": 74200
    },
    {
      "epoch": 24.084278768233386,
      "grad_norm": 1.003758192062378,
      "learning_rate": 4.762358092766786e-05,
      "loss": 3.25,
      "step": 74300
    },
    {
      "epoch": 24.116693679092382,
      "grad_norm": 0.8264837265014648,
      "learning_rate": 4.7620337333765815e-05,
      "loss": 3.2622,
      "step": 74400
    },
    {
      "epoch": 24.14910858995138,
      "grad_norm": 0.8535533547401428,
      "learning_rate": 4.761709373986377e-05,
      "loss": 3.2412,
      "step": 74500
    },
    {
      "epoch": 24.18152350081037,
      "grad_norm": 1.0274821519851685,
      "learning_rate": 4.761385014596173e-05,
      "loss": 3.2621,
      "step": 74600
    },
    {
      "epoch": 24.213938411669368,
      "grad_norm": 1.239328145980835,
      "learning_rate": 4.7610606552059684e-05,
      "loss": 3.2503,
      "step": 74700
    },
    {
      "epoch": 24.246353322528364,
      "grad_norm": 0.9451021552085876,
      "learning_rate": 4.760736295815764e-05,
      "loss": 3.2542,
      "step": 74800
    },
    {
      "epoch": 24.278768233387357,
      "grad_norm": 0.8727419376373291,
      "learning_rate": 4.7604119364255595e-05,
      "loss": 3.2482,
      "step": 74900
    },
    {
      "epoch": 24.311183144246353,
      "grad_norm": 0.8275892734527588,
      "learning_rate": 4.760090820629258e-05,
      "loss": 3.2577,
      "step": 75000
    },
    {
      "epoch": 24.34359805510535,
      "grad_norm": 0.7916747331619263,
      "learning_rate": 4.759766461239053e-05,
      "loss": 3.2446,
      "step": 75100
    },
    {
      "epoch": 24.376012965964343,
      "grad_norm": 0.8920164704322815,
      "learning_rate": 4.759442101848849e-05,
      "loss": 3.2702,
      "step": 75200
    },
    {
      "epoch": 24.40842787682334,
      "grad_norm": 0.8288471102714539,
      "learning_rate": 4.759117742458644e-05,
      "loss": 3.2693,
      "step": 75300
    },
    {
      "epoch": 24.440842787682335,
      "grad_norm": 0.8027697205543518,
      "learning_rate": 4.75879338306844e-05,
      "loss": 3.2294,
      "step": 75400
    },
    {
      "epoch": 24.473257698541328,
      "grad_norm": 0.8192620277404785,
      "learning_rate": 4.758469023678236e-05,
      "loss": 3.2815,
      "step": 75500
    },
    {
      "epoch": 24.505672609400325,
      "grad_norm": 0.8228100538253784,
      "learning_rate": 4.758144664288031e-05,
      "loss": 3.2714,
      "step": 75600
    },
    {
      "epoch": 24.53808752025932,
      "grad_norm": 1.0634491443634033,
      "learning_rate": 4.757820304897827e-05,
      "loss": 3.2411,
      "step": 75700
    },
    {
      "epoch": 24.570502431118314,
      "grad_norm": 0.8299837708473206,
      "learning_rate": 4.757495945507623e-05,
      "loss": 3.2751,
      "step": 75800
    },
    {
      "epoch": 24.60291734197731,
      "grad_norm": 0.9359004497528076,
      "learning_rate": 4.757171586117418e-05,
      "loss": 3.2453,
      "step": 75900
    },
    {
      "epoch": 24.635332252836303,
      "grad_norm": 0.8783425688743591,
      "learning_rate": 4.756847226727214e-05,
      "loss": 3.2521,
      "step": 76000
    },
    {
      "epoch": 24.6677471636953,
      "grad_norm": 1.055668830871582,
      "learning_rate": 4.756522867337009e-05,
      "loss": 3.2509,
      "step": 76100
    },
    {
      "epoch": 24.700162074554296,
      "grad_norm": 0.928293764591217,
      "learning_rate": 4.756198507946805e-05,
      "loss": 3.2541,
      "step": 76200
    },
    {
      "epoch": 24.73257698541329,
      "grad_norm": 0.8656312823295593,
      "learning_rate": 4.755874148556601e-05,
      "loss": 3.2669,
      "step": 76300
    },
    {
      "epoch": 24.764991896272285,
      "grad_norm": 0.8994680643081665,
      "learning_rate": 4.755549789166396e-05,
      "loss": 3.2656,
      "step": 76400
    },
    {
      "epoch": 24.79740680713128,
      "grad_norm": 0.7852749824523926,
      "learning_rate": 4.755225429776192e-05,
      "loss": 3.272,
      "step": 76500
    },
    {
      "epoch": 24.829821717990274,
      "grad_norm": 0.9707778692245483,
      "learning_rate": 4.754901070385988e-05,
      "loss": 3.2566,
      "step": 76600
    },
    {
      "epoch": 24.86223662884927,
      "grad_norm": 0.8130062818527222,
      "learning_rate": 4.754576710995784e-05,
      "loss": 3.2516,
      "step": 76700
    },
    {
      "epoch": 24.894651539708267,
      "grad_norm": 0.8701446056365967,
      "learning_rate": 4.754252351605579e-05,
      "loss": 3.252,
      "step": 76800
    },
    {
      "epoch": 24.92706645056726,
      "grad_norm": 0.8199581503868103,
      "learning_rate": 4.753927992215375e-05,
      "loss": 3.2603,
      "step": 76900
    },
    {
      "epoch": 24.959481361426256,
      "grad_norm": 0.8691790699958801,
      "learning_rate": 4.753603632825171e-05,
      "loss": 3.2492,
      "step": 77000
    },
    {
      "epoch": 24.991896272285253,
      "grad_norm": 0.8421449065208435,
      "learning_rate": 4.7532792734349665e-05,
      "loss": 3.2671,
      "step": 77100
    },
    {
      "epoch": 25.0,
      "eval_bleu": 1.171312991774984,
      "eval_loss": 3.715087890625,
      "eval_runtime": 4.8164,
      "eval_samples_per_second": 102.152,
      "eval_steps_per_second": 1.661,
      "step": 77125
    },
    {
      "epoch": 25.024311183144246,
      "grad_norm": 0.775773286819458,
      "learning_rate": 4.752954914044762e-05,
      "loss": 3.2364,
      "step": 77200
    },
    {
      "epoch": 25.056726094003242,
      "grad_norm": 0.9894416928291321,
      "learning_rate": 4.7526305546545576e-05,
      "loss": 3.2481,
      "step": 77300
    },
    {
      "epoch": 25.08914100486224,
      "grad_norm": 0.8933448791503906,
      "learning_rate": 4.7523061952643535e-05,
      "loss": 3.2473,
      "step": 77400
    },
    {
      "epoch": 25.12155591572123,
      "grad_norm": 0.8663082718849182,
      "learning_rate": 4.751981835874149e-05,
      "loss": 3.2644,
      "step": 77500
    },
    {
      "epoch": 25.153970826580228,
      "grad_norm": 0.8531897068023682,
      "learning_rate": 4.7516574764839446e-05,
      "loss": 3.2443,
      "step": 77600
    },
    {
      "epoch": 25.18638573743922,
      "grad_norm": 0.7778231501579285,
      "learning_rate": 4.7513331170937404e-05,
      "loss": 3.2436,
      "step": 77700
    },
    {
      "epoch": 25.218800648298217,
      "grad_norm": 0.8978093266487122,
      "learning_rate": 4.7510087577035356e-05,
      "loss": 3.2491,
      "step": 77800
    },
    {
      "epoch": 25.251215559157213,
      "grad_norm": 0.8269281983375549,
      "learning_rate": 4.7506843983133315e-05,
      "loss": 3.263,
      "step": 77900
    },
    {
      "epoch": 25.283630470016206,
      "grad_norm": 0.8854743242263794,
      "learning_rate": 4.7503600389231274e-05,
      "loss": 3.2633,
      "step": 78000
    },
    {
      "epoch": 25.316045380875202,
      "grad_norm": 0.7297553420066833,
      "learning_rate": 4.7500356795329226e-05,
      "loss": 3.2311,
      "step": 78100
    },
    {
      "epoch": 25.3484602917342,
      "grad_norm": 0.9313138723373413,
      "learning_rate": 4.7497113201427185e-05,
      "loss": 3.2383,
      "step": 78200
    },
    {
      "epoch": 25.38087520259319,
      "grad_norm": 0.8120205402374268,
      "learning_rate": 4.7493869607525137e-05,
      "loss": 3.2377,
      "step": 78300
    },
    {
      "epoch": 25.413290113452188,
      "grad_norm": 0.9985283017158508,
      "learning_rate": 4.7490626013623095e-05,
      "loss": 3.2651,
      "step": 78400
    },
    {
      "epoch": 25.445705024311184,
      "grad_norm": 0.8506268262863159,
      "learning_rate": 4.7487382419721054e-05,
      "loss": 3.2578,
      "step": 78500
    },
    {
      "epoch": 25.478119935170177,
      "grad_norm": 0.8653962016105652,
      "learning_rate": 4.7484138825819006e-05,
      "loss": 3.2404,
      "step": 78600
    },
    {
      "epoch": 25.510534846029174,
      "grad_norm": 0.838596522808075,
      "learning_rate": 4.7480895231916965e-05,
      "loss": 3.2751,
      "step": 78700
    },
    {
      "epoch": 25.54294975688817,
      "grad_norm": 1.0048962831497192,
      "learning_rate": 4.7477651638014923e-05,
      "loss": 3.2456,
      "step": 78800
    },
    {
      "epoch": 25.575364667747163,
      "grad_norm": 0.9100828766822815,
      "learning_rate": 4.7474408044112875e-05,
      "loss": 3.2443,
      "step": 78900
    },
    {
      "epoch": 25.60777957860616,
      "grad_norm": 0.8006051778793335,
      "learning_rate": 4.747119688614985e-05,
      "loss": 3.2588,
      "step": 79000
    },
    {
      "epoch": 25.640194489465156,
      "grad_norm": 0.7676663398742676,
      "learning_rate": 4.746795329224781e-05,
      "loss": 3.2519,
      "step": 79100
    },
    {
      "epoch": 25.67260940032415,
      "grad_norm": 0.9863914251327515,
      "learning_rate": 4.746470969834577e-05,
      "loss": 3.2667,
      "step": 79200
    },
    {
      "epoch": 25.705024311183145,
      "grad_norm": 0.8919060826301575,
      "learning_rate": 4.746146610444372e-05,
      "loss": 3.2336,
      "step": 79300
    },
    {
      "epoch": 25.737439222042138,
      "grad_norm": 0.9381189942359924,
      "learning_rate": 4.745822251054168e-05,
      "loss": 3.2536,
      "step": 79400
    },
    {
      "epoch": 25.769854132901134,
      "grad_norm": 0.978266179561615,
      "learning_rate": 4.745497891663963e-05,
      "loss": 3.2499,
      "step": 79500
    },
    {
      "epoch": 25.80226904376013,
      "grad_norm": 0.8508621454238892,
      "learning_rate": 4.745173532273759e-05,
      "loss": 3.248,
      "step": 79600
    },
    {
      "epoch": 25.834683954619123,
      "grad_norm": 1.161535620689392,
      "learning_rate": 4.744849172883555e-05,
      "loss": 3.2378,
      "step": 79700
    },
    {
      "epoch": 25.86709886547812,
      "grad_norm": 0.8118118643760681,
      "learning_rate": 4.744524813493351e-05,
      "loss": 3.2516,
      "step": 79800
    },
    {
      "epoch": 25.899513776337116,
      "grad_norm": 1.0001522302627563,
      "learning_rate": 4.744200454103147e-05,
      "loss": 3.2461,
      "step": 79900
    },
    {
      "epoch": 25.93192868719611,
      "grad_norm": 0.7949409484863281,
      "learning_rate": 4.743876094712942e-05,
      "loss": 3.2341,
      "step": 80000
    },
    {
      "epoch": 25.964343598055105,
      "grad_norm": 1.130479335784912,
      "learning_rate": 4.743551735322738e-05,
      "loss": 3.2442,
      "step": 80100
    },
    {
      "epoch": 25.996758508914102,
      "grad_norm": 0.869892954826355,
      "learning_rate": 4.743227375932534e-05,
      "loss": 3.2633,
      "step": 80200
    },
    {
      "epoch": 26.0,
      "eval_bleu": 1.1047696811784966,
      "eval_loss": 3.7164268493652344,
      "eval_runtime": 4.5687,
      "eval_samples_per_second": 107.69,
      "eval_steps_per_second": 1.751,
      "step": 80210
    },
    {
      "epoch": 26.029173419773095,
      "grad_norm": 0.7359393835067749,
      "learning_rate": 4.7429030165423297e-05,
      "loss": 3.2383,
      "step": 80300
    },
    {
      "epoch": 26.06158833063209,
      "grad_norm": 1.0057424306869507,
      "learning_rate": 4.742578657152125e-05,
      "loss": 3.2378,
      "step": 80400
    },
    {
      "epoch": 26.094003241491087,
      "grad_norm": 0.9581093192100525,
      "learning_rate": 4.742254297761921e-05,
      "loss": 3.2183,
      "step": 80500
    },
    {
      "epoch": 26.12641815235008,
      "grad_norm": 0.9035683870315552,
      "learning_rate": 4.741929938371716e-05,
      "loss": 3.2446,
      "step": 80600
    },
    {
      "epoch": 26.158833063209077,
      "grad_norm": 0.9605050683021545,
      "learning_rate": 4.741605578981512e-05,
      "loss": 3.2499,
      "step": 80700
    },
    {
      "epoch": 26.191247974068073,
      "grad_norm": 0.7987914085388184,
      "learning_rate": 4.741281219591308e-05,
      "loss": 3.2557,
      "step": 80800
    },
    {
      "epoch": 26.223662884927066,
      "grad_norm": 0.8493878841400146,
      "learning_rate": 4.740956860201103e-05,
      "loss": 3.2413,
      "step": 80900
    },
    {
      "epoch": 26.256077795786062,
      "grad_norm": 1.0518476963043213,
      "learning_rate": 4.7406357444048006e-05,
      "loss": 3.2423,
      "step": 81000
    },
    {
      "epoch": 26.288492706645055,
      "grad_norm": 0.8379240036010742,
      "learning_rate": 4.7403113850145965e-05,
      "loss": 3.259,
      "step": 81100
    },
    {
      "epoch": 26.32090761750405,
      "grad_norm": 0.7951277494430542,
      "learning_rate": 4.7399870256243924e-05,
      "loss": 3.2446,
      "step": 81200
    },
    {
      "epoch": 26.353322528363048,
      "grad_norm": 1.02846097946167,
      "learning_rate": 4.7396626662341876e-05,
      "loss": 3.2401,
      "step": 81300
    },
    {
      "epoch": 26.38573743922204,
      "grad_norm": 0.8962801098823547,
      "learning_rate": 4.7393383068439835e-05,
      "loss": 3.2395,
      "step": 81400
    },
    {
      "epoch": 26.418152350081037,
      "grad_norm": 0.8636525273323059,
      "learning_rate": 4.7390139474537793e-05,
      "loss": 3.2409,
      "step": 81500
    },
    {
      "epoch": 26.450567260940034,
      "grad_norm": 1.0217043161392212,
      "learning_rate": 4.7386895880635745e-05,
      "loss": 3.2415,
      "step": 81600
    },
    {
      "epoch": 26.482982171799026,
      "grad_norm": 0.8765149116516113,
      "learning_rate": 4.7383652286733704e-05,
      "loss": 3.2378,
      "step": 81700
    },
    {
      "epoch": 26.515397082658023,
      "grad_norm": 0.9284408092498779,
      "learning_rate": 4.7380408692831656e-05,
      "loss": 3.2301,
      "step": 81800
    },
    {
      "epoch": 26.54781199351702,
      "grad_norm": 0.9525735974311829,
      "learning_rate": 4.7377165098929615e-05,
      "loss": 3.2397,
      "step": 81900
    },
    {
      "epoch": 26.580226904376012,
      "grad_norm": 0.8759743571281433,
      "learning_rate": 4.7373921505027574e-05,
      "loss": 3.2462,
      "step": 82000
    },
    {
      "epoch": 26.61264181523501,
      "grad_norm": 0.8841198086738586,
      "learning_rate": 4.7370677911125526e-05,
      "loss": 3.2496,
      "step": 82100
    },
    {
      "epoch": 26.645056726094005,
      "grad_norm": 0.8010789155960083,
      "learning_rate": 4.7367434317223484e-05,
      "loss": 3.2418,
      "step": 82200
    },
    {
      "epoch": 26.677471636952998,
      "grad_norm": 0.9061064720153809,
      "learning_rate": 4.736419072332144e-05,
      "loss": 3.2486,
      "step": 82300
    },
    {
      "epoch": 26.709886547811994,
      "grad_norm": 0.9909858703613281,
      "learning_rate": 4.7360947129419395e-05,
      "loss": 3.2495,
      "step": 82400
    },
    {
      "epoch": 26.742301458670987,
      "grad_norm": 0.8410025835037231,
      "learning_rate": 4.7357703535517354e-05,
      "loss": 3.2491,
      "step": 82500
    },
    {
      "epoch": 26.774716369529983,
      "grad_norm": 1.1023732423782349,
      "learning_rate": 4.735445994161531e-05,
      "loss": 3.2434,
      "step": 82600
    },
    {
      "epoch": 26.80713128038898,
      "grad_norm": 0.969001054763794,
      "learning_rate": 4.7351216347713264e-05,
      "loss": 3.2344,
      "step": 82700
    },
    {
      "epoch": 26.839546191247972,
      "grad_norm": 0.8901473879814148,
      "learning_rate": 4.734797275381122e-05,
      "loss": 3.2526,
      "step": 82800
    },
    {
      "epoch": 26.87196110210697,
      "grad_norm": 0.9235983490943909,
      "learning_rate": 4.734472915990918e-05,
      "loss": 3.2354,
      "step": 82900
    },
    {
      "epoch": 26.904376012965965,
      "grad_norm": 0.9452083110809326,
      "learning_rate": 4.734151800194616e-05,
      "loss": 3.2288,
      "step": 83000
    },
    {
      "epoch": 26.936790923824958,
      "grad_norm": 0.8909045457839966,
      "learning_rate": 4.733827440804411e-05,
      "loss": 3.263,
      "step": 83100
    },
    {
      "epoch": 26.969205834683954,
      "grad_norm": 1.0283564329147339,
      "learning_rate": 4.733503081414207e-05,
      "loss": 3.2436,
      "step": 83200
    },
    {
      "epoch": 27.0,
      "eval_bleu": 1.1564511397397266,
      "eval_loss": 3.715927839279175,
      "eval_runtime": 4.6031,
      "eval_samples_per_second": 106.885,
      "eval_steps_per_second": 1.738,
      "step": 83295
    },
    {
      "epoch": 27.00162074554295,
      "grad_norm": 0.8801915049552917,
      "learning_rate": 4.733178722024003e-05,
      "loss": 3.24,
      "step": 83300
    },
    {
      "epoch": 27.034035656401944,
      "grad_norm": 0.8015052080154419,
      "learning_rate": 4.732854362633798e-05,
      "loss": 3.216,
      "step": 83400
    },
    {
      "epoch": 27.06645056726094,
      "grad_norm": 0.8208436965942383,
      "learning_rate": 4.732530003243594e-05,
      "loss": 3.2113,
      "step": 83500
    },
    {
      "epoch": 27.098865478119937,
      "grad_norm": 0.7837216258049011,
      "learning_rate": 4.73220564385339e-05,
      "loss": 3.23,
      "step": 83600
    },
    {
      "epoch": 27.13128038897893,
      "grad_norm": 0.8142829537391663,
      "learning_rate": 4.731881284463186e-05,
      "loss": 3.2334,
      "step": 83700
    },
    {
      "epoch": 27.163695299837926,
      "grad_norm": 1.0355675220489502,
      "learning_rate": 4.7315569250729816e-05,
      "loss": 3.2297,
      "step": 83800
    },
    {
      "epoch": 27.196110210696922,
      "grad_norm": 0.9307448863983154,
      "learning_rate": 4.731232565682777e-05,
      "loss": 3.214,
      "step": 83900
    },
    {
      "epoch": 27.228525121555915,
      "grad_norm": 0.9059737324714661,
      "learning_rate": 4.730908206292573e-05,
      "loss": 3.2345,
      "step": 84000
    },
    {
      "epoch": 27.26094003241491,
      "grad_norm": 0.9997667670249939,
      "learning_rate": 4.730583846902368e-05,
      "loss": 3.2181,
      "step": 84100
    },
    {
      "epoch": 27.293354943273904,
      "grad_norm": 0.8966726064682007,
      "learning_rate": 4.730259487512164e-05,
      "loss": 3.2458,
      "step": 84200
    },
    {
      "epoch": 27.3257698541329,
      "grad_norm": 0.8832192420959473,
      "learning_rate": 4.7299351281219596e-05,
      "loss": 3.2356,
      "step": 84300
    },
    {
      "epoch": 27.358184764991897,
      "grad_norm": 0.8023368716239929,
      "learning_rate": 4.729610768731755e-05,
      "loss": 3.246,
      "step": 84400
    },
    {
      "epoch": 27.39059967585089,
      "grad_norm": 0.9183096885681152,
      "learning_rate": 4.729286409341551e-05,
      "loss": 3.2391,
      "step": 84500
    },
    {
      "epoch": 27.423014586709886,
      "grad_norm": 0.9081838726997375,
      "learning_rate": 4.7289620499513466e-05,
      "loss": 3.2195,
      "step": 84600
    },
    {
      "epoch": 27.455429497568883,
      "grad_norm": 0.7576412558555603,
      "learning_rate": 4.728637690561142e-05,
      "loss": 3.2488,
      "step": 84700
    },
    {
      "epoch": 27.487844408427875,
      "grad_norm": 0.8169103860855103,
      "learning_rate": 4.7283133311709376e-05,
      "loss": 3.2495,
      "step": 84800
    },
    {
      "epoch": 27.520259319286872,
      "grad_norm": 0.8990625143051147,
      "learning_rate": 4.7279889717807335e-05,
      "loss": 3.2271,
      "step": 84900
    },
    {
      "epoch": 27.55267423014587,
      "grad_norm": 1.0501149892807007,
      "learning_rate": 4.727667855984431e-05,
      "loss": 3.2428,
      "step": 85000
    },
    {
      "epoch": 27.58508914100486,
      "grad_norm": 0.8705491423606873,
      "learning_rate": 4.7273434965942265e-05,
      "loss": 3.243,
      "step": 85100
    },
    {
      "epoch": 27.617504051863857,
      "grad_norm": 0.9048194289207458,
      "learning_rate": 4.7270191372040224e-05,
      "loss": 3.2198,
      "step": 85200
    },
    {
      "epoch": 27.649918962722854,
      "grad_norm": 0.9554929137229919,
      "learning_rate": 4.726694777813818e-05,
      "loss": 3.2332,
      "step": 85300
    },
    {
      "epoch": 27.682333873581847,
      "grad_norm": 0.8119944930076599,
      "learning_rate": 4.7263704184236134e-05,
      "loss": 3.2364,
      "step": 85400
    },
    {
      "epoch": 27.714748784440843,
      "grad_norm": 0.9195199608802795,
      "learning_rate": 4.726046059033409e-05,
      "loss": 3.2321,
      "step": 85500
    },
    {
      "epoch": 27.74716369529984,
      "grad_norm": 0.8603640198707581,
      "learning_rate": 4.7257216996432045e-05,
      "loss": 3.2276,
      "step": 85600
    },
    {
      "epoch": 27.779578606158832,
      "grad_norm": 0.9460744857788086,
      "learning_rate": 4.7253973402530004e-05,
      "loss": 3.2259,
      "step": 85700
    },
    {
      "epoch": 27.81199351701783,
      "grad_norm": 0.9148878455162048,
      "learning_rate": 4.725072980862796e-05,
      "loss": 3.2588,
      "step": 85800
    },
    {
      "epoch": 27.84440842787682,
      "grad_norm": 0.9529094099998474,
      "learning_rate": 4.7247486214725915e-05,
      "loss": 3.2448,
      "step": 85900
    },
    {
      "epoch": 27.876823338735818,
      "grad_norm": 0.8275254368782043,
      "learning_rate": 4.724424262082387e-05,
      "loss": 3.2496,
      "step": 86000
    },
    {
      "epoch": 27.909238249594814,
      "grad_norm": 0.9331241250038147,
      "learning_rate": 4.724099902692183e-05,
      "loss": 3.2446,
      "step": 86100
    },
    {
      "epoch": 27.941653160453807,
      "grad_norm": 0.8560378551483154,
      "learning_rate": 4.7237755433019784e-05,
      "loss": 3.2306,
      "step": 86200
    },
    {
      "epoch": 27.974068071312804,
      "grad_norm": 0.9922318458557129,
      "learning_rate": 4.723451183911774e-05,
      "loss": 3.2429,
      "step": 86300
    },
    {
      "epoch": 28.0,
      "eval_bleu": 1.016993195089588,
      "eval_loss": 3.71298885345459,
      "eval_runtime": 5.1454,
      "eval_samples_per_second": 95.619,
      "eval_steps_per_second": 1.555,
      "step": 86380
    },
    {
      "epoch": 28.0064829821718,
      "grad_norm": 0.7811623215675354,
      "learning_rate": 4.72312682452157e-05,
      "loss": 3.2411,
      "step": 86400
    },
    {
      "epoch": 28.038897893030793,
      "grad_norm": 0.9034388661384583,
      "learning_rate": 4.722802465131366e-05,
      "loss": 3.2331,
      "step": 86500
    },
    {
      "epoch": 28.07131280388979,
      "grad_norm": 0.8952441215515137,
      "learning_rate": 4.722478105741162e-05,
      "loss": 3.2252,
      "step": 86600
    },
    {
      "epoch": 28.103727714748786,
      "grad_norm": 0.9215511679649353,
      "learning_rate": 4.722153746350957e-05,
      "loss": 3.2347,
      "step": 86700
    },
    {
      "epoch": 28.13614262560778,
      "grad_norm": 1.0312906503677368,
      "learning_rate": 4.721829386960753e-05,
      "loss": 3.2296,
      "step": 86800
    },
    {
      "epoch": 28.168557536466775,
      "grad_norm": 0.9444610476493835,
      "learning_rate": 4.721505027570549e-05,
      "loss": 3.2212,
      "step": 86900
    },
    {
      "epoch": 28.20097244732577,
      "grad_norm": 0.7815536856651306,
      "learning_rate": 4.721183911774246e-05,
      "loss": 3.2357,
      "step": 87000
    },
    {
      "epoch": 28.233387358184764,
      "grad_norm": 0.8535163402557373,
      "learning_rate": 4.720859552384042e-05,
      "loss": 3.215,
      "step": 87100
    },
    {
      "epoch": 28.26580226904376,
      "grad_norm": 1.0644131898880005,
      "learning_rate": 4.720535192993838e-05,
      "loss": 3.2183,
      "step": 87200
    },
    {
      "epoch": 28.298217179902757,
      "grad_norm": 0.9090868234634399,
      "learning_rate": 4.7202108336036336e-05,
      "loss": 3.2183,
      "step": 87300
    },
    {
      "epoch": 28.33063209076175,
      "grad_norm": 0.858262300491333,
      "learning_rate": 4.719886474213429e-05,
      "loss": 3.2306,
      "step": 87400
    },
    {
      "epoch": 28.363047001620746,
      "grad_norm": 0.842239499092102,
      "learning_rate": 4.7195621148232246e-05,
      "loss": 3.2276,
      "step": 87500
    },
    {
      "epoch": 28.39546191247974,
      "grad_norm": 0.7752278447151184,
      "learning_rate": 4.7192377554330205e-05,
      "loss": 3.2371,
      "step": 87600
    },
    {
      "epoch": 28.427876823338735,
      "grad_norm": 1.0732678174972534,
      "learning_rate": 4.718913396042816e-05,
      "loss": 3.2213,
      "step": 87700
    },
    {
      "epoch": 28.46029173419773,
      "grad_norm": 0.80622398853302,
      "learning_rate": 4.7185890366526116e-05,
      "loss": 3.2155,
      "step": 87800
    },
    {
      "epoch": 28.492706645056725,
      "grad_norm": 0.9782589673995972,
      "learning_rate": 4.718264677262407e-05,
      "loss": 3.203,
      "step": 87900
    },
    {
      "epoch": 28.52512155591572,
      "grad_norm": 0.736560583114624,
      "learning_rate": 4.7179403178722027e-05,
      "loss": 3.2334,
      "step": 88000
    },
    {
      "epoch": 28.557536466774717,
      "grad_norm": 0.8794552087783813,
      "learning_rate": 4.7176159584819985e-05,
      "loss": 3.2235,
      "step": 88100
    },
    {
      "epoch": 28.58995137763371,
      "grad_norm": 0.7932811975479126,
      "learning_rate": 4.717291599091794e-05,
      "loss": 3.2219,
      "step": 88200
    },
    {
      "epoch": 28.622366288492707,
      "grad_norm": 0.7872946262359619,
      "learning_rate": 4.7169672397015896e-05,
      "loss": 3.2247,
      "step": 88300
    },
    {
      "epoch": 28.654781199351703,
      "grad_norm": 0.8411557674407959,
      "learning_rate": 4.7166428803113855e-05,
      "loss": 3.2225,
      "step": 88400
    },
    {
      "epoch": 28.687196110210696,
      "grad_norm": 0.8057785034179688,
      "learning_rate": 4.716318520921181e-05,
      "loss": 3.232,
      "step": 88500
    },
    {
      "epoch": 28.719611021069692,
      "grad_norm": 0.992669939994812,
      "learning_rate": 4.7159941615309765e-05,
      "loss": 3.2227,
      "step": 88600
    },
    {
      "epoch": 28.75202593192869,
      "grad_norm": 0.9484904408454895,
      "learning_rate": 4.7156698021407724e-05,
      "loss": 3.2604,
      "step": 88700
    },
    {
      "epoch": 28.78444084278768,
      "grad_norm": 0.807748019695282,
      "learning_rate": 4.7153454427505676e-05,
      "loss": 3.235,
      "step": 88800
    },
    {
      "epoch": 28.816855753646678,
      "grad_norm": 0.9000910520553589,
      "learning_rate": 4.7150210833603635e-05,
      "loss": 3.2303,
      "step": 88900
    },
    {
      "epoch": 28.849270664505674,
      "grad_norm": 0.977117657661438,
      "learning_rate": 4.714699967564061e-05,
      "loss": 3.2174,
      "step": 89000
    },
    {
      "epoch": 28.881685575364667,
      "grad_norm": 0.9425219297409058,
      "learning_rate": 4.7143756081738565e-05,
      "loss": 3.2345,
      "step": 89100
    },
    {
      "epoch": 28.914100486223663,
      "grad_norm": 0.9931532144546509,
      "learning_rate": 4.7140512487836523e-05,
      "loss": 3.2348,
      "step": 89200
    },
    {
      "epoch": 28.946515397082656,
      "grad_norm": 0.9346868991851807,
      "learning_rate": 4.713726889393448e-05,
      "loss": 3.2379,
      "step": 89300
    },
    {
      "epoch": 28.978930307941653,
      "grad_norm": 1.0380154848098755,
      "learning_rate": 4.7134025300032434e-05,
      "loss": 3.2344,
      "step": 89400
    },
    {
      "epoch": 29.0,
      "eval_bleu": 0.8620958450774207,
      "eval_loss": 3.7156527042388916,
      "eval_runtime": 4.903,
      "eval_samples_per_second": 100.346,
      "eval_steps_per_second": 1.632,
      "step": 89465
    },
    {
      "epoch": 29.01134521880065,
      "grad_norm": 0.9693220257759094,
      "learning_rate": 4.713078170613039e-05,
      "loss": 3.2472,
      "step": 89500
    },
    {
      "epoch": 29.043760129659642,
      "grad_norm": 0.836755633354187,
      "learning_rate": 4.712753811222835e-05,
      "loss": 3.2071,
      "step": 89600
    },
    {
      "epoch": 29.07617504051864,
      "grad_norm": 0.963408887386322,
      "learning_rate": 4.7124294518326304e-05,
      "loss": 3.2228,
      "step": 89700
    },
    {
      "epoch": 29.108589951377635,
      "grad_norm": 0.8612934350967407,
      "learning_rate": 4.712105092442426e-05,
      "loss": 3.2113,
      "step": 89800
    },
    {
      "epoch": 29.141004862236628,
      "grad_norm": 0.9327705502510071,
      "learning_rate": 4.711780733052222e-05,
      "loss": 3.2174,
      "step": 89900
    },
    {
      "epoch": 29.173419773095624,
      "grad_norm": 0.9117677211761475,
      "learning_rate": 4.711456373662017e-05,
      "loss": 3.2027,
      "step": 90000
    },
    {
      "epoch": 29.20583468395462,
      "grad_norm": 0.8437981605529785,
      "learning_rate": 4.711132014271813e-05,
      "loss": 3.2345,
      "step": 90100
    },
    {
      "epoch": 29.238249594813613,
      "grad_norm": 0.8107649683952332,
      "learning_rate": 4.710807654881609e-05,
      "loss": 3.2353,
      "step": 90200
    },
    {
      "epoch": 29.27066450567261,
      "grad_norm": 1.035867691040039,
      "learning_rate": 4.710483295491405e-05,
      "loss": 3.2256,
      "step": 90300
    },
    {
      "epoch": 29.303079416531606,
      "grad_norm": 0.8371264338493347,
      "learning_rate": 4.710158936101201e-05,
      "loss": 3.2348,
      "step": 90400
    },
    {
      "epoch": 29.3354943273906,
      "grad_norm": 0.8924967646598816,
      "learning_rate": 4.709834576710996e-05,
      "loss": 3.2159,
      "step": 90500
    },
    {
      "epoch": 29.367909238249595,
      "grad_norm": 0.8506584167480469,
      "learning_rate": 4.709510217320792e-05,
      "loss": 3.1907,
      "step": 90600
    },
    {
      "epoch": 29.40032414910859,
      "grad_norm": 0.9373581409454346,
      "learning_rate": 4.709185857930588e-05,
      "loss": 3.2246,
      "step": 90700
    },
    {
      "epoch": 29.432739059967584,
      "grad_norm": 0.9310516119003296,
      "learning_rate": 4.708861498540383e-05,
      "loss": 3.2302,
      "step": 90800
    },
    {
      "epoch": 29.46515397082658,
      "grad_norm": 1.0228257179260254,
      "learning_rate": 4.708537139150179e-05,
      "loss": 3.2273,
      "step": 90900
    },
    {
      "epoch": 29.497568881685574,
      "grad_norm": 0.8112310767173767,
      "learning_rate": 4.708212779759975e-05,
      "loss": 3.2216,
      "step": 91000
    },
    {
      "epoch": 29.52998379254457,
      "grad_norm": 0.8605124354362488,
      "learning_rate": 4.7078916639636725e-05,
      "loss": 3.2431,
      "step": 91100
    },
    {
      "epoch": 29.562398703403566,
      "grad_norm": 0.9410679340362549,
      "learning_rate": 4.707567304573468e-05,
      "loss": 3.2272,
      "step": 91200
    },
    {
      "epoch": 29.59481361426256,
      "grad_norm": 0.8497024178504944,
      "learning_rate": 4.7072429451832635e-05,
      "loss": 3.2274,
      "step": 91300
    },
    {
      "epoch": 29.627228525121556,
      "grad_norm": 0.9948247075080872,
      "learning_rate": 4.706918585793059e-05,
      "loss": 3.2056,
      "step": 91400
    },
    {
      "epoch": 29.659643435980552,
      "grad_norm": 0.9449138045310974,
      "learning_rate": 4.7065942264028546e-05,
      "loss": 3.2203,
      "step": 91500
    },
    {
      "epoch": 29.692058346839545,
      "grad_norm": 1.0411323308944702,
      "learning_rate": 4.7062698670126505e-05,
      "loss": 3.2096,
      "step": 91600
    },
    {
      "epoch": 29.72447325769854,
      "grad_norm": 1.0237008333206177,
      "learning_rate": 4.705945507622446e-05,
      "loss": 3.2025,
      "step": 91700
    },
    {
      "epoch": 29.756888168557538,
      "grad_norm": 0.852400004863739,
      "learning_rate": 4.7056211482322416e-05,
      "loss": 3.2229,
      "step": 91800
    },
    {
      "epoch": 29.78930307941653,
      "grad_norm": 0.9233277440071106,
      "learning_rate": 4.7052967888420374e-05,
      "loss": 3.2206,
      "step": 91900
    },
    {
      "epoch": 29.821717990275527,
      "grad_norm": 0.963147759437561,
      "learning_rate": 4.7049724294518326e-05,
      "loss": 3.2371,
      "step": 92000
    },
    {
      "epoch": 29.854132901134523,
      "grad_norm": 0.9863738417625427,
      "learning_rate": 4.7046480700616285e-05,
      "loss": 3.2193,
      "step": 92100
    },
    {
      "epoch": 29.886547811993516,
      "grad_norm": 0.9980084300041199,
      "learning_rate": 4.7043237106714244e-05,
      "loss": 3.2183,
      "step": 92200
    },
    {
      "epoch": 29.918962722852513,
      "grad_norm": 0.9427039623260498,
      "learning_rate": 4.7039993512812196e-05,
      "loss": 3.2205,
      "step": 92300
    },
    {
      "epoch": 29.95137763371151,
      "grad_norm": 0.744028627872467,
      "learning_rate": 4.7036749918910154e-05,
      "loss": 3.2197,
      "step": 92400
    },
    {
      "epoch": 29.983792544570502,
      "grad_norm": 0.8347026109695435,
      "learning_rate": 4.7033506325008106e-05,
      "loss": 3.2394,
      "step": 92500
    },
    {
      "epoch": 30.0,
      "eval_bleu": 1.0578432419301722,
      "eval_loss": 3.7217772006988525,
      "eval_runtime": 4.4621,
      "eval_samples_per_second": 110.262,
      "eval_steps_per_second": 1.793,
      "step": 92550
    },
    {
      "epoch": 30.016207455429498,
      "grad_norm": 0.9188928008079529,
      "learning_rate": 4.7030262731106065e-05,
      "loss": 3.211,
      "step": 92600
    },
    {
      "epoch": 30.04862236628849,
      "grad_norm": 0.822765588760376,
      "learning_rate": 4.7027019137204024e-05,
      "loss": 3.2024,
      "step": 92700
    },
    {
      "epoch": 30.081037277147487,
      "grad_norm": 0.896817147731781,
      "learning_rate": 4.7023775543301976e-05,
      "loss": 3.2004,
      "step": 92800
    },
    {
      "epoch": 30.113452188006484,
      "grad_norm": 0.8816471099853516,
      "learning_rate": 4.7020531949399935e-05,
      "loss": 3.2167,
      "step": 92900
    },
    {
      "epoch": 30.145867098865477,
      "grad_norm": 0.8790517449378967,
      "learning_rate": 4.7017288355497893e-05,
      "loss": 3.2176,
      "step": 93000
    },
    {
      "epoch": 30.178282009724473,
      "grad_norm": 0.8545476794242859,
      "learning_rate": 4.701407719753487e-05,
      "loss": 3.2229,
      "step": 93100
    },
    {
      "epoch": 30.21069692058347,
      "grad_norm": 1.0187726020812988,
      "learning_rate": 4.701083360363282e-05,
      "loss": 3.1946,
      "step": 93200
    },
    {
      "epoch": 30.243111831442462,
      "grad_norm": 0.9758113622665405,
      "learning_rate": 4.700759000973078e-05,
      "loss": 3.2237,
      "step": 93300
    },
    {
      "epoch": 30.27552674230146,
      "grad_norm": 0.8630130290985107,
      "learning_rate": 4.700434641582874e-05,
      "loss": 3.2132,
      "step": 93400
    },
    {
      "epoch": 30.307941653160455,
      "grad_norm": 0.9997749328613281,
      "learning_rate": 4.700110282192669e-05,
      "loss": 3.2152,
      "step": 93500
    },
    {
      "epoch": 30.340356564019448,
      "grad_norm": 1.017067790031433,
      "learning_rate": 4.699785922802465e-05,
      "loss": 3.2145,
      "step": 93600
    },
    {
      "epoch": 30.372771474878444,
      "grad_norm": 0.9030765295028687,
      "learning_rate": 4.699461563412261e-05,
      "loss": 3.2011,
      "step": 93700
    },
    {
      "epoch": 30.40518638573744,
      "grad_norm": 0.8409193754196167,
      "learning_rate": 4.699137204022057e-05,
      "loss": 3.2183,
      "step": 93800
    },
    {
      "epoch": 30.437601296596434,
      "grad_norm": 0.8864888548851013,
      "learning_rate": 4.698812844631853e-05,
      "loss": 3.2209,
      "step": 93900
    },
    {
      "epoch": 30.47001620745543,
      "grad_norm": 0.9642324447631836,
      "learning_rate": 4.698488485241648e-05,
      "loss": 3.2024,
      "step": 94000
    },
    {
      "epoch": 30.502431118314426,
      "grad_norm": 0.8766580820083618,
      "learning_rate": 4.698164125851444e-05,
      "loss": 3.2137,
      "step": 94100
    },
    {
      "epoch": 30.53484602917342,
      "grad_norm": 0.8159382343292236,
      "learning_rate": 4.69783976646124e-05,
      "loss": 3.227,
      "step": 94200
    },
    {
      "epoch": 30.567260940032416,
      "grad_norm": 0.9068295359611511,
      "learning_rate": 4.697515407071035e-05,
      "loss": 3.1983,
      "step": 94300
    },
    {
      "epoch": 30.59967585089141,
      "grad_norm": 0.9001442790031433,
      "learning_rate": 4.697191047680831e-05,
      "loss": 3.2331,
      "step": 94400
    },
    {
      "epoch": 30.632090761750405,
      "grad_norm": 0.9198023676872253,
      "learning_rate": 4.6968666882906266e-05,
      "loss": 3.1917,
      "step": 94500
    },
    {
      "epoch": 30.6645056726094,
      "grad_norm": 0.8896318674087524,
      "learning_rate": 4.696542328900422e-05,
      "loss": 3.2129,
      "step": 94600
    },
    {
      "epoch": 30.696920583468394,
      "grad_norm": 0.8808829188346863,
      "learning_rate": 4.696217969510218e-05,
      "loss": 3.2129,
      "step": 94700
    },
    {
      "epoch": 30.72933549432739,
      "grad_norm": 0.8463298678398132,
      "learning_rate": 4.695893610120013e-05,
      "loss": 3.2468,
      "step": 94800
    },
    {
      "epoch": 30.761750405186387,
      "grad_norm": 0.8516310453414917,
      "learning_rate": 4.6955724943237114e-05,
      "loss": 3.2171,
      "step": 94900
    },
    {
      "epoch": 30.79416531604538,
      "grad_norm": 0.8366274833679199,
      "learning_rate": 4.6952481349335066e-05,
      "loss": 3.2047,
      "step": 95000
    },
    {
      "epoch": 30.826580226904376,
      "grad_norm": 1.2102687358856201,
      "learning_rate": 4.6949237755433024e-05,
      "loss": 3.2134,
      "step": 95100
    },
    {
      "epoch": 30.858995137763372,
      "grad_norm": 0.798808753490448,
      "learning_rate": 4.6945994161530976e-05,
      "loss": 3.2321,
      "step": 95200
    },
    {
      "epoch": 30.891410048622365,
      "grad_norm": 0.8087403774261475,
      "learning_rate": 4.6942750567628935e-05,
      "loss": 3.2198,
      "step": 95300
    },
    {
      "epoch": 30.92382495948136,
      "grad_norm": 1.0594292879104614,
      "learning_rate": 4.6939506973726894e-05,
      "loss": 3.2213,
      "step": 95400
    },
    {
      "epoch": 30.956239870340358,
      "grad_norm": 1.0351459980010986,
      "learning_rate": 4.6936263379824846e-05,
      "loss": 3.2076,
      "step": 95500
    },
    {
      "epoch": 30.98865478119935,
      "grad_norm": 0.8211233615875244,
      "learning_rate": 4.6933019785922805e-05,
      "loss": 3.2204,
      "step": 95600
    },
    {
      "epoch": 31.0,
      "eval_bleu": 0.9521607640525667,
      "eval_loss": 3.7175779342651367,
      "eval_runtime": 4.702,
      "eval_samples_per_second": 104.635,
      "eval_steps_per_second": 1.701,
      "step": 95635
    },
    {
      "epoch": 31.021069692058347,
      "grad_norm": 0.8903776407241821,
      "learning_rate": 4.692977619202076e-05,
      "loss": 3.209,
      "step": 95700
    },
    {
      "epoch": 31.053484602917344,
      "grad_norm": 0.7941554188728333,
      "learning_rate": 4.6926532598118715e-05,
      "loss": 3.2002,
      "step": 95800
    },
    {
      "epoch": 31.085899513776337,
      "grad_norm": 1.108073353767395,
      "learning_rate": 4.6923289004216674e-05,
      "loss": 3.2104,
      "step": 95900
    },
    {
      "epoch": 31.118314424635333,
      "grad_norm": 0.9436637163162231,
      "learning_rate": 4.6920045410314626e-05,
      "loss": 3.2159,
      "step": 96000
    },
    {
      "epoch": 31.150729335494326,
      "grad_norm": 0.8878848552703857,
      "learning_rate": 4.6916801816412585e-05,
      "loss": 3.1952,
      "step": 96100
    },
    {
      "epoch": 31.183144246353322,
      "grad_norm": 0.84607994556427,
      "learning_rate": 4.6913558222510544e-05,
      "loss": 3.196,
      "step": 96200
    },
    {
      "epoch": 31.21555915721232,
      "grad_norm": 0.9180102944374084,
      "learning_rate": 4.6910314628608495e-05,
      "loss": 3.215,
      "step": 96300
    },
    {
      "epoch": 31.24797406807131,
      "grad_norm": 0.9465857148170471,
      "learning_rate": 4.6907071034706454e-05,
      "loss": 3.2246,
      "step": 96400
    },
    {
      "epoch": 31.280388978930308,
      "grad_norm": 0.9359623789787292,
      "learning_rate": 4.690382744080441e-05,
      "loss": 3.2047,
      "step": 96500
    },
    {
      "epoch": 31.312803889789304,
      "grad_norm": 0.9237015247344971,
      "learning_rate": 4.690058384690237e-05,
      "loss": 3.1847,
      "step": 96600
    },
    {
      "epoch": 31.345218800648297,
      "grad_norm": 0.9187662601470947,
      "learning_rate": 4.6897340253000324e-05,
      "loss": 3.198,
      "step": 96700
    },
    {
      "epoch": 31.377633711507293,
      "grad_norm": 0.8106099367141724,
      "learning_rate": 4.689409665909828e-05,
      "loss": 3.2106,
      "step": 96800
    },
    {
      "epoch": 31.41004862236629,
      "grad_norm": 0.8186467885971069,
      "learning_rate": 4.689085306519624e-05,
      "loss": 3.2212,
      "step": 96900
    },
    {
      "epoch": 31.442463533225283,
      "grad_norm": 0.8866676092147827,
      "learning_rate": 4.68876094712942e-05,
      "loss": 3.2205,
      "step": 97000
    },
    {
      "epoch": 31.47487844408428,
      "grad_norm": 0.8730195760726929,
      "learning_rate": 4.688436587739215e-05,
      "loss": 3.2141,
      "step": 97100
    },
    {
      "epoch": 31.507293354943275,
      "grad_norm": 1.1999715566635132,
      "learning_rate": 4.688112228349011e-05,
      "loss": 3.2132,
      "step": 97200
    },
    {
      "epoch": 31.53970826580227,
      "grad_norm": 0.9076035022735596,
      "learning_rate": 4.687787868958807e-05,
      "loss": 3.2142,
      "step": 97300
    },
    {
      "epoch": 31.572123176661265,
      "grad_norm": 0.8119577765464783,
      "learning_rate": 4.687463509568602e-05,
      "loss": 3.1928,
      "step": 97400
    },
    {
      "epoch": 31.60453808752026,
      "grad_norm": 0.884177565574646,
      "learning_rate": 4.687139150178398e-05,
      "loss": 3.2124,
      "step": 97500
    },
    {
      "epoch": 31.636952998379254,
      "grad_norm": 0.9078370928764343,
      "learning_rate": 4.686814790788194e-05,
      "loss": 3.1957,
      "step": 97600
    },
    {
      "epoch": 31.66936790923825,
      "grad_norm": 0.9091132879257202,
      "learning_rate": 4.686490431397989e-05,
      "loss": 3.2223,
      "step": 97700
    },
    {
      "epoch": 31.701782820097243,
      "grad_norm": 0.8677605390548706,
      "learning_rate": 4.686169315601687e-05,
      "loss": 3.1858,
      "step": 97800
    },
    {
      "epoch": 31.73419773095624,
      "grad_norm": 0.7816935181617737,
      "learning_rate": 4.685844956211483e-05,
      "loss": 3.215,
      "step": 97900
    },
    {
      "epoch": 31.766612641815236,
      "grad_norm": 0.8640094995498657,
      "learning_rate": 4.6855205968212786e-05,
      "loss": 3.2076,
      "step": 98000
    },
    {
      "epoch": 31.79902755267423,
      "grad_norm": 0.8532865047454834,
      "learning_rate": 4.685196237431074e-05,
      "loss": 3.2148,
      "step": 98100
    },
    {
      "epoch": 31.831442463533225,
      "grad_norm": 0.7971879243850708,
      "learning_rate": 4.68487187804087e-05,
      "loss": 3.2133,
      "step": 98200
    },
    {
      "epoch": 31.86385737439222,
      "grad_norm": 0.965357780456543,
      "learning_rate": 4.6845475186506656e-05,
      "loss": 3.1915,
      "step": 98300
    },
    {
      "epoch": 31.896272285251214,
      "grad_norm": 0.9404030442237854,
      "learning_rate": 4.684223159260461e-05,
      "loss": 3.2366,
      "step": 98400
    },
    {
      "epoch": 31.92868719611021,
      "grad_norm": 0.8669416904449463,
      "learning_rate": 4.6838987998702566e-05,
      "loss": 3.1912,
      "step": 98500
    },
    {
      "epoch": 31.961102106969207,
      "grad_norm": 0.9696821570396423,
      "learning_rate": 4.683574440480052e-05,
      "loss": 3.2218,
      "step": 98600
    },
    {
      "epoch": 31.9935170178282,
      "grad_norm": 0.9548887014389038,
      "learning_rate": 4.683250081089848e-05,
      "loss": 3.192,
      "step": 98700
    },
    {
      "epoch": 32.0,
      "eval_bleu": 1.039920174071523,
      "eval_loss": 3.718536853790283,
      "eval_runtime": 4.5374,
      "eval_samples_per_second": 108.433,
      "eval_steps_per_second": 1.763,
      "step": 98720
    },
    {
      "epoch": 32.02593192868719,
      "grad_norm": 0.9219508767127991,
      "learning_rate": 4.6829257216996436e-05,
      "loss": 3.23,
      "step": 98800
    },
    {
      "epoch": 32.05834683954619,
      "grad_norm": 0.7699658274650574,
      "learning_rate": 4.682601362309439e-05,
      "loss": 3.1939,
      "step": 98900
    },
    {
      "epoch": 32.090761750405186,
      "grad_norm": 1.0319527387619019,
      "learning_rate": 4.6822770029192346e-05,
      "loss": 3.2038,
      "step": 99000
    },
    {
      "epoch": 32.12317666126418,
      "grad_norm": 0.8969290852546692,
      "learning_rate": 4.6819526435290305e-05,
      "loss": 3.1965,
      "step": 99100
    },
    {
      "epoch": 32.15559157212318,
      "grad_norm": 0.9121366143226624,
      "learning_rate": 4.681628284138826e-05,
      "loss": 3.1915,
      "step": 99200
    },
    {
      "epoch": 32.188006482982175,
      "grad_norm": 0.8702734112739563,
      "learning_rate": 4.6813039247486216e-05,
      "loss": 3.1807,
      "step": 99300
    },
    {
      "epoch": 32.220421393841164,
      "grad_norm": 0.8458595871925354,
      "learning_rate": 4.680979565358417e-05,
      "loss": 3.2061,
      "step": 99400
    },
    {
      "epoch": 32.25283630470016,
      "grad_norm": 0.9202053546905518,
      "learning_rate": 4.6806552059682127e-05,
      "loss": 3.202,
      "step": 99500
    },
    {
      "epoch": 32.28525121555916,
      "grad_norm": 0.8985304832458496,
      "learning_rate": 4.6803308465780085e-05,
      "loss": 3.1888,
      "step": 99600
    },
    {
      "epoch": 32.31766612641815,
      "grad_norm": 0.9207409620285034,
      "learning_rate": 4.6800064871878044e-05,
      "loss": 3.1935,
      "step": 99700
    },
    {
      "epoch": 32.35008103727715,
      "grad_norm": 0.9339014291763306,
      "learning_rate": 4.6796821277976e-05,
      "loss": 3.1897,
      "step": 99800
    },
    {
      "epoch": 32.382495948136146,
      "grad_norm": 0.8218439817428589,
      "learning_rate": 4.679357768407396e-05,
      "loss": 3.1946,
      "step": 99900
    },
    {
      "epoch": 32.414910858995135,
      "grad_norm": 0.8805781006813049,
      "learning_rate": 4.6790334090171913e-05,
      "loss": 3.1876,
      "step": 100000
    },
    {
      "epoch": 32.44732576985413,
      "grad_norm": 0.8465056419372559,
      "learning_rate": 4.678709049626987e-05,
      "loss": 3.2054,
      "step": 100100
    },
    {
      "epoch": 32.47974068071313,
      "grad_norm": 0.8700856566429138,
      "learning_rate": 4.678384690236783e-05,
      "loss": 3.1968,
      "step": 100200
    },
    {
      "epoch": 32.512155591572125,
      "grad_norm": 0.8809384107589722,
      "learning_rate": 4.678060330846578e-05,
      "loss": 3.2232,
      "step": 100300
    },
    {
      "epoch": 32.54457050243112,
      "grad_norm": 0.9054728746414185,
      "learning_rate": 4.677735971456374e-05,
      "loss": 3.2031,
      "step": 100400
    },
    {
      "epoch": 32.57698541329011,
      "grad_norm": 0.980101466178894,
      "learning_rate": 4.6774116120661694e-05,
      "loss": 3.1988,
      "step": 100500
    },
    {
      "epoch": 32.60940032414911,
      "grad_norm": 0.9070025682449341,
      "learning_rate": 4.677087252675965e-05,
      "loss": 3.2061,
      "step": 100600
    },
    {
      "epoch": 32.6418152350081,
      "grad_norm": 0.9876022338867188,
      "learning_rate": 4.676762893285761e-05,
      "loss": 3.1936,
      "step": 100700
    },
    {
      "epoch": 32.6742301458671,
      "grad_norm": 0.9292892813682556,
      "learning_rate": 4.676438533895556e-05,
      "loss": 3.1939,
      "step": 100800
    },
    {
      "epoch": 32.706645056726096,
      "grad_norm": 1.0896451473236084,
      "learning_rate": 4.676114174505352e-05,
      "loss": 3.2067,
      "step": 100900
    },
    {
      "epoch": 32.73905996758509,
      "grad_norm": 0.8370418548583984,
      "learning_rate": 4.675789815115148e-05,
      "loss": 3.1983,
      "step": 101000
    },
    {
      "epoch": 32.77147487844408,
      "grad_norm": 0.9945325255393982,
      "learning_rate": 4.675465455724943e-05,
      "loss": 3.2005,
      "step": 101100
    },
    {
      "epoch": 32.80388978930308,
      "grad_norm": 0.8277724385261536,
      "learning_rate": 4.675141096334739e-05,
      "loss": 3.2054,
      "step": 101200
    },
    {
      "epoch": 32.836304700162074,
      "grad_norm": 0.8415814638137817,
      "learning_rate": 4.674816736944535e-05,
      "loss": 3.2016,
      "step": 101300
    },
    {
      "epoch": 32.86871961102107,
      "grad_norm": 0.7820727825164795,
      "learning_rate": 4.67449237755433e-05,
      "loss": 3.2141,
      "step": 101400
    },
    {
      "epoch": 32.90113452188007,
      "grad_norm": 0.8547714948654175,
      "learning_rate": 4.674168018164126e-05,
      "loss": 3.2348,
      "step": 101500
    },
    {
      "epoch": 32.93354943273906,
      "grad_norm": 0.8712201714515686,
      "learning_rate": 4.673843658773921e-05,
      "loss": 3.2147,
      "step": 101600
    },
    {
      "epoch": 32.96596434359805,
      "grad_norm": 0.8755193948745728,
      "learning_rate": 4.673519299383717e-05,
      "loss": 3.2031,
      "step": 101700
    },
    {
      "epoch": 32.99837925445705,
      "grad_norm": 1.0285942554473877,
      "learning_rate": 4.673198183587415e-05,
      "loss": 3.2101,
      "step": 101800
    },
    {
      "epoch": 33.0,
      "eval_bleu": 1.0968360674256468,
      "eval_loss": 3.722261428833008,
      "eval_runtime": 4.7657,
      "eval_samples_per_second": 103.238,
      "eval_steps_per_second": 1.679,
      "step": 101805
    },
    {
      "epoch": 33.030794165316046,
      "grad_norm": 0.7800317406654358,
      "learning_rate": 4.672873824197211e-05,
      "loss": 3.1935,
      "step": 101900
    },
    {
      "epoch": 33.06320907617504,
      "grad_norm": 0.8956158757209778,
      "learning_rate": 4.672549464807006e-05,
      "loss": 3.2043,
      "step": 102000
    },
    {
      "epoch": 33.09562398703404,
      "grad_norm": 0.8137602806091309,
      "learning_rate": 4.672225105416802e-05,
      "loss": 3.1623,
      "step": 102100
    },
    {
      "epoch": 33.12803889789303,
      "grad_norm": 0.9699781537055969,
      "learning_rate": 4.671900746026598e-05,
      "loss": 3.1906,
      "step": 102200
    },
    {
      "epoch": 33.160453808752024,
      "grad_norm": 1.0317703485488892,
      "learning_rate": 4.671576386636393e-05,
      "loss": 3.2022,
      "step": 102300
    },
    {
      "epoch": 33.19286871961102,
      "grad_norm": 1.0089045763015747,
      "learning_rate": 4.671252027246189e-05,
      "loss": 3.1933,
      "step": 102400
    },
    {
      "epoch": 33.22528363047002,
      "grad_norm": 0.843778133392334,
      "learning_rate": 4.670927667855985e-05,
      "loss": 3.1981,
      "step": 102500
    },
    {
      "epoch": 33.25769854132901,
      "grad_norm": 0.8670610189437866,
      "learning_rate": 4.67060330846578e-05,
      "loss": 3.1848,
      "step": 102600
    },
    {
      "epoch": 33.29011345218801,
      "grad_norm": 0.8825877904891968,
      "learning_rate": 4.670278949075576e-05,
      "loss": 3.1863,
      "step": 102700
    },
    {
      "epoch": 33.322528363047,
      "grad_norm": 1.0116809606552124,
      "learning_rate": 4.6699545896853716e-05,
      "loss": 3.1995,
      "step": 102800
    },
    {
      "epoch": 33.354943273905995,
      "grad_norm": 0.8166382312774658,
      "learning_rate": 4.6696302302951675e-05,
      "loss": 3.194,
      "step": 102900
    },
    {
      "epoch": 33.38735818476499,
      "grad_norm": 0.876072347164154,
      "learning_rate": 4.6693058709049634e-05,
      "loss": 3.1959,
      "step": 103000
    },
    {
      "epoch": 33.41977309562399,
      "grad_norm": 0.9637598991394043,
      "learning_rate": 4.6689815115147586e-05,
      "loss": 3.2152,
      "step": 103100
    },
    {
      "epoch": 33.452188006482984,
      "grad_norm": 0.9068995118141174,
      "learning_rate": 4.668663639312358e-05,
      "loss": 3.1907,
      "step": 103200
    },
    {
      "epoch": 33.48460291734198,
      "grad_norm": 0.9221773147583008,
      "learning_rate": 4.668342523516056e-05,
      "loss": 3.2087,
      "step": 103300
    },
    {
      "epoch": 33.51701782820097,
      "grad_norm": 0.9367987513542175,
      "learning_rate": 4.668018164125852e-05,
      "loss": 3.2072,
      "step": 103400
    },
    {
      "epoch": 33.54943273905997,
      "grad_norm": 0.8639792203903198,
      "learning_rate": 4.667693804735647e-05,
      "loss": 3.1885,
      "step": 103500
    },
    {
      "epoch": 33.58184764991896,
      "grad_norm": 0.9860755205154419,
      "learning_rate": 4.667369445345443e-05,
      "loss": 3.1888,
      "step": 103600
    },
    {
      "epoch": 33.61426256077796,
      "grad_norm": 0.8923941850662231,
      "learning_rate": 4.667045085955238e-05,
      "loss": 3.198,
      "step": 103700
    },
    {
      "epoch": 33.646677471636956,
      "grad_norm": 0.905695915222168,
      "learning_rate": 4.666720726565034e-05,
      "loss": 3.1742,
      "step": 103800
    },
    {
      "epoch": 33.679092382495945,
      "grad_norm": 0.8533588647842407,
      "learning_rate": 4.66639636717483e-05,
      "loss": 3.2165,
      "step": 103900
    },
    {
      "epoch": 33.71150729335494,
      "grad_norm": 1.045547366142273,
      "learning_rate": 4.666072007784625e-05,
      "loss": 3.1787,
      "step": 104000
    },
    {
      "epoch": 33.74392220421394,
      "grad_norm": 0.9620909690856934,
      "learning_rate": 4.665747648394421e-05,
      "loss": 3.1869,
      "step": 104100
    },
    {
      "epoch": 33.776337115072934,
      "grad_norm": 0.9421836733818054,
      "learning_rate": 4.665423289004217e-05,
      "loss": 3.2012,
      "step": 104200
    },
    {
      "epoch": 33.80875202593193,
      "grad_norm": 0.8454378247261047,
      "learning_rate": 4.665098929614012e-05,
      "loss": 3.2151,
      "step": 104300
    },
    {
      "epoch": 33.84116693679093,
      "grad_norm": 0.8711632490158081,
      "learning_rate": 4.664774570223808e-05,
      "loss": 3.1989,
      "step": 104400
    },
    {
      "epoch": 33.873581847649916,
      "grad_norm": 1.0074613094329834,
      "learning_rate": 4.664450210833604e-05,
      "loss": 3.182,
      "step": 104500
    },
    {
      "epoch": 33.90599675850891,
      "grad_norm": 0.9877121448516846,
      "learning_rate": 4.6641258514434e-05,
      "loss": 3.1884,
      "step": 104600
    },
    {
      "epoch": 33.93841166936791,
      "grad_norm": 0.8752614259719849,
      "learning_rate": 4.6638014920531956e-05,
      "loss": 3.1957,
      "step": 104700
    },
    {
      "epoch": 33.970826580226905,
      "grad_norm": 0.9652555584907532,
      "learning_rate": 4.663477132662991e-05,
      "loss": 3.2178,
      "step": 104800
    },
    {
      "epoch": 34.0,
      "eval_bleu": 0.9388188905439638,
      "eval_loss": 3.7274720668792725,
      "eval_runtime": 4.5169,
      "eval_samples_per_second": 108.924,
      "eval_steps_per_second": 1.771,
      "step": 104890
    },
    {
      "epoch": 34.0032414910859,
      "grad_norm": 0.8019259572029114,
      "learning_rate": 4.6631527732727866e-05,
      "loss": 3.1975,
      "step": 104900
    },
    {
      "epoch": 34.0356564019449,
      "grad_norm": 0.9212105870246887,
      "learning_rate": 4.6628284138825825e-05,
      "loss": 3.1752,
      "step": 105000
    },
    {
      "epoch": 34.06807131280389,
      "grad_norm": 0.9145721793174744,
      "learning_rate": 4.662504054492378e-05,
      "loss": 3.1795,
      "step": 105100
    },
    {
      "epoch": 34.100486223662884,
      "grad_norm": 1.1005901098251343,
      "learning_rate": 4.6621796951021736e-05,
      "loss": 3.1863,
      "step": 105200
    },
    {
      "epoch": 34.13290113452188,
      "grad_norm": 0.9118809700012207,
      "learning_rate": 4.6618585793058714e-05,
      "loss": 3.1949,
      "step": 105300
    },
    {
      "epoch": 34.16531604538088,
      "grad_norm": 1.0129868984222412,
      "learning_rate": 4.661534219915667e-05,
      "loss": 3.1917,
      "step": 105400
    },
    {
      "epoch": 34.19773095623987,
      "grad_norm": 0.9981558918952942,
      "learning_rate": 4.6612098605254624e-05,
      "loss": 3.1851,
      "step": 105500
    },
    {
      "epoch": 34.23014586709886,
      "grad_norm": 1.012673020362854,
      "learning_rate": 4.660885501135258e-05,
      "loss": 3.1679,
      "step": 105600
    },
    {
      "epoch": 34.26256077795786,
      "grad_norm": 0.9064047932624817,
      "learning_rate": 4.660561141745054e-05,
      "loss": 3.1959,
      "step": 105700
    },
    {
      "epoch": 34.294975688816855,
      "grad_norm": 0.873271107673645,
      "learning_rate": 4.6602367823548494e-05,
      "loss": 3.1854,
      "step": 105800
    },
    {
      "epoch": 34.32739059967585,
      "grad_norm": 0.8083904385566711,
      "learning_rate": 4.659912422964645e-05,
      "loss": 3.1858,
      "step": 105900
    },
    {
      "epoch": 34.35980551053485,
      "grad_norm": 0.8278627395629883,
      "learning_rate": 4.6595880635744405e-05,
      "loss": 3.1847,
      "step": 106000
    },
    {
      "epoch": 34.392220421393844,
      "grad_norm": 0.9548860192298889,
      "learning_rate": 4.659266947778139e-05,
      "loss": 3.2063,
      "step": 106100
    },
    {
      "epoch": 34.424635332252834,
      "grad_norm": 1.058962345123291,
      "learning_rate": 4.658945831981836e-05,
      "loss": 3.2031,
      "step": 106200
    },
    {
      "epoch": 34.45705024311183,
      "grad_norm": 0.9175042510032654,
      "learning_rate": 4.658621472591632e-05,
      "loss": 3.2078,
      "step": 106300
    },
    {
      "epoch": 34.489465153970826,
      "grad_norm": 1.0057713985443115,
      "learning_rate": 4.658297113201428e-05,
      "loss": 3.1923,
      "step": 106400
    },
    {
      "epoch": 34.52188006482982,
      "grad_norm": 0.8823800086975098,
      "learning_rate": 4.6579727538112236e-05,
      "loss": 3.1924,
      "step": 106500
    },
    {
      "epoch": 34.55429497568882,
      "grad_norm": 0.7858763337135315,
      "learning_rate": 4.657648394421019e-05,
      "loss": 3.1802,
      "step": 106600
    },
    {
      "epoch": 34.58670988654781,
      "grad_norm": 1.0265415906906128,
      "learning_rate": 4.657324035030815e-05,
      "loss": 3.1954,
      "step": 106700
    },
    {
      "epoch": 34.619124797406805,
      "grad_norm": 0.895444929599762,
      "learning_rate": 4.65699967564061e-05,
      "loss": 3.1942,
      "step": 106800
    },
    {
      "epoch": 34.6515397082658,
      "grad_norm": 0.942039430141449,
      "learning_rate": 4.656675316250406e-05,
      "loss": 3.1834,
      "step": 106900
    },
    {
      "epoch": 34.6839546191248,
      "grad_norm": 0.8262958526611328,
      "learning_rate": 4.6563509568602017e-05,
      "loss": 3.19,
      "step": 107000
    },
    {
      "epoch": 34.716369529983794,
      "grad_norm": 0.8327507376670837,
      "learning_rate": 4.656026597469997e-05,
      "loss": 3.2038,
      "step": 107100
    },
    {
      "epoch": 34.74878444084279,
      "grad_norm": 1.1137975454330444,
      "learning_rate": 4.655702238079793e-05,
      "loss": 3.1742,
      "step": 107200
    },
    {
      "epoch": 34.78119935170178,
      "grad_norm": 0.8229698538780212,
      "learning_rate": 4.6553778786895886e-05,
      "loss": 3.1955,
      "step": 107300
    },
    {
      "epoch": 34.813614262560776,
      "grad_norm": 0.878894567489624,
      "learning_rate": 4.655053519299384e-05,
      "loss": 3.1913,
      "step": 107400
    },
    {
      "epoch": 34.84602917341977,
      "grad_norm": 1.0076828002929688,
      "learning_rate": 4.65472915990918e-05,
      "loss": 3.1719,
      "step": 107500
    },
    {
      "epoch": 34.87844408427877,
      "grad_norm": 0.851231575012207,
      "learning_rate": 4.654404800518975e-05,
      "loss": 3.2083,
      "step": 107600
    },
    {
      "epoch": 34.910858995137765,
      "grad_norm": 0.9445328116416931,
      "learning_rate": 4.654080441128771e-05,
      "loss": 3.2006,
      "step": 107700
    },
    {
      "epoch": 34.94327390599676,
      "grad_norm": 0.9421026706695557,
      "learning_rate": 4.6537560817385666e-05,
      "loss": 3.1965,
      "step": 107800
    },
    {
      "epoch": 34.97568881685575,
      "grad_norm": 0.9222015142440796,
      "learning_rate": 4.653431722348362e-05,
      "loss": 3.1888,
      "step": 107900
    },
    {
      "epoch": 35.0,
      "eval_bleu": 0.9453184352973777,
      "eval_loss": 3.725027322769165,
      "eval_runtime": 4.6813,
      "eval_samples_per_second": 105.1,
      "eval_steps_per_second": 1.709,
      "step": 107975
    },
    {
      "epoch": 35.00810372771475,
      "grad_norm": 1.0478609800338745,
      "learning_rate": 4.653107362958158e-05,
      "loss": 3.2016,
      "step": 108000
    },
    {
      "epoch": 35.040518638573744,
      "grad_norm": 0.897853434085846,
      "learning_rate": 4.6527830035679536e-05,
      "loss": 3.1917,
      "step": 108100
    },
    {
      "epoch": 35.07293354943274,
      "grad_norm": 0.8081399202346802,
      "learning_rate": 4.652458644177749e-05,
      "loss": 3.1762,
      "step": 108200
    },
    {
      "epoch": 35.10534846029174,
      "grad_norm": 0.9911915063858032,
      "learning_rate": 4.6521342847875446e-05,
      "loss": 3.1802,
      "step": 108300
    },
    {
      "epoch": 35.137763371150726,
      "grad_norm": 0.9403888583183289,
      "learning_rate": 4.6518099253973405e-05,
      "loss": 3.1769,
      "step": 108400
    },
    {
      "epoch": 35.17017828200972,
      "grad_norm": 1.0647908449172974,
      "learning_rate": 4.651485566007136e-05,
      "loss": 3.1891,
      "step": 108500
    },
    {
      "epoch": 35.20259319286872,
      "grad_norm": 1.1439307928085327,
      "learning_rate": 4.6511612066169316e-05,
      "loss": 3.1984,
      "step": 108600
    },
    {
      "epoch": 35.235008103727715,
      "grad_norm": 0.8809279799461365,
      "learning_rate": 4.6508368472267275e-05,
      "loss": 3.1917,
      "step": 108700
    },
    {
      "epoch": 35.26742301458671,
      "grad_norm": 0.8126108050346375,
      "learning_rate": 4.650512487836523e-05,
      "loss": 3.1806,
      "step": 108800
    },
    {
      "epoch": 35.29983792544571,
      "grad_norm": 0.8341014981269836,
      "learning_rate": 4.650188128446319e-05,
      "loss": 3.1875,
      "step": 108900
    },
    {
      "epoch": 35.3322528363047,
      "grad_norm": 0.8766995072364807,
      "learning_rate": 4.6498637690561144e-05,
      "loss": 3.1847,
      "step": 109000
    },
    {
      "epoch": 35.36466774716369,
      "grad_norm": 0.83847975730896,
      "learning_rate": 4.64953940966591e-05,
      "loss": 3.1926,
      "step": 109100
    },
    {
      "epoch": 35.39708265802269,
      "grad_norm": 0.9519503712654114,
      "learning_rate": 4.649215050275706e-05,
      "loss": 3.1719,
      "step": 109200
    },
    {
      "epoch": 35.429497568881686,
      "grad_norm": 0.8990920186042786,
      "learning_rate": 4.6488906908855013e-05,
      "loss": 3.2037,
      "step": 109300
    },
    {
      "epoch": 35.46191247974068,
      "grad_norm": 0.9153710603713989,
      "learning_rate": 4.648566331495297e-05,
      "loss": 3.1885,
      "step": 109400
    },
    {
      "epoch": 35.49432739059968,
      "grad_norm": 0.9574485421180725,
      "learning_rate": 4.648241972105093e-05,
      "loss": 3.182,
      "step": 109500
    },
    {
      "epoch": 35.52674230145867,
      "grad_norm": 0.9595448970794678,
      "learning_rate": 4.647917612714888e-05,
      "loss": 3.1912,
      "step": 109600
    },
    {
      "epoch": 35.559157212317665,
      "grad_norm": 0.9367234110832214,
      "learning_rate": 4.647593253324684e-05,
      "loss": 3.1783,
      "step": 109700
    },
    {
      "epoch": 35.59157212317666,
      "grad_norm": 0.8334375023841858,
      "learning_rate": 4.6472688939344794e-05,
      "loss": 3.1527,
      "step": 109800
    },
    {
      "epoch": 35.62398703403566,
      "grad_norm": 0.9186452031135559,
      "learning_rate": 4.646944534544275e-05,
      "loss": 3.181,
      "step": 109900
    },
    {
      "epoch": 35.656401944894654,
      "grad_norm": 0.9090968370437622,
      "learning_rate": 4.646620175154071e-05,
      "loss": 3.1897,
      "step": 110000
    },
    {
      "epoch": 35.68881685575364,
      "grad_norm": 0.934284508228302,
      "learning_rate": 4.646295815763866e-05,
      "loss": 3.1905,
      "step": 110100
    },
    {
      "epoch": 35.72123176661264,
      "grad_norm": 0.8335790634155273,
      "learning_rate": 4.645971456373662e-05,
      "loss": 3.1872,
      "step": 110200
    },
    {
      "epoch": 35.753646677471636,
      "grad_norm": 0.9974843859672546,
      "learning_rate": 4.645647096983458e-05,
      "loss": 3.1825,
      "step": 110300
    },
    {
      "epoch": 35.78606158833063,
      "grad_norm": 0.9354469180107117,
      "learning_rate": 4.645322737593253e-05,
      "loss": 3.1645,
      "step": 110400
    },
    {
      "epoch": 35.81847649918963,
      "grad_norm": 0.9747728109359741,
      "learning_rate": 4.644998378203049e-05,
      "loss": 3.1787,
      "step": 110500
    },
    {
      "epoch": 35.850891410048625,
      "grad_norm": 0.7990772724151611,
      "learning_rate": 4.644674018812844e-05,
      "loss": 3.1839,
      "step": 110600
    },
    {
      "epoch": 35.883306320907614,
      "grad_norm": 0.8656535744667053,
      "learning_rate": 4.64434965942264e-05,
      "loss": 3.1914,
      "step": 110700
    },
    {
      "epoch": 35.91572123176661,
      "grad_norm": 0.9005745649337769,
      "learning_rate": 4.644025300032436e-05,
      "loss": 3.1845,
      "step": 110800
    },
    {
      "epoch": 35.94813614262561,
      "grad_norm": 0.8498059511184692,
      "learning_rate": 4.643700940642231e-05,
      "loss": 3.1928,
      "step": 110900
    },
    {
      "epoch": 35.980551053484604,
      "grad_norm": 1.0529240369796753,
      "learning_rate": 4.643376581252027e-05,
      "loss": 3.1835,
      "step": 111000
    },
    {
      "epoch": 36.0,
      "eval_bleu": 1.106127292779125,
      "eval_loss": 3.7246158123016357,
      "eval_runtime": 4.5746,
      "eval_samples_per_second": 107.551,
      "eval_steps_per_second": 1.749,
      "step": 111060
    },
    {
      "epoch": 36.0129659643436,
      "grad_norm": 0.9090566039085388,
      "learning_rate": 4.643052221861823e-05,
      "loss": 3.1735,
      "step": 111100
    },
    {
      "epoch": 36.045380875202596,
      "grad_norm": 1.0108298063278198,
      "learning_rate": 4.642727862471619e-05,
      "loss": 3.1741,
      "step": 111200
    },
    {
      "epoch": 36.077795786061586,
      "grad_norm": 1.2153499126434326,
      "learning_rate": 4.642403503081415e-05,
      "loss": 3.1561,
      "step": 111300
    },
    {
      "epoch": 36.11021069692058,
      "grad_norm": 0.9361422061920166,
      "learning_rate": 4.6420791436912106e-05,
      "loss": 3.1659,
      "step": 111400
    },
    {
      "epoch": 36.14262560777958,
      "grad_norm": 0.8187971711158752,
      "learning_rate": 4.641754784301006e-05,
      "loss": 3.1731,
      "step": 111500
    },
    {
      "epoch": 36.175040518638575,
      "grad_norm": 0.860126256942749,
      "learning_rate": 4.641430424910802e-05,
      "loss": 3.164,
      "step": 111600
    },
    {
      "epoch": 36.20745542949757,
      "grad_norm": 0.8220388293266296,
      "learning_rate": 4.641106065520597e-05,
      "loss": 3.1725,
      "step": 111700
    },
    {
      "epoch": 36.23987034035656,
      "grad_norm": 0.9482901096343994,
      "learning_rate": 4.640781706130393e-05,
      "loss": 3.1784,
      "step": 111800
    },
    {
      "epoch": 36.27228525121556,
      "grad_norm": 0.9371810555458069,
      "learning_rate": 4.6404573467401887e-05,
      "loss": 3.1986,
      "step": 111900
    },
    {
      "epoch": 36.30470016207455,
      "grad_norm": 0.9338274598121643,
      "learning_rate": 4.640132987349984e-05,
      "loss": 3.194,
      "step": 112000
    },
    {
      "epoch": 36.33711507293355,
      "grad_norm": 0.9595882892608643,
      "learning_rate": 4.63980862795978e-05,
      "loss": 3.1734,
      "step": 112100
    },
    {
      "epoch": 36.369529983792546,
      "grad_norm": 1.0485495328903198,
      "learning_rate": 4.6394842685695756e-05,
      "loss": 3.1719,
      "step": 112200
    },
    {
      "epoch": 36.40194489465154,
      "grad_norm": 0.9546681642532349,
      "learning_rate": 4.639159909179371e-05,
      "loss": 3.1788,
      "step": 112300
    },
    {
      "epoch": 36.43435980551053,
      "grad_norm": 0.8361537456512451,
      "learning_rate": 4.638835549789167e-05,
      "loss": 3.1829,
      "step": 112400
    },
    {
      "epoch": 36.46677471636953,
      "grad_norm": 0.9260414242744446,
      "learning_rate": 4.6385111903989625e-05,
      "loss": 3.1757,
      "step": 112500
    },
    {
      "epoch": 36.499189627228525,
      "grad_norm": 0.8211867213249207,
      "learning_rate": 4.638186831008758e-05,
      "loss": 3.1658,
      "step": 112600
    },
    {
      "epoch": 36.53160453808752,
      "grad_norm": 0.7724260687828064,
      "learning_rate": 4.6378657152124555e-05,
      "loss": 3.1636,
      "step": 112700
    },
    {
      "epoch": 36.56401944894652,
      "grad_norm": 0.9775712490081787,
      "learning_rate": 4.6375413558222514e-05,
      "loss": 3.1657,
      "step": 112800
    },
    {
      "epoch": 36.596434359805514,
      "grad_norm": 0.8928866386413574,
      "learning_rate": 4.6372169964320466e-05,
      "loss": 3.1788,
      "step": 112900
    },
    {
      "epoch": 36.6288492706645,
      "grad_norm": 0.8869342803955078,
      "learning_rate": 4.6368926370418425e-05,
      "loss": 3.1806,
      "step": 113000
    },
    {
      "epoch": 36.6612641815235,
      "grad_norm": 0.9338503479957581,
      "learning_rate": 4.6365682776516383e-05,
      "loss": 3.1947,
      "step": 113100
    },
    {
      "epoch": 36.693679092382496,
      "grad_norm": 1.1796499490737915,
      "learning_rate": 4.6362439182614335e-05,
      "loss": 3.182,
      "step": 113200
    },
    {
      "epoch": 36.72609400324149,
      "grad_norm": 0.8488435745239258,
      "learning_rate": 4.6359195588712294e-05,
      "loss": 3.1655,
      "step": 113300
    },
    {
      "epoch": 36.75850891410049,
      "grad_norm": 0.7764959931373596,
      "learning_rate": 4.635595199481025e-05,
      "loss": 3.1869,
      "step": 113400
    },
    {
      "epoch": 36.79092382495948,
      "grad_norm": 0.8976495265960693,
      "learning_rate": 4.6352708400908205e-05,
      "loss": 3.1751,
      "step": 113500
    },
    {
      "epoch": 36.823338735818474,
      "grad_norm": 0.9037179350852966,
      "learning_rate": 4.6349464807006164e-05,
      "loss": 3.1873,
      "step": 113600
    },
    {
      "epoch": 36.85575364667747,
      "grad_norm": 0.8857465386390686,
      "learning_rate": 4.634622121310412e-05,
      "loss": 3.1946,
      "step": 113700
    },
    {
      "epoch": 36.88816855753647,
      "grad_norm": 1.0443952083587646,
      "learning_rate": 4.6342977619202074e-05,
      "loss": 3.1912,
      "step": 113800
    },
    {
      "epoch": 36.92058346839546,
      "grad_norm": 0.8300617337226868,
      "learning_rate": 4.633973402530003e-05,
      "loss": 3.177,
      "step": 113900
    },
    {
      "epoch": 36.95299837925446,
      "grad_norm": 1.0912394523620605,
      "learning_rate": 4.633649043139799e-05,
      "loss": 3.196,
      "step": 114000
    },
    {
      "epoch": 36.98541329011345,
      "grad_norm": 0.9284029006958008,
      "learning_rate": 4.633324683749595e-05,
      "loss": 3.181,
      "step": 114100
    },
    {
      "epoch": 37.0,
      "eval_bleu": 1.1194100051281957,
      "eval_loss": 3.7273097038269043,
      "eval_runtime": 4.8956,
      "eval_samples_per_second": 100.499,
      "eval_steps_per_second": 1.634,
      "step": 114145
    },
    {
      "epoch": 37.017828200972446,
      "grad_norm": 0.9788601994514465,
      "learning_rate": 4.63300032435939e-05,
      "loss": 3.1866,
      "step": 114200
    },
    {
      "epoch": 37.05024311183144,
      "grad_norm": 0.8127401471138,
      "learning_rate": 4.632675964969186e-05,
      "loss": 3.1734,
      "step": 114300
    },
    {
      "epoch": 37.08265802269044,
      "grad_norm": 1.0080010890960693,
      "learning_rate": 4.632351605578982e-05,
      "loss": 3.1631,
      "step": 114400
    },
    {
      "epoch": 37.115072933549435,
      "grad_norm": 1.0841151475906372,
      "learning_rate": 4.632027246188778e-05,
      "loss": 3.1801,
      "step": 114500
    },
    {
      "epoch": 37.14748784440843,
      "grad_norm": 0.9313936233520508,
      "learning_rate": 4.631706130392475e-05,
      "loss": 3.1671,
      "step": 114600
    },
    {
      "epoch": 37.17990275526742,
      "grad_norm": 1.028465986251831,
      "learning_rate": 4.631381771002271e-05,
      "loss": 3.1712,
      "step": 114700
    },
    {
      "epoch": 37.21231766612642,
      "grad_norm": 0.9576243758201599,
      "learning_rate": 4.631057411612067e-05,
      "loss": 3.1629,
      "step": 114800
    },
    {
      "epoch": 37.24473257698541,
      "grad_norm": 0.834165096282959,
      "learning_rate": 4.6307330522218626e-05,
      "loss": 3.1558,
      "step": 114900
    },
    {
      "epoch": 37.27714748784441,
      "grad_norm": 0.8748229742050171,
      "learning_rate": 4.630408692831658e-05,
      "loss": 3.1547,
      "step": 115000
    },
    {
      "epoch": 37.309562398703406,
      "grad_norm": 0.812533438205719,
      "learning_rate": 4.630084333441454e-05,
      "loss": 3.173,
      "step": 115100
    },
    {
      "epoch": 37.341977309562395,
      "grad_norm": 0.8992028832435608,
      "learning_rate": 4.6297599740512495e-05,
      "loss": 3.1745,
      "step": 115200
    },
    {
      "epoch": 37.37439222042139,
      "grad_norm": 0.9124386310577393,
      "learning_rate": 4.629435614661045e-05,
      "loss": 3.1832,
      "step": 115300
    },
    {
      "epoch": 37.40680713128039,
      "grad_norm": 0.9429936408996582,
      "learning_rate": 4.6291112552708406e-05,
      "loss": 3.1899,
      "step": 115400
    },
    {
      "epoch": 37.439222042139384,
      "grad_norm": 0.8015742301940918,
      "learning_rate": 4.628786895880636e-05,
      "loss": 3.1919,
      "step": 115500
    },
    {
      "epoch": 37.47163695299838,
      "grad_norm": 0.9371631741523743,
      "learning_rate": 4.628462536490432e-05,
      "loss": 3.1737,
      "step": 115600
    },
    {
      "epoch": 37.50405186385738,
      "grad_norm": 0.8266986608505249,
      "learning_rate": 4.6281381771002276e-05,
      "loss": 3.156,
      "step": 115700
    },
    {
      "epoch": 37.53646677471637,
      "grad_norm": 1.0008533000946045,
      "learning_rate": 4.627813817710023e-05,
      "loss": 3.1735,
      "step": 115800
    },
    {
      "epoch": 37.56888168557536,
      "grad_norm": 0.9163750410079956,
      "learning_rate": 4.6274894583198186e-05,
      "loss": 3.1726,
      "step": 115900
    },
    {
      "epoch": 37.60129659643436,
      "grad_norm": 0.8883119225502014,
      "learning_rate": 4.6271650989296145e-05,
      "loss": 3.1727,
      "step": 116000
    },
    {
      "epoch": 37.633711507293356,
      "grad_norm": 0.9525693655014038,
      "learning_rate": 4.62684073953941e-05,
      "loss": 3.1743,
      "step": 116100
    },
    {
      "epoch": 37.66612641815235,
      "grad_norm": 1.0676506757736206,
      "learning_rate": 4.6265163801492056e-05,
      "loss": 3.1668,
      "step": 116200
    },
    {
      "epoch": 37.69854132901135,
      "grad_norm": 0.8236426115036011,
      "learning_rate": 4.626192020759001e-05,
      "loss": 3.171,
      "step": 116300
    },
    {
      "epoch": 37.73095623987034,
      "grad_norm": 0.9773117899894714,
      "learning_rate": 4.6258676613687966e-05,
      "loss": 3.1785,
      "step": 116400
    },
    {
      "epoch": 37.763371150729334,
      "grad_norm": 1.1350595951080322,
      "learning_rate": 4.6255433019785925e-05,
      "loss": 3.1882,
      "step": 116500
    },
    {
      "epoch": 37.79578606158833,
      "grad_norm": 1.0719459056854248,
      "learning_rate": 4.625218942588388e-05,
      "loss": 3.1705,
      "step": 116600
    },
    {
      "epoch": 37.82820097244733,
      "grad_norm": 0.9174105525016785,
      "learning_rate": 4.6248945831981836e-05,
      "loss": 3.1848,
      "step": 116700
    },
    {
      "epoch": 37.86061588330632,
      "grad_norm": 0.867039144039154,
      "learning_rate": 4.6245702238079795e-05,
      "loss": 3.1713,
      "step": 116800
    },
    {
      "epoch": 37.89303079416531,
      "grad_norm": 0.9754366278648376,
      "learning_rate": 4.624245864417775e-05,
      "loss": 3.1845,
      "step": 116900
    },
    {
      "epoch": 37.92544570502431,
      "grad_norm": 0.8129899501800537,
      "learning_rate": 4.6239215050275705e-05,
      "loss": 3.1558,
      "step": 117000
    },
    {
      "epoch": 37.957860615883305,
      "grad_norm": 0.9454236626625061,
      "learning_rate": 4.6235971456373664e-05,
      "loss": 3.1751,
      "step": 117100
    },
    {
      "epoch": 37.9902755267423,
      "grad_norm": 0.8291586637496948,
      "learning_rate": 4.623272786247162e-05,
      "loss": 3.1591,
      "step": 117200
    },
    {
      "epoch": 38.0,
      "eval_bleu": 1.1870226064188687,
      "eval_loss": 3.72916316986084,
      "eval_runtime": 4.5162,
      "eval_samples_per_second": 108.94,
      "eval_steps_per_second": 1.771,
      "step": 117230
    },
    {
      "epoch": 38.0226904376013,
      "grad_norm": 0.8337608575820923,
      "learning_rate": 4.622948426856958e-05,
      "loss": 3.1641,
      "step": 117300
    },
    {
      "epoch": 38.055105348460295,
      "grad_norm": 1.0819714069366455,
      "learning_rate": 4.6226240674667534e-05,
      "loss": 3.1667,
      "step": 117400
    },
    {
      "epoch": 38.087520259319284,
      "grad_norm": 0.9340637922286987,
      "learning_rate": 4.622299708076549e-05,
      "loss": 3.1447,
      "step": 117500
    },
    {
      "epoch": 38.11993517017828,
      "grad_norm": 0.7909200191497803,
      "learning_rate": 4.621975348686345e-05,
      "loss": 3.165,
      "step": 117600
    },
    {
      "epoch": 38.15235008103728,
      "grad_norm": 0.8327709436416626,
      "learning_rate": 4.62165098929614e-05,
      "loss": 3.1535,
      "step": 117700
    },
    {
      "epoch": 38.18476499189627,
      "grad_norm": 0.8214728236198425,
      "learning_rate": 4.621326629905936e-05,
      "loss": 3.1666,
      "step": 117800
    },
    {
      "epoch": 38.21717990275527,
      "grad_norm": 1.0969563722610474,
      "learning_rate": 4.621002270515732e-05,
      "loss": 3.161,
      "step": 117900
    },
    {
      "epoch": 38.249594813614266,
      "grad_norm": 0.8333291411399841,
      "learning_rate": 4.620677911125527e-05,
      "loss": 3.1499,
      "step": 118000
    },
    {
      "epoch": 38.282009724473255,
      "grad_norm": 0.9735608100891113,
      "learning_rate": 4.620353551735323e-05,
      "loss": 3.1773,
      "step": 118100
    },
    {
      "epoch": 38.31442463533225,
      "grad_norm": 0.8974720239639282,
      "learning_rate": 4.620029192345119e-05,
      "loss": 3.1605,
      "step": 118200
    },
    {
      "epoch": 38.34683954619125,
      "grad_norm": 1.0591739416122437,
      "learning_rate": 4.619704832954914e-05,
      "loss": 3.1901,
      "step": 118300
    },
    {
      "epoch": 38.379254457050244,
      "grad_norm": 1.033583164215088,
      "learning_rate": 4.61938047356471e-05,
      "loss": 3.1543,
      "step": 118400
    },
    {
      "epoch": 38.41166936790924,
      "grad_norm": 0.9921106696128845,
      "learning_rate": 4.619056114174505e-05,
      "loss": 3.1648,
      "step": 118500
    },
    {
      "epoch": 38.44408427876823,
      "grad_norm": 1.053819179534912,
      "learning_rate": 4.618731754784301e-05,
      "loss": 3.1751,
      "step": 118600
    },
    {
      "epoch": 38.476499189627226,
      "grad_norm": 0.8557673096656799,
      "learning_rate": 4.618407395394097e-05,
      "loss": 3.1792,
      "step": 118700
    },
    {
      "epoch": 38.50891410048622,
      "grad_norm": 0.7896551489830017,
      "learning_rate": 4.618083036003892e-05,
      "loss": 3.1631,
      "step": 118800
    },
    {
      "epoch": 38.54132901134522,
      "grad_norm": 0.8116888999938965,
      "learning_rate": 4.617758676613688e-05,
      "loss": 3.1552,
      "step": 118900
    },
    {
      "epoch": 38.573743922204216,
      "grad_norm": 0.9483992457389832,
      "learning_rate": 4.617434317223484e-05,
      "loss": 3.1461,
      "step": 119000
    },
    {
      "epoch": 38.60615883306321,
      "grad_norm": 0.8973764777183533,
      "learning_rate": 4.617109957833279e-05,
      "loss": 3.1594,
      "step": 119100
    },
    {
      "epoch": 38.6385737439222,
      "grad_norm": 0.8667300939559937,
      "learning_rate": 4.616785598443075e-05,
      "loss": 3.1883,
      "step": 119200
    },
    {
      "epoch": 38.6709886547812,
      "grad_norm": 1.0398048162460327,
      "learning_rate": 4.61646123905287e-05,
      "loss": 3.1773,
      "step": 119300
    },
    {
      "epoch": 38.703403565640194,
      "grad_norm": 0.9880317449569702,
      "learning_rate": 4.616136879662666e-05,
      "loss": 3.1759,
      "step": 119400
    },
    {
      "epoch": 38.73581847649919,
      "grad_norm": 0.8303514719009399,
      "learning_rate": 4.615812520272462e-05,
      "loss": 3.1551,
      "step": 119500
    },
    {
      "epoch": 38.76823338735819,
      "grad_norm": 0.9851863384246826,
      "learning_rate": 4.615488160882258e-05,
      "loss": 3.1803,
      "step": 119600
    },
    {
      "epoch": 38.80064829821718,
      "grad_norm": 0.8830580115318298,
      "learning_rate": 4.615163801492054e-05,
      "loss": 3.1589,
      "step": 119700
    },
    {
      "epoch": 38.83306320907617,
      "grad_norm": 0.8159946799278259,
      "learning_rate": 4.6148394421018496e-05,
      "loss": 3.1826,
      "step": 119800
    },
    {
      "epoch": 38.86547811993517,
      "grad_norm": 0.8236784934997559,
      "learning_rate": 4.614515082711645e-05,
      "loss": 3.1786,
      "step": 119900
    },
    {
      "epoch": 38.897893030794165,
      "grad_norm": 0.8387981653213501,
      "learning_rate": 4.614190723321441e-05,
      "loss": 3.1616,
      "step": 120000
    },
    {
      "epoch": 38.93030794165316,
      "grad_norm": 0.9352803230285645,
      "learning_rate": 4.6138663639312365e-05,
      "loss": 3.18,
      "step": 120100
    },
    {
      "epoch": 38.96272285251216,
      "grad_norm": 0.8862873911857605,
      "learning_rate": 4.613542004541032e-05,
      "loss": 3.166,
      "step": 120200
    },
    {
      "epoch": 38.99513776337115,
      "grad_norm": 1.0821279287338257,
      "learning_rate": 4.6132176451508276e-05,
      "loss": 3.1756,
      "step": 120300
    },
    {
      "epoch": 39.0,
      "eval_bleu": 1.033884315853997,
      "eval_loss": 3.7330241203308105,
      "eval_runtime": 4.6897,
      "eval_samples_per_second": 104.911,
      "eval_steps_per_second": 1.706,
      "step": 120315
    },
    {
      "epoch": 39.027552674230144,
      "grad_norm": 1.0518709421157837,
      "learning_rate": 4.612893285760623e-05,
      "loss": 3.1539,
      "step": 120400
    },
    {
      "epoch": 39.05996758508914,
      "grad_norm": 0.9576952457427979,
      "learning_rate": 4.612568926370419e-05,
      "loss": 3.144,
      "step": 120500
    },
    {
      "epoch": 39.09238249594814,
      "grad_norm": 0.9993543028831482,
      "learning_rate": 4.6122445669802146e-05,
      "loss": 3.1532,
      "step": 120600
    },
    {
      "epoch": 39.12479740680713,
      "grad_norm": 0.8721500039100647,
      "learning_rate": 4.61192020759001e-05,
      "loss": 3.1547,
      "step": 120700
    },
    {
      "epoch": 39.15721231766613,
      "grad_norm": 1.0198324918746948,
      "learning_rate": 4.6115958481998056e-05,
      "loss": 3.1666,
      "step": 120800
    },
    {
      "epoch": 39.18962722852512,
      "grad_norm": 0.9878070950508118,
      "learning_rate": 4.6112714888096015e-05,
      "loss": 3.1616,
      "step": 120900
    },
    {
      "epoch": 39.222042139384115,
      "grad_norm": 0.8289362192153931,
      "learning_rate": 4.610947129419397e-05,
      "loss": 3.1573,
      "step": 121000
    },
    {
      "epoch": 39.25445705024311,
      "grad_norm": 0.7403767108917236,
      "learning_rate": 4.6106227700291926e-05,
      "loss": 3.1657,
      "step": 121100
    },
    {
      "epoch": 39.28687196110211,
      "grad_norm": 0.8969583511352539,
      "learning_rate": 4.6102984106389884e-05,
      "loss": 3.1764,
      "step": 121200
    },
    {
      "epoch": 39.319286871961104,
      "grad_norm": 0.9757165312767029,
      "learning_rate": 4.6099740512487836e-05,
      "loss": 3.1786,
      "step": 121300
    },
    {
      "epoch": 39.3517017828201,
      "grad_norm": 0.8385777473449707,
      "learning_rate": 4.6096496918585795e-05,
      "loss": 3.1428,
      "step": 121400
    },
    {
      "epoch": 39.38411669367909,
      "grad_norm": 0.8289440274238586,
      "learning_rate": 4.609325332468375e-05,
      "loss": 3.1564,
      "step": 121500
    },
    {
      "epoch": 39.416531604538086,
      "grad_norm": 0.7768101096153259,
      "learning_rate": 4.6090009730781706e-05,
      "loss": 3.1627,
      "step": 121600
    },
    {
      "epoch": 39.44894651539708,
      "grad_norm": 0.9249829053878784,
      "learning_rate": 4.6086766136879665e-05,
      "loss": 3.1698,
      "step": 121700
    },
    {
      "epoch": 39.48136142625608,
      "grad_norm": 0.9315572381019592,
      "learning_rate": 4.608352254297762e-05,
      "loss": 3.1444,
      "step": 121800
    },
    {
      "epoch": 39.513776337115075,
      "grad_norm": 0.7677350044250488,
      "learning_rate": 4.6080278949075575e-05,
      "loss": 3.1714,
      "step": 121900
    },
    {
      "epoch": 39.546191247974065,
      "grad_norm": 0.9034419655799866,
      "learning_rate": 4.6077035355173534e-05,
      "loss": 3.1417,
      "step": 122000
    },
    {
      "epoch": 39.57860615883306,
      "grad_norm": 0.8502383232116699,
      "learning_rate": 4.607379176127149e-05,
      "loss": 3.1781,
      "step": 122100
    },
    {
      "epoch": 39.61102106969206,
      "grad_norm": 0.821972668170929,
      "learning_rate": 4.607054816736945e-05,
      "loss": 3.1483,
      "step": 122200
    },
    {
      "epoch": 39.643435980551054,
      "grad_norm": 1.0969666242599487,
      "learning_rate": 4.606733700940642e-05,
      "loss": 3.167,
      "step": 122300
    },
    {
      "epoch": 39.67585089141005,
      "grad_norm": 1.0272279977798462,
      "learning_rate": 4.606409341550438e-05,
      "loss": 3.1674,
      "step": 122400
    },
    {
      "epoch": 39.70826580226905,
      "grad_norm": 0.9367011189460754,
      "learning_rate": 4.606084982160233e-05,
      "loss": 3.1528,
      "step": 122500
    },
    {
      "epoch": 39.740680713128036,
      "grad_norm": 0.8662858605384827,
      "learning_rate": 4.605760622770029e-05,
      "loss": 3.1767,
      "step": 122600
    },
    {
      "epoch": 39.77309562398703,
      "grad_norm": 0.9076191782951355,
      "learning_rate": 4.605439506973727e-05,
      "loss": 3.1584,
      "step": 122700
    },
    {
      "epoch": 39.80551053484603,
      "grad_norm": 0.8437454700469971,
      "learning_rate": 4.605115147583523e-05,
      "loss": 3.1674,
      "step": 122800
    },
    {
      "epoch": 39.837925445705025,
      "grad_norm": 1.0088030099868774,
      "learning_rate": 4.604790788193318e-05,
      "loss": 3.1672,
      "step": 122900
    },
    {
      "epoch": 39.87034035656402,
      "grad_norm": 0.84013831615448,
      "learning_rate": 4.604466428803114e-05,
      "loss": 3.1706,
      "step": 123000
    },
    {
      "epoch": 39.90275526742302,
      "grad_norm": 0.7918412089347839,
      "learning_rate": 4.60414206941291e-05,
      "loss": 3.1663,
      "step": 123100
    },
    {
      "epoch": 39.93517017828201,
      "grad_norm": 0.8901945948600769,
      "learning_rate": 4.603817710022706e-05,
      "loss": 3.1705,
      "step": 123200
    },
    {
      "epoch": 39.967585089141004,
      "grad_norm": 0.9327324032783508,
      "learning_rate": 4.603493350632501e-05,
      "loss": 3.1637,
      "step": 123300
    },
    {
      "epoch": 40.0,
      "grad_norm": 0.9496197700500488,
      "learning_rate": 4.603168991242297e-05,
      "loss": 3.1537,
      "step": 123400
    },
    {
      "epoch": 40.0,
      "eval_bleu": 1.11621957322237,
      "eval_loss": 3.7393763065338135,
      "eval_runtime": 4.605,
      "eval_samples_per_second": 106.84,
      "eval_steps_per_second": 1.737,
      "step": 123400
    },
    {
      "epoch": 40.032414910858996,
      "grad_norm": 0.9844576120376587,
      "learning_rate": 4.6028446318520926e-05,
      "loss": 3.1476,
      "step": 123500
    },
    {
      "epoch": 40.06482982171799,
      "grad_norm": 0.9136404395103455,
      "learning_rate": 4.6025202724618885e-05,
      "loss": 3.1604,
      "step": 123600
    },
    {
      "epoch": 40.09724473257698,
      "grad_norm": 0.9235369563102722,
      "learning_rate": 4.602195913071684e-05,
      "loss": 3.1497,
      "step": 123700
    },
    {
      "epoch": 40.12965964343598,
      "grad_norm": 0.8370351791381836,
      "learning_rate": 4.6018715536814796e-05,
      "loss": 3.1323,
      "step": 123800
    },
    {
      "epoch": 40.162074554294975,
      "grad_norm": 1.2283134460449219,
      "learning_rate": 4.601547194291275e-05,
      "loss": 3.1568,
      "step": 123900
    },
    {
      "epoch": 40.19448946515397,
      "grad_norm": 0.8979170322418213,
      "learning_rate": 4.6012228349010706e-05,
      "loss": 3.1619,
      "step": 124000
    },
    {
      "epoch": 40.22690437601297,
      "grad_norm": 0.9249616861343384,
      "learning_rate": 4.6008984755108665e-05,
      "loss": 3.1527,
      "step": 124100
    },
    {
      "epoch": 40.259319286871964,
      "grad_norm": 0.8156850934028625,
      "learning_rate": 4.600574116120662e-05,
      "loss": 3.162,
      "step": 124200
    },
    {
      "epoch": 40.29173419773095,
      "grad_norm": 0.9723141193389893,
      "learning_rate": 4.6002497567304576e-05,
      "loss": 3.1551,
      "step": 124300
    },
    {
      "epoch": 40.32414910858995,
      "grad_norm": 0.8535184860229492,
      "learning_rate": 4.5999253973402535e-05,
      "loss": 3.1648,
      "step": 124400
    },
    {
      "epoch": 40.356564019448946,
      "grad_norm": 0.9572745561599731,
      "learning_rate": 4.5996010379500487e-05,
      "loss": 3.1609,
      "step": 124500
    },
    {
      "epoch": 40.38897893030794,
      "grad_norm": 1.0384132862091064,
      "learning_rate": 4.5992766785598445e-05,
      "loss": 3.1579,
      "step": 124600
    },
    {
      "epoch": 40.42139384116694,
      "grad_norm": 0.9565693140029907,
      "learning_rate": 4.5989523191696404e-05,
      "loss": 3.1589,
      "step": 124700
    },
    {
      "epoch": 40.453808752025935,
      "grad_norm": 0.9028939008712769,
      "learning_rate": 4.5986279597794356e-05,
      "loss": 3.149,
      "step": 124800
    },
    {
      "epoch": 40.486223662884925,
      "grad_norm": 0.9528378248214722,
      "learning_rate": 4.5983036003892315e-05,
      "loss": 3.1646,
      "step": 124900
    },
    {
      "epoch": 40.51863857374392,
      "grad_norm": 0.8544490933418274,
      "learning_rate": 4.597979240999027e-05,
      "loss": 3.1662,
      "step": 125000
    },
    {
      "epoch": 40.55105348460292,
      "grad_norm": 0.8833342790603638,
      "learning_rate": 4.5976548816088225e-05,
      "loss": 3.1709,
      "step": 125100
    },
    {
      "epoch": 40.583468395461914,
      "grad_norm": 1.0169634819030762,
      "learning_rate": 4.5973305222186184e-05,
      "loss": 3.1361,
      "step": 125200
    },
    {
      "epoch": 40.61588330632091,
      "grad_norm": 0.9381358623504639,
      "learning_rate": 4.5970061628284136e-05,
      "loss": 3.1569,
      "step": 125300
    },
    {
      "epoch": 40.6482982171799,
      "grad_norm": 1.085260272026062,
      "learning_rate": 4.5966818034382095e-05,
      "loss": 3.1694,
      "step": 125400
    },
    {
      "epoch": 40.680713128038896,
      "grad_norm": 0.903556227684021,
      "learning_rate": 4.5963574440480054e-05,
      "loss": 3.1631,
      "step": 125500
    },
    {
      "epoch": 40.71312803889789,
      "grad_norm": 1.0021151304244995,
      "learning_rate": 4.596033084657801e-05,
      "loss": 3.1377,
      "step": 125600
    },
    {
      "epoch": 40.74554294975689,
      "grad_norm": 0.832374095916748,
      "learning_rate": 4.5957087252675964e-05,
      "loss": 3.1673,
      "step": 125700
    },
    {
      "epoch": 40.777957860615885,
      "grad_norm": 0.8500869870185852,
      "learning_rate": 4.595384365877392e-05,
      "loss": 3.15,
      "step": 125800
    },
    {
      "epoch": 40.81037277147488,
      "grad_norm": 1.198673963546753,
      "learning_rate": 4.595060006487188e-05,
      "loss": 3.1695,
      "step": 125900
    },
    {
      "epoch": 40.84278768233387,
      "grad_norm": 0.8970091938972473,
      "learning_rate": 4.594735647096984e-05,
      "loss": 3.1737,
      "step": 126000
    },
    {
      "epoch": 40.87520259319287,
      "grad_norm": 0.8494666218757629,
      "learning_rate": 4.594411287706779e-05,
      "loss": 3.1521,
      "step": 126100
    },
    {
      "epoch": 40.90761750405186,
      "grad_norm": 1.0136501789093018,
      "learning_rate": 4.594086928316575e-05,
      "loss": 3.1653,
      "step": 126200
    },
    {
      "epoch": 40.94003241491086,
      "grad_norm": 0.8595350980758667,
      "learning_rate": 4.593762568926371e-05,
      "loss": 3.1367,
      "step": 126300
    },
    {
      "epoch": 40.972447325769856,
      "grad_norm": 0.8823848366737366,
      "learning_rate": 4.593438209536166e-05,
      "loss": 3.1384,
      "step": 126400
    },
    {
      "epoch": 41.0,
      "eval_bleu": 1.0345636036565988,
      "eval_loss": 3.7360057830810547,
      "eval_runtime": 4.3603,
      "eval_samples_per_second": 112.835,
      "eval_steps_per_second": 1.835,
      "step": 126485
    },
    {
      "epoch": 41.00486223662885,
      "grad_norm": 0.8960253000259399,
      "learning_rate": 4.593113850145962e-05,
      "loss": 3.158,
      "step": 126500
    },
    {
      "epoch": 41.03727714748784,
      "grad_norm": 0.8626531362533569,
      "learning_rate": 4.592789490755758e-05,
      "loss": 3.1418,
      "step": 126600
    },
    {
      "epoch": 41.06969205834684,
      "grad_norm": 1.1586333513259888,
      "learning_rate": 4.592465131365553e-05,
      "loss": 3.1414,
      "step": 126700
    },
    {
      "epoch": 41.102106969205835,
      "grad_norm": 0.9867116808891296,
      "learning_rate": 4.592140771975349e-05,
      "loss": 3.1354,
      "step": 126800
    },
    {
      "epoch": 41.13452188006483,
      "grad_norm": 0.851769208908081,
      "learning_rate": 4.591816412585145e-05,
      "loss": 3.1436,
      "step": 126900
    },
    {
      "epoch": 41.16693679092383,
      "grad_norm": 0.9926910400390625,
      "learning_rate": 4.59149205319494e-05,
      "loss": 3.139,
      "step": 127000
    },
    {
      "epoch": 41.19935170178282,
      "grad_norm": 0.7866466045379639,
      "learning_rate": 4.591167693804736e-05,
      "loss": 3.1601,
      "step": 127100
    },
    {
      "epoch": 41.23176661264181,
      "grad_norm": 0.9389259815216064,
      "learning_rate": 4.590843334414531e-05,
      "loss": 3.1513,
      "step": 127200
    },
    {
      "epoch": 41.26418152350081,
      "grad_norm": 0.9913707971572876,
      "learning_rate": 4.590518975024327e-05,
      "loss": 3.1431,
      "step": 127300
    },
    {
      "epoch": 41.296596434359806,
      "grad_norm": 0.9526100754737854,
      "learning_rate": 4.590194615634123e-05,
      "loss": 3.1783,
      "step": 127400
    },
    {
      "epoch": 41.3290113452188,
      "grad_norm": 1.092116355895996,
      "learning_rate": 4.589870256243918e-05,
      "loss": 3.1446,
      "step": 127500
    },
    {
      "epoch": 41.3614262560778,
      "grad_norm": 0.9582242965698242,
      "learning_rate": 4.589545896853714e-05,
      "loss": 3.1627,
      "step": 127600
    },
    {
      "epoch": 41.39384116693679,
      "grad_norm": 0.8177607655525208,
      "learning_rate": 4.58922153746351e-05,
      "loss": 3.1379,
      "step": 127700
    },
    {
      "epoch": 41.426256077795784,
      "grad_norm": 0.9119894504547119,
      "learning_rate": 4.588897178073305e-05,
      "loss": 3.1705,
      "step": 127800
    },
    {
      "epoch": 41.45867098865478,
      "grad_norm": 1.0474365949630737,
      "learning_rate": 4.588572818683101e-05,
      "loss": 3.1679,
      "step": 127900
    },
    {
      "epoch": 41.49108589951378,
      "grad_norm": 0.8267908096313477,
      "learning_rate": 4.588248459292897e-05,
      "loss": 3.1512,
      "step": 128000
    },
    {
      "epoch": 41.523500810372774,
      "grad_norm": 0.9483926296234131,
      "learning_rate": 4.587924099902693e-05,
      "loss": 3.1387,
      "step": 128100
    },
    {
      "epoch": 41.55591572123177,
      "grad_norm": 0.9541379809379578,
      "learning_rate": 4.587599740512488e-05,
      "loss": 3.1509,
      "step": 128200
    },
    {
      "epoch": 41.58833063209076,
      "grad_norm": 0.9950642585754395,
      "learning_rate": 4.587275381122284e-05,
      "loss": 3.1576,
      "step": 128300
    },
    {
      "epoch": 41.620745542949756,
      "grad_norm": 1.0187331438064575,
      "learning_rate": 4.5869510217320796e-05,
      "loss": 3.1426,
      "step": 128400
    },
    {
      "epoch": 41.65316045380875,
      "grad_norm": 0.8352580070495605,
      "learning_rate": 4.5866266623418755e-05,
      "loss": 3.1469,
      "step": 128500
    },
    {
      "epoch": 41.68557536466775,
      "grad_norm": 0.9997631907463074,
      "learning_rate": 4.586302302951671e-05,
      "loss": 3.1464,
      "step": 128600
    },
    {
      "epoch": 41.717990275526745,
      "grad_norm": 0.9713588953018188,
      "learning_rate": 4.5859779435614666e-05,
      "loss": 3.1519,
      "step": 128700
    },
    {
      "epoch": 41.750405186385734,
      "grad_norm": 0.8959241509437561,
      "learning_rate": 4.5856535841712624e-05,
      "loss": 3.1357,
      "step": 128800
    },
    {
      "epoch": 41.78282009724473,
      "grad_norm": 0.9071714282035828,
      "learning_rate": 4.5853292247810576e-05,
      "loss": 3.1528,
      "step": 128900
    },
    {
      "epoch": 41.81523500810373,
      "grad_norm": 0.8834446668624878,
      "learning_rate": 4.5850048653908535e-05,
      "loss": 3.1465,
      "step": 129000
    },
    {
      "epoch": 41.84764991896272,
      "grad_norm": 0.8347866535186768,
      "learning_rate": 4.584680506000649e-05,
      "loss": 3.1368,
      "step": 129100
    },
    {
      "epoch": 41.88006482982172,
      "grad_norm": 0.9023616909980774,
      "learning_rate": 4.5843561466104446e-05,
      "loss": 3.164,
      "step": 129200
    },
    {
      "epoch": 41.912479740680716,
      "grad_norm": 0.9065122008323669,
      "learning_rate": 4.5840317872202405e-05,
      "loss": 3.1676,
      "step": 129300
    },
    {
      "epoch": 41.944894651539705,
      "grad_norm": 0.9135403037071228,
      "learning_rate": 4.583710671423938e-05,
      "loss": 3.1559,
      "step": 129400
    },
    {
      "epoch": 41.9773095623987,
      "grad_norm": 0.7869455814361572,
      "learning_rate": 4.5833863120337334e-05,
      "loss": 3.1589,
      "step": 129500
    },
    {
      "epoch": 42.0,
      "eval_bleu": 1.1218241837245695,
      "eval_loss": 3.7363414764404297,
      "eval_runtime": 5.3371,
      "eval_samples_per_second": 92.185,
      "eval_steps_per_second": 1.499,
      "step": 129570
    },
    {
      "epoch": 42.0097244732577,
      "grad_norm": 0.8866503834724426,
      "learning_rate": 4.583061952643529e-05,
      "loss": 3.1418,
      "step": 129600
    },
    {
      "epoch": 42.042139384116695,
      "grad_norm": 0.7875815629959106,
      "learning_rate": 4.582737593253325e-05,
      "loss": 3.112,
      "step": 129700
    },
    {
      "epoch": 42.07455429497569,
      "grad_norm": 1.0215955972671509,
      "learning_rate": 4.5824132338631204e-05,
      "loss": 3.1528,
      "step": 129800
    },
    {
      "epoch": 42.10696920583469,
      "grad_norm": 1.0371382236480713,
      "learning_rate": 4.582088874472916e-05,
      "loss": 3.1602,
      "step": 129900
    },
    {
      "epoch": 42.13938411669368,
      "grad_norm": 0.9932175278663635,
      "learning_rate": 4.581764515082712e-05,
      "loss": 3.1435,
      "step": 130000
    },
    {
      "epoch": 42.17179902755267,
      "grad_norm": 0.8902833461761475,
      "learning_rate": 4.581440155692507e-05,
      "loss": 3.1398,
      "step": 130100
    },
    {
      "epoch": 42.20421393841167,
      "grad_norm": 0.9435009360313416,
      "learning_rate": 4.581115796302303e-05,
      "loss": 3.145,
      "step": 130200
    },
    {
      "epoch": 42.236628849270666,
      "grad_norm": 0.8556351065635681,
      "learning_rate": 4.5807914369120984e-05,
      "loss": 3.1582,
      "step": 130300
    },
    {
      "epoch": 42.26904376012966,
      "grad_norm": 0.9137068390846252,
      "learning_rate": 4.580467077521894e-05,
      "loss": 3.1527,
      "step": 130400
    },
    {
      "epoch": 42.30145867098865,
      "grad_norm": 1.0301969051361084,
      "learning_rate": 4.58014271813169e-05,
      "loss": 3.1487,
      "step": 130500
    },
    {
      "epoch": 42.33387358184765,
      "grad_norm": 0.9426267147064209,
      "learning_rate": 4.5798183587414853e-05,
      "loss": 3.1342,
      "step": 130600
    },
    {
      "epoch": 42.366288492706644,
      "grad_norm": 1.0616579055786133,
      "learning_rate": 4.579493999351281e-05,
      "loss": 3.1371,
      "step": 130700
    },
    {
      "epoch": 42.39870340356564,
      "grad_norm": 0.8589192628860474,
      "learning_rate": 4.579169639961077e-05,
      "loss": 3.1395,
      "step": 130800
    },
    {
      "epoch": 42.43111831442464,
      "grad_norm": 0.9385725259780884,
      "learning_rate": 4.578845280570872e-05,
      "loss": 3.1442,
      "step": 130900
    },
    {
      "epoch": 42.46353322528363,
      "grad_norm": 0.9380668997764587,
      "learning_rate": 4.578520921180668e-05,
      "loss": 3.1502,
      "step": 131000
    },
    {
      "epoch": 42.49594813614262,
      "grad_norm": 0.8761782646179199,
      "learning_rate": 4.578196561790464e-05,
      "loss": 3.142,
      "step": 131100
    },
    {
      "epoch": 42.52836304700162,
      "grad_norm": 0.898663341999054,
      "learning_rate": 4.57787220240026e-05,
      "loss": 3.1564,
      "step": 131200
    },
    {
      "epoch": 42.560777957860616,
      "grad_norm": 0.9238415360450745,
      "learning_rate": 4.577547843010056e-05,
      "loss": 3.1291,
      "step": 131300
    },
    {
      "epoch": 42.59319286871961,
      "grad_norm": 1.1190303564071655,
      "learning_rate": 4.577223483619851e-05,
      "loss": 3.1483,
      "step": 131400
    },
    {
      "epoch": 42.62560777957861,
      "grad_norm": 0.8780283331871033,
      "learning_rate": 4.576899124229647e-05,
      "loss": 3.1581,
      "step": 131500
    },
    {
      "epoch": 42.658022690437605,
      "grad_norm": 1.037509560585022,
      "learning_rate": 4.576574764839443e-05,
      "loss": 3.1424,
      "step": 131600
    },
    {
      "epoch": 42.690437601296594,
      "grad_norm": 0.8719333410263062,
      "learning_rate": 4.576250405449238e-05,
      "loss": 3.1294,
      "step": 131700
    },
    {
      "epoch": 42.72285251215559,
      "grad_norm": 1.2072104215621948,
      "learning_rate": 4.575926046059034e-05,
      "loss": 3.1269,
      "step": 131800
    },
    {
      "epoch": 42.75526742301459,
      "grad_norm": 0.8842899203300476,
      "learning_rate": 4.5756049302627316e-05,
      "loss": 3.1354,
      "step": 131900
    },
    {
      "epoch": 42.78768233387358,
      "grad_norm": 1.0913312435150146,
      "learning_rate": 4.5752805708725275e-05,
      "loss": 3.1493,
      "step": 132000
    },
    {
      "epoch": 42.82009724473258,
      "grad_norm": 0.8736574053764343,
      "learning_rate": 4.5749562114823227e-05,
      "loss": 3.1568,
      "step": 132100
    },
    {
      "epoch": 42.85251215559157,
      "grad_norm": 0.9154927730560303,
      "learning_rate": 4.5746318520921185e-05,
      "loss": 3.1645,
      "step": 132200
    },
    {
      "epoch": 42.884927066450565,
      "grad_norm": 1.0082712173461914,
      "learning_rate": 4.5743074927019144e-05,
      "loss": 3.1535,
      "step": 132300
    },
    {
      "epoch": 42.91734197730956,
      "grad_norm": 1.2893333435058594,
      "learning_rate": 4.5739831333117096e-05,
      "loss": 3.148,
      "step": 132400
    },
    {
      "epoch": 42.94975688816856,
      "grad_norm": 0.8718233108520508,
      "learning_rate": 4.5736587739215055e-05,
      "loss": 3.1404,
      "step": 132500
    },
    {
      "epoch": 42.982171799027554,
      "grad_norm": 1.044385313987732,
      "learning_rate": 4.573334414531301e-05,
      "loss": 3.173,
      "step": 132600
    },
    {
      "epoch": 43.0,
      "eval_bleu": 0.9481399823638387,
      "eval_loss": 3.7404379844665527,
      "eval_runtime": 4.7337,
      "eval_samples_per_second": 103.935,
      "eval_steps_per_second": 1.69,
      "step": 132655
    },
    {
      "epoch": 43.01458670988655,
      "grad_norm": 0.8583739995956421,
      "learning_rate": 4.5730100551410965e-05,
      "loss": 3.1475,
      "step": 132700
    },
    {
      "epoch": 43.04700162074554,
      "grad_norm": 0.9419816732406616,
      "learning_rate": 4.5726856957508924e-05,
      "loss": 3.1569,
      "step": 132800
    },
    {
      "epoch": 43.07941653160454,
      "grad_norm": 0.8981895446777344,
      "learning_rate": 4.5723613363606876e-05,
      "loss": 3.1424,
      "step": 132900
    },
    {
      "epoch": 43.11183144246353,
      "grad_norm": 0.8793110847473145,
      "learning_rate": 4.5720369769704835e-05,
      "loss": 3.1536,
      "step": 133000
    },
    {
      "epoch": 43.14424635332253,
      "grad_norm": 0.9396337270736694,
      "learning_rate": 4.5717126175802794e-05,
      "loss": 3.1611,
      "step": 133100
    },
    {
      "epoch": 43.176661264181526,
      "grad_norm": 1.0077484846115112,
      "learning_rate": 4.5713882581900746e-05,
      "loss": 3.1358,
      "step": 133200
    },
    {
      "epoch": 43.20907617504052,
      "grad_norm": 1.1445286273956299,
      "learning_rate": 4.5710638987998704e-05,
      "loss": 3.1516,
      "step": 133300
    },
    {
      "epoch": 43.24149108589951,
      "grad_norm": 1.014430284500122,
      "learning_rate": 4.570739539409666e-05,
      "loss": 3.1437,
      "step": 133400
    },
    {
      "epoch": 43.27390599675851,
      "grad_norm": 0.9071393609046936,
      "learning_rate": 4.5704151800194615e-05,
      "loss": 3.1173,
      "step": 133500
    },
    {
      "epoch": 43.306320907617504,
      "grad_norm": 0.9009402990341187,
      "learning_rate": 4.5700908206292574e-05,
      "loss": 3.1287,
      "step": 133600
    },
    {
      "epoch": 43.3387358184765,
      "grad_norm": 1.1579945087432861,
      "learning_rate": 4.5697664612390526e-05,
      "loss": 3.1463,
      "step": 133700
    },
    {
      "epoch": 43.3711507293355,
      "grad_norm": 0.9555280208587646,
      "learning_rate": 4.5694421018488484e-05,
      "loss": 3.1479,
      "step": 133800
    },
    {
      "epoch": 43.403565640194486,
      "grad_norm": 0.8718819618225098,
      "learning_rate": 4.569117742458644e-05,
      "loss": 3.1525,
      "step": 133900
    },
    {
      "epoch": 43.43598055105348,
      "grad_norm": 0.9767548441886902,
      "learning_rate": 4.5687933830684395e-05,
      "loss": 3.1435,
      "step": 134000
    },
    {
      "epoch": 43.46839546191248,
      "grad_norm": 0.9193469882011414,
      "learning_rate": 4.5684690236782354e-05,
      "loss": 3.1422,
      "step": 134100
    },
    {
      "epoch": 43.500810372771475,
      "grad_norm": 0.9274998903274536,
      "learning_rate": 4.568144664288031e-05,
      "loss": 3.1326,
      "step": 134200
    },
    {
      "epoch": 43.53322528363047,
      "grad_norm": 0.920428454875946,
      "learning_rate": 4.567820304897827e-05,
      "loss": 3.1352,
      "step": 134300
    },
    {
      "epoch": 43.56564019448947,
      "grad_norm": 0.8940868377685547,
      "learning_rate": 4.567495945507623e-05,
      "loss": 3.1376,
      "step": 134400
    },
    {
      "epoch": 43.59805510534846,
      "grad_norm": 0.9079470038414001,
      "learning_rate": 4.567171586117419e-05,
      "loss": 3.1282,
      "step": 134500
    },
    {
      "epoch": 43.630470016207454,
      "grad_norm": 0.8140437602996826,
      "learning_rate": 4.566847226727214e-05,
      "loss": 3.1444,
      "step": 134600
    },
    {
      "epoch": 43.66288492706645,
      "grad_norm": 0.9193212985992432,
      "learning_rate": 4.56652286733701e-05,
      "loss": 3.1347,
      "step": 134700
    },
    {
      "epoch": 43.69529983792545,
      "grad_norm": 0.7931756377220154,
      "learning_rate": 4.566198507946805e-05,
      "loss": 3.142,
      "step": 134800
    },
    {
      "epoch": 43.72771474878444,
      "grad_norm": 0.9885871410369873,
      "learning_rate": 4.565874148556601e-05,
      "loss": 3.1544,
      "step": 134900
    },
    {
      "epoch": 43.76012965964344,
      "grad_norm": 0.9460179805755615,
      "learning_rate": 4.565549789166397e-05,
      "loss": 3.1067,
      "step": 135000
    },
    {
      "epoch": 43.79254457050243,
      "grad_norm": 0.855376660823822,
      "learning_rate": 4.565225429776192e-05,
      "loss": 3.1422,
      "step": 135100
    },
    {
      "epoch": 43.824959481361425,
      "grad_norm": 1.0726900100708008,
      "learning_rate": 4.564901070385988e-05,
      "loss": 3.1323,
      "step": 135200
    },
    {
      "epoch": 43.85737439222042,
      "grad_norm": 0.9215888977050781,
      "learning_rate": 4.564576710995784e-05,
      "loss": 3.1299,
      "step": 135300
    },
    {
      "epoch": 43.88978930307942,
      "grad_norm": 0.9134015440940857,
      "learning_rate": 4.564252351605579e-05,
      "loss": 3.1314,
      "step": 135400
    },
    {
      "epoch": 43.922204213938414,
      "grad_norm": 0.8680312037467957,
      "learning_rate": 4.563927992215375e-05,
      "loss": 3.1362,
      "step": 135500
    },
    {
      "epoch": 43.954619124797404,
      "grad_norm": 1.0454591512680054,
      "learning_rate": 4.563603632825171e-05,
      "loss": 3.1362,
      "step": 135600
    },
    {
      "epoch": 43.9870340356564,
      "grad_norm": 0.9308097958564758,
      "learning_rate": 4.563279273434966e-05,
      "loss": 3.1554,
      "step": 135700
    },
    {
      "epoch": 44.0,
      "eval_bleu": 0.927797414892225,
      "eval_loss": 3.741178035736084,
      "eval_runtime": 4.3717,
      "eval_samples_per_second": 112.541,
      "eval_steps_per_second": 1.83,
      "step": 135740
    },
    {
      "epoch": 44.019448946515396,
      "grad_norm": 1.161232590675354,
      "learning_rate": 4.562954914044762e-05,
      "loss": 3.1355,
      "step": 135800
    },
    {
      "epoch": 44.05186385737439,
      "grad_norm": 0.9266771078109741,
      "learning_rate": 4.5626337982484596e-05,
      "loss": 3.1275,
      "step": 135900
    },
    {
      "epoch": 44.08427876823339,
      "grad_norm": 0.9338603615760803,
      "learning_rate": 4.562309438858255e-05,
      "loss": 3.1327,
      "step": 136000
    },
    {
      "epoch": 44.116693679092386,
      "grad_norm": 1.0332984924316406,
      "learning_rate": 4.561985079468051e-05,
      "loss": 3.1549,
      "step": 136100
    },
    {
      "epoch": 44.149108589951375,
      "grad_norm": 0.8439421653747559,
      "learning_rate": 4.5616607200778466e-05,
      "loss": 3.1272,
      "step": 136200
    },
    {
      "epoch": 44.18152350081037,
      "grad_norm": 0.8472484946250916,
      "learning_rate": 4.561336360687642e-05,
      "loss": 3.1436,
      "step": 136300
    },
    {
      "epoch": 44.21393841166937,
      "grad_norm": 0.9709866046905518,
      "learning_rate": 4.561012001297438e-05,
      "loss": 3.1243,
      "step": 136400
    },
    {
      "epoch": 44.246353322528364,
      "grad_norm": 1.0164843797683716,
      "learning_rate": 4.5606876419072335e-05,
      "loss": 3.1325,
      "step": 136500
    },
    {
      "epoch": 44.27876823338736,
      "grad_norm": 0.842199444770813,
      "learning_rate": 4.560363282517029e-05,
      "loss": 3.1414,
      "step": 136600
    },
    {
      "epoch": 44.31118314424636,
      "grad_norm": 0.8822439312934875,
      "learning_rate": 4.5600389231268246e-05,
      "loss": 3.1361,
      "step": 136700
    },
    {
      "epoch": 44.343598055105346,
      "grad_norm": 0.8919244408607483,
      "learning_rate": 4.5597145637366205e-05,
      "loss": 3.1225,
      "step": 136800
    },
    {
      "epoch": 44.37601296596434,
      "grad_norm": 0.9336486458778381,
      "learning_rate": 4.559390204346416e-05,
      "loss": 3.1304,
      "step": 136900
    },
    {
      "epoch": 44.40842787682334,
      "grad_norm": 0.9652215838432312,
      "learning_rate": 4.5590690885501135e-05,
      "loss": 3.1472,
      "step": 137000
    },
    {
      "epoch": 44.440842787682335,
      "grad_norm": 0.9500108361244202,
      "learning_rate": 4.558744729159909e-05,
      "loss": 3.1301,
      "step": 137100
    },
    {
      "epoch": 44.47325769854133,
      "grad_norm": 1.0220675468444824,
      "learning_rate": 4.558420369769705e-05,
      "loss": 3.144,
      "step": 137200
    },
    {
      "epoch": 44.50567260940032,
      "grad_norm": 0.9276295900344849,
      "learning_rate": 4.5580960103795004e-05,
      "loss": 3.143,
      "step": 137300
    },
    {
      "epoch": 44.53808752025932,
      "grad_norm": 0.9407332539558411,
      "learning_rate": 4.557774894583198e-05,
      "loss": 3.1414,
      "step": 137400
    },
    {
      "epoch": 44.570502431118314,
      "grad_norm": 1.0043888092041016,
      "learning_rate": 4.557450535192994e-05,
      "loss": 3.1438,
      "step": 137500
    },
    {
      "epoch": 44.60291734197731,
      "grad_norm": 0.8339277505874634,
      "learning_rate": 4.557126175802789e-05,
      "loss": 3.131,
      "step": 137600
    },
    {
      "epoch": 44.63533225283631,
      "grad_norm": 1.0543043613433838,
      "learning_rate": 4.556801816412585e-05,
      "loss": 3.1312,
      "step": 137700
    },
    {
      "epoch": 44.6677471636953,
      "grad_norm": 1.0692118406295776,
      "learning_rate": 4.556477457022381e-05,
      "loss": 3.1357,
      "step": 137800
    },
    {
      "epoch": 44.70016207455429,
      "grad_norm": 1.0194711685180664,
      "learning_rate": 4.556153097632176e-05,
      "loss": 3.1408,
      "step": 137900
    },
    {
      "epoch": 44.73257698541329,
      "grad_norm": 0.9197695851325989,
      "learning_rate": 4.555828738241972e-05,
      "loss": 3.1357,
      "step": 138000
    },
    {
      "epoch": 44.764991896272285,
      "grad_norm": 0.9247215390205383,
      "learning_rate": 4.555504378851768e-05,
      "loss": 3.1384,
      "step": 138100
    },
    {
      "epoch": 44.79740680713128,
      "grad_norm": 0.9304542541503906,
      "learning_rate": 4.555180019461563e-05,
      "loss": 3.1414,
      "step": 138200
    },
    {
      "epoch": 44.82982171799028,
      "grad_norm": 1.1118457317352295,
      "learning_rate": 4.554855660071359e-05,
      "loss": 3.1316,
      "step": 138300
    },
    {
      "epoch": 44.862236628849274,
      "grad_norm": 0.9765797853469849,
      "learning_rate": 4.554531300681155e-05,
      "loss": 3.1551,
      "step": 138400
    },
    {
      "epoch": 44.89465153970826,
      "grad_norm": 0.8029811382293701,
      "learning_rate": 4.554206941290951e-05,
      "loss": 3.1246,
      "step": 138500
    },
    {
      "epoch": 44.92706645056726,
      "grad_norm": 0.9488136172294617,
      "learning_rate": 4.5538825819007466e-05,
      "loss": 3.1228,
      "step": 138600
    },
    {
      "epoch": 44.959481361426256,
      "grad_norm": 1.005617618560791,
      "learning_rate": 4.553558222510542e-05,
      "loss": 3.13,
      "step": 138700
    },
    {
      "epoch": 44.99189627228525,
      "grad_norm": 0.9994165301322937,
      "learning_rate": 4.5532371067142396e-05,
      "loss": 3.1187,
      "step": 138800
    },
    {
      "epoch": 45.0,
      "eval_bleu": 0.9941232593031739,
      "eval_loss": 3.7480878829956055,
      "eval_runtime": 4.3048,
      "eval_samples_per_second": 114.292,
      "eval_steps_per_second": 1.858,
      "step": 138825
    },
    {
      "epoch": 45.02431118314425,
      "grad_norm": 0.9487804174423218,
      "learning_rate": 4.5529127473240355e-05,
      "loss": 3.1024,
      "step": 138900
    },
    {
      "epoch": 45.05672609400324,
      "grad_norm": 0.8566412925720215,
      "learning_rate": 4.552588387933831e-05,
      "loss": 3.1266,
      "step": 139000
    },
    {
      "epoch": 45.089141004862235,
      "grad_norm": 1.1083990335464478,
      "learning_rate": 4.5522640285436266e-05,
      "loss": 3.1213,
      "step": 139100
    },
    {
      "epoch": 45.12155591572123,
      "grad_norm": 0.9115878939628601,
      "learning_rate": 4.5519396691534224e-05,
      "loss": 3.1362,
      "step": 139200
    },
    {
      "epoch": 45.15397082658023,
      "grad_norm": 0.894212543964386,
      "learning_rate": 4.551615309763218e-05,
      "loss": 3.1232,
      "step": 139300
    },
    {
      "epoch": 45.186385737439224,
      "grad_norm": 0.9627618193626404,
      "learning_rate": 4.5512909503730135e-05,
      "loss": 3.1294,
      "step": 139400
    },
    {
      "epoch": 45.21880064829822,
      "grad_norm": 0.9603878855705261,
      "learning_rate": 4.5509665909828094e-05,
      "loss": 3.132,
      "step": 139500
    },
    {
      "epoch": 45.25121555915721,
      "grad_norm": 1.0172016620635986,
      "learning_rate": 4.550642231592605e-05,
      "loss": 3.1335,
      "step": 139600
    },
    {
      "epoch": 45.283630470016206,
      "grad_norm": 1.0706373453140259,
      "learning_rate": 4.5503178722024005e-05,
      "loss": 3.1221,
      "step": 139700
    },
    {
      "epoch": 45.3160453808752,
      "grad_norm": 0.9187244176864624,
      "learning_rate": 4.549993512812196e-05,
      "loss": 3.1189,
      "step": 139800
    },
    {
      "epoch": 45.3484602917342,
      "grad_norm": 1.0507376194000244,
      "learning_rate": 4.5496691534219915e-05,
      "loss": 3.1249,
      "step": 139900
    },
    {
      "epoch": 45.380875202593195,
      "grad_norm": 0.9996633529663086,
      "learning_rate": 4.5493447940317874e-05,
      "loss": 3.1377,
      "step": 140000
    },
    {
      "epoch": 45.41329011345219,
      "grad_norm": 1.0380547046661377,
      "learning_rate": 4.549020434641583e-05,
      "loss": 3.1178,
      "step": 140100
    },
    {
      "epoch": 45.44570502431118,
      "grad_norm": 0.920059084892273,
      "learning_rate": 4.5486960752513785e-05,
      "loss": 3.1304,
      "step": 140200
    },
    {
      "epoch": 45.47811993517018,
      "grad_norm": 0.9161081910133362,
      "learning_rate": 4.5483717158611743e-05,
      "loss": 3.1234,
      "step": 140300
    },
    {
      "epoch": 45.510534846029174,
      "grad_norm": 0.8582990169525146,
      "learning_rate": 4.54804735647097e-05,
      "loss": 3.1475,
      "step": 140400
    },
    {
      "epoch": 45.54294975688817,
      "grad_norm": 0.8895837068557739,
      "learning_rate": 4.5477229970807654e-05,
      "loss": 3.1277,
      "step": 140500
    },
    {
      "epoch": 45.575364667747166,
      "grad_norm": 0.9232614636421204,
      "learning_rate": 4.547398637690561e-05,
      "loss": 3.12,
      "step": 140600
    },
    {
      "epoch": 45.607779578606156,
      "grad_norm": 0.9658222198486328,
      "learning_rate": 4.547074278300357e-05,
      "loss": 3.1174,
      "step": 140700
    },
    {
      "epoch": 45.64019448946515,
      "grad_norm": 1.0038875341415405,
      "learning_rate": 4.5467499189101524e-05,
      "loss": 3.1242,
      "step": 140800
    },
    {
      "epoch": 45.67260940032415,
      "grad_norm": 0.7769636511802673,
      "learning_rate": 4.546425559519948e-05,
      "loss": 3.1499,
      "step": 140900
    },
    {
      "epoch": 45.705024311183145,
      "grad_norm": 1.023628830909729,
      "learning_rate": 4.5461012001297434e-05,
      "loss": 3.1594,
      "step": 141000
    },
    {
      "epoch": 45.73743922204214,
      "grad_norm": 0.9918527007102966,
      "learning_rate": 4.545776840739539e-05,
      "loss": 3.1412,
      "step": 141100
    },
    {
      "epoch": 45.76985413290114,
      "grad_norm": 1.061635971069336,
      "learning_rate": 4.545452481349335e-05,
      "loss": 3.1264,
      "step": 141200
    },
    {
      "epoch": 45.80226904376013,
      "grad_norm": 1.0102208852767944,
      "learning_rate": 4.545128121959131e-05,
      "loss": 3.1212,
      "step": 141300
    },
    {
      "epoch": 45.83468395461912,
      "grad_norm": 0.9276434779167175,
      "learning_rate": 4.544803762568926e-05,
      "loss": 3.1197,
      "step": 141400
    },
    {
      "epoch": 45.86709886547812,
      "grad_norm": 1.130599856376648,
      "learning_rate": 4.544479403178722e-05,
      "loss": 3.1444,
      "step": 141500
    },
    {
      "epoch": 45.899513776337116,
      "grad_norm": 0.907529890537262,
      "learning_rate": 4.544155043788518e-05,
      "loss": 3.1364,
      "step": 141600
    },
    {
      "epoch": 45.93192868719611,
      "grad_norm": 0.85237717628479,
      "learning_rate": 4.543830684398314e-05,
      "loss": 3.1277,
      "step": 141700
    },
    {
      "epoch": 45.96434359805511,
      "grad_norm": 0.9248133897781372,
      "learning_rate": 4.543509568602011e-05,
      "loss": 3.1483,
      "step": 141800
    },
    {
      "epoch": 45.9967585089141,
      "grad_norm": 0.9887186288833618,
      "learning_rate": 4.543185209211807e-05,
      "loss": 3.1149,
      "step": 141900
    },
    {
      "epoch": 46.0,
      "eval_bleu": 0.9432120466224351,
      "eval_loss": 3.7476415634155273,
      "eval_runtime": 3.9767,
      "eval_samples_per_second": 123.72,
      "eval_steps_per_second": 2.012,
      "step": 141910
    },
    {
      "epoch": 46.029173419773095,
      "grad_norm": 1.017760157585144,
      "learning_rate": 4.542860849821603e-05,
      "loss": 3.128,
      "step": 142000
    },
    {
      "epoch": 46.06158833063209,
      "grad_norm": 0.9456163644790649,
      "learning_rate": 4.5425364904313986e-05,
      "loss": 3.1086,
      "step": 142100
    },
    {
      "epoch": 46.09400324149109,
      "grad_norm": 1.1550788879394531,
      "learning_rate": 4.542212131041194e-05,
      "loss": 3.1311,
      "step": 142200
    },
    {
      "epoch": 46.126418152350084,
      "grad_norm": 1.026902198791504,
      "learning_rate": 4.54188777165099e-05,
      "loss": 3.1282,
      "step": 142300
    },
    {
      "epoch": 46.15883306320907,
      "grad_norm": 1.0089647769927979,
      "learning_rate": 4.5415634122607855e-05,
      "loss": 3.1263,
      "step": 142400
    },
    {
      "epoch": 46.19124797406807,
      "grad_norm": 0.8608373999595642,
      "learning_rate": 4.541239052870581e-05,
      "loss": 3.1194,
      "step": 142500
    },
    {
      "epoch": 46.223662884927066,
      "grad_norm": 0.8565713167190552,
      "learning_rate": 4.5409146934803766e-05,
      "loss": 3.0989,
      "step": 142600
    },
    {
      "epoch": 46.25607779578606,
      "grad_norm": 1.0183147192001343,
      "learning_rate": 4.5405903340901725e-05,
      "loss": 3.1237,
      "step": 142700
    },
    {
      "epoch": 46.28849270664506,
      "grad_norm": 0.8250812292098999,
      "learning_rate": 4.54026921829387e-05,
      "loss": 3.1225,
      "step": 142800
    },
    {
      "epoch": 46.320907617504055,
      "grad_norm": 0.8660604953765869,
      "learning_rate": 4.5399448589036655e-05,
      "loss": 3.1207,
      "step": 142900
    },
    {
      "epoch": 46.353322528363044,
      "grad_norm": 0.8320249319076538,
      "learning_rate": 4.5396204995134613e-05,
      "loss": 3.1262,
      "step": 143000
    },
    {
      "epoch": 46.38573743922204,
      "grad_norm": 0.9543217420578003,
      "learning_rate": 4.539296140123257e-05,
      "loss": 3.129,
      "step": 143100
    },
    {
      "epoch": 46.41815235008104,
      "grad_norm": 0.8417069911956787,
      "learning_rate": 4.5389717807330524e-05,
      "loss": 3.1332,
      "step": 143200
    },
    {
      "epoch": 46.45056726094003,
      "grad_norm": 0.9806358814239502,
      "learning_rate": 4.538647421342848e-05,
      "loss": 3.1239,
      "step": 143300
    },
    {
      "epoch": 46.48298217179903,
      "grad_norm": 1.0434421300888062,
      "learning_rate": 4.538323061952644e-05,
      "loss": 3.143,
      "step": 143400
    },
    {
      "epoch": 46.51539708265802,
      "grad_norm": 0.9713369011878967,
      "learning_rate": 4.5379987025624394e-05,
      "loss": 3.1246,
      "step": 143500
    },
    {
      "epoch": 46.547811993517016,
      "grad_norm": 1.0660046339035034,
      "learning_rate": 4.537674343172235e-05,
      "loss": 3.1373,
      "step": 143600
    },
    {
      "epoch": 46.58022690437601,
      "grad_norm": 1.0193437337875366,
      "learning_rate": 4.5373499837820304e-05,
      "loss": 3.1263,
      "step": 143700
    },
    {
      "epoch": 46.61264181523501,
      "grad_norm": 1.0791493654251099,
      "learning_rate": 4.537025624391826e-05,
      "loss": 3.1219,
      "step": 143800
    },
    {
      "epoch": 46.645056726094005,
      "grad_norm": 0.9801159501075745,
      "learning_rate": 4.536701265001622e-05,
      "loss": 3.1393,
      "step": 143900
    },
    {
      "epoch": 46.677471636953,
      "grad_norm": 0.9358268976211548,
      "learning_rate": 4.5363769056114174e-05,
      "loss": 3.1136,
      "step": 144000
    },
    {
      "epoch": 46.70988654781199,
      "grad_norm": 0.9196190237998962,
      "learning_rate": 4.536052546221213e-05,
      "loss": 3.1201,
      "step": 144100
    },
    {
      "epoch": 46.74230145867099,
      "grad_norm": 0.8364621996879578,
      "learning_rate": 4.535728186831009e-05,
      "loss": 3.1335,
      "step": 144200
    },
    {
      "epoch": 46.77471636952998,
      "grad_norm": 0.949940025806427,
      "learning_rate": 4.535403827440804e-05,
      "loss": 3.116,
      "step": 144300
    },
    {
      "epoch": 46.80713128038898,
      "grad_norm": 1.0605628490447998,
      "learning_rate": 4.5350794680506e-05,
      "loss": 3.1358,
      "step": 144400
    },
    {
      "epoch": 46.839546191247976,
      "grad_norm": 0.9642239212989807,
      "learning_rate": 4.534755108660396e-05,
      "loss": 3.1341,
      "step": 144500
    },
    {
      "epoch": 46.87196110210697,
      "grad_norm": 1.0234025716781616,
      "learning_rate": 4.534430749270191e-05,
      "loss": 3.1186,
      "step": 144600
    },
    {
      "epoch": 46.90437601296596,
      "grad_norm": 0.8895108699798584,
      "learning_rate": 4.534106389879987e-05,
      "loss": 3.1261,
      "step": 144700
    },
    {
      "epoch": 46.93679092382496,
      "grad_norm": 0.8197807669639587,
      "learning_rate": 4.533782030489782e-05,
      "loss": 3.1312,
      "step": 144800
    },
    {
      "epoch": 46.969205834683954,
      "grad_norm": 0.9053052663803101,
      "learning_rate": 4.533457671099578e-05,
      "loss": 3.1229,
      "step": 144900
    },
    {
      "epoch": 47.0,
      "eval_bleu": 1.165935607253103,
      "eval_loss": 3.7525534629821777,
      "eval_runtime": 4.4276,
      "eval_samples_per_second": 111.12,
      "eval_steps_per_second": 1.807,
      "step": 144995
    },
    {
      "epoch": 47.00162074554295,
      "grad_norm": 0.8532252907752991,
      "learning_rate": 4.533133311709374e-05,
      "loss": 3.1149,
      "step": 145000
    },
    {
      "epoch": 47.03403565640195,
      "grad_norm": 0.8322739005088806,
      "learning_rate": 4.53280895231917e-05,
      "loss": 3.1116,
      "step": 145100
    },
    {
      "epoch": 47.06645056726094,
      "grad_norm": 1.0524837970733643,
      "learning_rate": 4.532484592928966e-05,
      "loss": 3.1076,
      "step": 145200
    },
    {
      "epoch": 47.09886547811993,
      "grad_norm": 0.7654354572296143,
      "learning_rate": 4.532160233538762e-05,
      "loss": 3.0974,
      "step": 145300
    },
    {
      "epoch": 47.13128038897893,
      "grad_norm": 0.9560398459434509,
      "learning_rate": 4.531835874148557e-05,
      "loss": 3.1218,
      "step": 145400
    },
    {
      "epoch": 47.163695299837926,
      "grad_norm": 0.9176743626594543,
      "learning_rate": 4.531511514758353e-05,
      "loss": 3.1092,
      "step": 145500
    },
    {
      "epoch": 47.19611021069692,
      "grad_norm": 0.7933011651039124,
      "learning_rate": 4.531187155368148e-05,
      "loss": 3.1252,
      "step": 145600
    },
    {
      "epoch": 47.22852512155592,
      "grad_norm": 0.9406899213790894,
      "learning_rate": 4.530862795977944e-05,
      "loss": 3.1324,
      "step": 145700
    },
    {
      "epoch": 47.26094003241491,
      "grad_norm": 0.8715283274650574,
      "learning_rate": 4.53053843658774e-05,
      "loss": 3.1335,
      "step": 145800
    },
    {
      "epoch": 47.293354943273904,
      "grad_norm": 1.0689418315887451,
      "learning_rate": 4.530214077197535e-05,
      "loss": 3.1152,
      "step": 145900
    },
    {
      "epoch": 47.3257698541329,
      "grad_norm": 0.8630914092063904,
      "learning_rate": 4.529889717807331e-05,
      "loss": 3.1394,
      "step": 146000
    },
    {
      "epoch": 47.3581847649919,
      "grad_norm": 0.7609529495239258,
      "learning_rate": 4.529565358417127e-05,
      "loss": 3.127,
      "step": 146100
    },
    {
      "epoch": 47.39059967585089,
      "grad_norm": 1.000158667564392,
      "learning_rate": 4.529240999026922e-05,
      "loss": 3.1193,
      "step": 146200
    },
    {
      "epoch": 47.42301458670989,
      "grad_norm": 0.8744961619377136,
      "learning_rate": 4.528916639636718e-05,
      "loss": 3.0952,
      "step": 146300
    },
    {
      "epoch": 47.45542949756888,
      "grad_norm": 0.8220177888870239,
      "learning_rate": 4.5285922802465136e-05,
      "loss": 3.1318,
      "step": 146400
    },
    {
      "epoch": 47.487844408427875,
      "grad_norm": 0.9729081988334656,
      "learning_rate": 4.528267920856309e-05,
      "loss": 3.1127,
      "step": 146500
    },
    {
      "epoch": 47.52025931928687,
      "grad_norm": 0.8328625559806824,
      "learning_rate": 4.527943561466105e-05,
      "loss": 3.1158,
      "step": 146600
    },
    {
      "epoch": 47.55267423014587,
      "grad_norm": 0.9399292469024658,
      "learning_rate": 4.5276192020759e-05,
      "loss": 3.1369,
      "step": 146700
    },
    {
      "epoch": 47.585089141004865,
      "grad_norm": 0.8707639575004578,
      "learning_rate": 4.527294842685696e-05,
      "loss": 3.1197,
      "step": 146800
    },
    {
      "epoch": 47.617504051863854,
      "grad_norm": 0.9639185667037964,
      "learning_rate": 4.5269704832954916e-05,
      "loss": 3.1119,
      "step": 146900
    },
    {
      "epoch": 47.64991896272285,
      "grad_norm": 0.9133157134056091,
      "learning_rate": 4.526646123905287e-05,
      "loss": 3.1232,
      "step": 147000
    },
    {
      "epoch": 47.68233387358185,
      "grad_norm": 0.9493808150291443,
      "learning_rate": 4.526321764515083e-05,
      "loss": 3.122,
      "step": 147100
    },
    {
      "epoch": 47.71474878444084,
      "grad_norm": 0.93483966588974,
      "learning_rate": 4.5259974051248786e-05,
      "loss": 3.112,
      "step": 147200
    },
    {
      "epoch": 47.74716369529984,
      "grad_norm": 0.9953950643539429,
      "learning_rate": 4.525673045734674e-05,
      "loss": 3.1165,
      "step": 147300
    },
    {
      "epoch": 47.779578606158836,
      "grad_norm": 0.8445706963539124,
      "learning_rate": 4.5253486863444696e-05,
      "loss": 3.1358,
      "step": 147400
    },
    {
      "epoch": 47.811993517017825,
      "grad_norm": 0.9090886116027832,
      "learning_rate": 4.5250243269542655e-05,
      "loss": 3.1264,
      "step": 147500
    },
    {
      "epoch": 47.84440842787682,
      "grad_norm": 0.9368017911911011,
      "learning_rate": 4.5246999675640614e-05,
      "loss": 3.1401,
      "step": 147600
    },
    {
      "epoch": 47.87682333873582,
      "grad_norm": 1.0287601947784424,
      "learning_rate": 4.524375608173857e-05,
      "loss": 3.1109,
      "step": 147700
    },
    {
      "epoch": 47.909238249594814,
      "grad_norm": 0.8257893919944763,
      "learning_rate": 4.5240512487836525e-05,
      "loss": 3.1238,
      "step": 147800
    },
    {
      "epoch": 47.94165316045381,
      "grad_norm": 1.0042285919189453,
      "learning_rate": 4.5237268893934483e-05,
      "loss": 3.1255,
      "step": 147900
    },
    {
      "epoch": 47.97406807131281,
      "grad_norm": 0.8824120759963989,
      "learning_rate": 4.523402530003244e-05,
      "loss": 3.1168,
      "step": 148000
    },
    {
      "epoch": 48.0,
      "eval_bleu": 1.0739338508100271,
      "eval_loss": 3.750614881515503,
      "eval_runtime": 4.1137,
      "eval_samples_per_second": 119.601,
      "eval_steps_per_second": 1.945,
      "step": 148080
    },
    {
      "epoch": 48.006482982171796,
      "grad_norm": 0.8159235715866089,
      "learning_rate": 4.5230781706130394e-05,
      "loss": 3.1394,
      "step": 148100
    },
    {
      "epoch": 48.03889789303079,
      "grad_norm": 0.8922426104545593,
      "learning_rate": 4.522753811222835e-05,
      "loss": 3.0961,
      "step": 148200
    },
    {
      "epoch": 48.07131280388979,
      "grad_norm": 0.9675890207290649,
      "learning_rate": 4.522429451832631e-05,
      "loss": 3.1103,
      "step": 148300
    },
    {
      "epoch": 48.103727714748786,
      "grad_norm": 0.8389149904251099,
      "learning_rate": 4.5221050924424264e-05,
      "loss": 3.1008,
      "step": 148400
    },
    {
      "epoch": 48.13614262560778,
      "grad_norm": 0.8369548320770264,
      "learning_rate": 4.521780733052222e-05,
      "loss": 3.1158,
      "step": 148500
    },
    {
      "epoch": 48.16855753646677,
      "grad_norm": 0.8855810761451721,
      "learning_rate": 4.5214563736620174e-05,
      "loss": 3.1132,
      "step": 148600
    },
    {
      "epoch": 48.20097244732577,
      "grad_norm": 0.9860530495643616,
      "learning_rate": 4.521132014271813e-05,
      "loss": 3.1184,
      "step": 148700
    },
    {
      "epoch": 48.233387358184764,
      "grad_norm": 0.9256541132926941,
      "learning_rate": 4.520807654881609e-05,
      "loss": 3.1163,
      "step": 148800
    },
    {
      "epoch": 48.26580226904376,
      "grad_norm": 1.0195196866989136,
      "learning_rate": 4.5204832954914044e-05,
      "loss": 3.1047,
      "step": 148900
    },
    {
      "epoch": 48.29821717990276,
      "grad_norm": 0.8926693201065063,
      "learning_rate": 4.5201589361012e-05,
      "loss": 3.1243,
      "step": 149000
    },
    {
      "epoch": 48.33063209076175,
      "grad_norm": 0.8778586387634277,
      "learning_rate": 4.519834576710996e-05,
      "loss": 3.089,
      "step": 149100
    },
    {
      "epoch": 48.36304700162074,
      "grad_norm": 0.794789731502533,
      "learning_rate": 4.519510217320791e-05,
      "loss": 3.1159,
      "step": 149200
    },
    {
      "epoch": 48.39546191247974,
      "grad_norm": 0.9336301684379578,
      "learning_rate": 4.519185857930587e-05,
      "loss": 3.118,
      "step": 149300
    },
    {
      "epoch": 48.427876823338735,
      "grad_norm": 0.8335862159729004,
      "learning_rate": 4.518861498540383e-05,
      "loss": 3.1138,
      "step": 149400
    },
    {
      "epoch": 48.46029173419773,
      "grad_norm": 1.0472791194915771,
      "learning_rate": 4.518537139150178e-05,
      "loss": 3.1154,
      "step": 149500
    },
    {
      "epoch": 48.49270664505673,
      "grad_norm": 0.9193012118339539,
      "learning_rate": 4.518212779759974e-05,
      "loss": 3.1021,
      "step": 149600
    },
    {
      "epoch": 48.525121555915725,
      "grad_norm": 0.8649392127990723,
      "learning_rate": 4.517888420369769e-05,
      "loss": 3.1191,
      "step": 149700
    },
    {
      "epoch": 48.557536466774714,
      "grad_norm": 1.0279219150543213,
      "learning_rate": 4.517564060979565e-05,
      "loss": 3.1016,
      "step": 149800
    },
    {
      "epoch": 48.58995137763371,
      "grad_norm": 0.8999248147010803,
      "learning_rate": 4.517239701589361e-05,
      "loss": 3.1009,
      "step": 149900
    },
    {
      "epoch": 48.62236628849271,
      "grad_norm": 0.9124566316604614,
      "learning_rate": 4.516915342199157e-05,
      "loss": 3.1127,
      "step": 150000
    },
    {
      "epoch": 48.6547811993517,
      "grad_norm": 0.9918031692504883,
      "learning_rate": 4.516590982808953e-05,
      "loss": 3.1213,
      "step": 150100
    },
    {
      "epoch": 48.6871961102107,
      "grad_norm": 1.092551827430725,
      "learning_rate": 4.516266623418749e-05,
      "loss": 3.1237,
      "step": 150200
    },
    {
      "epoch": 48.71961102106969,
      "grad_norm": 1.0144835710525513,
      "learning_rate": 4.515942264028544e-05,
      "loss": 3.1318,
      "step": 150300
    },
    {
      "epoch": 48.752025931928685,
      "grad_norm": 1.1565043926239014,
      "learning_rate": 4.51561790463834e-05,
      "loss": 3.1163,
      "step": 150400
    },
    {
      "epoch": 48.78444084278768,
      "grad_norm": 0.8706660270690918,
      "learning_rate": 4.5152935452481357e-05,
      "loss": 3.14,
      "step": 150500
    },
    {
      "epoch": 48.81685575364668,
      "grad_norm": 0.8656037449836731,
      "learning_rate": 4.514972429451833e-05,
      "loss": 3.1181,
      "step": 150600
    },
    {
      "epoch": 48.849270664505674,
      "grad_norm": 1.071699857711792,
      "learning_rate": 4.5146480700616286e-05,
      "loss": 3.1083,
      "step": 150700
    },
    {
      "epoch": 48.88168557536467,
      "grad_norm": 1.0093107223510742,
      "learning_rate": 4.5143237106714245e-05,
      "loss": 3.1111,
      "step": 150800
    },
    {
      "epoch": 48.91410048622366,
      "grad_norm": 0.9132304787635803,
      "learning_rate": 4.51399935128122e-05,
      "loss": 3.1293,
      "step": 150900
    },
    {
      "epoch": 48.946515397082656,
      "grad_norm": 1.0481152534484863,
      "learning_rate": 4.5136749918910156e-05,
      "loss": 3.1225,
      "step": 151000
    },
    {
      "epoch": 48.97893030794165,
      "grad_norm": 0.8678814768791199,
      "learning_rate": 4.5133506325008114e-05,
      "loss": 3.1343,
      "step": 151100
    },
    {
      "epoch": 49.0,
      "eval_bleu": 1.1070525667943056,
      "eval_loss": 3.7557830810546875,
      "eval_runtime": 4.5072,
      "eval_samples_per_second": 109.159,
      "eval_steps_per_second": 1.775,
      "step": 151165
    },
    {
      "epoch": 49.01134521880065,
      "grad_norm": 0.8396968245506287,
      "learning_rate": 4.5130262731106066e-05,
      "loss": 3.1177,
      "step": 151200
    },
    {
      "epoch": 49.043760129659645,
      "grad_norm": 1.082525610923767,
      "learning_rate": 4.5127019137204025e-05,
      "loss": 3.103,
      "step": 151300
    },
    {
      "epoch": 49.07617504051864,
      "grad_norm": 0.8964900374412537,
      "learning_rate": 4.5123775543301984e-05,
      "loss": 3.0919,
      "step": 151400
    },
    {
      "epoch": 49.10858995137763,
      "grad_norm": 0.9359110593795776,
      "learning_rate": 4.5120531949399936e-05,
      "loss": 3.1033,
      "step": 151500
    },
    {
      "epoch": 49.14100486223663,
      "grad_norm": 0.9962767958641052,
      "learning_rate": 4.5117288355497895e-05,
      "loss": 3.1152,
      "step": 151600
    },
    {
      "epoch": 49.173419773095624,
      "grad_norm": 0.8432102799415588,
      "learning_rate": 4.5114044761595853e-05,
      "loss": 3.1214,
      "step": 151700
    },
    {
      "epoch": 49.20583468395462,
      "grad_norm": 0.8410413861274719,
      "learning_rate": 4.5110801167693805e-05,
      "loss": 3.1119,
      "step": 151800
    },
    {
      "epoch": 49.23824959481362,
      "grad_norm": 0.8476511240005493,
      "learning_rate": 4.5107557573791764e-05,
      "loss": 3.1066,
      "step": 151900
    },
    {
      "epoch": 49.270664505672606,
      "grad_norm": 0.9156368374824524,
      "learning_rate": 4.5104313979889716e-05,
      "loss": 3.1201,
      "step": 152000
    },
    {
      "epoch": 49.3030794165316,
      "grad_norm": 0.9691908359527588,
      "learning_rate": 4.5101070385987675e-05,
      "loss": 3.0979,
      "step": 152100
    },
    {
      "epoch": 49.3354943273906,
      "grad_norm": 0.8192843794822693,
      "learning_rate": 4.5097826792085634e-05,
      "loss": 3.0964,
      "step": 152200
    },
    {
      "epoch": 49.367909238249595,
      "grad_norm": 0.9507142305374146,
      "learning_rate": 4.5094583198183586e-05,
      "loss": 3.1096,
      "step": 152300
    },
    {
      "epoch": 49.40032414910859,
      "grad_norm": 0.898819625377655,
      "learning_rate": 4.5091339604281544e-05,
      "loss": 3.1102,
      "step": 152400
    },
    {
      "epoch": 49.43273905996759,
      "grad_norm": 0.9769629240036011,
      "learning_rate": 4.50880960103795e-05,
      "loss": 3.1047,
      "step": 152500
    },
    {
      "epoch": 49.46515397082658,
      "grad_norm": 1.0441793203353882,
      "learning_rate": 4.5084852416477455e-05,
      "loss": 3.1159,
      "step": 152600
    },
    {
      "epoch": 49.497568881685574,
      "grad_norm": 1.0027556419372559,
      "learning_rate": 4.5081608822575414e-05,
      "loss": 3.0913,
      "step": 152700
    },
    {
      "epoch": 49.52998379254457,
      "grad_norm": 0.8098018765449524,
      "learning_rate": 4.507836522867337e-05,
      "loss": 3.1238,
      "step": 152800
    },
    {
      "epoch": 49.56239870340357,
      "grad_norm": 1.3674529790878296,
      "learning_rate": 4.507512163477133e-05,
      "loss": 3.1124,
      "step": 152900
    },
    {
      "epoch": 49.59481361426256,
      "grad_norm": 0.9630165696144104,
      "learning_rate": 4.507187804086928e-05,
      "loss": 3.1084,
      "step": 153000
    },
    {
      "epoch": 49.62722852512156,
      "grad_norm": 1.055614709854126,
      "learning_rate": 4.506866688290626e-05,
      "loss": 3.1229,
      "step": 153100
    },
    {
      "epoch": 49.65964343598055,
      "grad_norm": 0.9126559495925903,
      "learning_rate": 4.506542328900421e-05,
      "loss": 3.1191,
      "step": 153200
    },
    {
      "epoch": 49.692058346839545,
      "grad_norm": 0.9396699070930481,
      "learning_rate": 4.506217969510217e-05,
      "loss": 3.1119,
      "step": 153300
    },
    {
      "epoch": 49.72447325769854,
      "grad_norm": 0.8181762099266052,
      "learning_rate": 4.505893610120013e-05,
      "loss": 3.1139,
      "step": 153400
    },
    {
      "epoch": 49.75688816855754,
      "grad_norm": 0.8413995504379272,
      "learning_rate": 4.505569250729809e-05,
      "loss": 3.1205,
      "step": 153500
    },
    {
      "epoch": 49.789303079416534,
      "grad_norm": 1.0115749835968018,
      "learning_rate": 4.505244891339605e-05,
      "loss": 3.1294,
      "step": 153600
    },
    {
      "epoch": 49.82171799027552,
      "grad_norm": 1.0071079730987549,
      "learning_rate": 4.504920531949401e-05,
      "loss": 3.0987,
      "step": 153700
    },
    {
      "epoch": 49.85413290113452,
      "grad_norm": 0.9735620617866516,
      "learning_rate": 4.504596172559196e-05,
      "loss": 3.1166,
      "step": 153800
    },
    {
      "epoch": 49.886547811993516,
      "grad_norm": 0.9350029826164246,
      "learning_rate": 4.504271813168992e-05,
      "loss": 3.0976,
      "step": 153900
    },
    {
      "epoch": 49.91896272285251,
      "grad_norm": 0.9160934090614319,
      "learning_rate": 4.5039474537787876e-05,
      "loss": 3.1148,
      "step": 154000
    },
    {
      "epoch": 49.95137763371151,
      "grad_norm": 0.9517815709114075,
      "learning_rate": 4.503623094388583e-05,
      "loss": 3.1225,
      "step": 154100
    },
    {
      "epoch": 49.983792544570505,
      "grad_norm": 0.8653106689453125,
      "learning_rate": 4.503298734998379e-05,
      "loss": 3.1102,
      "step": 154200
    },
    {
      "epoch": 50.0,
      "eval_bleu": 1.151145423420661,
      "eval_loss": 3.7576873302459717,
      "eval_runtime": 4.7736,
      "eval_samples_per_second": 103.066,
      "eval_steps_per_second": 1.676,
      "step": 154250
    },
    {
      "epoch": 50.016207455429495,
      "grad_norm": 0.9743425250053406,
      "learning_rate": 4.502974375608174e-05,
      "loss": 3.0765,
      "step": 154300
    },
    {
      "epoch": 50.04862236628849,
      "grad_norm": 0.9787591695785522,
      "learning_rate": 4.50265001621797e-05,
      "loss": 3.0961,
      "step": 154400
    },
    {
      "epoch": 50.08103727714749,
      "grad_norm": 1.0276235342025757,
      "learning_rate": 4.5023256568277656e-05,
      "loss": 3.088,
      "step": 154500
    },
    {
      "epoch": 50.113452188006484,
      "grad_norm": 1.0611143112182617,
      "learning_rate": 4.502001297437561e-05,
      "loss": 3.1196,
      "step": 154600
    },
    {
      "epoch": 50.14586709886548,
      "grad_norm": 0.9761902689933777,
      "learning_rate": 4.501676938047357e-05,
      "loss": 3.0945,
      "step": 154700
    },
    {
      "epoch": 50.17828200972448,
      "grad_norm": 1.027915120124817,
      "learning_rate": 4.5013525786571526e-05,
      "loss": 3.0989,
      "step": 154800
    },
    {
      "epoch": 50.210696920583466,
      "grad_norm": 1.0402449369430542,
      "learning_rate": 4.501028219266948e-05,
      "loss": 3.1152,
      "step": 154900
    },
    {
      "epoch": 50.24311183144246,
      "grad_norm": 1.0243490934371948,
      "learning_rate": 4.5007038598767436e-05,
      "loss": 3.0908,
      "step": 155000
    },
    {
      "epoch": 50.27552674230146,
      "grad_norm": 1.0416210889816284,
      "learning_rate": 4.5003795004865395e-05,
      "loss": 3.106,
      "step": 155100
    },
    {
      "epoch": 50.307941653160455,
      "grad_norm": 0.8598913550376892,
      "learning_rate": 4.500055141096335e-05,
      "loss": 3.1109,
      "step": 155200
    },
    {
      "epoch": 50.34035656401945,
      "grad_norm": 1.0525538921356201,
      "learning_rate": 4.4997307817061306e-05,
      "loss": 3.0902,
      "step": 155300
    },
    {
      "epoch": 50.37277147487844,
      "grad_norm": 0.9820732474327087,
      "learning_rate": 4.499406422315926e-05,
      "loss": 3.1129,
      "step": 155400
    },
    {
      "epoch": 50.40518638573744,
      "grad_norm": 1.0121688842773438,
      "learning_rate": 4.4990820629257217e-05,
      "loss": 3.0994,
      "step": 155500
    },
    {
      "epoch": 50.43760129659643,
      "grad_norm": 1.0766725540161133,
      "learning_rate": 4.4987577035355175e-05,
      "loss": 3.1057,
      "step": 155600
    },
    {
      "epoch": 50.47001620745543,
      "grad_norm": 0.9170451760292053,
      "learning_rate": 4.498433344145313e-05,
      "loss": 3.1154,
      "step": 155700
    },
    {
      "epoch": 50.502431118314426,
      "grad_norm": 0.7724522352218628,
      "learning_rate": 4.4981089847551086e-05,
      "loss": 3.1143,
      "step": 155800
    },
    {
      "epoch": 50.53484602917342,
      "grad_norm": 0.9767469763755798,
      "learning_rate": 4.4977846253649045e-05,
      "loss": 3.1113,
      "step": 155900
    },
    {
      "epoch": 50.56726094003241,
      "grad_norm": 0.8646152019500732,
      "learning_rate": 4.4974602659747004e-05,
      "loss": 3.114,
      "step": 156000
    },
    {
      "epoch": 50.59967585089141,
      "grad_norm": 1.0001672506332397,
      "learning_rate": 4.497135906584496e-05,
      "loss": 3.1226,
      "step": 156100
    },
    {
      "epoch": 50.632090761750405,
      "grad_norm": 1.0434032678604126,
      "learning_rate": 4.496811547194292e-05,
      "loss": 3.0943,
      "step": 156200
    },
    {
      "epoch": 50.6645056726094,
      "grad_norm": 0.795707643032074,
      "learning_rate": 4.496487187804087e-05,
      "loss": 3.1091,
      "step": 156300
    },
    {
      "epoch": 50.6969205834684,
      "grad_norm": 0.8914735913276672,
      "learning_rate": 4.496162828413883e-05,
      "loss": 3.1071,
      "step": 156400
    },
    {
      "epoch": 50.729335494327394,
      "grad_norm": 0.9482558965682983,
      "learning_rate": 4.4958384690236784e-05,
      "loss": 3.1289,
      "step": 156500
    },
    {
      "epoch": 50.76175040518638,
      "grad_norm": 0.9654344320297241,
      "learning_rate": 4.495514109633474e-05,
      "loss": 3.1056,
      "step": 156600
    },
    {
      "epoch": 50.79416531604538,
      "grad_norm": 1.178837776184082,
      "learning_rate": 4.49518975024327e-05,
      "loss": 3.0988,
      "step": 156700
    },
    {
      "epoch": 50.826580226904376,
      "grad_norm": 0.9621936082839966,
      "learning_rate": 4.494865390853065e-05,
      "loss": 3.1323,
      "step": 156800
    },
    {
      "epoch": 50.85899513776337,
      "grad_norm": 0.872199296951294,
      "learning_rate": 4.494541031462861e-05,
      "loss": 3.0888,
      "step": 156900
    },
    {
      "epoch": 50.89141004862237,
      "grad_norm": 1.0443679094314575,
      "learning_rate": 4.494216672072657e-05,
      "loss": 3.1046,
      "step": 157000
    },
    {
      "epoch": 50.92382495948136,
      "grad_norm": 0.9157425165176392,
      "learning_rate": 4.493892312682452e-05,
      "loss": 3.1032,
      "step": 157100
    },
    {
      "epoch": 50.956239870340355,
      "grad_norm": 0.915890634059906,
      "learning_rate": 4.493567953292248e-05,
      "loss": 3.1181,
      "step": 157200
    },
    {
      "epoch": 50.98865478119935,
      "grad_norm": 0.9522885084152222,
      "learning_rate": 4.493243593902043e-05,
      "loss": 3.098,
      "step": 157300
    },
    {
      "epoch": 51.0,
      "eval_bleu": 1.2205828035850521,
      "eval_loss": 3.7503809928894043,
      "eval_runtime": 5.3163,
      "eval_samples_per_second": 92.546,
      "eval_steps_per_second": 1.505,
      "step": 157335
    },
    {
      "epoch": 51.02106969205835,
      "grad_norm": 1.0643036365509033,
      "learning_rate": 4.492919234511839e-05,
      "loss": 3.0723,
      "step": 157400
    },
    {
      "epoch": 51.053484602917344,
      "grad_norm": 1.051707148551941,
      "learning_rate": 4.492594875121635e-05,
      "loss": 3.1002,
      "step": 157500
    },
    {
      "epoch": 51.08589951377634,
      "grad_norm": 0.9452869892120361,
      "learning_rate": 4.49227051573143e-05,
      "loss": 3.0817,
      "step": 157600
    },
    {
      "epoch": 51.11831442463533,
      "grad_norm": 0.8823714852333069,
      "learning_rate": 4.491946156341226e-05,
      "loss": 3.108,
      "step": 157700
    },
    {
      "epoch": 51.150729335494326,
      "grad_norm": 0.8517045378684998,
      "learning_rate": 4.491621796951022e-05,
      "loss": 3.0932,
      "step": 157800
    },
    {
      "epoch": 51.18314424635332,
      "grad_norm": 0.9068583250045776,
      "learning_rate": 4.491297437560817e-05,
      "loss": 3.0999,
      "step": 157900
    },
    {
      "epoch": 51.21555915721232,
      "grad_norm": 1.0560989379882812,
      "learning_rate": 4.490973078170613e-05,
      "loss": 3.1212,
      "step": 158000
    },
    {
      "epoch": 51.247974068071315,
      "grad_norm": 0.855201780796051,
      "learning_rate": 4.490648718780409e-05,
      "loss": 3.0948,
      "step": 158100
    },
    {
      "epoch": 51.28038897893031,
      "grad_norm": 0.9869187474250793,
      "learning_rate": 4.490324359390204e-05,
      "loss": 3.1053,
      "step": 158200
    },
    {
      "epoch": 51.3128038897893,
      "grad_norm": 1.0160491466522217,
      "learning_rate": 4.49e-05,
      "loss": 3.0873,
      "step": 158300
    },
    {
      "epoch": 51.3452188006483,
      "grad_norm": 0.8936554193496704,
      "learning_rate": 4.489675640609796e-05,
      "loss": 3.1123,
      "step": 158400
    },
    {
      "epoch": 51.37763371150729,
      "grad_norm": 0.8206405639648438,
      "learning_rate": 4.489351281219592e-05,
      "loss": 3.1013,
      "step": 158500
    },
    {
      "epoch": 51.41004862236629,
      "grad_norm": 1.0304954051971436,
      "learning_rate": 4.489026921829388e-05,
      "loss": 3.1137,
      "step": 158600
    },
    {
      "epoch": 51.442463533225286,
      "grad_norm": 0.9344852566719055,
      "learning_rate": 4.488705806033085e-05,
      "loss": 3.1094,
      "step": 158700
    },
    {
      "epoch": 51.474878444084275,
      "grad_norm": 0.9291937947273254,
      "learning_rate": 4.48838144664288e-05,
      "loss": 3.0685,
      "step": 158800
    },
    {
      "epoch": 51.50729335494327,
      "grad_norm": 1.0356419086456299,
      "learning_rate": 4.488057087252676e-05,
      "loss": 3.1302,
      "step": 158900
    },
    {
      "epoch": 51.53970826580227,
      "grad_norm": 0.9128406643867493,
      "learning_rate": 4.487732727862472e-05,
      "loss": 3.0881,
      "step": 159000
    },
    {
      "epoch": 51.572123176661265,
      "grad_norm": 0.988945722579956,
      "learning_rate": 4.4874083684722676e-05,
      "loss": 3.0647,
      "step": 159100
    },
    {
      "epoch": 51.60453808752026,
      "grad_norm": 1.0346429347991943,
      "learning_rate": 4.4870840090820635e-05,
      "loss": 3.1076,
      "step": 159200
    },
    {
      "epoch": 51.63695299837926,
      "grad_norm": 0.9595087170600891,
      "learning_rate": 4.486759649691859e-05,
      "loss": 3.1202,
      "step": 159300
    },
    {
      "epoch": 51.66936790923825,
      "grad_norm": 0.8875600099563599,
      "learning_rate": 4.4864352903016545e-05,
      "loss": 3.1216,
      "step": 159400
    },
    {
      "epoch": 51.70178282009724,
      "grad_norm": 0.9138034582138062,
      "learning_rate": 4.4861109309114504e-05,
      "loss": 3.1032,
      "step": 159500
    },
    {
      "epoch": 51.73419773095624,
      "grad_norm": 0.8980088829994202,
      "learning_rate": 4.4857865715212456e-05,
      "loss": 3.1101,
      "step": 159600
    },
    {
      "epoch": 51.766612641815236,
      "grad_norm": 1.0449392795562744,
      "learning_rate": 4.4854622121310415e-05,
      "loss": 3.1249,
      "step": 159700
    },
    {
      "epoch": 51.79902755267423,
      "grad_norm": 1.0568054914474487,
      "learning_rate": 4.4851378527408373e-05,
      "loss": 3.0834,
      "step": 159800
    },
    {
      "epoch": 51.83144246353323,
      "grad_norm": 0.8995320200920105,
      "learning_rate": 4.4848134933506325e-05,
      "loss": 3.0938,
      "step": 159900
    },
    {
      "epoch": 51.86385737439222,
      "grad_norm": 0.9072110056877136,
      "learning_rate": 4.4844891339604284e-05,
      "loss": 3.1207,
      "step": 160000
    },
    {
      "epoch": 51.896272285251214,
      "grad_norm": 1.0906175374984741,
      "learning_rate": 4.484164774570224e-05,
      "loss": 3.1037,
      "step": 160100
    },
    {
      "epoch": 51.92868719611021,
      "grad_norm": 0.8867667317390442,
      "learning_rate": 4.4838404151800195e-05,
      "loss": 3.1102,
      "step": 160200
    },
    {
      "epoch": 51.96110210696921,
      "grad_norm": 0.8926135301589966,
      "learning_rate": 4.4835160557898154e-05,
      "loss": 3.1065,
      "step": 160300
    },
    {
      "epoch": 51.993517017828204,
      "grad_norm": 0.9557908177375793,
      "learning_rate": 4.483191696399611e-05,
      "loss": 3.0838,
      "step": 160400
    },
    {
      "epoch": 52.0,
      "eval_bleu": 1.2410698298872123,
      "eval_loss": 3.762298107147217,
      "eval_runtime": 4.2085,
      "eval_samples_per_second": 116.906,
      "eval_steps_per_second": 1.901,
      "step": 160420
    },
    {
      "epoch": 52.02593192868719,
      "grad_norm": 1.0619851350784302,
      "learning_rate": 4.4828673370094064e-05,
      "loss": 3.0921,
      "step": 160500
    },
    {
      "epoch": 52.05834683954619,
      "grad_norm": 0.9836370944976807,
      "learning_rate": 4.482542977619202e-05,
      "loss": 3.1041,
      "step": 160600
    },
    {
      "epoch": 52.090761750405186,
      "grad_norm": 0.9678823351860046,
      "learning_rate": 4.4822218618229e-05,
      "loss": 3.0783,
      "step": 160700
    },
    {
      "epoch": 52.12317666126418,
      "grad_norm": 0.8701555728912354,
      "learning_rate": 4.481897502432696e-05,
      "loss": 3.1107,
      "step": 160800
    },
    {
      "epoch": 52.15559157212318,
      "grad_norm": 0.818109929561615,
      "learning_rate": 4.481573143042491e-05,
      "loss": 3.0953,
      "step": 160900
    },
    {
      "epoch": 52.188006482982175,
      "grad_norm": 0.9042512774467468,
      "learning_rate": 4.481248783652287e-05,
      "loss": 3.0954,
      "step": 161000
    },
    {
      "epoch": 52.220421393841164,
      "grad_norm": 0.876621663570404,
      "learning_rate": 4.480924424262082e-05,
      "loss": 3.0897,
      "step": 161100
    },
    {
      "epoch": 52.25283630470016,
      "grad_norm": 0.9201112985610962,
      "learning_rate": 4.480600064871878e-05,
      "loss": 3.088,
      "step": 161200
    },
    {
      "epoch": 52.28525121555916,
      "grad_norm": 0.9242194294929504,
      "learning_rate": 4.480275705481674e-05,
      "loss": 3.1033,
      "step": 161300
    },
    {
      "epoch": 52.31766612641815,
      "grad_norm": 1.023519515991211,
      "learning_rate": 4.479951346091469e-05,
      "loss": 3.0914,
      "step": 161400
    },
    {
      "epoch": 52.35008103727715,
      "grad_norm": 1.0146260261535645,
      "learning_rate": 4.479626986701265e-05,
      "loss": 3.0855,
      "step": 161500
    },
    {
      "epoch": 52.382495948136146,
      "grad_norm": 0.8553302884101868,
      "learning_rate": 4.479302627311061e-05,
      "loss": 3.1127,
      "step": 161600
    },
    {
      "epoch": 52.414910858995135,
      "grad_norm": 0.859031617641449,
      "learning_rate": 4.478978267920856e-05,
      "loss": 3.0913,
      "step": 161700
    },
    {
      "epoch": 52.44732576985413,
      "grad_norm": 1.03914475440979,
      "learning_rate": 4.478653908530652e-05,
      "loss": 3.0839,
      "step": 161800
    },
    {
      "epoch": 52.47974068071313,
      "grad_norm": 1.0289028882980347,
      "learning_rate": 4.478329549140448e-05,
      "loss": 3.1066,
      "step": 161900
    },
    {
      "epoch": 52.512155591572125,
      "grad_norm": 0.9043976068496704,
      "learning_rate": 4.478005189750244e-05,
      "loss": 3.086,
      "step": 162000
    },
    {
      "epoch": 52.54457050243112,
      "grad_norm": 1.1944093704223633,
      "learning_rate": 4.477680830360039e-05,
      "loss": 3.0913,
      "step": 162100
    },
    {
      "epoch": 52.57698541329011,
      "grad_norm": 0.9290409684181213,
      "learning_rate": 4.477356470969835e-05,
      "loss": 3.1126,
      "step": 162200
    },
    {
      "epoch": 52.60940032414911,
      "grad_norm": 1.0535515546798706,
      "learning_rate": 4.477032111579631e-05,
      "loss": 3.0916,
      "step": 162300
    },
    {
      "epoch": 52.6418152350081,
      "grad_norm": 0.8809988498687744,
      "learning_rate": 4.4767077521894266e-05,
      "loss": 3.0959,
      "step": 162400
    },
    {
      "epoch": 52.6742301458671,
      "grad_norm": 1.0527112483978271,
      "learning_rate": 4.476383392799222e-05,
      "loss": 3.0897,
      "step": 162500
    },
    {
      "epoch": 52.706645056726096,
      "grad_norm": 0.9187372326850891,
      "learning_rate": 4.4760590334090176e-05,
      "loss": 3.0941,
      "step": 162600
    },
    {
      "epoch": 52.73905996758509,
      "grad_norm": 0.9578344225883484,
      "learning_rate": 4.4757346740188135e-05,
      "loss": 3.1121,
      "step": 162700
    },
    {
      "epoch": 52.77147487844408,
      "grad_norm": 1.0419291257858276,
      "learning_rate": 4.475410314628609e-05,
      "loss": 3.1084,
      "step": 162800
    },
    {
      "epoch": 52.80388978930308,
      "grad_norm": 0.8757657408714294,
      "learning_rate": 4.4750859552384046e-05,
      "loss": 3.111,
      "step": 162900
    },
    {
      "epoch": 52.836304700162074,
      "grad_norm": 0.8772410750389099,
      "learning_rate": 4.4747615958482e-05,
      "loss": 3.104,
      "step": 163000
    },
    {
      "epoch": 52.86871961102107,
      "grad_norm": 0.808211088180542,
      "learning_rate": 4.4744372364579957e-05,
      "loss": 3.1025,
      "step": 163100
    },
    {
      "epoch": 52.90113452188007,
      "grad_norm": 0.944987952709198,
      "learning_rate": 4.4741128770677915e-05,
      "loss": 3.0956,
      "step": 163200
    },
    {
      "epoch": 52.93354943273906,
      "grad_norm": 0.8905817270278931,
      "learning_rate": 4.473788517677587e-05,
      "loss": 3.0855,
      "step": 163300
    },
    {
      "epoch": 52.96596434359805,
      "grad_norm": 1.0829766988754272,
      "learning_rate": 4.4734674018812845e-05,
      "loss": 3.0984,
      "step": 163400
    },
    {
      "epoch": 52.99837925445705,
      "grad_norm": 0.8204167485237122,
      "learning_rate": 4.4731430424910804e-05,
      "loss": 3.1057,
      "step": 163500
    },
    {
      "epoch": 53.0,
      "eval_bleu": 1.213402287289117,
      "eval_loss": 3.761035442352295,
      "eval_runtime": 4.4275,
      "eval_samples_per_second": 111.124,
      "eval_steps_per_second": 1.807,
      "step": 163505
    },
    {
      "epoch": 53.030794165316046,
      "grad_norm": 1.1334736347198486,
      "learning_rate": 4.472818683100876e-05,
      "loss": 3.0768,
      "step": 163600
    },
    {
      "epoch": 53.06320907617504,
      "grad_norm": 0.883377194404602,
      "learning_rate": 4.4724943237106714e-05,
      "loss": 3.087,
      "step": 163700
    },
    {
      "epoch": 53.09562398703404,
      "grad_norm": 1.0072332620620728,
      "learning_rate": 4.472169964320467e-05,
      "loss": 3.0932,
      "step": 163800
    },
    {
      "epoch": 53.12803889789303,
      "grad_norm": 0.8506906032562256,
      "learning_rate": 4.471845604930263e-05,
      "loss": 3.0956,
      "step": 163900
    },
    {
      "epoch": 53.160453808752024,
      "grad_norm": 1.008259654045105,
      "learning_rate": 4.4715212455400584e-05,
      "loss": 3.0802,
      "step": 164000
    },
    {
      "epoch": 53.19286871961102,
      "grad_norm": 1.0742640495300293,
      "learning_rate": 4.471196886149854e-05,
      "loss": 3.0779,
      "step": 164100
    },
    {
      "epoch": 53.22528363047002,
      "grad_norm": 0.8969210982322693,
      "learning_rate": 4.47087252675965e-05,
      "loss": 3.0921,
      "step": 164200
    },
    {
      "epoch": 53.25769854132901,
      "grad_norm": 1.0013072490692139,
      "learning_rate": 4.4705481673694453e-05,
      "loss": 3.0926,
      "step": 164300
    },
    {
      "epoch": 53.29011345218801,
      "grad_norm": 0.9881411194801331,
      "learning_rate": 4.470223807979241e-05,
      "loss": 3.1044,
      "step": 164400
    },
    {
      "epoch": 53.322528363047,
      "grad_norm": 1.044658899307251,
      "learning_rate": 4.4698994485890364e-05,
      "loss": 3.0998,
      "step": 164500
    },
    {
      "epoch": 53.354943273905995,
      "grad_norm": 0.8106368184089661,
      "learning_rate": 4.469575089198832e-05,
      "loss": 3.0894,
      "step": 164600
    },
    {
      "epoch": 53.38735818476499,
      "grad_norm": 0.8657239079475403,
      "learning_rate": 4.469250729808628e-05,
      "loss": 3.0815,
      "step": 164700
    },
    {
      "epoch": 53.41977309562399,
      "grad_norm": 0.8520392179489136,
      "learning_rate": 4.4689263704184234e-05,
      "loss": 3.0821,
      "step": 164800
    },
    {
      "epoch": 53.452188006482984,
      "grad_norm": 0.8988003730773926,
      "learning_rate": 4.468602011028219e-05,
      "loss": 3.0654,
      "step": 164900
    },
    {
      "epoch": 53.48460291734198,
      "grad_norm": 1.1772228479385376,
      "learning_rate": 4.468277651638015e-05,
      "loss": 3.1071,
      "step": 165000
    },
    {
      "epoch": 53.51701782820097,
      "grad_norm": 0.9131725430488586,
      "learning_rate": 4.467953292247811e-05,
      "loss": 3.0937,
      "step": 165100
    },
    {
      "epoch": 53.54943273905997,
      "grad_norm": 0.9171448349952698,
      "learning_rate": 4.467628932857607e-05,
      "loss": 3.0814,
      "step": 165200
    },
    {
      "epoch": 53.58184764991896,
      "grad_norm": 1.0460872650146484,
      "learning_rate": 4.467304573467402e-05,
      "loss": 3.0874,
      "step": 165300
    },
    {
      "epoch": 53.61426256077796,
      "grad_norm": 0.9420486092567444,
      "learning_rate": 4.466980214077198e-05,
      "loss": 3.1181,
      "step": 165400
    },
    {
      "epoch": 53.646677471636956,
      "grad_norm": 1.0744729042053223,
      "learning_rate": 4.466655854686994e-05,
      "loss": 3.0912,
      "step": 165500
    },
    {
      "epoch": 53.679092382495945,
      "grad_norm": 0.9328302145004272,
      "learning_rate": 4.466331495296789e-05,
      "loss": 3.1102,
      "step": 165600
    },
    {
      "epoch": 53.71150729335494,
      "grad_norm": 0.9722738265991211,
      "learning_rate": 4.466007135906585e-05,
      "loss": 3.1147,
      "step": 165700
    },
    {
      "epoch": 53.74392220421394,
      "grad_norm": 0.9706699252128601,
      "learning_rate": 4.465682776516381e-05,
      "loss": 3.113,
      "step": 165800
    },
    {
      "epoch": 53.776337115072934,
      "grad_norm": 0.8359207510948181,
      "learning_rate": 4.465358417126176e-05,
      "loss": 3.0815,
      "step": 165900
    },
    {
      "epoch": 53.80875202593193,
      "grad_norm": 0.9553876519203186,
      "learning_rate": 4.465034057735972e-05,
      "loss": 3.0988,
      "step": 166000
    },
    {
      "epoch": 53.84116693679093,
      "grad_norm": 0.957941472530365,
      "learning_rate": 4.464709698345768e-05,
      "loss": 3.0876,
      "step": 166100
    },
    {
      "epoch": 53.873581847649916,
      "grad_norm": 0.783126950263977,
      "learning_rate": 4.464385338955563e-05,
      "loss": 3.0967,
      "step": 166200
    },
    {
      "epoch": 53.90599675850891,
      "grad_norm": 0.9657486081123352,
      "learning_rate": 4.464060979565359e-05,
      "loss": 3.0855,
      "step": 166300
    },
    {
      "epoch": 53.93841166936791,
      "grad_norm": 0.8427544832229614,
      "learning_rate": 4.463736620175154e-05,
      "loss": 3.1013,
      "step": 166400
    },
    {
      "epoch": 53.970826580226905,
      "grad_norm": 0.9030826091766357,
      "learning_rate": 4.46341226078495e-05,
      "loss": 3.0989,
      "step": 166500
    },
    {
      "epoch": 54.0,
      "eval_bleu": 1.3060552815292787,
      "eval_loss": 3.7637887001037598,
      "eval_runtime": 4.3334,
      "eval_samples_per_second": 113.538,
      "eval_steps_per_second": 1.846,
      "step": 166590
    },
    {
      "epoch": 54.0032414910859,
      "grad_norm": 0.8096458911895752,
      "learning_rate": 4.463087901394746e-05,
      "loss": 3.0887,
      "step": 166600
    },
    {
      "epoch": 54.0356564019449,
      "grad_norm": 1.0502824783325195,
      "learning_rate": 4.462763542004541e-05,
      "loss": 3.0928,
      "step": 166700
    },
    {
      "epoch": 54.06807131280389,
      "grad_norm": 1.063646674156189,
      "learning_rate": 4.462439182614337e-05,
      "loss": 3.0826,
      "step": 166800
    },
    {
      "epoch": 54.100486223662884,
      "grad_norm": 0.8209788799285889,
      "learning_rate": 4.4621148232241327e-05,
      "loss": 3.0912,
      "step": 166900
    },
    {
      "epoch": 54.13290113452188,
      "grad_norm": 1.0074101686477661,
      "learning_rate": 4.461790463833928e-05,
      "loss": 3.09,
      "step": 167000
    },
    {
      "epoch": 54.16531604538088,
      "grad_norm": 0.9278899431228638,
      "learning_rate": 4.461466104443724e-05,
      "loss": 3.099,
      "step": 167100
    },
    {
      "epoch": 54.19773095623987,
      "grad_norm": 1.001463770866394,
      "learning_rate": 4.4611417450535196e-05,
      "loss": 3.1051,
      "step": 167200
    },
    {
      "epoch": 54.23014586709886,
      "grad_norm": 0.9740964770317078,
      "learning_rate": 4.460817385663315e-05,
      "loss": 3.0672,
      "step": 167300
    },
    {
      "epoch": 54.26256077795786,
      "grad_norm": 0.9629859328269958,
      "learning_rate": 4.4604962698670126e-05,
      "loss": 3.1002,
      "step": 167400
    },
    {
      "epoch": 54.294975688816855,
      "grad_norm": 0.8557560443878174,
      "learning_rate": 4.4601719104768084e-05,
      "loss": 3.088,
      "step": 167500
    },
    {
      "epoch": 54.32739059967585,
      "grad_norm": 0.9163900017738342,
      "learning_rate": 4.4598475510866036e-05,
      "loss": 3.0795,
      "step": 167600
    },
    {
      "epoch": 54.35980551053485,
      "grad_norm": 0.91778564453125,
      "learning_rate": 4.4595231916963995e-05,
      "loss": 3.1132,
      "step": 167700
    },
    {
      "epoch": 54.392220421393844,
      "grad_norm": 0.9968768358230591,
      "learning_rate": 4.4591988323061954e-05,
      "loss": 3.0952,
      "step": 167800
    },
    {
      "epoch": 54.424635332252834,
      "grad_norm": 1.096392273902893,
      "learning_rate": 4.4588744729159906e-05,
      "loss": 3.076,
      "step": 167900
    },
    {
      "epoch": 54.45705024311183,
      "grad_norm": 0.9892162084579468,
      "learning_rate": 4.4585533571196884e-05,
      "loss": 3.095,
      "step": 168000
    },
    {
      "epoch": 54.489465153970826,
      "grad_norm": 0.9461535811424255,
      "learning_rate": 4.458228997729484e-05,
      "loss": 3.0794,
      "step": 168100
    },
    {
      "epoch": 54.52188006482982,
      "grad_norm": 1.0456233024597168,
      "learning_rate": 4.45790463833928e-05,
      "loss": 3.0698,
      "step": 168200
    },
    {
      "epoch": 54.55429497568882,
      "grad_norm": 0.9592326879501343,
      "learning_rate": 4.457580278949075e-05,
      "loss": 3.0982,
      "step": 168300
    },
    {
      "epoch": 54.58670988654781,
      "grad_norm": 1.0210853815078735,
      "learning_rate": 4.457255919558871e-05,
      "loss": 3.0776,
      "step": 168400
    },
    {
      "epoch": 54.619124797406805,
      "grad_norm": 1.0267373323440552,
      "learning_rate": 4.456931560168667e-05,
      "loss": 3.0927,
      "step": 168500
    },
    {
      "epoch": 54.6515397082658,
      "grad_norm": 0.9276992678642273,
      "learning_rate": 4.456607200778463e-05,
      "loss": 3.0811,
      "step": 168600
    },
    {
      "epoch": 54.6839546191248,
      "grad_norm": 1.0178595781326294,
      "learning_rate": 4.456282841388258e-05,
      "loss": 3.0852,
      "step": 168700
    },
    {
      "epoch": 54.716369529983794,
      "grad_norm": 0.904073178768158,
      "learning_rate": 4.455961725591956e-05,
      "loss": 3.0707,
      "step": 168800
    },
    {
      "epoch": 54.74878444084279,
      "grad_norm": 0.9151212573051453,
      "learning_rate": 4.455637366201752e-05,
      "loss": 3.091,
      "step": 168900
    },
    {
      "epoch": 54.78119935170178,
      "grad_norm": 0.8964971303939819,
      "learning_rate": 4.455313006811547e-05,
      "loss": 3.0967,
      "step": 169000
    },
    {
      "epoch": 54.813614262560776,
      "grad_norm": 1.0192793607711792,
      "learning_rate": 4.454988647421343e-05,
      "loss": 3.0785,
      "step": 169100
    },
    {
      "epoch": 54.84602917341977,
      "grad_norm": 0.9625380635261536,
      "learning_rate": 4.454664288031139e-05,
      "loss": 3.0903,
      "step": 169200
    },
    {
      "epoch": 54.87844408427877,
      "grad_norm": 0.8561363220214844,
      "learning_rate": 4.4543399286409346e-05,
      "loss": 3.0963,
      "step": 169300
    },
    {
      "epoch": 54.910858995137765,
      "grad_norm": 0.859481692314148,
      "learning_rate": 4.4540155692507305e-05,
      "loss": 3.0785,
      "step": 169400
    },
    {
      "epoch": 54.94327390599676,
      "grad_norm": 1.1770014762878418,
      "learning_rate": 4.453691209860526e-05,
      "loss": 3.082,
      "step": 169500
    },
    {
      "epoch": 54.97568881685575,
      "grad_norm": 1.099898099899292,
      "learning_rate": 4.4533668504703216e-05,
      "loss": 3.1063,
      "step": 169600
    },
    {
      "epoch": 55.0,
      "eval_bleu": 1.2105644711835633,
      "eval_loss": 3.7647452354431152,
      "eval_runtime": 4.0576,
      "eval_samples_per_second": 121.255,
      "eval_steps_per_second": 1.972,
      "step": 169675
    },
    {
      "epoch": 55.00810372771475,
      "grad_norm": 0.9697479605674744,
      "learning_rate": 4.4530424910801174e-05,
      "loss": 3.0619,
      "step": 169700
    },
    {
      "epoch": 55.040518638573744,
      "grad_norm": 1.151084065437317,
      "learning_rate": 4.4527181316899126e-05,
      "loss": 3.0925,
      "step": 169800
    },
    {
      "epoch": 55.07293354943274,
      "grad_norm": 0.9920884370803833,
      "learning_rate": 4.4523937722997085e-05,
      "loss": 3.0907,
      "step": 169900
    },
    {
      "epoch": 55.10534846029174,
      "grad_norm": 0.9352954030036926,
      "learning_rate": 4.4520694129095044e-05,
      "loss": 3.0802,
      "step": 170000
    },
    {
      "epoch": 55.137763371150726,
      "grad_norm": 0.9317305088043213,
      "learning_rate": 4.4517450535192996e-05,
      "loss": 3.0523,
      "step": 170100
    },
    {
      "epoch": 55.17017828200972,
      "grad_norm": 1.0697829723358154,
      "learning_rate": 4.4514206941290954e-05,
      "loss": 3.0765,
      "step": 170200
    },
    {
      "epoch": 55.20259319286872,
      "grad_norm": 1.2021167278289795,
      "learning_rate": 4.4510963347388906e-05,
      "loss": 3.0809,
      "step": 170300
    },
    {
      "epoch": 55.235008103727715,
      "grad_norm": 1.0012785196304321,
      "learning_rate": 4.4507719753486865e-05,
      "loss": 3.0631,
      "step": 170400
    },
    {
      "epoch": 55.26742301458671,
      "grad_norm": 0.9638460278511047,
      "learning_rate": 4.4504476159584824e-05,
      "loss": 3.1021,
      "step": 170500
    },
    {
      "epoch": 55.29983792544571,
      "grad_norm": 0.9687654376029968,
      "learning_rate": 4.4501232565682776e-05,
      "loss": 3.0824,
      "step": 170600
    },
    {
      "epoch": 55.3322528363047,
      "grad_norm": 1.2248437404632568,
      "learning_rate": 4.4497988971780735e-05,
      "loss": 3.1048,
      "step": 170700
    },
    {
      "epoch": 55.36466774716369,
      "grad_norm": 0.9530860185623169,
      "learning_rate": 4.449474537787869e-05,
      "loss": 3.0808,
      "step": 170800
    },
    {
      "epoch": 55.39708265802269,
      "grad_norm": 0.9905874729156494,
      "learning_rate": 4.4491501783976645e-05,
      "loss": 3.0766,
      "step": 170900
    },
    {
      "epoch": 55.429497568881686,
      "grad_norm": 0.9625462889671326,
      "learning_rate": 4.4488258190074604e-05,
      "loss": 3.0647,
      "step": 171000
    },
    {
      "epoch": 55.46191247974068,
      "grad_norm": 0.9172755479812622,
      "learning_rate": 4.448501459617256e-05,
      "loss": 3.0836,
      "step": 171100
    },
    {
      "epoch": 55.49432739059968,
      "grad_norm": 0.9569387435913086,
      "learning_rate": 4.4481771002270515e-05,
      "loss": 3.0938,
      "step": 171200
    },
    {
      "epoch": 55.52674230145867,
      "grad_norm": 1.1024770736694336,
      "learning_rate": 4.4478527408368473e-05,
      "loss": 3.0852,
      "step": 171300
    },
    {
      "epoch": 55.559157212317665,
      "grad_norm": 1.0434993505477905,
      "learning_rate": 4.4475283814466425e-05,
      "loss": 3.1032,
      "step": 171400
    },
    {
      "epoch": 55.59157212317666,
      "grad_norm": 0.8440393209457397,
      "learning_rate": 4.4472040220564384e-05,
      "loss": 3.1046,
      "step": 171500
    },
    {
      "epoch": 55.62398703403566,
      "grad_norm": 0.9475730061531067,
      "learning_rate": 4.446879662666234e-05,
      "loss": 3.0975,
      "step": 171600
    },
    {
      "epoch": 55.656401944894654,
      "grad_norm": 1.1551638841629028,
      "learning_rate": 4.44655530327603e-05,
      "loss": 3.0728,
      "step": 171700
    },
    {
      "epoch": 55.68881685575364,
      "grad_norm": 1.0345134735107422,
      "learning_rate": 4.446230943885826e-05,
      "loss": 3.0675,
      "step": 171800
    },
    {
      "epoch": 55.72123176661264,
      "grad_norm": 1.1572818756103516,
      "learning_rate": 4.445906584495622e-05,
      "loss": 3.0823,
      "step": 171900
    },
    {
      "epoch": 55.753646677471636,
      "grad_norm": 1.1069427728652954,
      "learning_rate": 4.445582225105417e-05,
      "loss": 3.0942,
      "step": 172000
    },
    {
      "epoch": 55.78606158833063,
      "grad_norm": 1.0463335514068604,
      "learning_rate": 4.445257865715213e-05,
      "loss": 3.0908,
      "step": 172100
    },
    {
      "epoch": 55.81847649918963,
      "grad_norm": 0.9392539858818054,
      "learning_rate": 4.444933506325008e-05,
      "loss": 3.0734,
      "step": 172200
    },
    {
      "epoch": 55.850891410048625,
      "grad_norm": 0.8115541934967041,
      "learning_rate": 4.444609146934804e-05,
      "loss": 3.0818,
      "step": 172300
    },
    {
      "epoch": 55.883306320907614,
      "grad_norm": 0.9366373419761658,
      "learning_rate": 4.4442847875446e-05,
      "loss": 3.097,
      "step": 172400
    },
    {
      "epoch": 55.91572123176661,
      "grad_norm": 0.8653913140296936,
      "learning_rate": 4.443960428154395e-05,
      "loss": 3.0923,
      "step": 172500
    },
    {
      "epoch": 55.94813614262561,
      "grad_norm": 1.118883490562439,
      "learning_rate": 4.443636068764191e-05,
      "loss": 3.0945,
      "step": 172600
    },
    {
      "epoch": 55.980551053484604,
      "grad_norm": 0.8975049257278442,
      "learning_rate": 4.443311709373987e-05,
      "loss": 3.0723,
      "step": 172700
    },
    {
      "epoch": 56.0,
      "eval_bleu": 1.2919208120993049,
      "eval_loss": 3.7712948322296143,
      "eval_runtime": 4.8405,
      "eval_samples_per_second": 101.641,
      "eval_steps_per_second": 1.653,
      "step": 172760
    },
    {
      "epoch": 56.0129659643436,
      "grad_norm": 0.8876650333404541,
      "learning_rate": 4.442987349983782e-05,
      "loss": 3.0626,
      "step": 172800
    },
    {
      "epoch": 56.045380875202596,
      "grad_norm": 0.8401455879211426,
      "learning_rate": 4.442662990593578e-05,
      "loss": 3.0801,
      "step": 172900
    },
    {
      "epoch": 56.077795786061586,
      "grad_norm": 0.8640024662017822,
      "learning_rate": 4.442338631203374e-05,
      "loss": 3.0729,
      "step": 173000
    },
    {
      "epoch": 56.11021069692058,
      "grad_norm": 0.8933972120285034,
      "learning_rate": 4.442014271813169e-05,
      "loss": 3.0717,
      "step": 173100
    },
    {
      "epoch": 56.14262560777958,
      "grad_norm": 0.9339763522148132,
      "learning_rate": 4.441689912422965e-05,
      "loss": 3.0875,
      "step": 173200
    },
    {
      "epoch": 56.175040518638575,
      "grad_norm": 0.9704414010047913,
      "learning_rate": 4.44136555303276e-05,
      "loss": 3.078,
      "step": 173300
    },
    {
      "epoch": 56.20745542949757,
      "grad_norm": 0.8429357409477234,
      "learning_rate": 4.441041193642556e-05,
      "loss": 3.0801,
      "step": 173400
    },
    {
      "epoch": 56.23987034035656,
      "grad_norm": 0.8911164402961731,
      "learning_rate": 4.440716834252352e-05,
      "loss": 3.0851,
      "step": 173500
    },
    {
      "epoch": 56.27228525121556,
      "grad_norm": 0.8735036253929138,
      "learning_rate": 4.440392474862147e-05,
      "loss": 3.066,
      "step": 173600
    },
    {
      "epoch": 56.30470016207455,
      "grad_norm": 0.8166907429695129,
      "learning_rate": 4.440068115471943e-05,
      "loss": 3.0703,
      "step": 173700
    },
    {
      "epoch": 56.33711507293355,
      "grad_norm": 0.9348018765449524,
      "learning_rate": 4.439743756081739e-05,
      "loss": 3.0692,
      "step": 173800
    },
    {
      "epoch": 56.369529983792546,
      "grad_norm": 1.129471778869629,
      "learning_rate": 4.439419396691534e-05,
      "loss": 3.0846,
      "step": 173900
    },
    {
      "epoch": 56.40194489465154,
      "grad_norm": 0.8134592175483704,
      "learning_rate": 4.43909503730133e-05,
      "loss": 3.0957,
      "step": 174000
    },
    {
      "epoch": 56.43435980551053,
      "grad_norm": 0.9984643459320068,
      "learning_rate": 4.438770677911126e-05,
      "loss": 3.08,
      "step": 174100
    },
    {
      "epoch": 56.46677471636953,
      "grad_norm": 1.1238782405853271,
      "learning_rate": 4.4384463185209216e-05,
      "loss": 3.0834,
      "step": 174200
    },
    {
      "epoch": 56.499189627228525,
      "grad_norm": 0.9437011480331421,
      "learning_rate": 4.4381219591307175e-05,
      "loss": 3.0858,
      "step": 174300
    },
    {
      "epoch": 56.53160453808752,
      "grad_norm": 0.9418139457702637,
      "learning_rate": 4.437797599740513e-05,
      "loss": 3.0734,
      "step": 174400
    },
    {
      "epoch": 56.56401944894652,
      "grad_norm": 0.9815011024475098,
      "learning_rate": 4.4374732403503086e-05,
      "loss": 3.0809,
      "step": 174500
    },
    {
      "epoch": 56.596434359805514,
      "grad_norm": 1.0073981285095215,
      "learning_rate": 4.4371488809601044e-05,
      "loss": 3.0709,
      "step": 174600
    },
    {
      "epoch": 56.6288492706645,
      "grad_norm": 0.8952256441116333,
      "learning_rate": 4.4368245215698996e-05,
      "loss": 3.0831,
      "step": 174700
    },
    {
      "epoch": 56.6612641815235,
      "grad_norm": 1.0122112035751343,
      "learning_rate": 4.4365034057735974e-05,
      "loss": 3.095,
      "step": 174800
    },
    {
      "epoch": 56.693679092382496,
      "grad_norm": 1.1019912958145142,
      "learning_rate": 4.436179046383393e-05,
      "loss": 3.0921,
      "step": 174900
    },
    {
      "epoch": 56.72609400324149,
      "grad_norm": 1.0311636924743652,
      "learning_rate": 4.435854686993189e-05,
      "loss": 3.0615,
      "step": 175000
    },
    {
      "epoch": 56.75850891410049,
      "grad_norm": 0.920150876045227,
      "learning_rate": 4.4355303276029843e-05,
      "loss": 3.069,
      "step": 175100
    },
    {
      "epoch": 56.79092382495948,
      "grad_norm": 1.040536880493164,
      "learning_rate": 4.43520596821278e-05,
      "loss": 3.0791,
      "step": 175200
    },
    {
      "epoch": 56.823338735818474,
      "grad_norm": 0.8571653366088867,
      "learning_rate": 4.434881608822576e-05,
      "loss": 3.081,
      "step": 175300
    },
    {
      "epoch": 56.85575364667747,
      "grad_norm": 1.0282868146896362,
      "learning_rate": 4.434557249432371e-05,
      "loss": 3.0663,
      "step": 175400
    },
    {
      "epoch": 56.88816855753647,
      "grad_norm": 0.915986955165863,
      "learning_rate": 4.434232890042167e-05,
      "loss": 3.0762,
      "step": 175500
    },
    {
      "epoch": 56.92058346839546,
      "grad_norm": 0.9570252895355225,
      "learning_rate": 4.4339085306519624e-05,
      "loss": 3.0924,
      "step": 175600
    },
    {
      "epoch": 56.95299837925446,
      "grad_norm": 0.9513145685195923,
      "learning_rate": 4.433584171261758e-05,
      "loss": 3.0804,
      "step": 175700
    },
    {
      "epoch": 56.98541329011345,
      "grad_norm": 0.9313979744911194,
      "learning_rate": 4.433259811871554e-05,
      "loss": 3.0809,
      "step": 175800
    },
    {
      "epoch": 57.0,
      "eval_bleu": 1.0905839461629254,
      "eval_loss": 3.7697227001190186,
      "eval_runtime": 4.7383,
      "eval_samples_per_second": 103.835,
      "eval_steps_per_second": 1.688,
      "step": 175845
    },
    {
      "epoch": 57.017828200972446,
      "grad_norm": 0.9048677682876587,
      "learning_rate": 4.432935452481349e-05,
      "loss": 3.0576,
      "step": 175900
    },
    {
      "epoch": 57.05024311183144,
      "grad_norm": 0.897935688495636,
      "learning_rate": 4.432611093091145e-05,
      "loss": 3.0718,
      "step": 176000
    },
    {
      "epoch": 57.08265802269044,
      "grad_norm": 0.9602908492088318,
      "learning_rate": 4.432286733700941e-05,
      "loss": 3.0746,
      "step": 176100
    },
    {
      "epoch": 57.115072933549435,
      "grad_norm": 0.9436280727386475,
      "learning_rate": 4.431962374310736e-05,
      "loss": 3.0611,
      "step": 176200
    },
    {
      "epoch": 57.14748784440843,
      "grad_norm": 1.329330563545227,
      "learning_rate": 4.431638014920532e-05,
      "loss": 3.0811,
      "step": 176300
    },
    {
      "epoch": 57.17990275526742,
      "grad_norm": 0.8829637765884399,
      "learning_rate": 4.431313655530328e-05,
      "loss": 3.0748,
      "step": 176400
    },
    {
      "epoch": 57.21231766612642,
      "grad_norm": 1.1503254175186157,
      "learning_rate": 4.430989296140123e-05,
      "loss": 3.0562,
      "step": 176500
    },
    {
      "epoch": 57.24473257698541,
      "grad_norm": 0.9482676386833191,
      "learning_rate": 4.430664936749919e-05,
      "loss": 3.081,
      "step": 176600
    },
    {
      "epoch": 57.27714748784441,
      "grad_norm": 0.8969950675964355,
      "learning_rate": 4.430340577359714e-05,
      "loss": 3.0754,
      "step": 176700
    },
    {
      "epoch": 57.309562398703406,
      "grad_norm": 1.0222902297973633,
      "learning_rate": 4.430019461563413e-05,
      "loss": 3.0495,
      "step": 176800
    },
    {
      "epoch": 57.341977309562395,
      "grad_norm": 0.9790005087852478,
      "learning_rate": 4.429695102173208e-05,
      "loss": 3.078,
      "step": 176900
    },
    {
      "epoch": 57.37439222042139,
      "grad_norm": 1.0091127157211304,
      "learning_rate": 4.429370742783004e-05,
      "loss": 3.052,
      "step": 177000
    },
    {
      "epoch": 57.40680713128039,
      "grad_norm": 0.9778331518173218,
      "learning_rate": 4.429046383392799e-05,
      "loss": 3.0702,
      "step": 177100
    },
    {
      "epoch": 57.439222042139384,
      "grad_norm": 0.7851403951644897,
      "learning_rate": 4.428722024002595e-05,
      "loss": 3.0939,
      "step": 177200
    },
    {
      "epoch": 57.47163695299838,
      "grad_norm": 0.9741658568382263,
      "learning_rate": 4.428397664612391e-05,
      "loss": 3.0816,
      "step": 177300
    },
    {
      "epoch": 57.50405186385738,
      "grad_norm": 0.8399521708488464,
      "learning_rate": 4.428073305222186e-05,
      "loss": 3.0872,
      "step": 177400
    },
    {
      "epoch": 57.53646677471637,
      "grad_norm": 0.9239978790283203,
      "learning_rate": 4.427748945831982e-05,
      "loss": 3.0647,
      "step": 177500
    },
    {
      "epoch": 57.56888168557536,
      "grad_norm": 0.8352873921394348,
      "learning_rate": 4.427424586441778e-05,
      "loss": 3.0993,
      "step": 177600
    },
    {
      "epoch": 57.60129659643436,
      "grad_norm": 1.0507447719573975,
      "learning_rate": 4.4271002270515736e-05,
      "loss": 3.0738,
      "step": 177700
    },
    {
      "epoch": 57.633711507293356,
      "grad_norm": 0.8412164449691772,
      "learning_rate": 4.426775867661369e-05,
      "loss": 3.0629,
      "step": 177800
    },
    {
      "epoch": 57.66612641815235,
      "grad_norm": 0.9211402535438538,
      "learning_rate": 4.4264515082711646e-05,
      "loss": 3.0784,
      "step": 177900
    },
    {
      "epoch": 57.69854132901135,
      "grad_norm": 0.9064056277275085,
      "learning_rate": 4.4261271488809605e-05,
      "loss": 3.0827,
      "step": 178000
    },
    {
      "epoch": 57.73095623987034,
      "grad_norm": 0.9838758111000061,
      "learning_rate": 4.4258027894907564e-05,
      "loss": 3.0975,
      "step": 178100
    },
    {
      "epoch": 57.763371150729334,
      "grad_norm": 1.0438121557235718,
      "learning_rate": 4.4254784301005516e-05,
      "loss": 3.085,
      "step": 178200
    },
    {
      "epoch": 57.79578606158833,
      "grad_norm": 1.1364717483520508,
      "learning_rate": 4.4251540707103475e-05,
      "loss": 3.0736,
      "step": 178300
    },
    {
      "epoch": 57.82820097244733,
      "grad_norm": 0.8668408989906311,
      "learning_rate": 4.424829711320143e-05,
      "loss": 3.0676,
      "step": 178400
    },
    {
      "epoch": 57.86061588330632,
      "grad_norm": 0.9514926671981812,
      "learning_rate": 4.4245053519299385e-05,
      "loss": 3.0893,
      "step": 178500
    },
    {
      "epoch": 57.89303079416531,
      "grad_norm": 0.8557425737380981,
      "learning_rate": 4.4241809925397344e-05,
      "loss": 3.0838,
      "step": 178600
    },
    {
      "epoch": 57.92544570502431,
      "grad_norm": 0.9923211932182312,
      "learning_rate": 4.42385663314953e-05,
      "loss": 3.0638,
      "step": 178700
    },
    {
      "epoch": 57.957860615883305,
      "grad_norm": 0.8923498392105103,
      "learning_rate": 4.423535517353228e-05,
      "loss": 3.0919,
      "step": 178800
    },
    {
      "epoch": 57.9902755267423,
      "grad_norm": 0.9183527231216431,
      "learning_rate": 4.423214401556925e-05,
      "loss": 3.0716,
      "step": 178900
    },
    {
      "epoch": 58.0,
      "eval_bleu": 1.218304024487825,
      "eval_loss": 3.7746798992156982,
      "eval_runtime": 4.6522,
      "eval_samples_per_second": 105.757,
      "eval_steps_per_second": 1.72,
      "step": 178930
    },
    {
      "epoch": 58.0226904376013,
      "grad_norm": 0.9891571402549744,
      "learning_rate": 4.422890042166721e-05,
      "loss": 3.0686,
      "step": 179000
    },
    {
      "epoch": 58.055105348460295,
      "grad_norm": 1.034783124923706,
      "learning_rate": 4.422565682776517e-05,
      "loss": 3.0712,
      "step": 179100
    },
    {
      "epoch": 58.087520259319284,
      "grad_norm": 1.1537972688674927,
      "learning_rate": 4.422241323386313e-05,
      "loss": 3.0625,
      "step": 179200
    },
    {
      "epoch": 58.11993517017828,
      "grad_norm": 1.0361486673355103,
      "learning_rate": 4.421916963996108e-05,
      "loss": 3.0676,
      "step": 179300
    },
    {
      "epoch": 58.15235008103728,
      "grad_norm": 1.105081558227539,
      "learning_rate": 4.421592604605904e-05,
      "loss": 3.0897,
      "step": 179400
    },
    {
      "epoch": 58.18476499189627,
      "grad_norm": 1.1052169799804688,
      "learning_rate": 4.421268245215699e-05,
      "loss": 3.0568,
      "step": 179500
    },
    {
      "epoch": 58.21717990275527,
      "grad_norm": 0.8295475840568542,
      "learning_rate": 4.420943885825495e-05,
      "loss": 3.0565,
      "step": 179600
    },
    {
      "epoch": 58.249594813614266,
      "grad_norm": 0.8754131197929382,
      "learning_rate": 4.420619526435291e-05,
      "loss": 3.0649,
      "step": 179700
    },
    {
      "epoch": 58.282009724473255,
      "grad_norm": 1.1506290435791016,
      "learning_rate": 4.420295167045086e-05,
      "loss": 3.068,
      "step": 179800
    },
    {
      "epoch": 58.31442463533225,
      "grad_norm": 0.9906895756721497,
      "learning_rate": 4.419970807654882e-05,
      "loss": 3.0873,
      "step": 179900
    },
    {
      "epoch": 58.34683954619125,
      "grad_norm": 1.1105040311813354,
      "learning_rate": 4.419646448264678e-05,
      "loss": 3.0453,
      "step": 180000
    },
    {
      "epoch": 58.379254457050244,
      "grad_norm": 0.8596630096435547,
      "learning_rate": 4.419322088874473e-05,
      "loss": 3.0676,
      "step": 180100
    },
    {
      "epoch": 58.41166936790924,
      "grad_norm": 0.9051358103752136,
      "learning_rate": 4.418997729484269e-05,
      "loss": 3.0785,
      "step": 180200
    },
    {
      "epoch": 58.44408427876823,
      "grad_norm": 1.0401694774627686,
      "learning_rate": 4.418673370094065e-05,
      "loss": 3.0814,
      "step": 180300
    },
    {
      "epoch": 58.476499189627226,
      "grad_norm": 0.9706991314888,
      "learning_rate": 4.41834901070386e-05,
      "loss": 3.0666,
      "step": 180400
    },
    {
      "epoch": 58.50891410048622,
      "grad_norm": 0.9607043266296387,
      "learning_rate": 4.418024651313656e-05,
      "loss": 3.0686,
      "step": 180500
    },
    {
      "epoch": 58.54132901134522,
      "grad_norm": 1.1464266777038574,
      "learning_rate": 4.417700291923451e-05,
      "loss": 3.0679,
      "step": 180600
    },
    {
      "epoch": 58.573743922204216,
      "grad_norm": 0.9490071535110474,
      "learning_rate": 4.417375932533247e-05,
      "loss": 3.0705,
      "step": 180700
    },
    {
      "epoch": 58.60615883306321,
      "grad_norm": 0.99520343542099,
      "learning_rate": 4.417051573143043e-05,
      "loss": 3.0637,
      "step": 180800
    },
    {
      "epoch": 58.6385737439222,
      "grad_norm": 0.9586635828018188,
      "learning_rate": 4.416727213752838e-05,
      "loss": 3.0771,
      "step": 180900
    },
    {
      "epoch": 58.6709886547812,
      "grad_norm": 0.9579364657402039,
      "learning_rate": 4.416402854362634e-05,
      "loss": 3.066,
      "step": 181000
    },
    {
      "epoch": 58.703403565640194,
      "grad_norm": 1.0461047887802124,
      "learning_rate": 4.4160784949724296e-05,
      "loss": 3.0545,
      "step": 181100
    },
    {
      "epoch": 58.73581847649919,
      "grad_norm": 1.0280810594558716,
      "learning_rate": 4.415754135582225e-05,
      "loss": 3.0635,
      "step": 181200
    },
    {
      "epoch": 58.76823338735819,
      "grad_norm": 0.9722675085067749,
      "learning_rate": 4.415429776192021e-05,
      "loss": 3.0761,
      "step": 181300
    },
    {
      "epoch": 58.80064829821718,
      "grad_norm": 1.2333132028579712,
      "learning_rate": 4.4151054168018166e-05,
      "loss": 3.0773,
      "step": 181400
    },
    {
      "epoch": 58.83306320907617,
      "grad_norm": 1.0046762228012085,
      "learning_rate": 4.4147810574116125e-05,
      "loss": 3.0862,
      "step": 181500
    },
    {
      "epoch": 58.86547811993517,
      "grad_norm": 0.9400038123130798,
      "learning_rate": 4.4144566980214083e-05,
      "loss": 3.0838,
      "step": 181600
    },
    {
      "epoch": 58.897893030794165,
      "grad_norm": 1.117648959159851,
      "learning_rate": 4.4141323386312035e-05,
      "loss": 3.0705,
      "step": 181700
    },
    {
      "epoch": 58.93030794165316,
      "grad_norm": 0.8535289168357849,
      "learning_rate": 4.4138079792409994e-05,
      "loss": 3.0982,
      "step": 181800
    },
    {
      "epoch": 58.96272285251216,
      "grad_norm": 0.8865091800689697,
      "learning_rate": 4.413483619850795e-05,
      "loss": 3.0826,
      "step": 181900
    },
    {
      "epoch": 58.99513776337115,
      "grad_norm": 1.030324101448059,
      "learning_rate": 4.4131625040544924e-05,
      "loss": 3.0857,
      "step": 182000
    },
    {
      "epoch": 59.0,
      "eval_bleu": 1.3125946033464335,
      "eval_loss": 3.771334171295166,
      "eval_runtime": 4.9336,
      "eval_samples_per_second": 99.724,
      "eval_steps_per_second": 1.622,
      "step": 182015
    },
    {
      "epoch": 59.027552674230144,
      "grad_norm": 1.0162546634674072,
      "learning_rate": 4.412838144664288e-05,
      "loss": 3.0592,
      "step": 182100
    },
    {
      "epoch": 59.05996758508914,
      "grad_norm": 0.9468590021133423,
      "learning_rate": 4.412513785274084e-05,
      "loss": 3.0768,
      "step": 182200
    },
    {
      "epoch": 59.09238249594814,
      "grad_norm": 1.2393648624420166,
      "learning_rate": 4.41218942588388e-05,
      "loss": 3.0602,
      "step": 182300
    },
    {
      "epoch": 59.12479740680713,
      "grad_norm": 0.9997361898422241,
      "learning_rate": 4.411865066493675e-05,
      "loss": 3.0596,
      "step": 182400
    },
    {
      "epoch": 59.15721231766613,
      "grad_norm": 1.0541558265686035,
      "learning_rate": 4.411540707103471e-05,
      "loss": 3.0497,
      "step": 182500
    },
    {
      "epoch": 59.18962722852512,
      "grad_norm": 0.8438422679901123,
      "learning_rate": 4.411216347713267e-05,
      "loss": 3.0735,
      "step": 182600
    },
    {
      "epoch": 59.222042139384115,
      "grad_norm": 0.9710532426834106,
      "learning_rate": 4.410891988323062e-05,
      "loss": 3.0717,
      "step": 182700
    },
    {
      "epoch": 59.25445705024311,
      "grad_norm": 0.9883425235748291,
      "learning_rate": 4.410567628932858e-05,
      "loss": 3.0516,
      "step": 182800
    },
    {
      "epoch": 59.28687196110211,
      "grad_norm": 1.0859140157699585,
      "learning_rate": 4.410243269542653e-05,
      "loss": 3.067,
      "step": 182900
    },
    {
      "epoch": 59.319286871961104,
      "grad_norm": 0.9958247542381287,
      "learning_rate": 4.409918910152449e-05,
      "loss": 3.0697,
      "step": 183000
    },
    {
      "epoch": 59.3517017828201,
      "grad_norm": 1.1476421356201172,
      "learning_rate": 4.409594550762245e-05,
      "loss": 3.0758,
      "step": 183100
    },
    {
      "epoch": 59.38411669367909,
      "grad_norm": 1.0573232173919678,
      "learning_rate": 4.40927019137204e-05,
      "loss": 3.0792,
      "step": 183200
    },
    {
      "epoch": 59.416531604538086,
      "grad_norm": 1.0666143894195557,
      "learning_rate": 4.408949075575738e-05,
      "loss": 3.0739,
      "step": 183300
    },
    {
      "epoch": 59.44894651539708,
      "grad_norm": 0.8798184990882874,
      "learning_rate": 4.408624716185534e-05,
      "loss": 3.0409,
      "step": 183400
    },
    {
      "epoch": 59.48136142625608,
      "grad_norm": 0.9080670475959778,
      "learning_rate": 4.40830035679533e-05,
      "loss": 3.068,
      "step": 183500
    },
    {
      "epoch": 59.513776337115075,
      "grad_norm": 1.0992388725280762,
      "learning_rate": 4.407975997405125e-05,
      "loss": 3.0803,
      "step": 183600
    },
    {
      "epoch": 59.546191247974065,
      "grad_norm": 1.0189086198806763,
      "learning_rate": 4.407651638014921e-05,
      "loss": 3.0737,
      "step": 183700
    },
    {
      "epoch": 59.57860615883306,
      "grad_norm": 0.9760293364524841,
      "learning_rate": 4.4073272786247166e-05,
      "loss": 3.0659,
      "step": 183800
    },
    {
      "epoch": 59.61102106969206,
      "grad_norm": 1.0488795042037964,
      "learning_rate": 4.407002919234512e-05,
      "loss": 3.0511,
      "step": 183900
    },
    {
      "epoch": 59.643435980551054,
      "grad_norm": 0.8668627142906189,
      "learning_rate": 4.406678559844308e-05,
      "loss": 3.0973,
      "step": 184000
    },
    {
      "epoch": 59.67585089141005,
      "grad_norm": 0.9437422752380371,
      "learning_rate": 4.4063542004541036e-05,
      "loss": 3.0608,
      "step": 184100
    },
    {
      "epoch": 59.70826580226905,
      "grad_norm": 1.1421325206756592,
      "learning_rate": 4.406029841063899e-05,
      "loss": 3.0483,
      "step": 184200
    },
    {
      "epoch": 59.740680713128036,
      "grad_norm": 1.2751518487930298,
      "learning_rate": 4.4057054816736947e-05,
      "loss": 3.0669,
      "step": 184300
    },
    {
      "epoch": 59.77309562398703,
      "grad_norm": 0.9740843176841736,
      "learning_rate": 4.40538112228349e-05,
      "loss": 3.0476,
      "step": 184400
    },
    {
      "epoch": 59.80551053484603,
      "grad_norm": 0.8945028781890869,
      "learning_rate": 4.405056762893286e-05,
      "loss": 3.0732,
      "step": 184500
    },
    {
      "epoch": 59.837925445705025,
      "grad_norm": 0.9093775749206543,
      "learning_rate": 4.4047324035030816e-05,
      "loss": 3.0808,
      "step": 184600
    },
    {
      "epoch": 59.87034035656402,
      "grad_norm": 0.958479106426239,
      "learning_rate": 4.404408044112877e-05,
      "loss": 3.0865,
      "step": 184700
    },
    {
      "epoch": 59.90275526742302,
      "grad_norm": 0.9988131523132324,
      "learning_rate": 4.404083684722673e-05,
      "loss": 3.0767,
      "step": 184800
    },
    {
      "epoch": 59.93517017828201,
      "grad_norm": 0.9560626149177551,
      "learning_rate": 4.4037593253324685e-05,
      "loss": 3.0575,
      "step": 184900
    },
    {
      "epoch": 59.967585089141004,
      "grad_norm": 0.9048149585723877,
      "learning_rate": 4.4034349659422644e-05,
      "loss": 3.0515,
      "step": 185000
    },
    {
      "epoch": 60.0,
      "grad_norm": 0.9394872784614563,
      "learning_rate": 4.40311060655206e-05,
      "loss": 3.0761,
      "step": 185100
    },
    {
      "epoch": 60.0,
      "eval_bleu": 1.1745726010883217,
      "eval_loss": 3.771141767501831,
      "eval_runtime": 4.2558,
      "eval_samples_per_second": 115.607,
      "eval_steps_per_second": 1.88,
      "step": 185100
    },
    {
      "epoch": 60.032414910858996,
      "grad_norm": 0.8226368427276611,
      "learning_rate": 4.4027862471618555e-05,
      "loss": 3.0483,
      "step": 185200
    },
    {
      "epoch": 60.06482982171799,
      "grad_norm": 1.102583408355713,
      "learning_rate": 4.4024618877716514e-05,
      "loss": 3.033,
      "step": 185300
    },
    {
      "epoch": 60.09724473257698,
      "grad_norm": 1.00700843334198,
      "learning_rate": 4.4021407719753485e-05,
      "loss": 3.0708,
      "step": 185400
    },
    {
      "epoch": 60.12965964343598,
      "grad_norm": 1.1970701217651367,
      "learning_rate": 4.4018164125851443e-05,
      "loss": 3.0613,
      "step": 185500
    },
    {
      "epoch": 60.162074554294975,
      "grad_norm": 0.8917521834373474,
      "learning_rate": 4.40149205319494e-05,
      "loss": 3.0599,
      "step": 185600
    },
    {
      "epoch": 60.19448946515397,
      "grad_norm": 0.9084467887878418,
      "learning_rate": 4.401167693804736e-05,
      "loss": 3.0574,
      "step": 185700
    },
    {
      "epoch": 60.22690437601297,
      "grad_norm": 0.9764646887779236,
      "learning_rate": 4.400843334414532e-05,
      "loss": 3.0497,
      "step": 185800
    },
    {
      "epoch": 60.259319286871964,
      "grad_norm": 1.176646113395691,
      "learning_rate": 4.400518975024327e-05,
      "loss": 3.0653,
      "step": 185900
    },
    {
      "epoch": 60.29173419773095,
      "grad_norm": 0.9441313743591309,
      "learning_rate": 4.400194615634123e-05,
      "loss": 3.0587,
      "step": 186000
    },
    {
      "epoch": 60.32414910858995,
      "grad_norm": 1.0887404680252075,
      "learning_rate": 4.399870256243919e-05,
      "loss": 3.0532,
      "step": 186100
    },
    {
      "epoch": 60.356564019448946,
      "grad_norm": 0.8637223839759827,
      "learning_rate": 4.399545896853714e-05,
      "loss": 3.0559,
      "step": 186200
    },
    {
      "epoch": 60.38897893030794,
      "grad_norm": 1.1171196699142456,
      "learning_rate": 4.39922153746351e-05,
      "loss": 3.0678,
      "step": 186300
    },
    {
      "epoch": 60.42139384116694,
      "grad_norm": 0.8717026710510254,
      "learning_rate": 4.398897178073306e-05,
      "loss": 3.0423,
      "step": 186400
    },
    {
      "epoch": 60.453808752025935,
      "grad_norm": 0.8688693642616272,
      "learning_rate": 4.398572818683101e-05,
      "loss": 3.0486,
      "step": 186500
    },
    {
      "epoch": 60.486223662884925,
      "grad_norm": 0.9162328243255615,
      "learning_rate": 4.398248459292897e-05,
      "loss": 3.089,
      "step": 186600
    },
    {
      "epoch": 60.51863857374392,
      "grad_norm": 0.9879987835884094,
      "learning_rate": 4.397924099902692e-05,
      "loss": 3.0615,
      "step": 186700
    },
    {
      "epoch": 60.55105348460292,
      "grad_norm": 0.9252604246139526,
      "learning_rate": 4.397599740512488e-05,
      "loss": 3.0629,
      "step": 186800
    },
    {
      "epoch": 60.583468395461914,
      "grad_norm": 0.8941739201545715,
      "learning_rate": 4.397275381122284e-05,
      "loss": 3.0883,
      "step": 186900
    },
    {
      "epoch": 60.61588330632091,
      "grad_norm": 0.8336390256881714,
      "learning_rate": 4.396951021732079e-05,
      "loss": 3.0489,
      "step": 187000
    },
    {
      "epoch": 60.6482982171799,
      "grad_norm": 0.8716663122177124,
      "learning_rate": 4.396626662341875e-05,
      "loss": 3.073,
      "step": 187100
    },
    {
      "epoch": 60.680713128038896,
      "grad_norm": 1.0754162073135376,
      "learning_rate": 4.396302302951671e-05,
      "loss": 3.0498,
      "step": 187200
    },
    {
      "epoch": 60.71312803889789,
      "grad_norm": 1.0052999258041382,
      "learning_rate": 4.395977943561466e-05,
      "loss": 3.0716,
      "step": 187300
    },
    {
      "epoch": 60.74554294975689,
      "grad_norm": 0.9895509481430054,
      "learning_rate": 4.395653584171262e-05,
      "loss": 3.059,
      "step": 187400
    },
    {
      "epoch": 60.777957860615885,
      "grad_norm": 0.9713923335075378,
      "learning_rate": 4.395329224781057e-05,
      "loss": 3.0612,
      "step": 187500
    },
    {
      "epoch": 60.81037277147488,
      "grad_norm": 0.7779287099838257,
      "learning_rate": 4.395004865390853e-05,
      "loss": 3.0732,
      "step": 187600
    },
    {
      "epoch": 60.84278768233387,
      "grad_norm": 1.0104321241378784,
      "learning_rate": 4.394680506000649e-05,
      "loss": 3.0694,
      "step": 187700
    },
    {
      "epoch": 60.87520259319287,
      "grad_norm": 1.1676678657531738,
      "learning_rate": 4.394356146610444e-05,
      "loss": 3.0632,
      "step": 187800
    },
    {
      "epoch": 60.90761750405186,
      "grad_norm": 1.0543310642242432,
      "learning_rate": 4.39403178722024e-05,
      "loss": 3.0591,
      "step": 187900
    },
    {
      "epoch": 60.94003241491086,
      "grad_norm": 0.8929482698440552,
      "learning_rate": 4.393707427830036e-05,
      "loss": 3.0739,
      "step": 188000
    },
    {
      "epoch": 60.972447325769856,
      "grad_norm": 0.9342983961105347,
      "learning_rate": 4.3933830684398317e-05,
      "loss": 3.0719,
      "step": 188100
    },
    {
      "epoch": 61.0,
      "eval_bleu": 1.2771470495050232,
      "eval_loss": 3.770902156829834,
      "eval_runtime": 4.5149,
      "eval_samples_per_second": 108.974,
      "eval_steps_per_second": 1.772,
      "step": 188185
    },
    {
      "epoch": 61.00486223662885,
      "grad_norm": 1.0028547048568726,
      "learning_rate": 4.3930587090496275e-05,
      "loss": 3.0755,
      "step": 188200
    },
    {
      "epoch": 61.03727714748784,
      "grad_norm": 0.9024468660354614,
      "learning_rate": 4.3927343496594234e-05,
      "loss": 3.0713,
      "step": 188300
    },
    {
      "epoch": 61.06969205834684,
      "grad_norm": 0.9333835244178772,
      "learning_rate": 4.3924099902692186e-05,
      "loss": 3.0559,
      "step": 188400
    },
    {
      "epoch": 61.102106969205835,
      "grad_norm": 0.9430490136146545,
      "learning_rate": 4.3920856308790145e-05,
      "loss": 3.0534,
      "step": 188500
    },
    {
      "epoch": 61.13452188006483,
      "grad_norm": 0.9487960338592529,
      "learning_rate": 4.39176127148881e-05,
      "loss": 3.0513,
      "step": 188600
    },
    {
      "epoch": 61.16693679092383,
      "grad_norm": 0.957923412322998,
      "learning_rate": 4.3914369120986055e-05,
      "loss": 3.0542,
      "step": 188700
    },
    {
      "epoch": 61.19935170178282,
      "grad_norm": 0.9974782466888428,
      "learning_rate": 4.3911125527084014e-05,
      "loss": 3.0627,
      "step": 188800
    },
    {
      "epoch": 61.23176661264181,
      "grad_norm": 0.8664445281028748,
      "learning_rate": 4.3907881933181966e-05,
      "loss": 3.0594,
      "step": 188900
    },
    {
      "epoch": 61.26418152350081,
      "grad_norm": 1.0057573318481445,
      "learning_rate": 4.3904638339279925e-05,
      "loss": 3.0594,
      "step": 189000
    },
    {
      "epoch": 61.296596434359806,
      "grad_norm": 0.9491165280342102,
      "learning_rate": 4.3901394745377884e-05,
      "loss": 3.0544,
      "step": 189100
    },
    {
      "epoch": 61.3290113452188,
      "grad_norm": 1.1215214729309082,
      "learning_rate": 4.3898151151475836e-05,
      "loss": 3.0474,
      "step": 189200
    },
    {
      "epoch": 61.3614262560778,
      "grad_norm": 0.8649635314941406,
      "learning_rate": 4.3894907557573794e-05,
      "loss": 3.0616,
      "step": 189300
    },
    {
      "epoch": 61.39384116693679,
      "grad_norm": 1.1284451484680176,
      "learning_rate": 4.389166396367175e-05,
      "loss": 3.0563,
      "step": 189400
    },
    {
      "epoch": 61.426256077795784,
      "grad_norm": 1.0374317169189453,
      "learning_rate": 4.3888420369769705e-05,
      "loss": 3.0471,
      "step": 189500
    },
    {
      "epoch": 61.45867098865478,
      "grad_norm": 0.9241070747375488,
      "learning_rate": 4.3885176775867664e-05,
      "loss": 3.0656,
      "step": 189600
    },
    {
      "epoch": 61.49108589951378,
      "grad_norm": 0.9279916882514954,
      "learning_rate": 4.3881933181965616e-05,
      "loss": 3.0598,
      "step": 189700
    },
    {
      "epoch": 61.523500810372774,
      "grad_norm": 0.8678724765777588,
      "learning_rate": 4.3878689588063575e-05,
      "loss": 3.0584,
      "step": 189800
    },
    {
      "epoch": 61.55591572123177,
      "grad_norm": 0.8811473846435547,
      "learning_rate": 4.387547843010055e-05,
      "loss": 3.0662,
      "step": 189900
    },
    {
      "epoch": 61.58833063209076,
      "grad_norm": 0.9932101964950562,
      "learning_rate": 4.387223483619851e-05,
      "loss": 3.0573,
      "step": 190000
    },
    {
      "epoch": 61.620745542949756,
      "grad_norm": 0.8941240906715393,
      "learning_rate": 4.386899124229646e-05,
      "loss": 3.0543,
      "step": 190100
    },
    {
      "epoch": 61.65316045380875,
      "grad_norm": 0.8203635811805725,
      "learning_rate": 4.386574764839442e-05,
      "loss": 3.0671,
      "step": 190200
    },
    {
      "epoch": 61.68557536466775,
      "grad_norm": 1.3489412069320679,
      "learning_rate": 4.386250405449238e-05,
      "loss": 3.0537,
      "step": 190300
    },
    {
      "epoch": 61.717990275526745,
      "grad_norm": 0.8611129522323608,
      "learning_rate": 4.385926046059033e-05,
      "loss": 3.0517,
      "step": 190400
    },
    {
      "epoch": 61.750405186385734,
      "grad_norm": 1.0961743593215942,
      "learning_rate": 4.385601686668829e-05,
      "loss": 3.0718,
      "step": 190500
    },
    {
      "epoch": 61.78282009724473,
      "grad_norm": 1.045393705368042,
      "learning_rate": 4.385277327278625e-05,
      "loss": 3.0449,
      "step": 190600
    },
    {
      "epoch": 61.81523500810373,
      "grad_norm": 0.8827307224273682,
      "learning_rate": 4.38495296788842e-05,
      "loss": 3.0662,
      "step": 190700
    },
    {
      "epoch": 61.84764991896272,
      "grad_norm": 1.0554745197296143,
      "learning_rate": 4.384628608498216e-05,
      "loss": 3.0646,
      "step": 190800
    },
    {
      "epoch": 61.88006482982172,
      "grad_norm": 0.9538054466247559,
      "learning_rate": 4.384304249108012e-05,
      "loss": 3.0613,
      "step": 190900
    },
    {
      "epoch": 61.912479740680716,
      "grad_norm": 0.8263408541679382,
      "learning_rate": 4.383979889717808e-05,
      "loss": 3.0577,
      "step": 191000
    },
    {
      "epoch": 61.944894651539705,
      "grad_norm": 0.9366182088851929,
      "learning_rate": 4.383655530327603e-05,
      "loss": 3.0663,
      "step": 191100
    },
    {
      "epoch": 61.9773095623987,
      "grad_norm": 0.9944483637809753,
      "learning_rate": 4.383331170937399e-05,
      "loss": 3.0584,
      "step": 191200
    },
    {
      "epoch": 62.0,
      "eval_bleu": 1.2649112635978008,
      "eval_loss": 3.780653953552246,
      "eval_runtime": 4.3353,
      "eval_samples_per_second": 113.486,
      "eval_steps_per_second": 1.845,
      "step": 191270
    },
    {
      "epoch": 62.0097244732577,
      "grad_norm": 1.1764910221099854,
      "learning_rate": 4.383006811547195e-05,
      "loss": 3.0551,
      "step": 191300
    },
    {
      "epoch": 62.042139384116695,
      "grad_norm": 0.9573681354522705,
      "learning_rate": 4.3826824521569906e-05,
      "loss": 3.0488,
      "step": 191400
    },
    {
      "epoch": 62.07455429497569,
      "grad_norm": 1.127432942390442,
      "learning_rate": 4.382358092766786e-05,
      "loss": 3.0442,
      "step": 191500
    },
    {
      "epoch": 62.10696920583469,
      "grad_norm": 0.995548665523529,
      "learning_rate": 4.382033733376582e-05,
      "loss": 3.0632,
      "step": 191600
    },
    {
      "epoch": 62.13938411669368,
      "grad_norm": 1.067821979522705,
      "learning_rate": 4.3817093739863776e-05,
      "loss": 3.0468,
      "step": 191700
    },
    {
      "epoch": 62.17179902755267,
      "grad_norm": 1.134963870048523,
      "learning_rate": 4.381385014596173e-05,
      "loss": 3.0564,
      "step": 191800
    },
    {
      "epoch": 62.20421393841167,
      "grad_norm": 1.0378031730651855,
      "learning_rate": 4.3810606552059687e-05,
      "loss": 3.0478,
      "step": 191900
    },
    {
      "epoch": 62.236628849270666,
      "grad_norm": 0.9893219470977783,
      "learning_rate": 4.380736295815764e-05,
      "loss": 3.0635,
      "step": 192000
    },
    {
      "epoch": 62.26904376012966,
      "grad_norm": 0.8239917755126953,
      "learning_rate": 4.38041193642556e-05,
      "loss": 3.0492,
      "step": 192100
    },
    {
      "epoch": 62.30145867098865,
      "grad_norm": 0.8279983997344971,
      "learning_rate": 4.3800875770353556e-05,
      "loss": 3.0571,
      "step": 192200
    },
    {
      "epoch": 62.33387358184765,
      "grad_norm": 0.9591814279556274,
      "learning_rate": 4.379763217645151e-05,
      "loss": 3.058,
      "step": 192300
    },
    {
      "epoch": 62.366288492706644,
      "grad_norm": 1.0036828517913818,
      "learning_rate": 4.379438858254947e-05,
      "loss": 3.0675,
      "step": 192400
    },
    {
      "epoch": 62.39870340356564,
      "grad_norm": 1.0044517517089844,
      "learning_rate": 4.3791144988647425e-05,
      "loss": 3.0466,
      "step": 192500
    },
    {
      "epoch": 62.43111831442464,
      "grad_norm": 0.9899506568908691,
      "learning_rate": 4.378790139474538e-05,
      "loss": 3.0419,
      "step": 192600
    },
    {
      "epoch": 62.46353322528363,
      "grad_norm": 0.9122377634048462,
      "learning_rate": 4.3784657800843336e-05,
      "loss": 3.0709,
      "step": 192700
    },
    {
      "epoch": 62.49594813614262,
      "grad_norm": 0.8799233436584473,
      "learning_rate": 4.3781414206941295e-05,
      "loss": 3.0591,
      "step": 192800
    },
    {
      "epoch": 62.52836304700162,
      "grad_norm": 1.065356731414795,
      "learning_rate": 4.377817061303925e-05,
      "loss": 3.0632,
      "step": 192900
    },
    {
      "epoch": 62.560777957860616,
      "grad_norm": 1.002855896949768,
      "learning_rate": 4.3774927019137206e-05,
      "loss": 3.0546,
      "step": 193000
    },
    {
      "epoch": 62.59319286871961,
      "grad_norm": 0.9872782826423645,
      "learning_rate": 4.377168342523516e-05,
      "loss": 3.051,
      "step": 193100
    },
    {
      "epoch": 62.62560777957861,
      "grad_norm": 1.1321728229522705,
      "learning_rate": 4.3768439831333116e-05,
      "loss": 3.0648,
      "step": 193200
    },
    {
      "epoch": 62.658022690437605,
      "grad_norm": 1.1143913269042969,
      "learning_rate": 4.3765196237431075e-05,
      "loss": 3.0634,
      "step": 193300
    },
    {
      "epoch": 62.690437601296594,
      "grad_norm": 1.0388262271881104,
      "learning_rate": 4.3761952643529034e-05,
      "loss": 3.0639,
      "step": 193400
    },
    {
      "epoch": 62.72285251215559,
      "grad_norm": 1.0367424488067627,
      "learning_rate": 4.3758709049626986e-05,
      "loss": 3.045,
      "step": 193500
    },
    {
      "epoch": 62.75526742301459,
      "grad_norm": 0.8622238039970398,
      "learning_rate": 4.3755465455724944e-05,
      "loss": 3.0473,
      "step": 193600
    },
    {
      "epoch": 62.78768233387358,
      "grad_norm": 0.9953689575195312,
      "learning_rate": 4.37522218618229e-05,
      "loss": 3.0421,
      "step": 193700
    },
    {
      "epoch": 62.82009724473258,
      "grad_norm": 0.9002699255943298,
      "learning_rate": 4.374897826792086e-05,
      "loss": 3.042,
      "step": 193800
    },
    {
      "epoch": 62.85251215559157,
      "grad_norm": 1.1570343971252441,
      "learning_rate": 4.374576710995783e-05,
      "loss": 3.0577,
      "step": 193900
    },
    {
      "epoch": 62.884927066450565,
      "grad_norm": 0.996429979801178,
      "learning_rate": 4.374252351605579e-05,
      "loss": 3.0547,
      "step": 194000
    },
    {
      "epoch": 62.91734197730956,
      "grad_norm": 0.9824897646903992,
      "learning_rate": 4.373927992215375e-05,
      "loss": 3.0589,
      "step": 194100
    },
    {
      "epoch": 62.94975688816856,
      "grad_norm": 0.937312126159668,
      "learning_rate": 4.373603632825171e-05,
      "loss": 3.0687,
      "step": 194200
    },
    {
      "epoch": 62.982171799027554,
      "grad_norm": 1.00648033618927,
      "learning_rate": 4.373279273434966e-05,
      "loss": 3.0554,
      "step": 194300
    },
    {
      "epoch": 63.0,
      "eval_bleu": 1.1137214574588679,
      "eval_loss": 3.7799429893493652,
      "eval_runtime": 4.3569,
      "eval_samples_per_second": 112.924,
      "eval_steps_per_second": 1.836,
      "step": 194355
    },
    {
      "epoch": 63.01458670988655,
      "grad_norm": 0.8836382627487183,
      "learning_rate": 4.372954914044762e-05,
      "loss": 3.0423,
      "step": 194400
    },
    {
      "epoch": 63.04700162074554,
      "grad_norm": 1.0779087543487549,
      "learning_rate": 4.372630554654558e-05,
      "loss": 3.0483,
      "step": 194500
    },
    {
      "epoch": 63.07941653160454,
      "grad_norm": 0.9877982139587402,
      "learning_rate": 4.372306195264353e-05,
      "loss": 3.0431,
      "step": 194600
    },
    {
      "epoch": 63.11183144246353,
      "grad_norm": 0.8537886142730713,
      "learning_rate": 4.371985079468051e-05,
      "loss": 3.0402,
      "step": 194700
    },
    {
      "epoch": 63.14424635332253,
      "grad_norm": 0.8825653791427612,
      "learning_rate": 4.371660720077847e-05,
      "loss": 3.0589,
      "step": 194800
    },
    {
      "epoch": 63.176661264181526,
      "grad_norm": 1.0236682891845703,
      "learning_rate": 4.3713363606876426e-05,
      "loss": 3.0582,
      "step": 194900
    },
    {
      "epoch": 63.20907617504052,
      "grad_norm": 1.00654137134552,
      "learning_rate": 4.37101524489134e-05,
      "loss": 3.0507,
      "step": 195000
    },
    {
      "epoch": 63.24149108589951,
      "grad_norm": 1.160554051399231,
      "learning_rate": 4.3706908855011356e-05,
      "loss": 3.0509,
      "step": 195100
    },
    {
      "epoch": 63.27390599675851,
      "grad_norm": 0.9959549903869629,
      "learning_rate": 4.3703665261109314e-05,
      "loss": 3.0182,
      "step": 195200
    },
    {
      "epoch": 63.306320907617504,
      "grad_norm": Infinity,
      "learning_rate": 4.3700421667207266e-05,
      "loss": 3.0499,
      "step": 195300
    },
    {
      "epoch": 63.3387358184765,
      "grad_norm": 0.9056329131126404,
      "learning_rate": 4.3697210509244244e-05,
      "loss": 3.0219,
      "step": 195400
    },
    {
      "epoch": 63.3711507293355,
      "grad_norm": 0.9769321084022522,
      "learning_rate": 4.3693966915342196e-05,
      "loss": 3.0388,
      "step": 195500
    },
    {
      "epoch": 63.403565640194486,
      "grad_norm": 0.894415020942688,
      "learning_rate": 4.3690723321440155e-05,
      "loss": 3.026,
      "step": 195600
    },
    {
      "epoch": 63.43598055105348,
      "grad_norm": 1.0139333009719849,
      "learning_rate": 4.3687479727538114e-05,
      "loss": 3.0596,
      "step": 195700
    },
    {
      "epoch": 63.46839546191248,
      "grad_norm": 1.0630230903625488,
      "learning_rate": 4.368423613363607e-05,
      "loss": 3.0809,
      "step": 195800
    },
    {
      "epoch": 63.500810372771475,
      "grad_norm": 0.8489808440208435,
      "learning_rate": 4.368099253973403e-05,
      "loss": 3.0604,
      "step": 195900
    },
    {
      "epoch": 63.53322528363047,
      "grad_norm": 0.9256314039230347,
      "learning_rate": 4.367774894583199e-05,
      "loss": 3.0763,
      "step": 196000
    },
    {
      "epoch": 63.56564019448947,
      "grad_norm": 0.9444587230682373,
      "learning_rate": 4.367450535192994e-05,
      "loss": 3.0554,
      "step": 196100
    },
    {
      "epoch": 63.59805510534846,
      "grad_norm": 0.8484342098236084,
      "learning_rate": 4.36712617580279e-05,
      "loss": 3.0554,
      "step": 196200
    },
    {
      "epoch": 63.630470016207454,
      "grad_norm": 1.0074366331100464,
      "learning_rate": 4.366801816412585e-05,
      "loss": 3.0363,
      "step": 196300
    },
    {
      "epoch": 63.66288492706645,
      "grad_norm": 1.222711443901062,
      "learning_rate": 4.366477457022381e-05,
      "loss": 3.0453,
      "step": 196400
    },
    {
      "epoch": 63.69529983792545,
      "grad_norm": 0.9307461380958557,
      "learning_rate": 4.366153097632177e-05,
      "loss": 3.0585,
      "step": 196500
    },
    {
      "epoch": 63.72771474878444,
      "grad_norm": 1.1002087593078613,
      "learning_rate": 4.365828738241972e-05,
      "loss": 3.052,
      "step": 196600
    },
    {
      "epoch": 63.76012965964344,
      "grad_norm": 0.9807111024856567,
      "learning_rate": 4.365504378851768e-05,
      "loss": 3.0441,
      "step": 196700
    },
    {
      "epoch": 63.79254457050243,
      "grad_norm": 0.9719679951667786,
      "learning_rate": 4.365180019461564e-05,
      "loss": 3.0644,
      "step": 196800
    },
    {
      "epoch": 63.824959481361425,
      "grad_norm": 0.9792425036430359,
      "learning_rate": 4.364855660071359e-05,
      "loss": 3.07,
      "step": 196900
    },
    {
      "epoch": 63.85737439222042,
      "grad_norm": 0.9182358980178833,
      "learning_rate": 4.364531300681155e-05,
      "loss": 3.0524,
      "step": 197000
    },
    {
      "epoch": 63.88978930307942,
      "grad_norm": 0.9443842768669128,
      "learning_rate": 4.36420694129095e-05,
      "loss": 3.0403,
      "step": 197100
    },
    {
      "epoch": 63.922204213938414,
      "grad_norm": 0.9240365028381348,
      "learning_rate": 4.363882581900746e-05,
      "loss": 3.0597,
      "step": 197200
    },
    {
      "epoch": 63.954619124797404,
      "grad_norm": 0.934362530708313,
      "learning_rate": 4.363558222510542e-05,
      "loss": 3.0525,
      "step": 197300
    },
    {
      "epoch": 63.9870340356564,
      "grad_norm": 0.7999933362007141,
      "learning_rate": 4.363233863120337e-05,
      "loss": 3.0532,
      "step": 197400
    },
    {
      "epoch": 64.0,
      "eval_bleu": 1.1755134202342348,
      "eval_loss": 3.7782771587371826,
      "eval_runtime": 4.6327,
      "eval_samples_per_second": 106.202,
      "eval_steps_per_second": 1.727,
      "step": 197440
    },
    {
      "epoch": 64.0194489465154,
      "grad_norm": 0.9670348167419434,
      "learning_rate": 4.362909503730133e-05,
      "loss": 3.0626,
      "step": 197500
    },
    {
      "epoch": 64.05186385737439,
      "grad_norm": 0.9270984530448914,
      "learning_rate": 4.362585144339929e-05,
      "loss": 3.0215,
      "step": 197600
    },
    {
      "epoch": 64.08427876823339,
      "grad_norm": 0.9270864725112915,
      "learning_rate": 4.362260784949724e-05,
      "loss": 3.0506,
      "step": 197700
    },
    {
      "epoch": 64.11669367909238,
      "grad_norm": 1.0777394771575928,
      "learning_rate": 4.36193642555952e-05,
      "loss": 3.0286,
      "step": 197800
    },
    {
      "epoch": 64.14910858995138,
      "grad_norm": 0.9762635231018066,
      "learning_rate": 4.361612066169316e-05,
      "loss": 3.0405,
      "step": 197900
    },
    {
      "epoch": 64.18152350081037,
      "grad_norm": 0.8856582641601562,
      "learning_rate": 4.361287706779111e-05,
      "loss": 3.0424,
      "step": 198000
    },
    {
      "epoch": 64.21393841166937,
      "grad_norm": 0.995111882686615,
      "learning_rate": 4.360963347388907e-05,
      "loss": 3.0648,
      "step": 198100
    },
    {
      "epoch": 64.24635332252836,
      "grad_norm": 0.9787915945053101,
      "learning_rate": 4.360638987998703e-05,
      "loss": 3.0477,
      "step": 198200
    },
    {
      "epoch": 64.27876823338735,
      "grad_norm": 1.0421338081359863,
      "learning_rate": 4.360314628608499e-05,
      "loss": 3.029,
      "step": 198300
    },
    {
      "epoch": 64.31118314424636,
      "grad_norm": 1.049673080444336,
      "learning_rate": 4.3599902692182946e-05,
      "loss": 3.0566,
      "step": 198400
    },
    {
      "epoch": 64.34359805510535,
      "grad_norm": 1.0479387044906616,
      "learning_rate": 4.35966590982809e-05,
      "loss": 3.0574,
      "step": 198500
    },
    {
      "epoch": 64.37601296596435,
      "grad_norm": 1.015287160873413,
      "learning_rate": 4.3593415504378856e-05,
      "loss": 3.0419,
      "step": 198600
    },
    {
      "epoch": 64.40842787682334,
      "grad_norm": 0.8631727695465088,
      "learning_rate": 4.3590171910476815e-05,
      "loss": 3.0385,
      "step": 198700
    },
    {
      "epoch": 64.44084278768233,
      "grad_norm": 1.1546121835708618,
      "learning_rate": 4.358692831657477e-05,
      "loss": 3.0364,
      "step": 198800
    },
    {
      "epoch": 64.47325769854133,
      "grad_norm": 1.0086637735366821,
      "learning_rate": 4.3583684722672726e-05,
      "loss": 3.0422,
      "step": 198900
    },
    {
      "epoch": 64.50567260940032,
      "grad_norm": 0.9907424449920654,
      "learning_rate": 4.3580441128770684e-05,
      "loss": 3.0484,
      "step": 199000
    },
    {
      "epoch": 64.53808752025932,
      "grad_norm": 1.0428730249404907,
      "learning_rate": 4.3577197534868636e-05,
      "loss": 3.0574,
      "step": 199100
    },
    {
      "epoch": 64.57050243111831,
      "grad_norm": 0.934903621673584,
      "learning_rate": 4.3573953940966595e-05,
      "loss": 3.0273,
      "step": 199200
    },
    {
      "epoch": 64.6029173419773,
      "grad_norm": 1.0910265445709229,
      "learning_rate": 4.357071034706455e-05,
      "loss": 3.0428,
      "step": 199300
    },
    {
      "epoch": 64.6353322528363,
      "grad_norm": 0.9274861812591553,
      "learning_rate": 4.3567466753162506e-05,
      "loss": 3.0326,
      "step": 199400
    },
    {
      "epoch": 64.6677471636953,
      "grad_norm": 1.1793371438980103,
      "learning_rate": 4.3564223159260465e-05,
      "loss": 3.0593,
      "step": 199500
    },
    {
      "epoch": 64.7001620745543,
      "grad_norm": 1.0921708345413208,
      "learning_rate": 4.3560979565358417e-05,
      "loss": 3.0407,
      "step": 199600
    },
    {
      "epoch": 64.73257698541329,
      "grad_norm": 0.9744971394538879,
      "learning_rate": 4.3557735971456375e-05,
      "loss": 3.0461,
      "step": 199700
    },
    {
      "epoch": 64.76499189627229,
      "grad_norm": 0.9142928123474121,
      "learning_rate": 4.3554492377554334e-05,
      "loss": 3.0497,
      "step": 199800
    },
    {
      "epoch": 64.79740680713128,
      "grad_norm": 0.9186406135559082,
      "learning_rate": 4.3551248783652286e-05,
      "loss": 3.0488,
      "step": 199900
    },
    {
      "epoch": 64.82982171799027,
      "grad_norm": 0.9940789341926575,
      "learning_rate": 4.3548005189750245e-05,
      "loss": 3.0484,
      "step": 200000
    },
    {
      "epoch": 64.86223662884927,
      "grad_norm": 1.054619550704956,
      "learning_rate": 4.35447615958482e-05,
      "loss": 3.0502,
      "step": 200100
    },
    {
      "epoch": 64.89465153970826,
      "grad_norm": 0.9971835613250732,
      "learning_rate": 4.3541518001946155e-05,
      "loss": 3.0571,
      "step": 200200
    },
    {
      "epoch": 64.92706645056727,
      "grad_norm": 1.1745442152023315,
      "learning_rate": 4.3538274408044114e-05,
      "loss": 3.0492,
      "step": 200300
    },
    {
      "epoch": 64.95948136142626,
      "grad_norm": 1.0438578128814697,
      "learning_rate": 4.3535030814142066e-05,
      "loss": 3.051,
      "step": 200400
    },
    {
      "epoch": 64.99189627228525,
      "grad_norm": 1.006197452545166,
      "learning_rate": 4.3531787220240025e-05,
      "loss": 3.0711,
      "step": 200500
    },
    {
      "epoch": 65.0,
      "eval_bleu": 1.1254436470330103,
      "eval_loss": 3.780184268951416,
      "eval_runtime": 4.8496,
      "eval_samples_per_second": 101.453,
      "eval_steps_per_second": 1.65,
      "step": 200525
    },
    {
      "epoch": 65.02431118314425,
      "grad_norm": 1.1703344583511353,
      "learning_rate": 4.3528543626337984e-05,
      "loss": 3.0549,
      "step": 200600
    },
    {
      "epoch": 65.05672609400324,
      "grad_norm": 0.8846217393875122,
      "learning_rate": 4.352530003243594e-05,
      "loss": 3.0183,
      "step": 200700
    },
    {
      "epoch": 65.08914100486224,
      "grad_norm": 1.1856942176818848,
      "learning_rate": 4.35220564385339e-05,
      "loss": 3.0464,
      "step": 200800
    },
    {
      "epoch": 65.12155591572123,
      "grad_norm": 1.0172741413116455,
      "learning_rate": 4.351881284463186e-05,
      "loss": 3.035,
      "step": 200900
    },
    {
      "epoch": 65.15397082658022,
      "grad_norm": 1.0093344449996948,
      "learning_rate": 4.351556925072981e-05,
      "loss": 3.0404,
      "step": 201000
    },
    {
      "epoch": 65.18638573743922,
      "grad_norm": 0.8948166966438293,
      "learning_rate": 4.351232565682777e-05,
      "loss": 3.0395,
      "step": 201100
    },
    {
      "epoch": 65.21880064829821,
      "grad_norm": 0.9195936322212219,
      "learning_rate": 4.350908206292572e-05,
      "loss": 3.0467,
      "step": 201200
    },
    {
      "epoch": 65.25121555915722,
      "grad_norm": 0.9950392246246338,
      "learning_rate": 4.350583846902368e-05,
      "loss": 3.0566,
      "step": 201300
    },
    {
      "epoch": 65.2836304700162,
      "grad_norm": 0.9310985803604126,
      "learning_rate": 4.350259487512164e-05,
      "loss": 3.0639,
      "step": 201400
    },
    {
      "epoch": 65.31604538087521,
      "grad_norm": 0.9778191447257996,
      "learning_rate": 4.349935128121959e-05,
      "loss": 3.047,
      "step": 201500
    },
    {
      "epoch": 65.3484602917342,
      "grad_norm": 1.0078799724578857,
      "learning_rate": 4.349610768731755e-05,
      "loss": 3.0207,
      "step": 201600
    },
    {
      "epoch": 65.38087520259319,
      "grad_norm": 1.0354281663894653,
      "learning_rate": 4.349286409341551e-05,
      "loss": 3.0475,
      "step": 201700
    },
    {
      "epoch": 65.41329011345219,
      "grad_norm": 1.084295630455017,
      "learning_rate": 4.348965293545249e-05,
      "loss": 3.0419,
      "step": 201800
    },
    {
      "epoch": 65.44570502431118,
      "grad_norm": 0.8872249126434326,
      "learning_rate": 4.348640934155044e-05,
      "loss": 3.0301,
      "step": 201900
    },
    {
      "epoch": 65.47811993517018,
      "grad_norm": 1.1794567108154297,
      "learning_rate": 4.34831657476484e-05,
      "loss": 3.0187,
      "step": 202000
    },
    {
      "epoch": 65.51053484602917,
      "grad_norm": 0.8854323029518127,
      "learning_rate": 4.347992215374636e-05,
      "loss": 3.0428,
      "step": 202100
    },
    {
      "epoch": 65.54294975688816,
      "grad_norm": 0.9921071529388428,
      "learning_rate": 4.347667855984431e-05,
      "loss": 3.0428,
      "step": 202200
    },
    {
      "epoch": 65.57536466774717,
      "grad_norm": 1.0995851755142212,
      "learning_rate": 4.347343496594227e-05,
      "loss": 3.0527,
      "step": 202300
    },
    {
      "epoch": 65.60777957860616,
      "grad_norm": 0.998117208480835,
      "learning_rate": 4.3470191372040226e-05,
      "loss": 3.0543,
      "step": 202400
    },
    {
      "epoch": 65.64019448946516,
      "grad_norm": 1.1028380393981934,
      "learning_rate": 4.346694777813818e-05,
      "loss": 3.0457,
      "step": 202500
    },
    {
      "epoch": 65.67260940032415,
      "grad_norm": 1.0331721305847168,
      "learning_rate": 4.346370418423614e-05,
      "loss": 3.0399,
      "step": 202600
    },
    {
      "epoch": 65.70502431118314,
      "grad_norm": 1.0302934646606445,
      "learning_rate": 4.346046059033409e-05,
      "loss": 3.046,
      "step": 202700
    },
    {
      "epoch": 65.73743922204214,
      "grad_norm": 0.9718247652053833,
      "learning_rate": 4.345721699643205e-05,
      "loss": 3.0522,
      "step": 202800
    },
    {
      "epoch": 65.76985413290113,
      "grad_norm": 1.0012487173080444,
      "learning_rate": 4.3453973402530006e-05,
      "loss": 3.0521,
      "step": 202900
    },
    {
      "epoch": 65.80226904376013,
      "grad_norm": 0.9215603470802307,
      "learning_rate": 4.345072980862796e-05,
      "loss": 3.0503,
      "step": 203000
    },
    {
      "epoch": 65.83468395461912,
      "grad_norm": 1.1252351999282837,
      "learning_rate": 4.344748621472592e-05,
      "loss": 3.0411,
      "step": 203100
    },
    {
      "epoch": 65.86709886547813,
      "grad_norm": 0.8905367851257324,
      "learning_rate": 4.3444242620823876e-05,
      "loss": 3.0476,
      "step": 203200
    },
    {
      "epoch": 65.89951377633712,
      "grad_norm": 1.1351019144058228,
      "learning_rate": 4.344099902692183e-05,
      "loss": 3.0438,
      "step": 203300
    },
    {
      "epoch": 65.9319286871961,
      "grad_norm": 0.8879687786102295,
      "learning_rate": 4.3437755433019787e-05,
      "loss": 3.0629,
      "step": 203400
    },
    {
      "epoch": 65.96434359805511,
      "grad_norm": 0.8141821622848511,
      "learning_rate": 4.3434511839117745e-05,
      "loss": 3.0376,
      "step": 203500
    },
    {
      "epoch": 65.9967585089141,
      "grad_norm": 0.9039072394371033,
      "learning_rate": 4.34312682452157e-05,
      "loss": 3.03,
      "step": 203600
    },
    {
      "epoch": 66.0,
      "eval_bleu": 1.1631872084580528,
      "eval_loss": 3.780597686767578,
      "eval_runtime": 4.1385,
      "eval_samples_per_second": 118.882,
      "eval_steps_per_second": 1.933,
      "step": 203610
    },
    {
      "epoch": 66.0291734197731,
      "grad_norm": 1.0182334184646606,
      "learning_rate": 4.3428024651313656e-05,
      "loss": 3.0221,
      "step": 203700
    },
    {
      "epoch": 66.06158833063209,
      "grad_norm": 1.0733033418655396,
      "learning_rate": 4.3424781057411615e-05,
      "loss": 3.0342,
      "step": 203800
    },
    {
      "epoch": 66.09400324149108,
      "grad_norm": 0.9387120604515076,
      "learning_rate": 4.3421537463509573e-05,
      "loss": 3.0391,
      "step": 203900
    },
    {
      "epoch": 66.12641815235008,
      "grad_norm": 0.9388461709022522,
      "learning_rate": 4.341829386960753e-05,
      "loss": 3.0376,
      "step": 204000
    },
    {
      "epoch": 66.15883306320907,
      "grad_norm": 0.9730757474899292,
      "learning_rate": 4.3415050275705484e-05,
      "loss": 3.0401,
      "step": 204100
    },
    {
      "epoch": 66.19124797406808,
      "grad_norm": 0.9859522581100464,
      "learning_rate": 4.341180668180344e-05,
      "loss": 3.0343,
      "step": 204200
    },
    {
      "epoch": 66.22366288492707,
      "grad_norm": 0.9018638730049133,
      "learning_rate": 4.34085630879014e-05,
      "loss": 3.0381,
      "step": 204300
    },
    {
      "epoch": 66.25607779578606,
      "grad_norm": 0.9052261114120483,
      "learning_rate": 4.3405319493999354e-05,
      "loss": 3.0473,
      "step": 204400
    },
    {
      "epoch": 66.28849270664506,
      "grad_norm": 1.137751579284668,
      "learning_rate": 4.340207590009731e-05,
      "loss": 3.0141,
      "step": 204500
    },
    {
      "epoch": 66.32090761750405,
      "grad_norm": 0.8870409727096558,
      "learning_rate": 4.3398832306195264e-05,
      "loss": 3.0427,
      "step": 204600
    },
    {
      "epoch": 66.35332252836305,
      "grad_norm": 1.0963623523712158,
      "learning_rate": 4.339558871229322e-05,
      "loss": 3.029,
      "step": 204700
    },
    {
      "epoch": 66.38573743922204,
      "grad_norm": 0.9300582408905029,
      "learning_rate": 4.33923775543302e-05,
      "loss": 3.0254,
      "step": 204800
    },
    {
      "epoch": 66.41815235008104,
      "grad_norm": 0.8602294921875,
      "learning_rate": 4.338913396042816e-05,
      "loss": 3.0307,
      "step": 204900
    },
    {
      "epoch": 66.45056726094003,
      "grad_norm": 1.0046895742416382,
      "learning_rate": 4.338589036652611e-05,
      "loss": 3.056,
      "step": 205000
    },
    {
      "epoch": 66.48298217179902,
      "grad_norm": 0.9586430191993713,
      "learning_rate": 4.338264677262407e-05,
      "loss": 3.0301,
      "step": 205100
    },
    {
      "epoch": 66.51539708265803,
      "grad_norm": 1.1178067922592163,
      "learning_rate": 4.337940317872203e-05,
      "loss": 3.0394,
      "step": 205200
    },
    {
      "epoch": 66.54781199351702,
      "grad_norm": 0.8983089923858643,
      "learning_rate": 4.337615958481998e-05,
      "loss": 3.0492,
      "step": 205300
    },
    {
      "epoch": 66.58022690437602,
      "grad_norm": 0.8471813201904297,
      "learning_rate": 4.337291599091794e-05,
      "loss": 3.0508,
      "step": 205400
    },
    {
      "epoch": 66.61264181523501,
      "grad_norm": 0.9718757271766663,
      "learning_rate": 4.33696723970159e-05,
      "loss": 3.0443,
      "step": 205500
    },
    {
      "epoch": 66.645056726094,
      "grad_norm": 0.9958570003509521,
      "learning_rate": 4.336642880311385e-05,
      "loss": 3.0196,
      "step": 205600
    },
    {
      "epoch": 66.677471636953,
      "grad_norm": 1.0056426525115967,
      "learning_rate": 4.336318520921181e-05,
      "loss": 3.0505,
      "step": 205700
    },
    {
      "epoch": 66.70988654781199,
      "grad_norm": 0.9996539950370789,
      "learning_rate": 4.335994161530976e-05,
      "loss": 3.0322,
      "step": 205800
    },
    {
      "epoch": 66.742301458671,
      "grad_norm": 1.2080883979797363,
      "learning_rate": 4.335669802140772e-05,
      "loss": 3.033,
      "step": 205900
    },
    {
      "epoch": 66.77471636952998,
      "grad_norm": 0.8555617928504944,
      "learning_rate": 4.335345442750568e-05,
      "loss": 3.0445,
      "step": 206000
    },
    {
      "epoch": 66.80713128038897,
      "grad_norm": 1.0349833965301514,
      "learning_rate": 4.335021083360363e-05,
      "loss": 3.0359,
      "step": 206100
    },
    {
      "epoch": 66.83954619124798,
      "grad_norm": 1.106604814529419,
      "learning_rate": 4.334696723970159e-05,
      "loss": 3.0335,
      "step": 206200
    },
    {
      "epoch": 66.87196110210697,
      "grad_norm": 0.9045965671539307,
      "learning_rate": 4.334372364579955e-05,
      "loss": 3.0482,
      "step": 206300
    },
    {
      "epoch": 66.90437601296597,
      "grad_norm": 1.0949839353561401,
      "learning_rate": 4.33404800518975e-05,
      "loss": 3.0344,
      "step": 206400
    },
    {
      "epoch": 66.93679092382496,
      "grad_norm": 0.9657273888587952,
      "learning_rate": 4.333723645799546e-05,
      "loss": 3.0468,
      "step": 206500
    },
    {
      "epoch": 66.96920583468396,
      "grad_norm": 1.0171905755996704,
      "learning_rate": 4.333399286409342e-05,
      "loss": 3.0468,
      "step": 206600
    },
    {
      "epoch": 67.0,
      "eval_bleu": 1.1639881616521353,
      "eval_loss": 3.7845418453216553,
      "eval_runtime": 4.3897,
      "eval_samples_per_second": 112.08,
      "eval_steps_per_second": 1.822,
      "step": 206695
    },
    {
      "epoch": 67.00162074554295,
      "grad_norm": 0.8773579597473145,
      "learning_rate": 4.3330749270191376e-05,
      "loss": 3.0432,
      "step": 206700
    },
    {
      "epoch": 67.03403565640194,
      "grad_norm": 0.8395187258720398,
      "learning_rate": 4.332750567628933e-05,
      "loss": 3.0305,
      "step": 206800
    },
    {
      "epoch": 67.06645056726094,
      "grad_norm": 1.0187628269195557,
      "learning_rate": 4.332426208238729e-05,
      "loss": 3.0444,
      "step": 206900
    },
    {
      "epoch": 67.09886547811993,
      "grad_norm": 0.9841111898422241,
      "learning_rate": 4.3321018488485246e-05,
      "loss": 3.0263,
      "step": 207000
    },
    {
      "epoch": 67.13128038897894,
      "grad_norm": 0.9976674914360046,
      "learning_rate": 4.3317774894583205e-05,
      "loss": 3.0285,
      "step": 207100
    },
    {
      "epoch": 67.16369529983793,
      "grad_norm": 0.9882503747940063,
      "learning_rate": 4.3314531300681156e-05,
      "loss": 3.0254,
      "step": 207200
    },
    {
      "epoch": 67.19611021069692,
      "grad_norm": 1.1670362949371338,
      "learning_rate": 4.3311320142718134e-05,
      "loss": 3.0224,
      "step": 207300
    },
    {
      "epoch": 67.22852512155592,
      "grad_norm": 1.1286765336990356,
      "learning_rate": 4.330807654881609e-05,
      "loss": 3.0173,
      "step": 207400
    },
    {
      "epoch": 67.26094003241491,
      "grad_norm": 1.1004703044891357,
      "learning_rate": 4.330483295491405e-05,
      "loss": 3.0278,
      "step": 207500
    },
    {
      "epoch": 67.29335494327391,
      "grad_norm": 1.075177550315857,
      "learning_rate": 4.3301589361012004e-05,
      "loss": 3.0555,
      "step": 207600
    },
    {
      "epoch": 67.3257698541329,
      "grad_norm": 1.0277799367904663,
      "learning_rate": 4.329834576710996e-05,
      "loss": 3.0382,
      "step": 207700
    },
    {
      "epoch": 67.35818476499189,
      "grad_norm": 1.0440131425857544,
      "learning_rate": 4.329510217320792e-05,
      "loss": 3.0338,
      "step": 207800
    },
    {
      "epoch": 67.3905996758509,
      "grad_norm": 1.0381040573120117,
      "learning_rate": 4.329185857930587e-05,
      "loss": 3.0433,
      "step": 207900
    },
    {
      "epoch": 67.42301458670988,
      "grad_norm": 0.9683030843734741,
      "learning_rate": 4.328861498540383e-05,
      "loss": 3.0525,
      "step": 208000
    },
    {
      "epoch": 67.45542949756889,
      "grad_norm": 1.0808918476104736,
      "learning_rate": 4.3285371391501784e-05,
      "loss": 3.037,
      "step": 208100
    },
    {
      "epoch": 67.48784440842788,
      "grad_norm": 1.0482556819915771,
      "learning_rate": 4.328212779759974e-05,
      "loss": 3.0249,
      "step": 208200
    },
    {
      "epoch": 67.52025931928688,
      "grad_norm": 1.020625352859497,
      "learning_rate": 4.32788842036977e-05,
      "loss": 3.0244,
      "step": 208300
    },
    {
      "epoch": 67.55267423014587,
      "grad_norm": 1.09775710105896,
      "learning_rate": 4.327564060979565e-05,
      "loss": 3.0291,
      "step": 208400
    },
    {
      "epoch": 67.58508914100486,
      "grad_norm": 1.2418582439422607,
      "learning_rate": 4.327239701589361e-05,
      "loss": 3.0254,
      "step": 208500
    },
    {
      "epoch": 67.61750405186386,
      "grad_norm": 0.9640125632286072,
      "learning_rate": 4.326915342199157e-05,
      "loss": 3.0328,
      "step": 208600
    },
    {
      "epoch": 67.64991896272285,
      "grad_norm": 0.8825747966766357,
      "learning_rate": 4.326590982808952e-05,
      "loss": 3.0479,
      "step": 208700
    },
    {
      "epoch": 67.68233387358185,
      "grad_norm": 1.1742922067642212,
      "learning_rate": 4.326266623418748e-05,
      "loss": 3.0169,
      "step": 208800
    },
    {
      "epoch": 67.71474878444084,
      "grad_norm": 0.8814970850944519,
      "learning_rate": 4.325942264028544e-05,
      "loss": 3.0236,
      "step": 208900
    },
    {
      "epoch": 67.74716369529983,
      "grad_norm": 1.1444956064224243,
      "learning_rate": 4.325617904638339e-05,
      "loss": 3.037,
      "step": 209000
    },
    {
      "epoch": 67.77957860615884,
      "grad_norm": 0.9761528968811035,
      "learning_rate": 4.325293545248135e-05,
      "loss": 3.0462,
      "step": 209100
    },
    {
      "epoch": 67.81199351701783,
      "grad_norm": 0.8449925184249878,
      "learning_rate": 4.32496918585793e-05,
      "loss": 3.0416,
      "step": 209200
    },
    {
      "epoch": 67.84440842787683,
      "grad_norm": 1.0891598463058472,
      "learning_rate": 4.324644826467726e-05,
      "loss": 3.0318,
      "step": 209300
    },
    {
      "epoch": 67.87682333873582,
      "grad_norm": 0.8694517612457275,
      "learning_rate": 4.324320467077522e-05,
      "loss": 3.0256,
      "step": 209400
    },
    {
      "epoch": 67.90923824959481,
      "grad_norm": 1.0017534494400024,
      "learning_rate": 4.323996107687317e-05,
      "loss": 3.0455,
      "step": 209500
    },
    {
      "epoch": 67.94165316045381,
      "grad_norm": 0.9121502041816711,
      "learning_rate": 4.323671748297113e-05,
      "loss": 3.0609,
      "step": 209600
    },
    {
      "epoch": 67.9740680713128,
      "grad_norm": 0.931928813457489,
      "learning_rate": 4.323347388906909e-05,
      "loss": 3.0473,
      "step": 209700
    },
    {
      "epoch": 68.0,
      "eval_bleu": 1.2699031740351658,
      "eval_loss": 3.783064842224121,
      "eval_runtime": 4.5067,
      "eval_samples_per_second": 109.17,
      "eval_steps_per_second": 1.775,
      "step": 209780
    },
    {
      "epoch": 68.0064829821718,
      "grad_norm": 1.0818344354629517,
      "learning_rate": 4.323023029516705e-05,
      "loss": 3.0378,
      "step": 209800
    },
    {
      "epoch": 68.03889789303079,
      "grad_norm": 0.8633217215538025,
      "learning_rate": 4.322698670126501e-05,
      "loss": 3.0349,
      "step": 209900
    },
    {
      "epoch": 68.0713128038898,
      "grad_norm": 1.1368777751922607,
      "learning_rate": 4.3223743107362966e-05,
      "loss": 3.0245,
      "step": 210000
    },
    {
      "epoch": 68.10372771474879,
      "grad_norm": 0.9364138245582581,
      "learning_rate": 4.322049951346092e-05,
      "loss": 3.0212,
      "step": 210100
    },
    {
      "epoch": 68.13614262560777,
      "grad_norm": 0.885876476764679,
      "learning_rate": 4.321725591955888e-05,
      "loss": 3.014,
      "step": 210200
    },
    {
      "epoch": 68.16855753646678,
      "grad_norm": 0.8717752695083618,
      "learning_rate": 4.321401232565683e-05,
      "loss": 3.0371,
      "step": 210300
    },
    {
      "epoch": 68.20097244732577,
      "grad_norm": 1.102931261062622,
      "learning_rate": 4.321076873175479e-05,
      "loss": 3.0147,
      "step": 210400
    },
    {
      "epoch": 68.23338735818477,
      "grad_norm": 0.8952922821044922,
      "learning_rate": 4.3207525137852746e-05,
      "loss": 3.0496,
      "step": 210500
    },
    {
      "epoch": 68.26580226904376,
      "grad_norm": 1.0068033933639526,
      "learning_rate": 4.32042815439507e-05,
      "loss": 3.0346,
      "step": 210600
    },
    {
      "epoch": 68.29821717990275,
      "grad_norm": 1.0138667821884155,
      "learning_rate": 4.320103795004866e-05,
      "loss": 3.0334,
      "step": 210700
    },
    {
      "epoch": 68.33063209076175,
      "grad_norm": 1.0862351655960083,
      "learning_rate": 4.3197794356146616e-05,
      "loss": 3.0199,
      "step": 210800
    },
    {
      "epoch": 68.36304700162074,
      "grad_norm": 1.0013558864593506,
      "learning_rate": 4.319455076224457e-05,
      "loss": 3.0274,
      "step": 210900
    },
    {
      "epoch": 68.39546191247975,
      "grad_norm": 1.0750834941864014,
      "learning_rate": 4.3191307168342526e-05,
      "loss": 3.0354,
      "step": 211000
    },
    {
      "epoch": 68.42787682333874,
      "grad_norm": 1.1346707344055176,
      "learning_rate": 4.318806357444048e-05,
      "loss": 3.0444,
      "step": 211100
    },
    {
      "epoch": 68.46029173419772,
      "grad_norm": 1.0610458850860596,
      "learning_rate": 4.318481998053844e-05,
      "loss": 3.0315,
      "step": 211200
    },
    {
      "epoch": 68.49270664505673,
      "grad_norm": 0.9880039095878601,
      "learning_rate": 4.3181608822575415e-05,
      "loss": 3.0159,
      "step": 211300
    },
    {
      "epoch": 68.52512155591572,
      "grad_norm": 0.987166702747345,
      "learning_rate": 4.317839766461239e-05,
      "loss": 3.0253,
      "step": 211400
    },
    {
      "epoch": 68.55753646677472,
      "grad_norm": 1.2054524421691895,
      "learning_rate": 4.317515407071035e-05,
      "loss": 3.0296,
      "step": 211500
    },
    {
      "epoch": 68.58995137763371,
      "grad_norm": 0.869670033454895,
      "learning_rate": 4.317191047680831e-05,
      "loss": 3.0205,
      "step": 211600
    },
    {
      "epoch": 68.62236628849271,
      "grad_norm": 1.0144855976104736,
      "learning_rate": 4.316866688290626e-05,
      "loss": 3.0278,
      "step": 211700
    },
    {
      "epoch": 68.6547811993517,
      "grad_norm": 1.07700777053833,
      "learning_rate": 4.316542328900422e-05,
      "loss": 3.032,
      "step": 211800
    },
    {
      "epoch": 68.68719611021069,
      "grad_norm": 1.1793724298477173,
      "learning_rate": 4.316217969510217e-05,
      "loss": 3.0133,
      "step": 211900
    },
    {
      "epoch": 68.7196110210697,
      "grad_norm": 0.8309473395347595,
      "learning_rate": 4.315893610120013e-05,
      "loss": 3.0314,
      "step": 212000
    },
    {
      "epoch": 68.75202593192869,
      "grad_norm": 0.9605224132537842,
      "learning_rate": 4.315569250729809e-05,
      "loss": 3.049,
      "step": 212100
    },
    {
      "epoch": 68.78444084278769,
      "grad_norm": 0.8828433752059937,
      "learning_rate": 4.315244891339604e-05,
      "loss": 3.0383,
      "step": 212200
    },
    {
      "epoch": 68.81685575364668,
      "grad_norm": 1.1991353034973145,
      "learning_rate": 4.3149205319494e-05,
      "loss": 3.0398,
      "step": 212300
    },
    {
      "epoch": 68.84927066450567,
      "grad_norm": 0.9896833300590515,
      "learning_rate": 4.314596172559196e-05,
      "loss": 3.0302,
      "step": 212400
    },
    {
      "epoch": 68.88168557536467,
      "grad_norm": 0.8955299258232117,
      "learning_rate": 4.314271813168991e-05,
      "loss": 3.0367,
      "step": 212500
    },
    {
      "epoch": 68.91410048622366,
      "grad_norm": 1.232329249382019,
      "learning_rate": 4.313947453778787e-05,
      "loss": 3.0597,
      "step": 212600
    },
    {
      "epoch": 68.94651539708266,
      "grad_norm": 0.9713438153266907,
      "learning_rate": 4.313623094388583e-05,
      "loss": 3.0264,
      "step": 212700
    },
    {
      "epoch": 68.97893030794165,
      "grad_norm": 1.0412213802337646,
      "learning_rate": 4.313298734998378e-05,
      "loss": 3.0445,
      "step": 212800
    },
    {
      "epoch": 69.0,
      "eval_bleu": 1.0766369044935227,
      "eval_loss": 3.7870562076568604,
      "eval_runtime": 4.7714,
      "eval_samples_per_second": 103.115,
      "eval_steps_per_second": 1.677,
      "step": 212865
    },
    {
      "epoch": 69.01134521880064,
      "grad_norm": 1.010398268699646,
      "learning_rate": 4.312974375608174e-05,
      "loss": 3.0384,
      "step": 212900
    },
    {
      "epoch": 69.04376012965965,
      "grad_norm": 1.0025935173034668,
      "learning_rate": 4.312650016217969e-05,
      "loss": 3.0269,
      "step": 213000
    },
    {
      "epoch": 69.07617504051863,
      "grad_norm": 0.9946103096008301,
      "learning_rate": 4.312325656827765e-05,
      "loss": 3.025,
      "step": 213100
    },
    {
      "epoch": 69.10858995137764,
      "grad_norm": 0.9961799383163452,
      "learning_rate": 4.312001297437561e-05,
      "loss": 3.0198,
      "step": 213200
    },
    {
      "epoch": 69.14100486223663,
      "grad_norm": 1.0547457933425903,
      "learning_rate": 4.311676938047357e-05,
      "loss": 3.016,
      "step": 213300
    },
    {
      "epoch": 69.17341977309563,
      "grad_norm": 0.8803642392158508,
      "learning_rate": 4.311352578657153e-05,
      "loss": 3.0196,
      "step": 213400
    },
    {
      "epoch": 69.20583468395462,
      "grad_norm": 1.0730785131454468,
      "learning_rate": 4.311028219266948e-05,
      "loss": 3.0176,
      "step": 213500
    },
    {
      "epoch": 69.23824959481361,
      "grad_norm": 0.9427257776260376,
      "learning_rate": 4.310703859876744e-05,
      "loss": 3.0287,
      "step": 213600
    },
    {
      "epoch": 69.27066450567261,
      "grad_norm": 0.903620183467865,
      "learning_rate": 4.3103795004865396e-05,
      "loss": 3.0339,
      "step": 213700
    },
    {
      "epoch": 69.3030794165316,
      "grad_norm": 0.9232144951820374,
      "learning_rate": 4.310055141096335e-05,
      "loss": 3.0275,
      "step": 213800
    },
    {
      "epoch": 69.3354943273906,
      "grad_norm": 0.9259741902351379,
      "learning_rate": 4.309730781706131e-05,
      "loss": 3.0353,
      "step": 213900
    },
    {
      "epoch": 69.3679092382496,
      "grad_norm": 0.9461127519607544,
      "learning_rate": 4.3094064223159266e-05,
      "loss": 3.0304,
      "step": 214000
    },
    {
      "epoch": 69.40032414910858,
      "grad_norm": 1.1046639680862427,
      "learning_rate": 4.309082062925722e-05,
      "loss": 3.04,
      "step": 214100
    },
    {
      "epoch": 69.43273905996759,
      "grad_norm": 1.1786025762557983,
      "learning_rate": 4.3087577035355177e-05,
      "loss": 3.0181,
      "step": 214200
    },
    {
      "epoch": 69.46515397082658,
      "grad_norm": 1.036787986755371,
      "learning_rate": 4.3084333441453135e-05,
      "loss": 3.0378,
      "step": 214300
    },
    {
      "epoch": 69.49756888168558,
      "grad_norm": 0.953580915927887,
      "learning_rate": 4.308108984755109e-05,
      "loss": 3.0434,
      "step": 214400
    },
    {
      "epoch": 69.52998379254457,
      "grad_norm": 0.8663904666900635,
      "learning_rate": 4.3077846253649046e-05,
      "loss": 3.0428,
      "step": 214500
    },
    {
      "epoch": 69.56239870340356,
      "grad_norm": 0.9828327894210815,
      "learning_rate": 4.3074602659747005e-05,
      "loss": 3.0166,
      "step": 214600
    },
    {
      "epoch": 69.59481361426256,
      "grad_norm": 1.028395175933838,
      "learning_rate": 4.307135906584496e-05,
      "loss": 3.0139,
      "step": 214700
    },
    {
      "epoch": 69.62722852512155,
      "grad_norm": 1.0558754205703735,
      "learning_rate": 4.3068115471942915e-05,
      "loss": 3.0208,
      "step": 214800
    },
    {
      "epoch": 69.65964343598056,
      "grad_norm": 0.9291805028915405,
      "learning_rate": 4.306487187804087e-05,
      "loss": 3.0215,
      "step": 214900
    },
    {
      "epoch": 69.69205834683954,
      "grad_norm": 1.0174826383590698,
      "learning_rate": 4.3061628284138826e-05,
      "loss": 3.0317,
      "step": 215000
    },
    {
      "epoch": 69.72447325769855,
      "grad_norm": 0.9753902554512024,
      "learning_rate": 4.3058384690236785e-05,
      "loss": 3.043,
      "step": 215100
    },
    {
      "epoch": 69.75688816855754,
      "grad_norm": 0.9500665664672852,
      "learning_rate": 4.305514109633474e-05,
      "loss": 3.0297,
      "step": 215200
    },
    {
      "epoch": 69.78930307941653,
      "grad_norm": 1.0280424356460571,
      "learning_rate": 4.3051897502432696e-05,
      "loss": 3.0267,
      "step": 215300
    },
    {
      "epoch": 69.82171799027553,
      "grad_norm": 0.9077160954475403,
      "learning_rate": 4.3048653908530654e-05,
      "loss": 3.0216,
      "step": 215400
    },
    {
      "epoch": 69.85413290113452,
      "grad_norm": 0.9583052396774292,
      "learning_rate": 4.304544275056763e-05,
      "loss": 3.0267,
      "step": 215500
    },
    {
      "epoch": 69.88654781199352,
      "grad_norm": 0.992850124835968,
      "learning_rate": 4.3042199156665584e-05,
      "loss": 3.0065,
      "step": 215600
    },
    {
      "epoch": 69.91896272285251,
      "grad_norm": 0.9679660797119141,
      "learning_rate": 4.303895556276354e-05,
      "loss": 3.0337,
      "step": 215700
    },
    {
      "epoch": 69.9513776337115,
      "grad_norm": 0.9956992864608765,
      "learning_rate": 4.30357119688615e-05,
      "loss": 3.0369,
      "step": 215800
    },
    {
      "epoch": 69.9837925445705,
      "grad_norm": 1.2153536081314087,
      "learning_rate": 4.3032468374959454e-05,
      "loss": 3.0149,
      "step": 215900
    },
    {
      "epoch": 70.0,
      "eval_bleu": 1.1147630693110406,
      "eval_loss": 3.788935899734497,
      "eval_runtime": 4.4701,
      "eval_samples_per_second": 110.064,
      "eval_steps_per_second": 1.79,
      "step": 215950
    },
    {
      "epoch": 70.0162074554295,
      "grad_norm": 0.9911654591560364,
      "learning_rate": 4.302922478105741e-05,
      "loss": 3.0349,
      "step": 216000
    },
    {
      "epoch": 70.0486223662885,
      "grad_norm": 0.912945568561554,
      "learning_rate": 4.3025981187155364e-05,
      "loss": 3.0032,
      "step": 216100
    },
    {
      "epoch": 70.08103727714749,
      "grad_norm": 0.9901794791221619,
      "learning_rate": 4.302273759325332e-05,
      "loss": 3.0238,
      "step": 216200
    },
    {
      "epoch": 70.11345218800648,
      "grad_norm": 0.9724683165550232,
      "learning_rate": 4.301949399935128e-05,
      "loss": 3.0163,
      "step": 216300
    },
    {
      "epoch": 70.14586709886548,
      "grad_norm": 0.9655687212944031,
      "learning_rate": 4.301625040544924e-05,
      "loss": 3.0276,
      "step": 216400
    },
    {
      "epoch": 70.17828200972447,
      "grad_norm": 0.961936354637146,
      "learning_rate": 4.30130068115472e-05,
      "loss": 3.0104,
      "step": 216500
    },
    {
      "epoch": 70.21069692058347,
      "grad_norm": 1.0181795358657837,
      "learning_rate": 4.300976321764516e-05,
      "loss": 3.0264,
      "step": 216600
    },
    {
      "epoch": 70.24311183144246,
      "grad_norm": 1.0663292407989502,
      "learning_rate": 4.300651962374311e-05,
      "loss": 3.022,
      "step": 216700
    },
    {
      "epoch": 70.27552674230145,
      "grad_norm": 0.8903969526290894,
      "learning_rate": 4.300327602984107e-05,
      "loss": 3.0343,
      "step": 216800
    },
    {
      "epoch": 70.30794165316046,
      "grad_norm": 1.017232060432434,
      "learning_rate": 4.300003243593903e-05,
      "loss": 3.0228,
      "step": 216900
    },
    {
      "epoch": 70.34035656401944,
      "grad_norm": 1.1320340633392334,
      "learning_rate": 4.299678884203698e-05,
      "loss": 3.0265,
      "step": 217000
    },
    {
      "epoch": 70.37277147487845,
      "grad_norm": 1.040643334388733,
      "learning_rate": 4.299354524813494e-05,
      "loss": 3.0062,
      "step": 217100
    },
    {
      "epoch": 70.40518638573744,
      "grad_norm": 1.0996603965759277,
      "learning_rate": 4.299030165423289e-05,
      "loss": 3.0236,
      "step": 217200
    },
    {
      "epoch": 70.43760129659644,
      "grad_norm": 0.9473301768302917,
      "learning_rate": 4.298705806033085e-05,
      "loss": 3.0306,
      "step": 217300
    },
    {
      "epoch": 70.47001620745543,
      "grad_norm": 0.9167338013648987,
      "learning_rate": 4.298381446642881e-05,
      "loss": 3.0275,
      "step": 217400
    },
    {
      "epoch": 70.50243111831442,
      "grad_norm": 0.9238314032554626,
      "learning_rate": 4.2980603308465785e-05,
      "loss": 3.0135,
      "step": 217500
    },
    {
      "epoch": 70.53484602917342,
      "grad_norm": 0.9137956500053406,
      "learning_rate": 4.297735971456374e-05,
      "loss": 3.0358,
      "step": 217600
    },
    {
      "epoch": 70.56726094003241,
      "grad_norm": 0.9303743839263916,
      "learning_rate": 4.2974116120661696e-05,
      "loss": 3.0336,
      "step": 217700
    },
    {
      "epoch": 70.59967585089142,
      "grad_norm": 1.151563286781311,
      "learning_rate": 4.2970872526759655e-05,
      "loss": 3.0274,
      "step": 217800
    },
    {
      "epoch": 70.6320907617504,
      "grad_norm": 1.001854419708252,
      "learning_rate": 4.296762893285761e-05,
      "loss": 3.0186,
      "step": 217900
    },
    {
      "epoch": 70.6645056726094,
      "grad_norm": 0.9404789805412292,
      "learning_rate": 4.2964385338955566e-05,
      "loss": 3.0211,
      "step": 218000
    },
    {
      "epoch": 70.6969205834684,
      "grad_norm": 1.009298324584961,
      "learning_rate": 4.2961141745053524e-05,
      "loss": 3.0318,
      "step": 218100
    },
    {
      "epoch": 70.72933549432739,
      "grad_norm": 0.9352536201477051,
      "learning_rate": 4.2957898151151476e-05,
      "loss": 3.009,
      "step": 218200
    },
    {
      "epoch": 70.76175040518639,
      "grad_norm": 0.9798349142074585,
      "learning_rate": 4.2954654557249435e-05,
      "loss": 3.0395,
      "step": 218300
    },
    {
      "epoch": 70.79416531604538,
      "grad_norm": 1.139697551727295,
      "learning_rate": 4.295144339928641e-05,
      "loss": 3.0279,
      "step": 218400
    },
    {
      "epoch": 70.82658022690438,
      "grad_norm": 0.9239896535873413,
      "learning_rate": 4.294819980538437e-05,
      "loss": 3.0115,
      "step": 218500
    },
    {
      "epoch": 70.85899513776337,
      "grad_norm": 0.9393025040626526,
      "learning_rate": 4.2944956211482324e-05,
      "loss": 3.0273,
      "step": 218600
    },
    {
      "epoch": 70.89141004862236,
      "grad_norm": 0.9062139391899109,
      "learning_rate": 4.294171261758028e-05,
      "loss": 3.0279,
      "step": 218700
    },
    {
      "epoch": 70.92382495948137,
      "grad_norm": 0.967012345790863,
      "learning_rate": 4.2938469023678234e-05,
      "loss": 3.0294,
      "step": 218800
    },
    {
      "epoch": 70.95623987034035,
      "grad_norm": 0.969709038734436,
      "learning_rate": 4.293522542977619e-05,
      "loss": 3.0313,
      "step": 218900
    },
    {
      "epoch": 70.98865478119936,
      "grad_norm": 0.958719789981842,
      "learning_rate": 4.293201427181317e-05,
      "loss": 3.0284,
      "step": 219000
    },
    {
      "epoch": 71.0,
      "eval_bleu": 1.2419782083813002,
      "eval_loss": 3.797208547592163,
      "eval_runtime": 4.52,
      "eval_samples_per_second": 108.849,
      "eval_steps_per_second": 1.77,
      "step": 219035
    },
    {
      "epoch": 71.02106969205835,
      "grad_norm": 1.0118567943572998,
      "learning_rate": 4.292877067791113e-05,
      "loss": 3.0013,
      "step": 219100
    },
    {
      "epoch": 71.05348460291734,
      "grad_norm": 1.051900029182434,
      "learning_rate": 4.292552708400908e-05,
      "loss": 3.0158,
      "step": 219200
    },
    {
      "epoch": 71.08589951377634,
      "grad_norm": 0.9952330589294434,
      "learning_rate": 4.292228349010704e-05,
      "loss": 3.0118,
      "step": 219300
    },
    {
      "epoch": 71.11831442463533,
      "grad_norm": 1.017245888710022,
      "learning_rate": 4.2919039896205e-05,
      "loss": 3.009,
      "step": 219400
    },
    {
      "epoch": 71.15072933549433,
      "grad_norm": 1.037263035774231,
      "learning_rate": 4.291579630230295e-05,
      "loss": 3.023,
      "step": 219500
    },
    {
      "epoch": 71.18314424635332,
      "grad_norm": 0.8933784365653992,
      "learning_rate": 4.291255270840091e-05,
      "loss": 3.0069,
      "step": 219600
    },
    {
      "epoch": 71.21555915721231,
      "grad_norm": 1.033177137374878,
      "learning_rate": 4.290930911449887e-05,
      "loss": 3.0202,
      "step": 219700
    },
    {
      "epoch": 71.24797406807131,
      "grad_norm": 1.140195369720459,
      "learning_rate": 4.2906097956535846e-05,
      "loss": 3.0068,
      "step": 219800
    },
    {
      "epoch": 71.2803889789303,
      "grad_norm": 0.9221016764640808,
      "learning_rate": 4.29028543626338e-05,
      "loss": 3.0132,
      "step": 219900
    },
    {
      "epoch": 71.31280388978931,
      "grad_norm": 0.9290106892585754,
      "learning_rate": 4.289961076873176e-05,
      "loss": 3.014,
      "step": 220000
    },
    {
      "epoch": 71.3452188006483,
      "grad_norm": 0.9443814754486084,
      "learning_rate": 4.2896367174829716e-05,
      "loss": 3.0297,
      "step": 220100
    },
    {
      "epoch": 71.37763371150729,
      "grad_norm": 1.1420356035232544,
      "learning_rate": 4.289312358092767e-05,
      "loss": 3.021,
      "step": 220200
    },
    {
      "epoch": 71.41004862236629,
      "grad_norm": 1.023417592048645,
      "learning_rate": 4.2889879987025626e-05,
      "loss": 3.0161,
      "step": 220300
    },
    {
      "epoch": 71.44246353322528,
      "grad_norm": 0.9789552688598633,
      "learning_rate": 4.288663639312358e-05,
      "loss": 3.0495,
      "step": 220400
    },
    {
      "epoch": 71.47487844408428,
      "grad_norm": 0.8732361793518066,
      "learning_rate": 4.288339279922154e-05,
      "loss": 3.0238,
      "step": 220500
    },
    {
      "epoch": 71.50729335494327,
      "grad_norm": 1.0645778179168701,
      "learning_rate": 4.2880149205319496e-05,
      "loss": 3.0487,
      "step": 220600
    },
    {
      "epoch": 71.53970826580228,
      "grad_norm": 1.0098110437393188,
      "learning_rate": 4.287690561141745e-05,
      "loss": 3.008,
      "step": 220700
    },
    {
      "epoch": 71.57212317666126,
      "grad_norm": 1.0844104290008545,
      "learning_rate": 4.2873662017515407e-05,
      "loss": 3.0015,
      "step": 220800
    },
    {
      "epoch": 71.60453808752025,
      "grad_norm": 1.0342564582824707,
      "learning_rate": 4.2870418423613365e-05,
      "loss": 3.0425,
      "step": 220900
    },
    {
      "epoch": 71.63695299837926,
      "grad_norm": 0.8161481618881226,
      "learning_rate": 4.2867174829711324e-05,
      "loss": 3.0193,
      "step": 221000
    },
    {
      "epoch": 71.66936790923825,
      "grad_norm": 1.0681778192520142,
      "learning_rate": 4.2863931235809276e-05,
      "loss": 3.0091,
      "step": 221100
    },
    {
      "epoch": 71.70178282009725,
      "grad_norm": 1.0427581071853638,
      "learning_rate": 4.2860687641907235e-05,
      "loss": 3.0301,
      "step": 221200
    },
    {
      "epoch": 71.73419773095624,
      "grad_norm": 1.0320615768432617,
      "learning_rate": 4.2857444048005194e-05,
      "loss": 3.0169,
      "step": 221300
    },
    {
      "epoch": 71.76661264181523,
      "grad_norm": 0.9785946607589722,
      "learning_rate": 4.285420045410315e-05,
      "loss": 3.003,
      "step": 221400
    },
    {
      "epoch": 71.79902755267423,
      "grad_norm": 0.9665758609771729,
      "learning_rate": 4.2850956860201104e-05,
      "loss": 3.023,
      "step": 221500
    },
    {
      "epoch": 71.83144246353322,
      "grad_norm": 1.001916766166687,
      "learning_rate": 4.284771326629906e-05,
      "loss": 3.0214,
      "step": 221600
    },
    {
      "epoch": 71.86385737439223,
      "grad_norm": 1.1073857545852661,
      "learning_rate": 4.284446967239702e-05,
      "loss": 3.0323,
      "step": 221700
    },
    {
      "epoch": 71.89627228525121,
      "grad_norm": 0.968226969242096,
      "learning_rate": 4.2841226078494974e-05,
      "loss": 3.0123,
      "step": 221800
    },
    {
      "epoch": 71.9286871961102,
      "grad_norm": 1.0064232349395752,
      "learning_rate": 4.283798248459293e-05,
      "loss": 3.0194,
      "step": 221900
    },
    {
      "epoch": 71.96110210696921,
      "grad_norm": 0.9383906722068787,
      "learning_rate": 4.283473889069089e-05,
      "loss": 3.0157,
      "step": 222000
    },
    {
      "epoch": 71.9935170178282,
      "grad_norm": 0.9446894526481628,
      "learning_rate": 4.283149529678884e-05,
      "loss": 3.0499,
      "step": 222100
    },
    {
      "epoch": 72.0,
      "eval_bleu": 1.2161152276497207,
      "eval_loss": 3.7963578701019287,
      "eval_runtime": 4.9128,
      "eval_samples_per_second": 100.147,
      "eval_steps_per_second": 1.628,
      "step": 222120
    },
    {
      "epoch": 72.0259319286872,
      "grad_norm": 0.9312269687652588,
      "learning_rate": 4.28282517028868e-05,
      "loss": 3.0178,
      "step": 222200
    },
    {
      "epoch": 72.05834683954619,
      "grad_norm": 0.9115213751792908,
      "learning_rate": 4.282500810898476e-05,
      "loss": 3.0233,
      "step": 222300
    },
    {
      "epoch": 72.09076175040519,
      "grad_norm": 0.9317170977592468,
      "learning_rate": 4.282176451508271e-05,
      "loss": 2.9856,
      "step": 222400
    },
    {
      "epoch": 72.12317666126418,
      "grad_norm": 0.9798657298088074,
      "learning_rate": 4.281852092118067e-05,
      "loss": 3.006,
      "step": 222500
    },
    {
      "epoch": 72.15559157212317,
      "grad_norm": 1.1900506019592285,
      "learning_rate": 4.281527732727862e-05,
      "loss": 3.0283,
      "step": 222600
    },
    {
      "epoch": 72.18800648298217,
      "grad_norm": 0.9025070071220398,
      "learning_rate": 4.281203373337658e-05,
      "loss": 3.023,
      "step": 222700
    },
    {
      "epoch": 72.22042139384116,
      "grad_norm": 0.9814870357513428,
      "learning_rate": 4.280879013947454e-05,
      "loss": 3.0373,
      "step": 222800
    },
    {
      "epoch": 72.25283630470017,
      "grad_norm": 0.9578638672828674,
      "learning_rate": 4.280554654557249e-05,
      "loss": 2.9953,
      "step": 222900
    },
    {
      "epoch": 72.28525121555916,
      "grad_norm": 0.9381853342056274,
      "learning_rate": 4.280233538760947e-05,
      "loss": 3.0097,
      "step": 223000
    },
    {
      "epoch": 72.31766612641815,
      "grad_norm": 0.9146835803985596,
      "learning_rate": 4.279909179370743e-05,
      "loss": 3.0281,
      "step": 223100
    },
    {
      "epoch": 72.35008103727715,
      "grad_norm": 0.9878574013710022,
      "learning_rate": 4.279584819980539e-05,
      "loss": 3.0099,
      "step": 223200
    },
    {
      "epoch": 72.38249594813614,
      "grad_norm": 0.9801392555236816,
      "learning_rate": 4.279260460590334e-05,
      "loss": 2.9928,
      "step": 223300
    },
    {
      "epoch": 72.41491085899514,
      "grad_norm": 1.1284607648849487,
      "learning_rate": 4.27893610120013e-05,
      "loss": 3.0316,
      "step": 223400
    },
    {
      "epoch": 72.44732576985413,
      "grad_norm": 1.0011674165725708,
      "learning_rate": 4.278611741809926e-05,
      "loss": 3.0036,
      "step": 223500
    },
    {
      "epoch": 72.47974068071312,
      "grad_norm": 1.0524137020111084,
      "learning_rate": 4.278287382419721e-05,
      "loss": 3.0321,
      "step": 223600
    },
    {
      "epoch": 72.51215559157212,
      "grad_norm": 0.930594265460968,
      "learning_rate": 4.277963023029517e-05,
      "loss": 3.0026,
      "step": 223700
    },
    {
      "epoch": 72.54457050243111,
      "grad_norm": 0.9482957124710083,
      "learning_rate": 4.277638663639312e-05,
      "loss": 3.0222,
      "step": 223800
    },
    {
      "epoch": 72.57698541329012,
      "grad_norm": 0.8861395120620728,
      "learning_rate": 4.277314304249108e-05,
      "loss": 3.0167,
      "step": 223900
    },
    {
      "epoch": 72.6094003241491,
      "grad_norm": 0.9695923924446106,
      "learning_rate": 4.276989944858904e-05,
      "loss": 3.0121,
      "step": 224000
    },
    {
      "epoch": 72.64181523500811,
      "grad_norm": 1.1232049465179443,
      "learning_rate": 4.2766655854686996e-05,
      "loss": 3.0144,
      "step": 224100
    },
    {
      "epoch": 72.6742301458671,
      "grad_norm": 0.9545140862464905,
      "learning_rate": 4.2763412260784955e-05,
      "loss": 3.0117,
      "step": 224200
    },
    {
      "epoch": 72.70664505672609,
      "grad_norm": 1.1483705043792725,
      "learning_rate": 4.276016866688291e-05,
      "loss": 3.0277,
      "step": 224300
    },
    {
      "epoch": 72.73905996758509,
      "grad_norm": 0.93214350938797,
      "learning_rate": 4.2756925072980866e-05,
      "loss": 3.0245,
      "step": 224400
    },
    {
      "epoch": 72.77147487844408,
      "grad_norm": 0.864465594291687,
      "learning_rate": 4.2753681479078825e-05,
      "loss": 3.0329,
      "step": 224500
    },
    {
      "epoch": 72.80388978930308,
      "grad_norm": 0.9146140813827515,
      "learning_rate": 4.275043788517678e-05,
      "loss": 3.0218,
      "step": 224600
    },
    {
      "epoch": 72.83630470016207,
      "grad_norm": 0.9060612320899963,
      "learning_rate": 4.2747194291274735e-05,
      "loss": 3.0205,
      "step": 224700
    },
    {
      "epoch": 72.86871961102106,
      "grad_norm": 1.025734305381775,
      "learning_rate": 4.2743950697372694e-05,
      "loss": 3.0278,
      "step": 224800
    },
    {
      "epoch": 72.90113452188007,
      "grad_norm": 0.858806312084198,
      "learning_rate": 4.2740707103470646e-05,
      "loss": 3.0173,
      "step": 224900
    },
    {
      "epoch": 72.93354943273906,
      "grad_norm": 1.1431022882461548,
      "learning_rate": 4.2737463509568605e-05,
      "loss": 2.9943,
      "step": 225000
    },
    {
      "epoch": 72.96596434359806,
      "grad_norm": 0.9559085369110107,
      "learning_rate": 4.2734219915666564e-05,
      "loss": 3.0323,
      "step": 225100
    },
    {
      "epoch": 72.99837925445705,
      "grad_norm": 0.9115543365478516,
      "learning_rate": 4.2730976321764515e-05,
      "loss": 3.0185,
      "step": 225200
    },
    {
      "epoch": 73.0,
      "eval_bleu": 1.1877252859460952,
      "eval_loss": 3.7988929748535156,
      "eval_runtime": 4.3275,
      "eval_samples_per_second": 113.691,
      "eval_steps_per_second": 1.849,
      "step": 225205
    },
    {
      "epoch": 73.03079416531604,
      "grad_norm": 0.9660592675209045,
      "learning_rate": 4.2727732727862474e-05,
      "loss": 3.0197,
      "step": 225300
    },
    {
      "epoch": 73.06320907617504,
      "grad_norm": 0.9544588327407837,
      "learning_rate": 4.272452156989945e-05,
      "loss": 3.0037,
      "step": 225400
    },
    {
      "epoch": 73.09562398703403,
      "grad_norm": 0.9786755442619324,
      "learning_rate": 4.272127797599741e-05,
      "loss": 3.0124,
      "step": 225500
    },
    {
      "epoch": 73.12803889789303,
      "grad_norm": 0.9595096111297607,
      "learning_rate": 4.271803438209536e-05,
      "loss": 3.0072,
      "step": 225600
    },
    {
      "epoch": 73.16045380875202,
      "grad_norm": 0.9169298410415649,
      "learning_rate": 4.271479078819332e-05,
      "loss": 3.004,
      "step": 225700
    },
    {
      "epoch": 73.19286871961103,
      "grad_norm": 0.8988928198814392,
      "learning_rate": 4.271154719429128e-05,
      "loss": 3.0198,
      "step": 225800
    },
    {
      "epoch": 73.22528363047002,
      "grad_norm": 1.0933564901351929,
      "learning_rate": 4.270830360038923e-05,
      "loss": 3.008,
      "step": 225900
    },
    {
      "epoch": 73.257698541329,
      "grad_norm": 0.9606413841247559,
      "learning_rate": 4.270506000648719e-05,
      "loss": 2.9973,
      "step": 226000
    },
    {
      "epoch": 73.29011345218801,
      "grad_norm": 0.8578967452049255,
      "learning_rate": 4.270181641258514e-05,
      "loss": 3.0271,
      "step": 226100
    },
    {
      "epoch": 73.322528363047,
      "grad_norm": 0.9813776612281799,
      "learning_rate": 4.26985728186831e-05,
      "loss": 3.0047,
      "step": 226200
    },
    {
      "epoch": 73.354943273906,
      "grad_norm": 1.113995909690857,
      "learning_rate": 4.269532922478106e-05,
      "loss": 3.0166,
      "step": 226300
    },
    {
      "epoch": 73.38735818476499,
      "grad_norm": 0.9273882508277893,
      "learning_rate": 4.269208563087901e-05,
      "loss": 2.9938,
      "step": 226400
    },
    {
      "epoch": 73.41977309562398,
      "grad_norm": 1.006885290145874,
      "learning_rate": 4.268884203697697e-05,
      "loss": 3.008,
      "step": 226500
    },
    {
      "epoch": 73.45218800648298,
      "grad_norm": 0.9885825514793396,
      "learning_rate": 4.268559844307493e-05,
      "loss": 3.0201,
      "step": 226600
    },
    {
      "epoch": 73.48460291734197,
      "grad_norm": 1.0788192749023438,
      "learning_rate": 4.268235484917288e-05,
      "loss": 3.0077,
      "step": 226700
    },
    {
      "epoch": 73.51701782820098,
      "grad_norm": 1.1505763530731201,
      "learning_rate": 4.267911125527084e-05,
      "loss": 2.9988,
      "step": 226800
    },
    {
      "epoch": 73.54943273905997,
      "grad_norm": 1.137434482574463,
      "learning_rate": 4.26758676613688e-05,
      "loss": 3.0277,
      "step": 226900
    },
    {
      "epoch": 73.58184764991896,
      "grad_norm": 1.179266095161438,
      "learning_rate": 4.267262406746675e-05,
      "loss": 3.0177,
      "step": 227000
    },
    {
      "epoch": 73.61426256077796,
      "grad_norm": 1.612559199333191,
      "learning_rate": 4.266938047356471e-05,
      "loss": 3.0153,
      "step": 227100
    },
    {
      "epoch": 73.64667747163695,
      "grad_norm": 1.1719450950622559,
      "learning_rate": 4.266613687966267e-05,
      "loss": 2.9997,
      "step": 227200
    },
    {
      "epoch": 73.67909238249595,
      "grad_norm": 0.9841035008430481,
      "learning_rate": 4.266289328576063e-05,
      "loss": 3.0217,
      "step": 227300
    },
    {
      "epoch": 73.71150729335494,
      "grad_norm": 0.9195095300674438,
      "learning_rate": 4.2659649691858586e-05,
      "loss": 3.02,
      "step": 227400
    },
    {
      "epoch": 73.74392220421394,
      "grad_norm": 1.0675787925720215,
      "learning_rate": 4.265640609795654e-05,
      "loss": 3.0344,
      "step": 227500
    },
    {
      "epoch": 73.77633711507293,
      "grad_norm": 1.0421648025512695,
      "learning_rate": 4.26531625040545e-05,
      "loss": 3.0202,
      "step": 227600
    },
    {
      "epoch": 73.80875202593192,
      "grad_norm": 0.9331057667732239,
      "learning_rate": 4.264995134609147e-05,
      "loss": 3.011,
      "step": 227700
    },
    {
      "epoch": 73.84116693679093,
      "grad_norm": 1.2325910329818726,
      "learning_rate": 4.264670775218943e-05,
      "loss": 3.015,
      "step": 227800
    },
    {
      "epoch": 73.87358184764992,
      "grad_norm": 0.8995665907859802,
      "learning_rate": 4.2643464158287385e-05,
      "loss": 3.0121,
      "step": 227900
    },
    {
      "epoch": 73.90599675850892,
      "grad_norm": 0.97135990858078,
      "learning_rate": 4.2640220564385344e-05,
      "loss": 3.0049,
      "step": 228000
    },
    {
      "epoch": 73.93841166936791,
      "grad_norm": 1.2022418975830078,
      "learning_rate": 4.26369769704833e-05,
      "loss": 3.0233,
      "step": 228100
    },
    {
      "epoch": 73.9708265802269,
      "grad_norm": 1.0371406078338623,
      "learning_rate": 4.2633733376581255e-05,
      "loss": 3.0224,
      "step": 228200
    },
    {
      "epoch": 74.0,
      "eval_bleu": 1.1753084178980808,
      "eval_loss": 3.798126697540283,
      "eval_runtime": 4.2834,
      "eval_samples_per_second": 114.862,
      "eval_steps_per_second": 1.868,
      "step": 228290
    },
    {
      "epoch": 74.0032414910859,
      "grad_norm": 1.0116922855377197,
      "learning_rate": 4.2630489782679214e-05,
      "loss": 3.0252,
      "step": 228300
    },
    {
      "epoch": 74.03565640194489,
      "grad_norm": 0.9767429828643799,
      "learning_rate": 4.2627246188777166e-05,
      "loss": 3.0081,
      "step": 228400
    },
    {
      "epoch": 74.0680713128039,
      "grad_norm": 1.0876984596252441,
      "learning_rate": 4.2624002594875124e-05,
      "loss": 2.997,
      "step": 228500
    },
    {
      "epoch": 74.10048622366288,
      "grad_norm": 1.0080631971359253,
      "learning_rate": 4.262075900097308e-05,
      "loss": 3.0094,
      "step": 228600
    },
    {
      "epoch": 74.13290113452187,
      "grad_norm": 0.9725315570831299,
      "learning_rate": 4.2617515407071035e-05,
      "loss": 3.0165,
      "step": 228700
    },
    {
      "epoch": 74.16531604538088,
      "grad_norm": 1.0368995666503906,
      "learning_rate": 4.2614271813168994e-05,
      "loss": 3.0104,
      "step": 228800
    },
    {
      "epoch": 74.19773095623987,
      "grad_norm": 0.9370968341827393,
      "learning_rate": 4.261102821926695e-05,
      "loss": 2.9984,
      "step": 228900
    },
    {
      "epoch": 74.23014586709887,
      "grad_norm": 0.9337444305419922,
      "learning_rate": 4.2607784625364905e-05,
      "loss": 3.0095,
      "step": 229000
    },
    {
      "epoch": 74.26256077795786,
      "grad_norm": 0.9701345562934875,
      "learning_rate": 4.260454103146286e-05,
      "loss": 3.0046,
      "step": 229100
    },
    {
      "epoch": 74.29497568881686,
      "grad_norm": 1.1010020971298218,
      "learning_rate": 4.260129743756082e-05,
      "loss": 3.01,
      "step": 229200
    },
    {
      "epoch": 74.32739059967585,
      "grad_norm": 0.9829874038696289,
      "learning_rate": 4.2598053843658774e-05,
      "loss": 2.9903,
      "step": 229300
    },
    {
      "epoch": 74.35980551053484,
      "grad_norm": 1.0713329315185547,
      "learning_rate": 4.259481024975673e-05,
      "loss": 3.0223,
      "step": 229400
    },
    {
      "epoch": 74.39222042139384,
      "grad_norm": 0.9982863068580627,
      "learning_rate": 4.2591566655854685e-05,
      "loss": 3.0078,
      "step": 229500
    },
    {
      "epoch": 74.42463533225283,
      "grad_norm": 1.0079549551010132,
      "learning_rate": 4.2588323061952643e-05,
      "loss": 3.0068,
      "step": 229600
    },
    {
      "epoch": 74.45705024311184,
      "grad_norm": 1.029446005821228,
      "learning_rate": 4.25850794680506e-05,
      "loss": 2.9986,
      "step": 229700
    },
    {
      "epoch": 74.48946515397083,
      "grad_norm": 0.9587920308113098,
      "learning_rate": 4.2581835874148554e-05,
      "loss": 3.0174,
      "step": 229800
    },
    {
      "epoch": 74.52188006482982,
      "grad_norm": 0.9145508408546448,
      "learning_rate": 4.257859228024651e-05,
      "loss": 3.0167,
      "step": 229900
    },
    {
      "epoch": 74.55429497568882,
      "grad_norm": 1.0937868356704712,
      "learning_rate": 4.257534868634447e-05,
      "loss": 3.0204,
      "step": 230000
    },
    {
      "epoch": 74.58670988654781,
      "grad_norm": 1.0900925397872925,
      "learning_rate": 4.2572105092442424e-05,
      "loss": 3.0276,
      "step": 230100
    },
    {
      "epoch": 74.61912479740681,
      "grad_norm": 1.081492304801941,
      "learning_rate": 4.256886149854038e-05,
      "loss": 3.0287,
      "step": 230200
    },
    {
      "epoch": 74.6515397082658,
      "grad_norm": 0.9215598106384277,
      "learning_rate": 4.256561790463834e-05,
      "loss": 3.0071,
      "step": 230300
    },
    {
      "epoch": 74.68395461912479,
      "grad_norm": 1.163475513458252,
      "learning_rate": 4.25623743107363e-05,
      "loss": 2.9909,
      "step": 230400
    },
    {
      "epoch": 74.7163695299838,
      "grad_norm": 1.0637071132659912,
      "learning_rate": 4.255913071683426e-05,
      "loss": 2.9842,
      "step": 230500
    },
    {
      "epoch": 74.74878444084278,
      "grad_norm": 1.0246329307556152,
      "learning_rate": 4.255588712293221e-05,
      "loss": 3.0242,
      "step": 230600
    },
    {
      "epoch": 74.78119935170179,
      "grad_norm": 1.035521388053894,
      "learning_rate": 4.255264352903017e-05,
      "loss": 2.9949,
      "step": 230700
    },
    {
      "epoch": 74.81361426256078,
      "grad_norm": 1.066938042640686,
      "learning_rate": 4.254939993512813e-05,
      "loss": 3.0088,
      "step": 230800
    },
    {
      "epoch": 74.84602917341978,
      "grad_norm": 0.9991336464881897,
      "learning_rate": 4.254615634122608e-05,
      "loss": 3.002,
      "step": 230900
    },
    {
      "epoch": 74.87844408427877,
      "grad_norm": 0.9491111040115356,
      "learning_rate": 4.254291274732404e-05,
      "loss": 3.0198,
      "step": 231000
    },
    {
      "epoch": 74.91085899513776,
      "grad_norm": 1.0364127159118652,
      "learning_rate": 4.2539669153422e-05,
      "loss": 3.0054,
      "step": 231100
    },
    {
      "epoch": 74.94327390599676,
      "grad_norm": 1.060845136642456,
      "learning_rate": 4.253642555951995e-05,
      "loss": 2.9953,
      "step": 231200
    },
    {
      "epoch": 74.97568881685575,
      "grad_norm": 0.9655767679214478,
      "learning_rate": 4.253318196561791e-05,
      "loss": 3.0255,
      "step": 231300
    },
    {
      "epoch": 75.0,
      "eval_bleu": 1.1132553024299694,
      "eval_loss": 3.804202079772949,
      "eval_runtime": 4.6413,
      "eval_samples_per_second": 106.005,
      "eval_steps_per_second": 1.724,
      "step": 231375
    },
    {
      "epoch": 75.00810372771475,
      "grad_norm": 1.0661981105804443,
      "learning_rate": 4.252993837171586e-05,
      "loss": 3.0097,
      "step": 231400
    },
    {
      "epoch": 75.04051863857374,
      "grad_norm": 0.9939681887626648,
      "learning_rate": 4.252669477781382e-05,
      "loss": 3.013,
      "step": 231500
    },
    {
      "epoch": 75.07293354943273,
      "grad_norm": 0.9460613131523132,
      "learning_rate": 4.252345118391178e-05,
      "loss": 3.0009,
      "step": 231600
    },
    {
      "epoch": 75.10534846029174,
      "grad_norm": 0.980370044708252,
      "learning_rate": 4.252020759000973e-05,
      "loss": 2.9882,
      "step": 231700
    },
    {
      "epoch": 75.13776337115073,
      "grad_norm": 1.0232484340667725,
      "learning_rate": 4.251696399610769e-05,
      "loss": 3.0041,
      "step": 231800
    },
    {
      "epoch": 75.17017828200973,
      "grad_norm": 1.1084588766098022,
      "learning_rate": 4.251372040220565e-05,
      "loss": 3.0056,
      "step": 231900
    },
    {
      "epoch": 75.20259319286872,
      "grad_norm": 1.069765329360962,
      "learning_rate": 4.25104768083036e-05,
      "loss": 2.9922,
      "step": 232000
    },
    {
      "epoch": 75.23500810372771,
      "grad_norm": 1.0407143831253052,
      "learning_rate": 4.250723321440156e-05,
      "loss": 3.0172,
      "step": 232100
    },
    {
      "epoch": 75.26742301458671,
      "grad_norm": 1.0512853860855103,
      "learning_rate": 4.2503989620499517e-05,
      "loss": 3.0127,
      "step": 232200
    },
    {
      "epoch": 75.2998379254457,
      "grad_norm": 0.953782856464386,
      "learning_rate": 4.250074602659747e-05,
      "loss": 3.0056,
      "step": 232300
    },
    {
      "epoch": 75.3322528363047,
      "grad_norm": 0.9374799132347107,
      "learning_rate": 4.249750243269543e-05,
      "loss": 3.0029,
      "step": 232400
    },
    {
      "epoch": 75.3646677471637,
      "grad_norm": 0.9331226348876953,
      "learning_rate": 4.2494258838793386e-05,
      "loss": 2.9835,
      "step": 232500
    },
    {
      "epoch": 75.3970826580227,
      "grad_norm": 0.8374287486076355,
      "learning_rate": 4.249101524489134e-05,
      "loss": 2.9961,
      "step": 232600
    },
    {
      "epoch": 75.42949756888169,
      "grad_norm": 1.1221234798431396,
      "learning_rate": 4.24877716509893e-05,
      "loss": 3.0005,
      "step": 232700
    },
    {
      "epoch": 75.46191247974068,
      "grad_norm": 0.8951229453086853,
      "learning_rate": 4.2484528057087255e-05,
      "loss": 2.9991,
      "step": 232800
    },
    {
      "epoch": 75.49432739059968,
      "grad_norm": 0.8981218934059143,
      "learning_rate": 4.2481284463185214e-05,
      "loss": 3.0139,
      "step": 232900
    },
    {
      "epoch": 75.52674230145867,
      "grad_norm": 0.9372380971908569,
      "learning_rate": 4.247804086928317e-05,
      "loss": 3.0006,
      "step": 233000
    },
    {
      "epoch": 75.55915721231767,
      "grad_norm": 0.9201141595840454,
      "learning_rate": 4.2474797275381125e-05,
      "loss": 3.0146,
      "step": 233100
    },
    {
      "epoch": 75.59157212317666,
      "grad_norm": 0.9490286111831665,
      "learning_rate": 4.2471553681479084e-05,
      "loss": 2.9922,
      "step": 233200
    },
    {
      "epoch": 75.62398703403565,
      "grad_norm": 1.0318760871887207,
      "learning_rate": 4.246831008757704e-05,
      "loss": 3.0127,
      "step": 233300
    },
    {
      "epoch": 75.65640194489465,
      "grad_norm": 0.9602853059768677,
      "learning_rate": 4.2465066493674994e-05,
      "loss": 2.9996,
      "step": 233400
    },
    {
      "epoch": 75.68881685575364,
      "grad_norm": 0.9295884370803833,
      "learning_rate": 4.246182289977295e-05,
      "loss": 3.0075,
      "step": 233500
    },
    {
      "epoch": 75.72123176661265,
      "grad_norm": 1.282928705215454,
      "learning_rate": 4.2458579305870905e-05,
      "loss": 3.0059,
      "step": 233600
    },
    {
      "epoch": 75.75364667747164,
      "grad_norm": 0.9187174439430237,
      "learning_rate": 4.2455335711968864e-05,
      "loss": 3.0281,
      "step": 233700
    },
    {
      "epoch": 75.78606158833063,
      "grad_norm": 0.8854692578315735,
      "learning_rate": 4.245209211806682e-05,
      "loss": 3.0211,
      "step": 233800
    },
    {
      "epoch": 75.81847649918963,
      "grad_norm": 1.0703424215316772,
      "learning_rate": 4.2448848524164774e-05,
      "loss": 3.013,
      "step": 233900
    },
    {
      "epoch": 75.85089141004862,
      "grad_norm": 0.9728807210922241,
      "learning_rate": 4.244560493026273e-05,
      "loss": 2.9857,
      "step": 234000
    },
    {
      "epoch": 75.88330632090762,
      "grad_norm": 1.1877434253692627,
      "learning_rate": 4.244236133636069e-05,
      "loss": 3.0001,
      "step": 234100
    },
    {
      "epoch": 75.91572123176661,
      "grad_norm": 1.0294973850250244,
      "learning_rate": 4.2439117742458644e-05,
      "loss": 3.0312,
      "step": 234200
    },
    {
      "epoch": 75.94813614262561,
      "grad_norm": 1.11995530128479,
      "learning_rate": 4.24358741485566e-05,
      "loss": 3.025,
      "step": 234300
    },
    {
      "epoch": 75.9805510534846,
      "grad_norm": 1.129256248474121,
      "learning_rate": 4.2432630554654555e-05,
      "loss": 3.012,
      "step": 234400
    },
    {
      "epoch": 76.0,
      "eval_bleu": 1.216223642396467,
      "eval_loss": 3.804739475250244,
      "eval_runtime": 4.4389,
      "eval_samples_per_second": 110.839,
      "eval_steps_per_second": 1.802,
      "step": 234460
    },
    {
      "epoch": 76.01296596434359,
      "grad_norm": 1.0385600328445435,
      "learning_rate": 4.242938696075251e-05,
      "loss": 2.9962,
      "step": 234500
    },
    {
      "epoch": 76.0453808752026,
      "grad_norm": 0.9845083355903625,
      "learning_rate": 4.242614336685047e-05,
      "loss": 2.9909,
      "step": 234600
    },
    {
      "epoch": 76.07779578606159,
      "grad_norm": 1.0607292652130127,
      "learning_rate": 4.2422899772948424e-05,
      "loss": 2.9836,
      "step": 234700
    },
    {
      "epoch": 76.11021069692059,
      "grad_norm": 1.032041072845459,
      "learning_rate": 4.241965617904638e-05,
      "loss": 2.9955,
      "step": 234800
    },
    {
      "epoch": 76.14262560777958,
      "grad_norm": 1.1964490413665771,
      "learning_rate": 4.241641258514434e-05,
      "loss": 2.9966,
      "step": 234900
    },
    {
      "epoch": 76.17504051863857,
      "grad_norm": 1.012805700302124,
      "learning_rate": 4.2413168991242294e-05,
      "loss": 2.9967,
      "step": 235000
    },
    {
      "epoch": 76.20745542949757,
      "grad_norm": 0.9071995615959167,
      "learning_rate": 4.240992539734025e-05,
      "loss": 2.9913,
      "step": 235100
    },
    {
      "epoch": 76.23987034035656,
      "grad_norm": 0.8962684869766235,
      "learning_rate": 4.240668180343821e-05,
      "loss": 3.0239,
      "step": 235200
    },
    {
      "epoch": 76.27228525121556,
      "grad_norm": 1.0281341075897217,
      "learning_rate": 4.240343820953617e-05,
      "loss": 2.99,
      "step": 235300
    },
    {
      "epoch": 76.30470016207455,
      "grad_norm": 0.9536156058311462,
      "learning_rate": 4.240019461563413e-05,
      "loss": 2.9933,
      "step": 235400
    },
    {
      "epoch": 76.33711507293354,
      "grad_norm": 1.0238627195358276,
      "learning_rate": 4.239695102173208e-05,
      "loss": 3.0169,
      "step": 235500
    },
    {
      "epoch": 76.36952998379255,
      "grad_norm": 0.8831833600997925,
      "learning_rate": 4.239370742783004e-05,
      "loss": 2.9986,
      "step": 235600
    },
    {
      "epoch": 76.40194489465154,
      "grad_norm": 0.9366862177848816,
      "learning_rate": 4.2390463833928e-05,
      "loss": 2.9866,
      "step": 235700
    },
    {
      "epoch": 76.43435980551054,
      "grad_norm": 0.9637255072593689,
      "learning_rate": 4.238728511190399e-05,
      "loss": 3.0109,
      "step": 235800
    },
    {
      "epoch": 76.46677471636953,
      "grad_norm": 1.1511610746383667,
      "learning_rate": 4.238404151800195e-05,
      "loss": 2.9913,
      "step": 235900
    },
    {
      "epoch": 76.49918962722853,
      "grad_norm": 1.0542547702789307,
      "learning_rate": 4.23807979240999e-05,
      "loss": 3.0118,
      "step": 236000
    },
    {
      "epoch": 76.53160453808752,
      "grad_norm": 1.082669973373413,
      "learning_rate": 4.237755433019786e-05,
      "loss": 3.0156,
      "step": 236100
    },
    {
      "epoch": 76.56401944894651,
      "grad_norm": 1.1171941757202148,
      "learning_rate": 4.2374310736295816e-05,
      "loss": 3.0053,
      "step": 236200
    },
    {
      "epoch": 76.59643435980551,
      "grad_norm": 0.9839821457862854,
      "learning_rate": 4.2371067142393775e-05,
      "loss": 3.0035,
      "step": 236300
    },
    {
      "epoch": 76.6288492706645,
      "grad_norm": 0.8989562392234802,
      "learning_rate": 4.2367823548491734e-05,
      "loss": 3.0039,
      "step": 236400
    },
    {
      "epoch": 76.6612641815235,
      "grad_norm": 1.0268731117248535,
      "learning_rate": 4.236457995458969e-05,
      "loss": 3.0116,
      "step": 236500
    },
    {
      "epoch": 76.6936790923825,
      "grad_norm": 1.1760584115982056,
      "learning_rate": 4.2361336360687644e-05,
      "loss": 3.0083,
      "step": 236600
    },
    {
      "epoch": 76.72609400324149,
      "grad_norm": 0.9828698635101318,
      "learning_rate": 4.235812520272462e-05,
      "loss": 3.0137,
      "step": 236700
    },
    {
      "epoch": 76.75850891410049,
      "grad_norm": 1.0208523273468018,
      "learning_rate": 4.2354881608822574e-05,
      "loss": 3.0072,
      "step": 236800
    },
    {
      "epoch": 76.79092382495948,
      "grad_norm": 1.058044672012329,
      "learning_rate": 4.235163801492053e-05,
      "loss": 3.0019,
      "step": 236900
    },
    {
      "epoch": 76.82333873581848,
      "grad_norm": 0.9478278756141663,
      "learning_rate": 4.234839442101849e-05,
      "loss": 3.0141,
      "step": 237000
    },
    {
      "epoch": 76.85575364667747,
      "grad_norm": 1.2612597942352295,
      "learning_rate": 4.234515082711645e-05,
      "loss": 3.0054,
      "step": 237100
    },
    {
      "epoch": 76.88816855753646,
      "grad_norm": 0.9625019431114197,
      "learning_rate": 4.234190723321441e-05,
      "loss": 2.992,
      "step": 237200
    },
    {
      "epoch": 76.92058346839546,
      "grad_norm": 0.9551793932914734,
      "learning_rate": 4.233866363931236e-05,
      "loss": 2.9971,
      "step": 237300
    },
    {
      "epoch": 76.95299837925445,
      "grad_norm": 0.9317455887794495,
      "learning_rate": 4.233542004541032e-05,
      "loss": 3.0169,
      "step": 237400
    },
    {
      "epoch": 76.98541329011346,
      "grad_norm": 1.0252002477645874,
      "learning_rate": 4.233217645150827e-05,
      "loss": 3.0081,
      "step": 237500
    },
    {
      "epoch": 77.0,
      "eval_bleu": 1.1896778888382245,
      "eval_loss": 3.8075873851776123,
      "eval_runtime": 4.2821,
      "eval_samples_per_second": 114.898,
      "eval_steps_per_second": 1.868,
      "step": 237545
    },
    {
      "epoch": 77.01782820097245,
      "grad_norm": 1.0049386024475098,
      "learning_rate": 4.232893285760623e-05,
      "loss": 3.0032,
      "step": 237600
    },
    {
      "epoch": 77.05024311183145,
      "grad_norm": 0.9806503057479858,
      "learning_rate": 4.232568926370419e-05,
      "loss": 3.0039,
      "step": 237700
    },
    {
      "epoch": 77.08265802269044,
      "grad_norm": 0.9090269804000854,
      "learning_rate": 4.232244566980214e-05,
      "loss": 2.9927,
      "step": 237800
    },
    {
      "epoch": 77.11507293354943,
      "grad_norm": 1.1171976327896118,
      "learning_rate": 4.23192020759001e-05,
      "loss": 2.9949,
      "step": 237900
    },
    {
      "epoch": 77.14748784440843,
      "grad_norm": 0.9307818412780762,
      "learning_rate": 4.231595848199806e-05,
      "loss": 2.9891,
      "step": 238000
    },
    {
      "epoch": 77.17990275526742,
      "grad_norm": 1.127889633178711,
      "learning_rate": 4.231271488809601e-05,
      "loss": 2.9897,
      "step": 238100
    },
    {
      "epoch": 77.21231766612642,
      "grad_norm": 0.9421783685684204,
      "learning_rate": 4.230947129419397e-05,
      "loss": 3.0033,
      "step": 238200
    },
    {
      "epoch": 77.24473257698541,
      "grad_norm": 1.0958640575408936,
      "learning_rate": 4.230622770029192e-05,
      "loss": 2.9873,
      "step": 238300
    },
    {
      "epoch": 77.2771474878444,
      "grad_norm": 0.9370099306106567,
      "learning_rate": 4.230298410638988e-05,
      "loss": 2.9863,
      "step": 238400
    },
    {
      "epoch": 77.3095623987034,
      "grad_norm": 1.0054668188095093,
      "learning_rate": 4.229974051248784e-05,
      "loss": 2.9838,
      "step": 238500
    },
    {
      "epoch": 77.3419773095624,
      "grad_norm": 1.0436930656433105,
      "learning_rate": 4.229649691858579e-05,
      "loss": 2.999,
      "step": 238600
    },
    {
      "epoch": 77.3743922204214,
      "grad_norm": 0.9957905411720276,
      "learning_rate": 4.229325332468375e-05,
      "loss": 2.9995,
      "step": 238700
    },
    {
      "epoch": 77.40680713128039,
      "grad_norm": 0.9681530594825745,
      "learning_rate": 4.229000973078171e-05,
      "loss": 3.0048,
      "step": 238800
    },
    {
      "epoch": 77.43922204213938,
      "grad_norm": 0.8996002674102783,
      "learning_rate": 4.228676613687966e-05,
      "loss": 2.994,
      "step": 238900
    },
    {
      "epoch": 77.47163695299838,
      "grad_norm": 0.9322336316108704,
      "learning_rate": 4.228352254297762e-05,
      "loss": 2.9956,
      "step": 239000
    },
    {
      "epoch": 77.50405186385737,
      "grad_norm": 1.0864554643630981,
      "learning_rate": 4.228027894907558e-05,
      "loss": 2.9872,
      "step": 239100
    },
    {
      "epoch": 77.53646677471637,
      "grad_norm": 0.9105396270751953,
      "learning_rate": 4.227703535517353e-05,
      "loss": 3.0048,
      "step": 239200
    },
    {
      "epoch": 77.56888168557536,
      "grad_norm": 0.9718434810638428,
      "learning_rate": 4.227379176127149e-05,
      "loss": 3.0041,
      "step": 239300
    },
    {
      "epoch": 77.60129659643437,
      "grad_norm": 1.0722901821136475,
      "learning_rate": 4.227054816736945e-05,
      "loss": 3.0033,
      "step": 239400
    },
    {
      "epoch": 77.63371150729336,
      "grad_norm": 0.9034110903739929,
      "learning_rate": 4.2267304573467406e-05,
      "loss": 3.0146,
      "step": 239500
    },
    {
      "epoch": 77.66612641815234,
      "grad_norm": 0.9494526982307434,
      "learning_rate": 4.2264060979565365e-05,
      "loss": 3.0038,
      "step": 239600
    },
    {
      "epoch": 77.69854132901135,
      "grad_norm": 0.9205394983291626,
      "learning_rate": 4.226081738566332e-05,
      "loss": 3.0064,
      "step": 239700
    },
    {
      "epoch": 77.73095623987034,
      "grad_norm": 0.8740229606628418,
      "learning_rate": 4.2257573791761276e-05,
      "loss": 3.0011,
      "step": 239800
    },
    {
      "epoch": 77.76337115072934,
      "grad_norm": 1.0882816314697266,
      "learning_rate": 4.2254330197859234e-05,
      "loss": 2.9997,
      "step": 239900
    },
    {
      "epoch": 77.79578606158833,
      "grad_norm": 1.0719482898712158,
      "learning_rate": 4.2251086603957186e-05,
      "loss": 3.0021,
      "step": 240000
    },
    {
      "epoch": 77.82820097244732,
      "grad_norm": 0.9387555122375488,
      "learning_rate": 4.2247843010055145e-05,
      "loss": 3.0236,
      "step": 240100
    },
    {
      "epoch": 77.86061588330632,
      "grad_norm": 0.9850307703018188,
      "learning_rate": 4.2244599416153104e-05,
      "loss": 2.994,
      "step": 240200
    },
    {
      "epoch": 77.89303079416531,
      "grad_norm": 0.9032101035118103,
      "learning_rate": 4.2241355822251056e-05,
      "loss": 2.985,
      "step": 240300
    },
    {
      "epoch": 77.92544570502432,
      "grad_norm": 0.9962813258171082,
      "learning_rate": 4.2238112228349014e-05,
      "loss": 2.9927,
      "step": 240400
    },
    {
      "epoch": 77.9578606158833,
      "grad_norm": 0.9173154830932617,
      "learning_rate": 4.2234868634446966e-05,
      "loss": 2.9967,
      "step": 240500
    },
    {
      "epoch": 77.9902755267423,
      "grad_norm": 0.9339150190353394,
      "learning_rate": 4.2231625040544925e-05,
      "loss": 2.9965,
      "step": 240600
    },
    {
      "epoch": 78.0,
      "eval_bleu": 1.3074630469917226,
      "eval_loss": 3.805577278137207,
      "eval_runtime": 4.7128,
      "eval_samples_per_second": 104.396,
      "eval_steps_per_second": 1.697,
      "step": 240630
    },
    {
      "epoch": 78.0226904376013,
      "grad_norm": 1.0139206647872925,
      "learning_rate": 4.2228381446642884e-05,
      "loss": 2.9745,
      "step": 240700
    },
    {
      "epoch": 78.05510534846029,
      "grad_norm": 1.0710432529449463,
      "learning_rate": 4.2225137852740836e-05,
      "loss": 2.9717,
      "step": 240800
    },
    {
      "epoch": 78.08752025931929,
      "grad_norm": 1.127888798713684,
      "learning_rate": 4.2221894258838795e-05,
      "loss": 2.9855,
      "step": 240900
    },
    {
      "epoch": 78.11993517017828,
      "grad_norm": 1.0247498750686646,
      "learning_rate": 4.221865066493675e-05,
      "loss": 2.9827,
      "step": 241000
    },
    {
      "epoch": 78.15235008103728,
      "grad_norm": 1.3383569717407227,
      "learning_rate": 4.2215407071034705e-05,
      "loss": 2.9913,
      "step": 241100
    },
    {
      "epoch": 78.18476499189627,
      "grad_norm": 1.1304913759231567,
      "learning_rate": 4.2212163477132664e-05,
      "loss": 2.992,
      "step": 241200
    },
    {
      "epoch": 78.21717990275526,
      "grad_norm": 0.9650110602378845,
      "learning_rate": 4.220891988323062e-05,
      "loss": 3.0025,
      "step": 241300
    },
    {
      "epoch": 78.24959481361427,
      "grad_norm": 1.07081139087677,
      "learning_rate": 4.2205676289328575e-05,
      "loss": 2.973,
      "step": 241400
    },
    {
      "epoch": 78.28200972447326,
      "grad_norm": 1.0440435409545898,
      "learning_rate": 4.2202432695426533e-05,
      "loss": 2.9961,
      "step": 241500
    },
    {
      "epoch": 78.31442463533226,
      "grad_norm": 0.9601117372512817,
      "learning_rate": 4.219918910152449e-05,
      "loss": 2.9859,
      "step": 241600
    },
    {
      "epoch": 78.34683954619125,
      "grad_norm": 1.0170058012008667,
      "learning_rate": 4.2195945507622444e-05,
      "loss": 2.9838,
      "step": 241700
    },
    {
      "epoch": 78.37925445705024,
      "grad_norm": 1.0493425130844116,
      "learning_rate": 4.21927019137204e-05,
      "loss": 2.9952,
      "step": 241800
    },
    {
      "epoch": 78.41166936790924,
      "grad_norm": 0.9884775876998901,
      "learning_rate": 4.218945831981836e-05,
      "loss": 2.9919,
      "step": 241900
    },
    {
      "epoch": 78.44408427876823,
      "grad_norm": 0.9454813599586487,
      "learning_rate": 4.218621472591632e-05,
      "loss": 2.9915,
      "step": 242000
    },
    {
      "epoch": 78.47649918962723,
      "grad_norm": 1.1734586954116821,
      "learning_rate": 4.218297113201428e-05,
      "loss": 2.9939,
      "step": 242100
    },
    {
      "epoch": 78.50891410048622,
      "grad_norm": 1.0445871353149414,
      "learning_rate": 4.217972753811223e-05,
      "loss": 3.0104,
      "step": 242200
    },
    {
      "epoch": 78.54132901134521,
      "grad_norm": 0.8376871943473816,
      "learning_rate": 4.217648394421019e-05,
      "loss": 3.0138,
      "step": 242300
    },
    {
      "epoch": 78.57374392220422,
      "grad_norm": 0.9801080226898193,
      "learning_rate": 4.217324035030814e-05,
      "loss": 3.002,
      "step": 242400
    },
    {
      "epoch": 78.6061588330632,
      "grad_norm": 0.9383784532546997,
      "learning_rate": 4.217002919234512e-05,
      "loss": 3.0166,
      "step": 242500
    },
    {
      "epoch": 78.63857374392221,
      "grad_norm": 1.1309434175491333,
      "learning_rate": 4.216678559844308e-05,
      "loss": 2.9975,
      "step": 242600
    },
    {
      "epoch": 78.6709886547812,
      "grad_norm": 1.1967657804489136,
      "learning_rate": 4.216354200454104e-05,
      "loss": 3.0127,
      "step": 242700
    },
    {
      "epoch": 78.7034035656402,
      "grad_norm": 1.1365493535995483,
      "learning_rate": 4.216029841063899e-05,
      "loss": 2.9958,
      "step": 242800
    },
    {
      "epoch": 78.73581847649919,
      "grad_norm": 1.00609290599823,
      "learning_rate": 4.215705481673695e-05,
      "loss": 2.9853,
      "step": 242900
    },
    {
      "epoch": 78.76823338735818,
      "grad_norm": 1.07161545753479,
      "learning_rate": 4.2153811222834907e-05,
      "loss": 2.9889,
      "step": 243000
    },
    {
      "epoch": 78.80064829821718,
      "grad_norm": 1.014479160308838,
      "learning_rate": 4.215056762893286e-05,
      "loss": 2.9945,
      "step": 243100
    },
    {
      "epoch": 78.83306320907617,
      "grad_norm": 0.9551103711128235,
      "learning_rate": 4.214732403503082e-05,
      "loss": 3.0181,
      "step": 243200
    },
    {
      "epoch": 78.86547811993518,
      "grad_norm": 1.5022351741790771,
      "learning_rate": 4.2144080441128776e-05,
      "loss": 2.9904,
      "step": 243300
    },
    {
      "epoch": 78.89789303079417,
      "grad_norm": 1.035441517829895,
      "learning_rate": 4.214083684722673e-05,
      "loss": 3.0231,
      "step": 243400
    },
    {
      "epoch": 78.93030794165315,
      "grad_norm": 1.1410961151123047,
      "learning_rate": 4.213759325332469e-05,
      "loss": 3.022,
      "step": 243500
    },
    {
      "epoch": 78.96272285251216,
      "grad_norm": 0.9812159538269043,
      "learning_rate": 4.2134349659422645e-05,
      "loss": 2.9906,
      "step": 243600
    },
    {
      "epoch": 78.99513776337115,
      "grad_norm": 0.9964342713356018,
      "learning_rate": 4.21311060655206e-05,
      "loss": 2.9927,
      "step": 243700
    },
    {
      "epoch": 79.0,
      "eval_bleu": 1.188713651540546,
      "eval_loss": 3.802879810333252,
      "eval_runtime": 4.2695,
      "eval_samples_per_second": 115.235,
      "eval_steps_per_second": 1.874,
      "step": 243715
    },
    {
      "epoch": 79.02755267423015,
      "grad_norm": 1.0627212524414062,
      "learning_rate": 4.2127862471618556e-05,
      "loss": 2.9757,
      "step": 243800
    },
    {
      "epoch": 79.05996758508914,
      "grad_norm": 1.0484381914138794,
      "learning_rate": 4.212461887771651e-05,
      "loss": 2.9971,
      "step": 243900
    },
    {
      "epoch": 79.09238249594813,
      "grad_norm": 1.1147669553756714,
      "learning_rate": 4.212137528381447e-05,
      "loss": 2.9848,
      "step": 244000
    },
    {
      "epoch": 79.12479740680713,
      "grad_norm": 1.0276782512664795,
      "learning_rate": 4.2118131689912426e-05,
      "loss": 2.9751,
      "step": 244100
    },
    {
      "epoch": 79.15721231766612,
      "grad_norm": 0.8545264601707458,
      "learning_rate": 4.211488809601038e-05,
      "loss": 3.0048,
      "step": 244200
    },
    {
      "epoch": 79.18962722852513,
      "grad_norm": 1.2353538274765015,
      "learning_rate": 4.2111644502108336e-05,
      "loss": 2.9969,
      "step": 244300
    },
    {
      "epoch": 79.22204213938411,
      "grad_norm": 1.1516671180725098,
      "learning_rate": 4.2108400908206295e-05,
      "loss": 2.9663,
      "step": 244400
    },
    {
      "epoch": 79.25445705024312,
      "grad_norm": 1.2498854398727417,
      "learning_rate": 4.210515731430425e-05,
      "loss": 2.9883,
      "step": 244500
    },
    {
      "epoch": 79.28687196110211,
      "grad_norm": 0.953555166721344,
      "learning_rate": 4.2101913720402206e-05,
      "loss": 2.982,
      "step": 244600
    },
    {
      "epoch": 79.3192868719611,
      "grad_norm": 0.9437752366065979,
      "learning_rate": 4.2098702562439184e-05,
      "loss": 2.9664,
      "step": 244700
    },
    {
      "epoch": 79.3517017828201,
      "grad_norm": 0.9116535782814026,
      "learning_rate": 4.209545896853714e-05,
      "loss": 2.9898,
      "step": 244800
    },
    {
      "epoch": 79.38411669367909,
      "grad_norm": 0.969352126121521,
      "learning_rate": 4.2092215374635094e-05,
      "loss": 2.9872,
      "step": 244900
    },
    {
      "epoch": 79.4165316045381,
      "grad_norm": 0.9580119252204895,
      "learning_rate": 4.208897178073305e-05,
      "loss": 2.9828,
      "step": 245000
    },
    {
      "epoch": 79.44894651539708,
      "grad_norm": 0.9817381501197815,
      "learning_rate": 4.2085728186831005e-05,
      "loss": 3.0071,
      "step": 245100
    },
    {
      "epoch": 79.48136142625607,
      "grad_norm": 0.8978882431983948,
      "learning_rate": 4.2082484592928964e-05,
      "loss": 2.9936,
      "step": 245200
    },
    {
      "epoch": 79.51377633711508,
      "grad_norm": 1.344700574874878,
      "learning_rate": 4.207924099902692e-05,
      "loss": 2.9868,
      "step": 245300
    },
    {
      "epoch": 79.54619124797406,
      "grad_norm": 1.0571684837341309,
      "learning_rate": 4.207599740512488e-05,
      "loss": 3.0048,
      "step": 245400
    },
    {
      "epoch": 79.57860615883307,
      "grad_norm": 0.86741703748703,
      "learning_rate": 4.207275381122284e-05,
      "loss": 3.0016,
      "step": 245500
    },
    {
      "epoch": 79.61102106969206,
      "grad_norm": 0.9551133513450623,
      "learning_rate": 4.20695102173208e-05,
      "loss": 3.0007,
      "step": 245600
    },
    {
      "epoch": 79.64343598055105,
      "grad_norm": 1.0571993589401245,
      "learning_rate": 4.206626662341875e-05,
      "loss": 3.0018,
      "step": 245700
    },
    {
      "epoch": 79.67585089141005,
      "grad_norm": 0.9514489769935608,
      "learning_rate": 4.206302302951671e-05,
      "loss": 2.9947,
      "step": 245800
    },
    {
      "epoch": 79.70826580226904,
      "grad_norm": 0.9495440125465393,
      "learning_rate": 4.205977943561467e-05,
      "loss": 2.9918,
      "step": 245900
    },
    {
      "epoch": 79.74068071312804,
      "grad_norm": 1.0559308528900146,
      "learning_rate": 4.205653584171262e-05,
      "loss": 2.983,
      "step": 246000
    },
    {
      "epoch": 79.77309562398703,
      "grad_norm": 1.0481146574020386,
      "learning_rate": 4.205329224781058e-05,
      "loss": 2.9991,
      "step": 246100
    },
    {
      "epoch": 79.80551053484604,
      "grad_norm": 0.8659568428993225,
      "learning_rate": 4.205004865390853e-05,
      "loss": 3.0264,
      "step": 246200
    },
    {
      "epoch": 79.83792544570503,
      "grad_norm": 1.1229313611984253,
      "learning_rate": 4.204680506000649e-05,
      "loss": 3.0037,
      "step": 246300
    },
    {
      "epoch": 79.87034035656401,
      "grad_norm": 1.0346320867538452,
      "learning_rate": 4.204356146610445e-05,
      "loss": 3.0012,
      "step": 246400
    },
    {
      "epoch": 79.90275526742302,
      "grad_norm": 1.0432714223861694,
      "learning_rate": 4.20403178722024e-05,
      "loss": 2.975,
      "step": 246500
    },
    {
      "epoch": 79.93517017828201,
      "grad_norm": 1.397632122039795,
      "learning_rate": 4.203707427830036e-05,
      "loss": 2.9959,
      "step": 246600
    },
    {
      "epoch": 79.96758508914101,
      "grad_norm": 0.9996395111083984,
      "learning_rate": 4.203383068439832e-05,
      "loss": 2.9773,
      "step": 246700
    },
    {
      "epoch": 80.0,
      "grad_norm": 0.9635169506072998,
      "learning_rate": 4.203058709049627e-05,
      "loss": 2.9778,
      "step": 246800
    },
    {
      "epoch": 80.0,
      "eval_bleu": 1.4050730653141843,
      "eval_loss": 3.806168556213379,
      "eval_runtime": 4.382,
      "eval_samples_per_second": 112.277,
      "eval_steps_per_second": 1.826,
      "step": 246800
    }
  ],
  "logging_steps": 100,
  "max_steps": 1542500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 500,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.347908600194662e+16,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
