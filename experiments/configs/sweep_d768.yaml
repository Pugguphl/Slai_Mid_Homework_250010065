seed: 1337

model:
  # Hyperparameter sweep: d_model = 768, num_heads = 12 (baseline = 512, 8)
  d_model: 768
  num_heads: 12
  num_encoder_layers: 6
  num_decoder_layers: 6
  dim_feedforward: 3072  # Scaled proportionally (768 * 4)
  dropout: 0.1
  activation: relu
  max_position_embeddings: 256
  max_decode_len: 120
  share_embeddings: true
  tie_output: true
  norm_first: true
  positional_encoding: sinusoidal
  norm_type: layernorm

training:
  batch_size: 128
  eval_batch_size: 64
  learning_rate: 0.0005
  num_epochs: 500
  warmup_steps: 4000
  weight_decay: 0.01
  gradient_accumulation_steps: 2
  max_grad_norm: 1.0
  label_smoothing: 0.1
  num_workers: 4

data:
  train_file: data/processed/train.jsonl
  valid_file: data/processed/valid.jsonl
  tokenizer_file: data/vocab/tokenizer_config.json
  src_key: zh
  tgt_key: en

evaluation:
  model_save_path: experiments/logs/sweep_d768_best.pt

experiments:
  metrics_file: experiments/results/sweep_d768_metrics.csv
