seed: 1337

model:
  d_model: 384
  num_heads: 6
  num_encoder_layers: 6
  num_decoder_layers: 6
  dim_feedforward: 1536
  dropout: 0.15
  activation: relu
  max_position_embeddings: 128
  max_decode_len: 50
  share_embeddings: true
  tie_output: true
  norm_first: true
  positional_encoding: sinusoidal
  norm_type: rmsnorm

training:
  batch_size: 128
  eval_batch_size: 64
  learning_rate: 0.0008
  num_epochs: 150
  warmup_steps: 3000
  weight_decay: 0.01
  gradient_accumulation_steps: 2
  max_grad_norm: 1.0
  label_smoothing: 0.1
  num_workers: 4
  lr_scheduler_type: cosine

data:
  train_file: data/processed/train_100k.jsonl
  valid_file: data/processed/valid.jsonl
  tokenizer_file: data/vocab/tokenizer_config.json
  src_key: zh
  tgt_key: en

evaluation:
  model_save_path: experiments/logs/transformer_100k_medium_best.pt

experiments:
  metrics_file: experiments/results/transformer_100k_medium_metrics.csv
